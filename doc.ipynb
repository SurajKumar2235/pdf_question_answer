{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub,LLMChain\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader('papers/LEE.pdf')\n",
    "\n",
    "data=loader.load()\n",
    "pagecontent=[i.page_content for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sensors\\nLetter\\nDeep-Learning-Based Detection of Infants with\\nAutism Spectrum Disorder Using Auto-Encoder\\nFeature Representation\\nJung Hyuk Lee1, Geon Woo Lee1, Guiyoung Bong2, Hee Jeong Yoo2,3and Hong Kook Kim1,*\\n1School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology,\\nGwangju 61005, Korea; ljh0412@gist.ac.kr (J.H.L.); geonwoo0801@gist.ac.kr (G.W.L.)\\n2Department of Psychiatry, Seoul National University Bundang Hospital, Seongnam-si,\\nGyeonggi-do 13620, Korea; 20409@snubh.org (G.B.); hjyoo@snu.ac.kr (H.J.Y.)\\n3Department of Psychiatry, College of Medicine, Seoul National University, Seoul 03980, Korea\\n*Correspondence: hongkook@gist.ac.kr\\nReceived: 29 October 2020; Accepted: 24 November 2020; Published: 26 November 2020\\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001\\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\\nAbstract: Autism spectrum disorder (ASD) is a developmental disorder with a life-span disability.\\nWhile diagnostic instruments have been developed and qualiﬁed based on the accuracy of the\\ndiscrimination of children with ASD from typical development (TD) children, the stability of such\\nprocedures can be disrupted by limitations pertaining to time expenses and the subjectivity of\\nclinicians. Consequently, automated diagnostic methods have been developed for acquiring objective\\nmeasures of autism, and in various ﬁelds of research, vocal characteristics have not only been reported\\nas distinctive characteristics by clinicians, but have also shown promising performance in several\\nstudies utilizing deep learning models based on the automated discrimination of children with\\nASD from children with TD. However, di ﬃculties still exist in terms of the characteristics of the\\ndata, the complexity of the analysis, and the lack of arranged data caused by the low accessibility\\nfor diagnosis and the need to secure anonymity. In order to address these issues, we introduce\\na pre-trained feature extraction auto-encoder model and a joint optimization scheme, which can\\nachieve robustness for widely distributed and unreﬁned data using a deep-learning-based method\\nfor the detection of autism that utilizes various models. By adopting this auto-encoder-based\\nfeature extraction and joint optimization in the extended version of the Geneva minimalistic acoustic\\nparameter set (eGeMAPS) speech feature data set, we acquire improved performance in the detection\\nof ASD in infants compared to the raw data set.\\nKeywords: auto-encoder; bidirectional long short-term memory (BLSTM); joint optimization; acoustic\\nfeature extraction; autism spectrum disorder\\n1. Introduction\\nAutism spectrum disorder (ASD) is a developmental disorder with a high probability of causing\\ndiﬃculties in social interactions with other people [ 1]. According to the Diagnostic and Statistical\\nManual of Mental Disorders, Fifth Edition (DSM-5), ASD involves several characteristics such as being\\nconﬁned to speciﬁc interests or behaviors, delayed linguistic development, and poor functionality in\\nterms of communicating or functioning in social situations [ 2]. As there is wide variation in terms of\\nthe types and severities of ASD based on its characteristics, the disorder is referred to as a spectrum [ 1].\\nNot only does ASD have the characteristics of a developmental disorder with a life-span disability, but\\nits prevalence is also increasing—from 1 in 150 children in 2000 to 1 in 54 children in 2016 [ 3]. As diverse\\nevidence has been obtained from previous research showing that the chance of improvement in the\\nSensors 2020 ,20, 6762; doi:10.3390 /s20236762 www.mdpi.com /journal /sensors',\n",
       " 'Sensors 2020 ,20, 6762 2 of 11\\nsocial abilities of people with ASD increases when an earlier clinical intervention is performed [ 4], the\\nearly detection of ASD characteristics has become a key point of current ASD research.\\nVarious instruments for discriminating ASD have been developed, and the commonly accepted\\ngold standard schemes are behavioral assessments, which are time-consuming procedures and require\\nmultidisciplinary teams (MDTs). However, most behavioral assessments su ﬀer in terms of the stability\\nof their ASD diagnosis as a result of the issues of accessibility or subjectivity and interpretive bias\\nbetween professions [ 5]. Therefore, several attempts to develop objective and precise diagnostic\\nmethods have been made in multiple ﬁelds, such as genetic determination [ 6], principle analysis of\\nbrain images [7], and physiological approaches [8].\\nOne prominent area of behavioral observations is that of infants’ vocal characteristics. Children\\nwith ASD are known to have abnormalities in their prosody resulting from deﬁcits in their ability to\\nrecognize the inherent mental conditions of others [ 9], and their atypical vocalizations are known to be\\nmonotonous or exaggerated, which can be revealed using various acoustic characteristics, followed\\nby engineering approaches for the discrimination of ASD or typical development (TD) in children\\nbased on the vocal and acoustic features. For example, in [ 10], the researchers estimated deﬁcits in the\\nvocalization of children with ASD at an average age of 18 months, such as “ﬂat” intonation, atypical\\npitch, or control of volume based on the variability of pitch and the long-term average spectrum\\n(LTAS) using fast Fourier transform, where signiﬁcant di ﬀerences were observed in the spectral\\ncomponents at low-band frequencies, as well as spectral peaks and larger pitch ranges and standard\\ndeviations. The development of linguistic abilities is also considered to be a distinguishable feature of\\ndelayed development in children with ASD. Earlier vocal patterns at age 6–18 months were proven\\nto be di ﬀerentiable in a study [ 11] that aimed to conﬁrm the hypothetical vocal patterns and social\\nquality of vocal behavior in order to di ﬀerentiate between ASD and TD cohorts in groups of children\\naged 0–6, 6–12, and 12–18 months in terms of categorized speech patterns consisting of vocalization,\\nlong reduplicated babbling, two-syllable babbling, and ﬁrst words. Evidence of abnormalities in\\nchildren with ASD were shown, in these cases, as a signiﬁcant decrease in vocalization and ﬁrst word\\nrate, while the di ﬀerence in babbling ability between children with ASD and TD was negligible.\\nGiven the development and improvement of machine learning algorithms, as the achievement\\nin the performance of state-of-the-art classiﬁcation and discrimination tasks [ 12], recent attempts to\\ndevelop automated classiﬁcation methods based on machine learning techniques have been based\\non the distinctiveness of vocal characteristics, and have been shown to be promising alternatives to\\nthe conventional methods in many publications [ 13]. For examples of machine learning classiﬁcation,\\nthe researchers of [ 14] employed various acoustic–prosodic features, including fundamental frequency,\\nformant frequencies, harmonics, and root mean square signal energy. In their research, support vector\\nmachines (SVMs) and probabilistic neural networks (PNNs) were adopted as classiﬁers, which showed\\neﬀectual accuracy in discriminating children with ASD from children with TD. Meanwhile, the authors\\nof [15] employed more recent deep learning techniques, such as convolutional neural networks (CNNs)\\nand recurrent neural networks (RNNs) with spectral features from short-time Fourier transform (STFT)\\nand constant Q transform (CQT), to classify children diagnosed using the autism diagnostic observation\\nschedule (ADOS), also showing promising results in multiple outcomes from SVMs, RNNs, and a\\ncombination of CNN and RNN classiﬁers.\\nA generalized acoustic feature set, an extended version of the Geneva minimalistic acoustic\\nparameter set (eGeMAPS) [ 16], and the bidirectional long short-term memory (BLSTM) model were\\nadopted to di ﬀerentiate between children with ASD and children with TD in [ 17], showing that 75% of\\nthe subjects’ utterances were correctly classiﬁed with the simple application of a deep learning model\\nand feature sets. While the quality of previous research based on various acoustic features has proven\\nthe eﬀectiveness of acoustic features and classiﬁcation algorithms for the detection of abnormalities in\\nchildren’s voices in ASD group compared to those of TD group, the complexity and relationship being\\ninherent between the features will remain uncertain until a large amount of data can be accumulated.\\nFurthermore, a limitation still remains in terms of the problems regarding data collection, since there are',\n",
       " 'Sensors 2020 ,20, 6762 3 of 11\\ndiﬃculties pertaining to the need to secure the anonymity of infant subjects, as well as the unintended\\nignorance of parents at earlier stages of their infant’s development. The data of infants are, accordingly,\\ndispersed by gender, age, and number of vocalizations, or consist of comparably small volumes of\\naudio engineering data in general. These problems were typically overlooked by previous research\\nwith controlled and small amounts of data.\\nIn order to provide suggestions for a method to overcome the abovementioned restrictions, we focus\\non examining the feasibility of neural networks as a feature extractor, employing an auto-encoder (AE),\\nwhich can modify acoustic features into lowered and separable feature dimensions [ 18]. We construct a\\nsimple six-layered stacked AE that contains an input layer, three fully connected (FC) layers, an output\\nlayer, and one auxiliary output layer, which has categorical targets for ASD and TD for the optimization\\nof the latent feature space of the AE. We train the AE and deep learning models and compare the\\nresults for each model based on SVMs and vanilla BLSTM, while adopting the same model parameters\\nfrom the method suggested in [17].\\nThe remainder of this paper is organized as follows. Section 2 describes the speciﬁcations of the\\nparticipants’ data, data processing, feature extraction, statistical analysis, and experimental setup.\\nSection 3 presents the performance evaluations for each algorithm of the SVMs and vanilla BLSTM.\\nLastly, Section 4 concludes the paper.\\n2. Proposed Method\\n2.1. Data Collection and Acoustic Feature Extraction\\nThis study was based on the audio data from video recordings of ASD diagnoses, which were\\ncollected from 2016 to 2018 at Seoul National University Bundang Hospital (SNUBH). We received\\napproval from the Institutional Review Board (IRB) at SNUBH to use fully anonymized data for\\nretrospective analysis (IRB no: B-1909 /567-110) from existing research (IRB no: B-1607 /353-005).\\nWe collected the audio data of 39 infants who were assessed using seven multiple instruments,\\nconsisting of (1) ADOS, second edition (ADOS-2), (2) the autism diagnostic interview, revised (ADI-R),\\n(3) the behavior development screening for toddlers interview (BeDevel-I), (4) the behavior development\\nscreening for toddlers play (BeDevel-P), (5) the Korean version of the childhood autism rating scale\\n(K-CARS) reﬁned from CARS-2, (6) the social communication questionnaire (SCQ), and (7) the social\\nresponsiveness scale (SRS) [ 19–22]. The ﬁnal diagnosis was based on the best clinical estimate diagnosis\\naccording to the DSM-5 ASD criteria by a licensed child psychiatrist using all of the available participant\\ninformation. The participants’ ages ranged between 6 and 24 months, where the average age was\\n19.20 months with a standard deviation (SD) of 2.52 months. Note here that the age means the age\\nat the time when each infant visited the hospital to undergo an initial diagnosis examination. There\\nwere four males and six females diagnosed with ASD, whose average age was 14.72 months with a\\nSD of 2.45. The remaining participants consisted of TD children (19 males and 10 females). Table 1\\ndisplays the collected data distribution, while Table 2 shows detailed information of collected data\\nfrom the infants.\\nTable 1. Distribution of age and gender (male /female).\\nAges (Month)No. of Subjects\\nDiagnosed as ASDNo. of Subjects\\nDiagnosed as TDNo. of Infant Subjects\\n6–12 months 0 5 M /1 F 5 M /1 F\\n12–18 months 1 M /3 F 14 M /9 F 15 M /12 F\\n18–24 months 3 M /3 F 0 3 M /3 F\\nAge (average±SD) 19.20 ±2.52 14.72 ±2.45 15.92 ±3.17',\n",
       " 'Sensors 2020 ,20, 6762 4 of 11\\nTable 2. Detailed information on the age, gender, and initial and deﬁnite diagnosis dates of each infant\\nin Table 1.\\nInfant IDAge (Months) on\\nInitial Diagnosis\\nDateGenderInitial Diagnosis\\nDate\\n(Year /Month /Day)Deﬁnite Final\\nDiagnosis Date\\n(Year /Month /Day)ASD /TD\\n1 18 Male 2018 /07/28 2018 /08/28 TD\\n2 18 Male 2017 /07/27 2017 /08/27 TD\\n3 10 Male 2018 /08/10 2018 /09/10 TD\\n4 13 Male 2017 /06/10 2017 /07/10 TD\\n5 22 Female 2018 /01/31 2018 /02/28 ASD\\n6 16 Male 2018 /03/17 2018 /04/17 TD\\n7 17 Female 2018 /06/30 2018 /07/30 TD\\n8 14 Female 2018 /01/06 2018 /02/06 TD\\n9 18 Male 2018 /07/17 2018 /08/17 TD\\n10 14 Male 2017 /11/04 2017 /12/04 TD\\n11 17 Female 2017 /06/29 2017 /07/29 ASD\\n12 12 Female 2018 /01/20 2018 /02/20 TD\\n13 9 Male 2017 /02/18 2017 /03/18 TD\\n14 18 Female 2017 /03/04 2017 /04/04 ASD\\n15 18 Male 2018 /05/19 2018 /06/19 TD\\n16 24 Female 2018 /08/08 2018 /09/08 ASD\\n17 19 Male 2018 /02/24 2018 /03/24 ASD\\n18 19 Male 2017 /04/18 2017 /05/18 ASD\\n19 18 Female 2017 /03/04 2017 /04/04 TD\\n20 12 Male 2016 /12/31 2017 /01/31 TD\\n21 16 Female 2018 /03/16 2018 /04/16 TD\\n22 20 Male 2017 /10/14 2017 /11/14 ASD\\n23 15 Male 2018 /05/09 2018 /06/09 ASD\\n24 17 Female 2017 /02/04 2017 /03/04 TD\\n25 16 Male 2018 /03/17 2018 /04/17 TD\\n26 12 Male 2018 /03/29 2018 /04/29 TD\\n27 17 Female 2017 /01/25 2017 /02/25 TD\\n28 17 Male 2018 /02/08 2018 /03/08 ASD\\n29 14 Male 2018 /01/13 2018 /02/13 TD\\n30 16 Male 2016 /11/30 2016 /12/30 TD\\n31 12 Male 2017 /03/22 2017 /04/22 TD\\n32 15 Male 2017 /03/11 2017 /04/11 TD\\n33 16 Male 2017 /12/05 2018 /01/05 TD\\n34 13 Female 2017 /12/13 2018 /01/13 TD\\n35 15 Female 2017 /03/25 2018 /04/25 TD\\n36 13 Male 2018 /08/25 2018 /09/25 TD\\n37 21 Male 2017 /06/24 2017 /07/24 ASD\\n38 14 Male 2017 /02/22 2017 /03/22 TD\\n39 14 Male 2018 /01/27 2018 /02/27 TD\\nAs each infant’s audio data were recorded during the clinical procedure to elicit behaviors from\\ninfants, with the attendance of one doctor or clinician and one or both parents with the child in the\\nclinical area, the audio components consisted of various speeches from the child, the clinician, and the\\nparent(s), as well as noises from toys or dragging chairs. Note here that the recordings were done in one\\nof two typical clinical rooms in SNUBH, where the room dimensions were 365 cm×400 cm×270 cm\\nand 350 cm×350 cm×270 cm, and the hospital noise level was around 40 dB. In order to analyze\\nthe vocal characteristics of the infants, each audio clip was processed and split into audio segments\\ncontaining the infant’s voice, not disturbed by music or clattering noises from toys or overlapped\\nby the voices of the clinician or parent(s). Each segment was classiﬁed into one of ﬁve categories,\\nlabeled from 0 to 4, for measuring the data distribution. Each label was intended to show di ﬀerentiable\\ncharacteristics relative to the children’s linguistic development: (1) 0 for one syllable, which is a short,',\n",
       " 'Sensors 2020 ,20, 6762 5 of 11\\nmomentary single vocalization such as “ah” or “ba”; (2) 1 for two syllables, commonly denoted as\\ncanonical babbling, as a reduplication of clear babbling of two identical or variant syllables such as\\n“baba” or “baga”; (3) 2 for babbling, not containing syllables; (4) 3 for ﬁrst word, such as “mother” or\\n“father”; and (5) 4 for atypical voice, including screaming or crying. The distribution of each type of\\nvocalization in seconds is shown in Table 3. The number of vocalizations per category is presented\\nalong with a rational value considering the di ﬀerence between the ASD and TD groups. While the\\ndata were unbalanced and very small, the distribution of ASD and TD vocalizations show the same\\ntendency as reported in [ 10], where the ASD group showed a signiﬁcantly lower ratio of ﬁrst words\\nand an increased ratio of atypical vocalizations, revealing developmental delay in linguistic ability.\\nTable 3. Amount (ratio) of each type of vocalization in seconds.\\nVocal Label ASD TD\\n0 80.134 (0.104) 267.897 (0.250)\\n1 314.405 (0.409) 443.498 (0.414)\\n2 33.241 (0.043) 34.766 (0.032)\\n3 8.311 (0.011) 57.286 (0.054)\\n4 333.400 (0.433) 266.794 (0.249)\\nTotal 769.491 1070.241\\nFor acquiring qualiﬁed and e ﬀective feature sets for the vocal data, eGeMAPS was employed\\nfor voice feature extraction. GeMAPS is a popular feature set providing minimalistic speech features\\ngenerally utilized for automatic voice analysis rather than as a large brute force parameter set. As an\\nextended version, eGeMAPS contains 88 acoustic features that were fully utilized in this experiment.\\nEach recorded set of audio data stored as a 48 kHz stereo ﬁle was down-sampled and down-mixed into a\\n16 kHz mono-audio ﬁle, taking into consideration its usability and resolution in mel-frequency cepstral\\ncoeﬃcients (MFCCs). To extract the speech features for ASD classiﬁcation, each infant’s utterances\\nwere segmented into 25 ms frames with a 10 ms overlap between frames. Then, 88 di ﬀerent features of\\nthe eGeMAPS were extracted for each frame with open source speech and music interpretation using\\nthe large-space extraction (OpenSMILE) toolkit [ 23], and these features were normalized by mean and\\nstandard deviation. The normalization scaling was acquired and ﬁxed by normalizing the factors of\\nthe training data set. The features were grouped for each ﬁve frames considering the time-relevant\\ncharacteristics of the speech data.\\n2.2. Pre-Trained AE for Acoustic Features\\nTo further process and reﬁne the acoustic data, a feature-extracting AE was introduced. An AE is a\\nhierarchical structure that is trained as a regression model for reproducing the input parameters. The AE\\ntakes inputs and converts them into latent representations, and then reconstructs the input parameters\\nfrom the latent values [ 24]. If we consider an input of AE, x∈Rd, then the latent representation z∈Rd′\\nand the reconstruction of the input y∈Rdare obtained by applying a nonlinear activation function\\nfto the weight sum of zusing a weighting matrix W∈Rd×d′and a bias vector b∈Rd′, such as\\nz=f(\\nWTx+b)\\n(1)\\ny=f(\\nWTz+b′)\\n(2)\\nwhere Tis a matrix transpose operator. When the latent dimension d′<d, the output from the latent\\nlayer is considered to be a compressed, meaningful value extracted from the input, which is also noted\\nas a bottleneck feature [25].\\nThe normalized eGeMAPS features were applied to train the feature-extracting AE, applying\\nthe same data as the input and the target. The AE model contained a latent layer with a lowered,\\ncompacted feature dimension compared to the input layer to achieve the useful bottleneck feature.',\n",
       " 'Sensors 2020 ,20, 6762 6 of 11\\nThe model was symmetrically structured, centering around the latent layer, and the model could be\\ndivided into two components: the encoder, consisting of layers from the input to the latent layers,\\nand a decoder, consisting of layers from the bottleneck to the output layers.\\nThe AE structure is depicted in Figure 1. Our AE model consisted of FC layers, with the dimensions\\nof 88, 70, 54, 70, and 88 nodes for the input, hidden, latent, hidden, and output layers, respectively.\\nThe hidden dimension was selected experimentally and the bottleneck feature dimension was used for\\ncomparison with previous research [ 17], where 54 features were selected considering the statistical\\ndissimilarity of the distributions between the ASD and TD features based on the Mann–Whitney U\\ntest [ 26]. We additionally introduced an auxiliary output as the binary categorical target for ASD\\nand TD, which is known as the semi-supervised method, to train the AE model e ﬀectively [ 27]. The\\nauxiliary output is depicted as Aux in Figure 1. The reconstructed features and auxiliary classiﬁcation\\ncan be written as\\nzi=f(Wi−1,izi−1+bi−1,i) (3)\\nwhere z1=f(W0,1x+b0,1), and\\nyrec=W3,4z3+b3,4 (4)\\nyaux=∂(W2,az2+b2,a) (5)\\nwhere yrecrefers to the reconstructed eGeMAPS features, yauxis the auxiliary classiﬁcation result, fis\\nthe activation function, and ∂is the softmax activation.\\nSensors 2020, 20, x FOR PEER REVIEW  6 of 12 \\n where T is a matrix transpose operator. When the latent dimension 𝑑′<𝑑, the output from the latent \\nlayer is considered to be a compressed, meaningful value extracted from the input, which is also \\nnoted as a bottleneck feature [25].  \\nThe normalized eGeMAPS feature s were applied to train the feature -extracting AE, applying the \\nsame data as the input and the target. The AE model contained a latent layer with a lowered, \\ncompacted feature dimension compared to the input layer to achieve the useful bottleneck feature. \\nThe model was symmetrically structured, centering around the latent layer, and the model could be \\ndivided into two components: the encoder, consisting of layers from the input to the latent layers, \\nand a decoder, consisting of layers from the bottleneck to the output layers.  \\nThe AE structure is depicted in Figure 1. Our AE model consisted of FC layers, with the \\ndimensions of 88, 70, 54, 70, and 88 nodes for the input, hidden, latent, hidden, and output layers, \\nrespectively. The hidden dimension was selected experimentally and the bottleneck feature \\ndimension was used for comparison with previous research [17], where 54 features were selected \\nconsidering the statistical dissimilarity of the distributions between the ASD and TD features based \\non the Mann –Whitne y U test [26]. We additionally introduced an auxiliary output as the binary \\ncategorical target for ASD and TD, which is known as the semi -supervised method, to train the AE \\nmodel effectively [27]. The auxiliary output is depicted as Aux in Figure 1. The reconstructed features \\nand auxiliary classification can be written as  \\n𝒛𝑖=𝑓(𝑾𝑖−1,𝑖𝒛𝑖−1+𝒃𝑖−1,𝑖) (3) \\nwhere 𝒛1=𝑓(𝑾0,1𝒙+𝒃0,1), and  \\n𝒚𝑟𝑒𝑐=𝑾3,4𝒛3+𝒃3,4 (4) \\n  𝒚𝑎𝑢𝑥=𝜕(𝑾2,𝑎𝒛2+𝒃2,𝑎) (5) \\nwhere 𝒚𝑟𝑒𝑐 refers to the reconstructed eGeMAPS features, 𝒚𝑎𝑢𝑥 is the auxiliary classification result, \\n𝑓 is the activation function, and 𝜕 is the softmax activation.  \\n \\nFigure 1. Structure of a semi -supervised auto -encoder (AE) model. eGeMAPS , extended version of \\nthe Geneva minimalistic acoustic parameter set; ASD, autism spectrum disorder; TD, typical \\ndevelopment.  \\nThe losses of the reconstruction error for main AE target are measured using the mean absolute \\nerror, while the auxiliary ASD/TD t arget loss is the binary cross -entropy, and they are added and \\nsimultaneously optimized with rational hyper -parameters. The overall loss equation is  \\nFigure 1. Structure of a semi-supervised auto-encoder (AE) model. eGeMAPS, extended version of the\\nGeneva minimalistic acoustic parameter set; ASD, autism spectrum disorder; TD, typical development.\\nThe losses of the reconstruction error for main AE target are measured using the mean absolute\\nerror, while the auxiliary ASD /TD target loss is the binary cross-entropy, and they are added and\\nsimultaneously optimized with rational hyper-parameters. The overall loss equation is\\nLrecon =1\\nNN∑\\ni=1⏐⏐⏐yirec−yigt⏐⏐⏐ (6)\\nLaux=−y1gtlog(y1aux)−(1−t)y1gtlog(y1aux) (7)\\nLtotal=Lrecon +αLaux (8)\\nwhere Lrecon,Laux, and Ltotaldenote the reconstruction error, auxiliary loss using a binary cross-entropy\\nloss function, and total loss, respectively.',\n",
       " 'Sensors 2020 ,20, 6762 7 of 11\\nFor our stacked AE model, a rational value of α=0.3was selected experimentally, considering\\nthe proportion of each loss. In order to train the AE e ﬀectively, both L2 normalization for weight\\nnormalization and batch normalization were adopted [ 28,29]. After the training was completed,\\nwe fetched the encoder of the AE as the feature extraction part for the joint optimization model in the\\ntraining procedures of the deep learning model.\\n2.3. Establishing and Training the Deep Learning Model for ASD Detection\\nAs the eGeMAPS data were set and the AE was trained through semi-supervised learning,\\nthe machine learning models, such as SVMs, BLSTM, and joint optimized BLSTM were constructed.\\nEach model had its own input parameter dimensions and the same output targets as ASD and TD\\nclassiﬁcation labels. The eGeMAPS feature data were paired with the diagnostic results for the\\nsupervised learning of the neural network models. For the binary decision, ASD was labeled as\\na positive data point, with a label of (0, 1), while TD was labeled as a negative data point (1, 0).\\nWe composed four kinds of models with the paired data: SVMs with linear kernel, the vanilla BLSTM\\nwith 88 eGeMAPS features, the vanilla BLSTM with 54 eGeMAPS features, and the jointly optimized\\nBLSTM layer with the AE. The joint optimization model is depicted in Figure 2. As the data set was\\nprepared as the input with ﬁve sequential frames, i.e., the grouped eGeMAPS features in Figure 2,\\nthe SVMs received a single frame parameter of 440 dimension which was ﬂattened from the original\\nﬁve input frames. For the deep learning models, batch normalization, rectangular linear unit (ReLU)\\nactivation, and dropout were applied for each layer, except for the output layer [ 30,31], and the\\nadaptive momentum (ADAM) optimizer [ 32] was used to train the network. The training procedure\\nwas controlled by early stopping for minimizing the validation error with 100 epoch patience, while\\nsaving the best models for improvement of the validation loss by each epoch. Because the amount\\nof speech data was relatively small for a deep learning model compared to the disparate ﬁeld of\\naudio engineering, we grouped the data into ﬁve segments, while the test utterances were separated\\nformerly, which were selected randomly for 10% of the total data, were evenly distributed across each\\nvocalization type, and underwent ﬁve-fold cross-validation for training; then, the best-performing\\nmodel was chosen. Our model was trained with the TensorFlow framework [ 33]. For comparison,\\nan SVM model with linear kernel was trained with the same data split as the proposed deep learning\\nmodel, and as well as the vanilla BLSTM suggested in [17], which has single BLSTM with eight cells.\\nSensors 2020, 20, x FOR PEER REVIEW  8 of 12 \\n  \\nFigure 2. Structure of a joint optimization model of an auto -encoder (AE) and bidirectional long short -\\nterm memory (BLSTM).  \\n3. Performance Evaluation  \\nThe performance of each method was evaluated through five -fold cross validation, where 95 \\naverage ASD utterances and 130 average TD utterances were proportionally distributed over five \\ncases of vocalizations for the gener alized estimation of unconcentrated utterance data. The averaged \\nperformances of the five validation splits of each model are described in Table 4. The labeled names \\nof the BLSTM were used as the features for training the BLSTM model, where eGeMAPS -88 deno tes \\n88 features of eGeMAPS, eGeMAPS -54 denotes 54 features selected by the Mann –Whitney U test, and \\nAE-encoded denotes the joint optimized model. In the classification stage, one utterance was \\nprocessed in the frame -wise method and the softmax output was c onverted to class indices 0 and 1, \\nand if the average of class indices of the frames was over 0.5, then the utterance was considered an \\nASD child’s utterance. The performances were scored with conventional measures, as well as \\nunweighted average recall (UA R) and weighted average recall (WAR), chosen in the INTERSPEECH \\n2009 Emotion challenge, which considered imbalanced classes [34]. In the experiment, the SVM \\nmodel showed very low precision, which was extremely biased toward the TD class. The BLSTM \\nclassifi er with 88 features of eGeMAPS and the AE model showed considerable quality in terms of \\nclassifying ASD and TD children, while the AE model showed only marginal improvement in \\ncorrectly classifying children with ASD compared to eGeMAPS -88. The 54 selected features showed \\ndegraded quality compared to eGeMAPS -88, obtaining more biased results toward children with TD.  \\nTable 4. Classification results from the support vector machine (SVM), BLSTM with 88 or 54 \\neGeMAPS features, 54 selected eGeMAPS features, and B LSTM with AE -encoded features.  \\nModels  SVM  BLSTM \\n(eGeMAPS -54) BLSTM \\n(eGeMAPS -88) BLSTM  (AE-\\nEncoded)  \\nPredicted To ASD  TD ASD  TD ASD  TD ASD  TD \\nASD  62 18 170 103 196 99 215 98 \\nTD 413 632 305 547 279 551 260 552 \\nAccuracy  0.6178  0.6373  0.6640  0.6818  \\nPrecision  0.1305  0.3579  0.4126  0.4526  \\nRecall  0.7750  0.6227  0.6644  0.6869  \\nF1 score  0.2234  0.4545  0.5091  0.5457  \\nUAR  0.5514  0.5997  0.6302  0.6509  \\nUAR, unweighted average recall.  \\n  \\nFigure 2. Structure of a joint optimization model of an auto-encoder (AE) and bidirectional long\\nshort-term memory (BLSTM).\\n3. Performance Evaluation\\nThe performance of each method was evaluated through ﬁve-fold cross validation, where\\n95 average ASD utterances and 130 average TD utterances were proportionally distributed over ﬁve',\n",
       " 'Sensors 2020 ,20, 6762 8 of 11\\ncases of vocalizations for the generalized estimation of unconcentrated utterance data. The averaged\\nperformances of the ﬁve validation splits of each model are described in Table 4. The labeled names\\nof the BLSTM were used as the features for training the BLSTM model, where eGeMAPS-88 denotes\\n88 features of eGeMAPS, eGeMAPS-54 denotes 54 features selected by the Mann–Whitney Utest,\\nand AE-encoded denotes the joint optimized model. In the classiﬁcation stage, one utterance was\\nprocessed in the frame-wise method and the softmax output was converted to class indices 0 and 1, and\\nif the average of class indices of the frames was over 0.5, then the utterance was considered an ASD\\nchild’s utterance. The performances were scored with conventional measures, as well as unweighted\\naverage recall (UAR) and weighted average recall (WAR), chosen in the INTERSPEECH 2009 Emotion\\nchallenge, which considered imbalanced classes [ 34]. In the experiment, the SVM model showed very\\nlow precision, which was extremely biased toward the TD class. The BLSTM classiﬁer with 88 features\\nof eGeMAPS and the AE model showed considerable quality in terms of classifying ASD and TD\\nchildren, while the AE model showed only marginal improvement in correctly classifying children\\nwith ASD compared to eGeMAPS-88. The 54 selected features showed degraded quality compared to\\neGeMAPS-88, obtaining more biased results toward children with TD.\\nTable 4. Classiﬁcation results from the support vector machine (SVM), BLSTM with 88 or 54 eGeMAPS\\nfeatures, 54 selected eGeMAPS features, and BLSTM with AE-encoded features.\\nModels SVMBLSTM\\n(eGeMAPS-54)BLSTM\\n(eGeMAPS-88)BLSTM\\n(AE-Encoded)\\nPredicted To ASD TD ASD TD ASD TD ASD TD\\nASD 62 18 170 103 196 99 215 98\\nTD 413 632 305 547 279 551 260 552\\nAccuracy 0.6178 0.6373 0.6640 0.6818\\nPrecision 0.1305 0.3579 0.4126 0.4526\\nRecall 0.7750 0.6227 0.6644 0.6869\\nF1 score 0.2234 0.4545 0.5091 0.5457\\nUAR 0.5514 0.5997 0.6302 0.6509\\nUAR, unweighted average recall.\\n4. Discussion\\nThe vanilla BLSTM model presented in [ 17] conducted discrimination on well-classiﬁed subjects\\nwith 10-month-old children and sorted 54 features from eGeMAPS that had a distinctive distribution\\nbetween ASD and TD selected by the Mann–Whitney Utest using the three-fold cross-validation\\nmethod. However, because the di ﬀerence in the data distribution failed to achieve the same eGeMAPS\\nfeature selection between the test and classiﬁcation results with the speciﬁed feature set presented\\nherein, the application of an identical model structure and the adoption of the same feature domain\\nwill allow both approaches to be indirectly comparable.\\nThese results can be interpreted by the data distributions, and we performed t-stochastic neighbor\\nembedding (t-SNE) analysis [ 35] on the training data set, which can nonlinearly squeeze the data\\ndimension based on a machine learning algorithm. Figure 3 shows each data distribution as a\\ntwo-dimensional scatter plot. In the ﬁgure, the eGeMAPS features from eGeMAPS-88 and eGeMAPS-54\\nshowed almost identical distribution, except for the amount of ASD outliers, which implies that the\\nASD and TD features in the eGeMAPS features show similar distributions in this experiment. As shown\\nin [16], eGeMAPS includes temporal features that are relevant to vocalizations and utterances; thus, these\\nfeatures might cause confusion regarding the discrimination between ASD and TD. The AE-encoded\\nfeatures, however, showed a redistributed feature map with a more characteristic distribution compared\\nto the eGeMAPS features. This is because the AE-encoded features were compressed into a bottleneck\\nfeature, which was derived by weighting the matrix, paying attention to the signiﬁcant parameters',\n",
       " 'Sensors 2020 ,20, 6762 9 of 11\\nwhile reducing the inﬂuence from the ambiguous parameters. While the joint optimization model\\nachieved only marginally improved results compared to eGeMAPS-88, the distribution of the feature\\nmap would be more noticeable in improved feature extraction models, as well as more di ﬀerentiable in\\ncomplex models, although BLSTM with eight cells was employed for a comparison with conventional\\nresearch in this experiment.\\nSensors 2020, 20, x FOR PEER REVIEW  9 of 12 \\n 4. Discussion  \\nThe vanilla BLSTM model presented in [17] conducted discrimination on well -classified subjects \\nwith 10 -month -old children and sorted 54 features from eGeMAPS that had a distinctive distribution \\nbetween ASD and TD selected by the Mann –Whitney U test using t he three -fold cross -validation \\nmethod. However, because the difference in the data distribution failed to achieve the same \\neGeMAPS feature selection between the test and classification results with the specified feature set \\npresented herein, the applicatio n of an identical model structure and the adoption of the same feature \\ndomain will allow both approaches to be indirectly comparable.  \\nThese results can be interpreted by the data distributions, and we performed t -stochastic \\nneighbor embedding (t -SNE) analy sis [35] on the training data set, which can nonlinearly squeeze the \\ndata dimension based on a machine learning algorithm. Figure 3 shows each data distribution as a \\ntwo-dimensional scatter plot. In the figure, the eGeMAPS features from eGeMAPS -88 and eGeM APS -\\n54 showed almost identical distribution, except for the amount of ASD outliers, which implies that \\nthe ASD and TD features in the eGeMAPS features show similar distributions in this experiment. As \\nshown in [16], eGeMAPS includes temporal features that are relevant to vocalizations and utterances; \\nthus, these features might cause confusion regarding the discrimination between ASD and TD. The \\nAE-encoded features, however, showed a redistributed feature map with a more characteristic \\ndistribution compared to the eGeMAPS features. This is because the AE -encoded features were \\ncompressed into a bottleneck feature, which was derived by weighting the matrix, paying attention \\nto the significant parameters while reducing the influence from the ambiguous parameters . While the \\njoint optimization model achieved only marginally improved results compared to eGeMAPS -88, the \\ndistribution of the feature map would be more noticeable in improved feature extraction models, as \\nwell as more differentiable in complex models, alt hough BLSTM with eight cells was employed for a \\ncomparison with conventional research in this experiment.  \\nWhile the overall performance scores were comparably low for general classification problems \\non account of the subjectivity and complexity of problems, and the limitation in terms of the shortage \\nof data, the results of the jointly optimized model imply the possibility of deep -learning -based feature \\nextraction for the improvement of automated ASD/TD diagnosis under restricted circumstances.  \\n   \\n(a) (b) (c) \\nFigure 3. Two -dimensional scatter plot for ( a) eGeMAPS -88, ( b) eGeMAPS -54, and ( c) the AE \\nprocessed by t -stochastic neighbor embedding (t -SNE).  \\n5. Conclusion s \\nIn this paper, we conducted experiments for discovering the possibility of auto -encoder -based \\nfeature extraction and a joint optimization  method for the automated detection of atypicality in voices \\nof children with ASD during early developmental stages. Un der the condition of an insufficient and \\ndispersed data set, the clas sification results were relatively poor in comparison to the general \\nclassification tasks based on deep learning. Although our investigation used a limited number of \\nsubjects and an unbal anced data set, the suggested auto -encoder -based feature extraction and joint \\noptimization method revealed the possibility of feature dimension and a slight improvement in \\nmodel -based diagnosis under such uncertain circumstances.  \\nFigure 3. Two-dimensional scatter plot for ( a) eGeMAPS-88, ( b) eGeMAPS-54, and ( c) the AE processed\\nby t-stochastic neighbor embedding (t-SNE).\\nWhile the overall performance scores were comparably low for general classiﬁcation problems on\\naccount of the subjectivity and complexity of problems, and the limitation in terms of the shortage of\\ndata, the results of the jointly optimized model imply the possibility of deep-learning-based feature\\nextraction for the improvement of automated ASD /TD diagnosis under restricted circumstances.\\n5. Conclusions\\nIn this paper, we conducted experiments for discovering the possibility of auto-encoder-based\\nfeature extraction and a joint optimization method for the automated detection of atypicality in voices\\nof children with ASD during early developmental stages. Under the condition of an insu ﬃcient\\nand dispersed data set, the classiﬁcation results were relatively poor in comparison to the general\\nclassiﬁcation tasks based on deep learning. Although our investigation used a limited number of\\nsubjects and an unbalanced data set, the suggested auto-encoder-based feature extraction and joint\\noptimization method revealed the possibility of feature dimension and a slight improvement in\\nmodel-based diagnosis under such uncertain circumstances.\\nIn future work, we will focus on increasing the reliability of the proposed method by addition\\nof a number of infants’ speech data, reﬁnement of the acoustic features, an auto-encoder for feature\\nextraction, and better, deeper, and up-to-date model structures. This research can also be extended to\\nchildren with the age of 3 or 4 who can speak several sentences. In this case, we will investigate the\\nlinguistic features, as well as acoustic features, such as we have done in this paper. In addition to ASD\\ndetection, this research can be applied to the detection of infants with development delays.\\nAuthor Contributions: All authors discussed the contents of the manuscript. H.K.K. contributed to the research\\nidea and the framework of this study; G.B. and H.J.Y. provided the database and helped with the discussion; J.H.L.\\nperformed the experiments; G.W.L. contributed to the data collection and pre-processing. All authors have read\\nand agreed to the published version of the manuscript.\\nFunding: This work was supported by the Institute of Information & communications Technology Planning &\\nevaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00330, Development of AI Technology\\nfor Early Screening of Infant /Child Autism Spectrum Disorders based on Cognition of the Psychological Behavior\\nand Response).\\nConﬂicts of Interest: The authors declare no conﬂict of interest.',\n",
       " 'Sensors 2020 ,20, 6762 10 of 11\\nReferences\\n1. National Institute of Mental Health. Autism Spectrum Disorder. Available online: https: //www.nimh.nih.\\ngov/health /topics /autism-spectrum-disorders-asd /index.shtml (accessed on 26 October 2020).\\n2. American Psychiatric Association. Diagnostic and Statistical Manual of Mental Disorders: DSM-5 ; American\\nPsychiatric Publishing: Washington, DC, USA, 2013.\\n3. Centers for Disease Control and Prevention (CDC). Data & Statistics on Autism Spectrum Disorder. Available\\nonline: https: //www.cdc.gov /ncbddd /autism /data.html (accessed on 26 October 2020).\\n4. Fenske, E.C.; Zalenski, S.; Krantz, P .J.; McClannahan, L.E. Age at intervention and treatment outcome for\\nautistic children in a comprehensive intervention program. Anal. Interv. Devel. Disabil. 1985 ,5, 49–58.\\n[CrossRef]\\n5. Falkmer, T.; Anderson, K.; Falkmer, M.; Horlin, C. Diagnostic procedures in autism spectrum disorders:\\nA systematic literature review. Eur. Child Adolesc. Psychiatry 2013 ,22, 329–340. [CrossRef] [PubMed]\\n6. Bailey, A.; Le Couteur, A.; Gottesman, I.; Bolton, P .; Simono ﬀ, E.; Yuzda, E.; Rutter, M. Autism as a strongly\\ngenetic disorder: Evidence from a British twin study. Physiol. Med. 1995 ,25, 63–77. [CrossRef] [PubMed]\\n7. Duﬀy, F.H.; Als, H. A stable pattern of EEG spectral coherence distinguishes children with autism from\\nneuro-typical controls—A large case control study. BMC Med. 2012 ,10, 64. [CrossRef] [PubMed]\\n8. Chaspari, T.; Lee, C.-C.; Narayanan, S.S. Interplay between verbal response latency and physiology of\\nchildren with autism during ECA interactions. In Proceedings of the Annual Conference of the International\\nSpeech Communication Association (Interspeech), Portland, OR, USA, 9–13 September 2012; pp. 1319–1322.\\n9. Baron-Cohen, S. Social and pragmatic deﬁcits in autism: Cognitive or a ﬀective? J. Autism Dev. Disord. 1988 ,\\n18, 379–402. [CrossRef] [PubMed]\\n10. Bonneh, Y.S.; Levanon, Y.; Dean-Pardo, O.; Lossos, L.; Adini, Y. Abnormal speech spectrum and increased\\npitch variability in young autistic children. Front. Hum. Neurosci. 2011 ,4, 237. [CrossRef] [PubMed]\\n11. Chericoni, N.; de Brito Wanderley, D.; Costanzo, V .; Diniz-Gonçalves, A.; Gille, M.L.; Parlato, E.; Cohen, D.;\\nApicella, F.; Calderoni, S.; Muratori, F. Pre-linguistic vocal trajectories at 6–18 months of age as early markers\\nof autism. Front. Psychol. 2016 ,7, 1595. [CrossRef] [PubMed]\\n12. Alom, M.Z.; Taha, T.M.; Yakopcic, C.; Westberg, S.; Sidike, P .; Nasrin, M.S.; Hasan, M.; van Essen, B.C.;\\nAwwal, A.A.S.; Asari, V .K. A state-of-the-art survey on deep learning theory and architectures. Electronics\\n2019 ,8, 292. [CrossRef]\\n13. Song, D.-Y.; Kim, S.Y.; Bong, G.; Kim, J.M.; Yoo, H.J. The use of artiﬁcial intelligence in screening and\\ndiagnosis of autism spectrum disorder: A literature review. J. Korean Acad. Child. Adolesc. Psychiatry 2019 ,30,\\n145–152. [CrossRef] [PubMed]\\n14. Santos, J.F.; Brosh, N.; Falk, T.H.; Zwaigenbaum, L.; Bryson, S.E.; Roberts, W.; Smith, I.M.; Szatmari, P .;\\nBrian, J.A. Very early detection of autism spectrum disorders based on acoustic analysis of pre-verbal\\nvocalizations of 18-month old toddlers. In Proceedings of the IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), Vancouver, BC, Canada, 26–31 May 2013; pp. 7567–7571.\\n15. Li, M.; Tang, D.; Zeng, J.; Zhou, T.; Zhu, H.; Chen, B.; Zou, X. An automated assessment framework for\\natypical prosody and stereotyped idiosyncratic phrases related to autism spectrum disorder. Comput. Speech\\nLang. 2019 ,56, 80–94. [CrossRef]\\n16. Eyben, F.; Scherer, K.R.; Schuller, B.W.; Sundberg, J.; Andr é, E.; Busso, C.; Devillers, L.Y.; Epps, J.; Laukka, P .;\\nNarayanan, S.S.; et al. The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and\\naﬀective computing. IEEE Trans. A ﬀect. Comput. 2016 ,7, 190–202. [CrossRef]\\n17. Pokorny, F.B.; Schuller, B.W.; Marschik, P .B.; Brueckner, R.; Nyström, P .; Cummins, N.; Bölte, S.;\\nEinspieler, C.; Falck-Ytter, T. Earlier identiﬁcation of children with autism spectrum disorder: An automatic\\nvocalisation-based approach. In Proceedings of the Annual Conference of the International Speech\\nCommunication Association (Interspeech), Stockholm, Sweden, 20–24 August 2017; pp. 309–313.\\n18. Xing, C.; Ma, L.; Yang, X. Stacked denoise autoencoder based feature extraction and classiﬁcation for\\nhyperspectral images. J. Sens. 2016 ,2016 , 3632943. [CrossRef]\\n19. Bong, G.; Kim, J.; Hong, Y.; Yoon, N.; Sunwoo, H.; Jang, J.; Oh, M.; Lee, K.; Jung, S.; Yoo, H. The feasibility and\\nvalidity of autism spectrum disorder screening instrument: Behavior development screening for toddlers\\n(BeDevel)—A pilot study. Autism Res. 2019 ,12, 1112–1128. [CrossRef] [PubMed]',\n",
       " 'Sensors 2020 ,20, 6762 11 of 11\\n20. Center for Autism Research. Social Communication Questionnaire (SCQ). Available online: https: //www.\\ncarautismroadmap.org /social-communication-questionnaire-scq /?print =pdf (accessed on 26 October 2020).\\n21. Center for Autism Research. Childhood Autism Rating Scale, 2nd Edition (CARS2). Available online: https:\\n//www.carautismroadmap.org /childhood-autism-rating-scale /?print =pdf (accessed on 26 October 2020).\\n22. Center for Autism Research. Social Responsiveness Scale, 2nd Edition (SRS-2). Available online: https:\\n//www.carautismroadmap.org /social-responsiveness-scale /?print =pdf (accessed on 26 October 2020).\\n23. Eyben, F.; Wöllmer, M.; Schuller, B. OpenSMILE—The Munich versatile and fast open-source audio feature\\nextractor. In Proceedings of the 18th ACM International Conference on Multimedia, Firenze, Italy, 25–29\\nOctober 2010; pp. 1459–1462.\\n24. Masci, J.; Meier, U.; Cire¸ san, D.; Schmidhuber, J. Stacked Convolutional Auto-Encoders for Hierarchical\\nFeature Extraction. In Artiﬁcial Neural Networks and Machine-ICANN 2011 ; Honkela, T., Duch, W., Girolami, M.,\\nKaski, S., Eds.; Springer: Berlin /Heidelberg, Germany, 2011; pp. 52–59.\\n25. Sainath, T.; Kingsbury, B.; Ramabhadran, B. Auto-encoder bottleneck features using deep belief networks. In\\nProceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\\nKyoto, Japan, 25–30 March 2012.\\n26. Nachar, N. The Mann-Whitney U: A test for assessing whether two independent samples come from the\\nsame distribution. Tutor. Quant. Methods Psychol. 2008 ,4, 13–20. [CrossRef]\\n27. Le, L.; Patterson, A.; White, M. Supervised autoencoders: Improving generalization performance with\\nunsupervised regularizers. In Advances in Neural Information Processing Systems ; Bengio, S., Wallach, H.,\\nLarochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R., Eds.; Curran Associates, Inc.: New York, NY,\\nUSA, 2018; pp. 107–117.\\n28. van Laarhoven, T. L2 regularization versus batch and weight normalization. arXiv 2017 , arXiv:1706.05350.\\n29. Ioﬀe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate\\nshift. In Proceedings of the International Conference on Machine Learning, Lille, France, 6–11 July 2015;\\npp. 448–456.\\n30. Nair, V .; Hinton, G.E. Rectiﬁed linear units improve restricted Boltzmann machines. In Proceedings of the\\n27th International Conference on Machine Learning, Haifa, Israel, 21–24 June 2010; pp. 807–814.\\n31. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent\\nneural networks from overﬁtting. J. Mach. Learn. Res. 2014 ,15, 929–1958.\\n32. Kingma, D.P .; Ba, J.L. ADAM: A method for stochastic optimization. In Proceedings of the 3rd International\\nConference on Learning Representations, San Diego, CA, USA, 7–9 May 2015; pp. 1–15.\\n33. Abadi, M.; Barham, P .; Chen, J.; Chen, Z.; Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; Irving, G.; Isard, M.;\\net al. TensorFlow: A system for large-scale machine learning. In Proceedings of the 12th USENIX Symposium\\non Operating Systems Design and Implementation, Savannah, GA, USA, 2–4 November 2016; pp. 265–283.\\n34. Schuller, B.; Steidl, S.; Batliner, A. The Interspeech 2009 emotion challenge. In Proceedings of the Annual\\nConference of the International Speech Communication Association (Interspeech), Brighton, UK, 6–10\\nSeptember 2009; pp. 312–315.\\n35. van der Maaten, L.; Hinton, G. Visualizing data using t-SNE. J. Mach. Learn. Res. 2008 ,9, 2579–2605.\\nPublisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional\\naﬃliations.\\n©2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access\\narticle distributed under the terms and conditions of the Creative Commons Attribution\\n(CC BY) license (http: //creativecommons.org /licenses /by/4.0/).']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagecontent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15_Nazneen.pdf',\n",
       " '1_Ramırez-Duque_.pdf',\n",
       " '22_Ouss_ASD.pdf',\n",
       " 'Abbas_2018.pdf',\n",
       " 'Abbas_2020.pdf',\n",
       " 'Asd_Cry_patterns.pdf',\n",
       " 'carpenter2020 (1).pdf',\n",
       " 'Dawson.pdf',\n",
       " 'LEE.pdf',\n",
       " 'Patten_Audio.pdf',\n",
       " 'Qiu.pdf',\n",
       " 'Tariq2018.pdf',\n",
       " 'Tariq_2019.pdf',\n",
       " 'Young_Behavior.pdf',\n",
       " 'zhao2020.pdf']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "files=os.listdir('papers')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files=[file for file in files if file.endswith(\".txt\")]\n",
    "# for file in files:\n",
    "#     if file.endswith(\".txt\"):\n",
    "\n",
    "#         files_path=os.path.join('papers',file)\n",
    "#         os.remove(files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15_Nazneen.pdf',\n",
       " '1_Ramırez-Duque_.pdf',\n",
       " '22_Ouss_ASD.pdf',\n",
       " 'Abbas_2018.pdf',\n",
       " 'Abbas_2020.pdf',\n",
       " 'Asd_Cry_patterns.pdf',\n",
       " 'carpenter2020 (1).pdf',\n",
       " 'Dawson.pdf',\n",
       " 'LEE.pdf',\n",
       " 'Patten_Audio.pdf',\n",
       " 'Qiu.pdf',\n",
       " 'Tariq2018.pdf',\n",
       " 'Tariq_2019.pdf',\n",
       " 'Young_Behavior.pdf',\n",
       " 'zhao2020.pdf']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_text(text, output_file):\n",
    "    # Write the cleaned text to a new file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_clean_text(pdf_file):\n",
    "    # Open the PDF file\n",
    "    loader=PyPDFLoader(file_path=pdf_file)\n",
    "    doc = loader.load()\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        # print(page.page_content)\n",
    "        # Remove images and tables (if any)\n",
    "        page_text = re.sub(r'\\[Image\\].*?', '', page.page_content)\n",
    "        page_text = re.sub(r'\\[Table\\].*?', '', page_text)\n",
    "\n",
    "        # Remove in-text citations (assuming they are in the format \"[1]\")\n",
    "        page_text = re.sub(r'\\[\\d+\\]', '', page_text)\n",
    "\n",
    "        # Append page text to the overall text\n",
    "        text += page_text\n",
    "\n",
    "    return text,doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files=[file for file in files if file.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing -----> papers/15_Nazneen.pdf\n",
      "processed -----> papers/15_Nazneen.pdf\n",
      "processing -----> papers/1_Ramırez-Duque_.pdf\n",
      "processed -----> papers/1_Ramırez-Duque_.pdf\n",
      "processing -----> papers/22_Ouss_ASD.pdf\n",
      "processed -----> papers/22_Ouss_ASD.pdf\n",
      "processing -----> papers/Abbas_2018.pdf\n",
      "processed -----> papers/Abbas_2018.pdf\n",
      "processing -----> papers/Abbas_2020.pdf\n",
      "processed -----> papers/Abbas_2020.pdf\n",
      "processing -----> papers/Asd_Cry_patterns.pdf\n",
      "processed -----> papers/Asd_Cry_patterns.pdf\n",
      "processing -----> papers/carpenter2020 (1).pdf\n",
      "processed -----> papers/carpenter2020 (1).pdf\n",
      "processing -----> papers/Dawson.pdf\n",
      "processed -----> papers/Dawson.pdf\n",
      "processing -----> papers/LEE.pdf\n",
      "processed -----> papers/LEE.pdf\n",
      "processing -----> papers/Patten_Audio.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x1cc6b for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1ce61 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1d014 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1d1ae for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1d33d for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1d4af for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1d699 for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1d85b for key /MediaBox\n",
      "Multiple definitions in dictionary at byte 0x1db05 for key /MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed -----> papers/Patten_Audio.pdf\n",
      "processing -----> papers/Qiu.pdf\n",
      "processed -----> papers/Qiu.pdf\n",
      "processing -----> papers/Tariq2018.pdf\n",
      "processed -----> papers/Tariq2018.pdf\n",
      "processing -----> papers/Tariq_2019.pdf\n",
      "processed -----> papers/Tariq_2019.pdf\n",
      "processing -----> papers/Young_Behavior.pdf\n",
      "processed -----> papers/Young_Behavior.pdf\n",
      "processing -----> papers/zhao2020.pdf\n",
      "processed -----> papers/zhao2020.pdf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "texts=[]\n",
    "for file in files:\n",
    "    if file.endswith(\".pdf\"):\n",
    "\n",
    "        files_path=os.path.join('papers',file)\n",
    "        print('processing ----->',files_path)\n",
    "\n",
    "        clean_text,pdf_content = extract_clean_text(files_path)\n",
    "        output_file = os.path.splitext(files_path)[0] + \"_clean.txt\"\n",
    "        save_clean_text(clean_text, output_file)  \n",
    "        texts.append(pdf_content)     \n",
    "        print('processed ----->',files_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 0}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 1}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 2}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 3}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 4}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 5}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 6}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 7}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 8}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 9}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 10}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 11}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 12}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 13}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 14}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 15}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 16}),\n",
       "  Document(page_content='', metadata={'source': 'papers/15_Nazneen.pdf', 'page': 17})],\n",
       " [Document(page_content='https://doi.org/10.1007/s10846-018-00975-y\\nRobot-AssistedAutismSpectrumDisorderDiagnosticBased\\nonArtiﬁcialReasoning\\nAndr´esA.Ram ´ırez-Duque1·AnselmoFrizera-Neto1·TeodianoFreireBastos1\\nReceived:25April2018/Accepted:20December2018\\n©SpringerNatureB.V.2019\\nAbstract\\nAutism spectrum disorder (ASD) is a neurodevelopmental disorder that affects people from birth, whose symptoms are\\nfound in the early developmental period. The ASD diagnosis is usually performed through several sessions of behavioral\\nobservation, exhaustive screening, and manual coding behavior. The early detection of ASD signs in naturalistic behavioral\\nobservation may be improved through Child-Robot Interaction (CRI) and technological-based tools for automated behavior\\nassessment. Robot-assisted tools using CRI theories have been of interest in intervention for children with Autism Spectrum\\nDisorder (CwASD), elucidating faster and more significant gains from the diagnosis and therapeutic intervention when\\ncompared to classical methods. Additionally, using computer vision to analyze child’s behaviors and automated video coding\\nto summarize the responses would help clinicians to reduce the delay of ASD diagnosis. In this article, a CRI to enhance\\nthe traditional tools for ASD diagnosis is proposed. The system relies on computer vision and an unstructured and scalable\\nnetwork of RGBD sensors built upon Robot Operating System (ROS) and machine learning algorithms for automated face\\nanalysis. Also, a proof of concept is presented, with participation of three typically developing (TD) children and three\\nchildren in risk of suffering from ASD.\\nKeywords Child-Robot interaction ·Autism spectrum disorder ·Convolutional neural network ·Robot reasoning model ·\\nStatistical shape modeling\\n1Introduction\\nResearch in Child-Robot Interaction (CRI) aims to provide\\nthe necessary conditions for the interaction between a\\nchild and a robotic device taking into account some\\nfundamental features, such as child’s neurophysical and\\nphysical condition, and the child’s mental health [ 1].\\nThat is how Robot-Assisted Therapies (RAT) using CRI\\ntheories have been of interest as an intervention for\\nCwASD, elucidating faster and more significant gains from\\nthe therapeutic intervention when compared to traditional\\ntherapies [ 2–4].\\nASD is a neurodevelopmental disorder that affects people\\nfrom birth, and its symptoms are found in the early\\n/envelopebackAndr ´es A. Ram ´ırez-Duque\\naaramirezd@gmail.com\\n1Universidade Federal do Espir ´ıto Santo., Av. Fernando Ferrari,\\n514 (29075-910), Vitoria, Brazildevelopmental period. Individuals suffering from ASD\\nexhibit persistent deficits in social communication, social\\ninteraction and repetitive patterns of behavior, interests, or\\nactivities [ 5]. Some of the ASD signs may be observed\\nbefore the age of 10 months, although a reliable diagnosis\\ncan only be performed at 18 months of age, according to [ 6],\\nor 24 months according to [ 7].\\nThe use of computer vision to analyze the child’s\\nbehaviors, and automated video coding to summarize the\\ninterventions, can help the clinicians to reduce the delay\\nof ASD diagnosis, providing the CwASD with access\\nto early therapeutic interventions. In addition, CRI-based\\nintervention can transform traditional diagnosis methods\\nthrough a robotic device to systematically elicit child’s\\nbehaviors that exhibit ASD signs [ 8].\\nSome of the first systems developed to assist ASD\\ntherapists and make diagnosis based on robotic devices\\nhave primarily been open loop and remotely operated sys-\\ntems. However, these approaches are unable to perform\\nautonomous feedback to enhance the interaction [ 9–11].JournalofIntelligent&RoboticSystems(2019)96:267–281\\n/Publishedonline:29 2019March\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 0}),\n",
       "  Document(page_content='Nevertheless, different systems are able to modify the\\nbehavior of the robot according to environmental interac-\\ntions and the child’s response, using a closed-loop and\\nartificial cognition approaches [ 12–16]. These systems have\\nbeen hypothesized to offer technological mechanisms for\\nsupporting more flexible and potentially more naturalistic\\ninteraction [ 17]. In fact, literature reports that automatic\\nrobot’s social behaviors modulation according to specifics\\nscenarios has a strong effect on child’s social behavior\\n[12]. However, despite the increase of positive evidence,\\nthis technology has rarely been applied to specific ASD\\ndiagnosis.\\nThis work aims to present a robot-assisted framework\\nusing an artificial reasoning module to assist clinicians with\\nthe ASD diagnostic process. The framework is composed of\\na responsive robotic platform, a flexible and scalable vision\\nsensor network, and an automated face analysis algorithm\\nbased on machine learning models. In this research we take\\nadvantage of some neural models available as open sources\\nprojects to build a completely new pipeline algorithm for\\nglobal recognition and tracking of child’s face among many\\nfaces present in a typical unstructured clinical intervention,\\nin order to estimate the child’s visual focus of attention\\nalong the time. The proposed system can be used in different\\nbehavioral analysis scenarios typical of an ASD diagnostic\\nprocess. In order to illustrate the feasibility of the proposed\\nsystem, in this paper an experimental trial to assess joint-\\nattention behavior is presented employing an in-clinic setup\\n(unstructured environment).\\nThe main contributions of this paper are: (i) the\\ndevelopment of a new artificial reasoning module upon\\na flexible and scalable ROS-based vision system using\\nstate-of-the-art machine learning neural models; (ii) the\\nproposal and implementation of a supervised CRI (child-\\nrobot interaction) based on an open source social robotic\\nplatform to enhance the traditional tools for ASD diagnosis\\nusing an in-clinic setup protocol. For the best of our\\nknowledge, there are no open source projects available for\\nface analysis based on a multi-camera approach using ROS\\nwith the characteristics described in our research.\\n2RelatedWork\\nRecent researches have shown the acceptance and efficiency\\nof technologies used as auxiliary tools for therapy\\nand teaching of individuals with ASD [ 18–21]. Such\\ntechnologies may also be useful for people surrounding\\nASD individuals (therapists, caregivers, family members).\\nFor example, the use of artificial vision systems to measure\\nand analyze the child’s behavior can lead to alternative\\nscreening and monitoring tools that help the clinicians to\\nget feedback from the effectiveness of the intervention [ 22].Additionally, social robots have great potential for aid in\\nthe diagnosis and therapy of children with ASD [ 18,23].\\nA higher degree of control, prediction and simplicity may\\nbe achieved in interactions with robots, impacting directly\\non frustration and reducing the anxiety of these individuals\\n[24].\\nRespect to the use of computer vision techniques,\\nprevious studies already analyzed child’s behaviors, such\\nas visual attention, eye gaze, eye contact, smile events, and\\nvisual exploration using cameras and eye trackers [ 25,26]\\nand RGBd cameras [ 27,28]. These studies have shown\\nthe potential of vision systems in improving the behavioral\\ncoding in ASD therapies. However, these studies did not\\nimplement techniques of CRI to enhance the intervention.\\nOn the other hand, studies about how CwASD respond\\nto a robot mediator compared to human mediator have\\nbeen reported, such as intervention scenarios with imitation\\ngames [ 29,30], telling stories [ 9] and free play tasks\\n[12,31]. These works used features, such as proxemics,\\nbody gestures, visual contact and eye gaze as behavioral\\ndescriptors, whereas the behavior analysis was estimated\\nusing manual video coding.\\nResearchers of Vanderbilt University published a series\\nof research showing an experimental protocol to assess joint\\nattention (JA) tasks defined as the capacity for coordinated\\norientation of two people toward an object or event [ 6].\\nThe protocol consisted of directing the attention of the\\nchild towards objects located in the room through adaptive\\nprompts [ 32]. Bekele et al. inferred the participant’s eye\\ngaze by the head pose, which was calculated in real-time by\\nan IR camera array [ 17]. In their last works, Zheng et al. and\\nWarren et al. used a commercial eye tracker to estimated the\\nchildren’s eye gaze around the robot and manual behavioral\\ncoding for global evaluation [ 10,33]. However, eye tracker\\ndevices require pre-calibration and may limit the movement\\nof the individual. The results of these works showed that the\\nrobot attracted children’s attention and that CwASD reached\\nall JA task. Nevertheless, developing JA tasks is more\\ndifficult with a robot than with humans [ 10]. Anzalone et al.\\ndeveloped a CRI scenario using the NAO robot to perform\\nJA tasks, in which the authors used an RGBD camera\\nto estimate only body and head movements. The results\\nshowed that JA performance of children with ASD was\\nsimilar to the performance of TD children when interacting\\nwith the human mediator, however, with a robot mediator,\\nthe children with ASD presented a lower performance than\\nthe TD children, i.e, the children with ASD needed more\\nsocial cues to finalize the task [ 34]. Chevalier et al. analyzed\\nin their study, some features, such as proprioceptive and\\nvisual integration in CwASD, using an RGBD sensor\\nto record the interventions sessions and manual behavior\\ncoding to analyzed the participants’ performance [ 35]. In\\nnone of the previous works, a closed-loop subsystem wasJIntellRobotSyst(2019)96:267–281 268\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 1}),\n",
       "  Document(page_content='implemented to provide some level of artificial cognition to\\nenable automated robot behavior.\\nIn contrast with the aforementioned researches, other\\nworks implemented automated face analysis and artificial\\ncognition through robot-mediator and computer vision,\\nwhich analyzed child’s engagement [ 36,37], emotions\\nrecognition capability [ 13,15,38] and child’s intentions\\n[14,16]. In these works, two different strategies were\\nimplemented, where the most common is based on mono-\\ncamera approach using an external RGB or RGBd sensor\\n[15,36,37] or using on-board RGB cameras mounted\\non the robotic-platform [ 13,16]. Other strategies are\\nbased on a highly structured environment composed of\\nan external camera plus an on-board camera [ 38]o ra\\nnetwork of vision sensors attached to a small table [ 14].\\nThese strategies based on multi-camera methods improve\\nthe system’s performance, but remain constrained in relation\\nto desired features, such as flexibility, scalability, and\\nmodularity. Thus, despite the potential that these techniques\\nhave shown, achieving automated child’s behavior analysis\\nin a naturalistic way into unstructured clinical-setups with\\nrobots that interact accordingly remains a challenge in CRI.\\n3SystemArchitectureOverview\\nThe ROS system used in this work is a flexible and\\nscalable open framework for writing modular robot-\\ncentered systems. Similar to a computing operating system,\\nROS manages the interface between robot hardware and\\nsoftware modules and provides common device drivers, data\\nstructures and tool-based packages, such as visualization\\nand debugging tools. In addition, ROS uses an interface\\ndefinition language (IDL) to describe the messages sent\\nbetween process or nodes, this feature facilitates the multi-\\nlanguage (C++, Python and Lisp) development [ 39].\\nThe overall system developed here was built using a\\nnode graph architecture, taking advantages of the principal\\nROS design criteria. As with ROS, our system consists of\\na number of nodes to local video processing together a\\nrobot’s behavior estimation, distributed around a number\\nof different hosts and connected at runtime in a peer-to-\\npeer topology. The inter-node connection is implemented\\nas a hand-shaking and occurs in XML-RPC protocol\\nalong with a web-socket communication for robot’s web-\\nbased node (/ONO node, see Fig. 1). The node structure\\nis flexible, scalable and can be dynamically modified,\\ni.e., each node can be started and left running along an\\nexperimental session or resumed and connected to each\\nother at runtime. In addition, from a general perspective,\\nany robotic platform with web-socket communication can\\nbe integrated. The developed system is composed of two\\ninterconnected modules as shown in Fig. 1: an artificialreasoning module and a CRI-channel module. The module\\narchitectures are detailed in the following subsections.\\n3.1ArchitectureofReasoningModule\\nIn this module, a distributed architecture for local video\\nprocessing is implemented. The data of each RGBD sensor\\nin the multi-camera system are processed for two nodes,\\nin which the first is a driver level node and the second\\nis a processing node. The driver1node transforms the\\nstreaming data of the RGBD sensor into the ROS messages\\nformat. The driver addresses the data through a specialized\\ntransport provided by plugings to publishes images in a\\ncompressed representations while the receptor node only\\nsees sensor msgs/Image messages. The data processing\\nnode executes the face analysis algorithm. This node uses\\naimage transport subscriber and a ROS packages called\\nCvBridge to turn the data into a image format supported for\\nthe typical computer vision algorithms. Later, the same node\\npublishes the head pose and eye gaze direction by means of\\na ROS navigation message defined as nav msgs/Odometry .\\nAn additional node hosted in the most powerful\\nworkstation carries out a data fusion of all navigation\\nmessages that were generated in the local processing\\nstage. In addition to the fusion, this node computes the\\nvisual focus of attention (VFOA) and publishes this as a\\nstd msgs/Header , in which the time stamp and the target\\nname of the VFOA estimation are registered.\\n3.2ArchitectureofCRI-Channel\\nThe system proposed here has two bidirectional communi-\\ncation channels, a robot-device, and a web-based applica-\\ntion to interact with both the child and the therapist. The\\nrobot device can interact with the CwASD executing differ-\\nent physical actions, such as facial expression, upper limb\\nposes, and verbal communication. Thus, according to the\\nchild’s performance, the reasoning module can modify the\\nrobot’s behavior through automatic gaze shifting, chang-\\ning the facial expression and providing sound rewards. The\\nclient-side application was developed to allow the therapist\\nto control and register all step of the intervention proto-\\ncol. This interface was also used to supervise and control\\nthe robot’s behavior and to offer feedback to the therapist\\nabout the child’s performance along the intervention. This\\nApp has two channels of communication for interacting\\nwith the reasoning module. The first connection uses a web-\\nsocket protocol and a RosBridge suite package to support\\nthe interpretation of ROS messages, as well as, JSON-based\\ncommands in ROS. The second one uses a ROS module\\n1Tools for using the Kinect One (Kinect V2) in ROS, https://github.\\ncom/code-iai/iai kinect2 .JIntellRobotSyst(2019)96:267–281 269\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 2}),\n",
       "  Document(page_content='Fig.1 Node graph architecture\\nof the proposed ROS-based\\nsystem. The system is composed\\nof two interconnected modules,\\nan artificial reasoning module\\nand a CRI-channel module. The\\nONO web server has two way of\\nbidirectional communication: a\\nwebsocket and a standard ROS\\nSubscriber\\ndeveloped in the server-side application to directly run a\\nROS node and communicate with standard ROS publishers\\nand subscribers.\\n4TheRoboticPlatformONO\\nThe CRI is implemented through the open source platform\\nfor social robotics (OPSORO),2which is a promising\\nand straightforward system developed for face to face\\ncommunication composed of a low-cost modular robot\\ncalled ONO (see Fig. 2) and web-based applications\\n[40]. Some of the most important requirements and\\ncharacteristics that make ONO interesting for this CRI\\nstrategy are explained in the following sections.\\n4.1AppearanceandIdentity\\nThe robot is covered in foam and also fabric to have a more\\ninviting and huggable appearance to the children. The robot\\nhas an oversized head to make its facial expressions more\\nprominent and to highlight the importance for communica-\\ntion and emotional interaction. As a consequence of its size\\nand pose, children can interact with the robot at eye height\\nwhen the robot is placed on a table.\\nThe robot ONO has not a predefined identity, as the\\nonly element previously conceived is the name. Unlike other\\nrobots that have well-defined identities, such as Probo [ 9]\\nor Kaspar [ 41], in this work the ONO’s identity is built with\\nthe participation of the child through a co-creation process.\\nFor this reason, a neutral appearance is initially used. In the\\n2Open Source Platform for Social Robotics (OPSORO) http://www.\\nopsoro.com .intervention, the therapist can provide the child with clothes\\nand accessories to define the identity of ONO.\\n4.2MechanicsPlatform\\nAs the initial design of ONO is composed only of the\\nactuated face, in this work it was needed to provide the ONO\\nwith some body language. For this purpose, motorized arms\\nwere designed and implemented.\\nThe new design of ONO has a fully face and two\\narms actuated, giving a total of 17 Degrees of Freedom\\n(DOF). The ONO is able to perform facial expressions and\\nnonverbal cues, such as waving, shake hands and pointing\\ntowards objects, moving its arms (2 DOF x 2), eyes (2 DOF\\nx 2), eyelids (2 DOF x 2), eyebrows (1 DOF x 2), and mouth\\n(3 DOF). The robot has also a sound module that allows\\nexplicit positive feedback as well as reinforcement learning\\nthrough playing words, conversations and other sounds.\\n4.3SocialExpressiveness\\nIn order to improve social interaction with a child, the ONO\\nis able to exhibit different facial expressions. The ONO’s\\nexpressiveness is based on the Facial Action Coding System\\n(FACS) developed in [ 42]. Each DOF that composes the\\nONO’s face is linked with a set of Action Units (AU) defined\\nby the FACT, and each facial expression is determined for\\nspecific AU values. The facial expressions are represented as\\na 2D vector fe=(v, a) in the emotion circumplex model\\ndefined by valence and arousal [ 9]. In this context, the basic\\nfacial expressions are specified on a unit circle, where the\\nneutral expression corresponds to the origin of the space\\nfe0=(0,0). The relation between the DOF position and\\nthe AU values is resolved through a lookup table algorithm\\nusing a predefined configuration file [ 40].JIntellRobotSyst(2019)96:267–281 270\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 3}),\n",
       "  Document(page_content='Fig.2 ONO robot, developed\\nthrough the open source\\nplatform for social robotics\\n(OPSORO)\\n4.4AdaptabilityandReproducibility\\nThe application of the Do-It-Yourself (DIY) concept is\\nthe principal feature of ONO’s design, which facilitates\\nits dissemination and use in research areas other than\\nengineering as health care. These characteristics allow ONO\\nbuilding for any person without specialized engineering\\nknowledge. Additionally, it is possible to replicate ONO\\nwithout the need for high-end components or manufacturing\\nmachines [ 40]. The electronic system is based on a\\nRaspberry Pi single-board computer combined with a\\ncustom OPSORO module with circuitry to control up to\\n32 servos, drive speakers and touch sensors. Any sensor\\nor actuator compatible with the embedded communication\\nprotocols (UART, I2C, SPI) implemented on the Raspberry\\nPi can be used by this platform.\\n4.5ControlandAutonomy\\nWith the information delivered for the automated reasoning\\nmodule, it was possible to automate the ONO’s behavior\\nand, then, the robot can infer and interpret the children’s\\nintentions to react most accurately to the action performed\\nby them, thus enabling a more efficient and dynamic\\ninteraction with ONO. In this work, the automated ONO’s\\nbehavior is partially implemented, i.e., the framework can\\nmodify some physical actions of ONO using the feedback\\ninformation about the child’s behavior. The actions suitable\\nto be modified are gaze shift toward the child in specifics\\nevents, changing from neutral to positive facial expression\\nwhen the child looks toward the target, and providing\\nsound rewards. Also, an Aliveness Behavior Module (ABM)\\nis implemented to improve the CRI, which consist of\\nblinking the robot’s eyes and changing its arms amongsome predefined poses. Also, the robot can be manually\\noperated through a remote controller hosted in the client-\\nside application.\\n5ReasoningModule:MachineLearning\\nMethodsforChild’sFaceAnalysis\\nThe automated child’s face analysis consists of monitoring\\nnonverbal cues, such as head and body movements, head\\npose, eye gaze, visual contact and visual focus of attention.\\nIn this work, a pipeline algorithm is implemented using\\nmachine learning neural models for face analysis. The\\nchosen methods were developed using state-of-art trained\\nneural models, available by Dlib3[43] and OpenFace4\\n[44]. Some modification such as, turn the neural model an\\nattribute of the ROS node class and evaluate this in each\\ntopic callback, were needed to run the neural models into a\\ncommon ROS node.\\nThe algorithm proposed for child’s face analysis involves\\nface detection, recognition, segmentation and tracking,\\nlandmarks detection and tracking, head pose, eye gaze and\\nvisual focus of attention (VFOA) estimation. In addition, the\\narchitecture proposed here also implement new methods for\\nasynchronous matching and fusion of all local data, visual\\nfocus of attention estimation based on Hidden Markov\\nModel (HMM) and direct connection with the CRI-channel\\nto influence the robot’s behaviors. A scheme of the pipeline\\nalgorithm is shown in Fig. 3.\\n3Dlib C++ Library http://dlib.net/ .\\n4A Open Source Facial Behavior Analysis https://github.com/\\nTadasBaltrusaitis/OpenFace .JIntellRobotSyst(2019)96:267–281 271\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 4}),\n",
       "  Document(page_content='Fig.3 Pipeline algorithm of the\\nautomated child’s face analysis\\n5.1Child’sFaceDetectionandRecognition\\nThe in-clinic setup requires differentiate the child’s face\\nfrom other faces detected and found in the scene. For this\\nreason, a face recognition process was also implemented in\\nthis work. First, the face detection is executed to initialize\\nthe face recognition process and, subsequently, initialize\\nthe landmarks detection. In this work, both detection and\\nrecognition are implemented using deep learning models,\\nwhich are described in this section.\\nIn the detection process, a Convolutional Neural Network\\n(CNN) based face detector with a Max-Margin Object\\nDetection (MMOD) as loss layer is used [ 45]. The CNN\\nconsist first of a block composed of three downsampling\\nlayers, which apply convolution with a 5x5 filter size and\\n2×2 stride to reduce the size of the image up to eight\\ntimes its original size and generate a feature map with 16\\ndimensions. Later, the result are processed for one more\\nblock composed of four convolutional layers to get the final\\noutput of the network. The three first layers of the last\\nblock have 5 ×5 filter size and 1x1 stride, but, the last layer\\nhas only 1 channel and a 9 ×9 filter size. The values in\\nthe last channel are large when the network thinks it has\\nfound a face at a particular location. All convolutional block\\nabove are implemented with two additional layers among\\nconvolutional layers, pointwise linear transformation, and\\nRectified Linear Units (RELU) to apply the non-saturating\\nactivation function f( x)=max(0,x). The training dataset\\nused to create the model is composed of 6975 faces and is\\navailable at Dlib’s homepage.5\\nThe face recognition algorithm used in this work is\\ninspired on the deep residual model from [ 46]. The\\n5http://dlib.net/files/data/dlib face detection dataset-2016-09-30.tar.\\ngz.residual network (ResNet) model developed by He et. al\\nreformulates the convolutional layers to learn a residual\\nfunctions F(x) :=H(x) −xwith reference to the layer\\ninputs x, instead of learning unreferenced functions. In the\\npractical implementation, the previous formulation means\\ninserting shortcut connections, which turn the network into\\nits counterpart residual version [ 46]. The CNN model then\\ntransforms each face detected to a 128D vector space in\\nwhich images from the same person will be close to each\\nother, but faces from different people will be far apart.\\nFinally, the faces are classified as child’s face, caregiver’s\\nface and therapist’s face.\\nBoth detection and recognition CNN model were\\nimplemented and trained from [ 43] and released in Dlib\\n19.6.\\n5.2FaceAnalysis,Landmarks,HeadPoseandEye\\nGaze\\nThis work uses the technique for landmarks detection, head\\npose and eye gaze estimation developed by Baltru ˇsaitis et\\nal., named Conditional Local Neural Fields (CLNF) [ 47].\\nThis technique is an extension of the Constrained Local\\nModel (CLM) algorithm using specialized local detectors\\nor patch experts. CNLF model consists of a statistical\\nshape model, which its learned from data examples and is\\nparametrized for mcomponents of linear deformation to\\ncontrol the possible shape variations of the non-rigid objects\\n[48]. Approaches based on CLM [ 49,50]a n dC L N F[ 47]\\nmodel the object appearance in a local fashion, i.e, each\\nfeature point has its own appearance model to describe the\\namount of misalignment.\\nCLNF-based landmark detection consists of three main\\nparts: the shape model, the local detectors or patch experts,\\nand the fitting algorithm, which are detailed below.JIntellRobotSyst(2019)96:267–281 272\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 5}),\n",
       "  Document(page_content='5.2.1ShapeModel\\nThe CLNF technique uses a linear model to describe non-\\nrigid deformations called Point Distribution Model (PDM).\\nThe PDM is used to estimate the likelihood of the shapes\\nbeing in a specific class, given a set of feature points [ 48].\\nThis is important for model fitting and shape recognition.\\nThe shape of a face that has nlandmark points can be\\ndescribed as:\\nX=[X1,X2,...,X n,Y1,Y2,...,Y n,Z1,Z2,...,Z n],\\n(1)\\nand the class that describes a valid instance of a face using\\nPDM can be represented as:\\nX=¯X+/Phi1q, (2)\\nwhere ¯Xis the mean shape of the face, /Phi1described\\nthe principal deformation modes of the shape, and q\\nrepresent the non-rigid deformation parameters. Both ¯X\\nand/Phi1are learned automatically from labeled data using\\nPrincipal Component Analysis (PCA). The probability\\ndensity distribution of the instances into the shape class is\\nexpressed as a zero mean Gaussian with Covariance matrix\\n/Lambda1=([λ1;...;λm])evaluated at q:\\np(q)=N(q;0;/Lambda1)=1√(2π)m|/Lambda1|exp{\\n−1\\n2(qT/Lambda1−1q)}\\n(3)\\nOnce the model is defined, it is necessary to place the 3D\\nPDM in an image space. The following equation is used\\nto transform between 3D space to image space using weak\\nperspective projection [ 49]:\\nxi=s·R2D·(¯Xi+/Phi1iq)+t, (4)\\nwhere ¯Xi=[ ¯xi,¯yi,¯zi]Tis the mean value of the ith\\nlandmark. The instance of the face in an image is, therefore,\\ncontrolled using the parameter vector p=[s,w,t,q ],\\nwhere qrepresents the local non-rigid deformation, sis a\\nscaling term, wis the rotation term that controls the 2 ×3\\nmatrix R2D,a n dtis the translation term.\\nThe global parameters are used to estimate the head\\npose in reference to the camera space using orthographic\\ncamera projection and solving the Perspective-n-Point\\n(PnP) problem respect to the detected landmarks. The PDM\\nused in [ 44] was trained on two public datasets [ 51,52].\\nThis result in a model with 34 non-rigid (Principal modes)\\nand 6 rigid shape parameters.\\n5.2.2PatchExperts\\nThe patch experts scheme is the main novelty implemented\\nin the CLNF model. The new Local Neural Field (LNF)patch expert takes advantage of the non linear relationship\\nbetween pixel values and the patch response maps. The LNF\\ncaptures two kinds of spatial characteristics between pixels,\\nsuch as similarity and sparsity [ 47].\\nLNF patch expert can be interpreted as a three layer\\nperceptron with a sigmoid activation function followed by\\na weighted sum of the hidden layers. It is also similar to\\nthe first layer of a Convolutional Neural Network [ 44].\\nThe new LNF patch expert is able to learn from multiple\\nilluminations and retain accuracy. This becomes important\\nwhen creating landmark detectors and trackers that are\\nexpected to work in unseen environments and on unseen\\npeople.\\nThe learning and inference process is developed using\\na gradient-based optimization method to help in finding\\nlocally optimal model parameters faster and more accu-\\nrately.\\nIn the CLNF model implemented in [ 44], 28 set in total\\nof LNF patch experts were trained for seven views and\\nfour scales. The framework uses patch experts specifically\\ntrained to recognize the eyelids, iris and the pupil, in order\\nto estimate the eye gaze [ 44].\\n5.2.3FittingAlgorithm\\nFor each new image or video frame, the fitting algorithm\\nof CLNF-based landmark detection process attempts to\\nfind the value of the local and global deformable model\\nparameters pthat minimizes the following function [ 49]:\\nE(p)=R(p)n∑\\ni=1Di(xi;I), (5)\\nwhereRis a weight to penalize unlikely shapes, which\\ndepends on the shape model, and Drepresents the\\nmisalignment of the ithlandmark in the image I,w h i c h\\nis function of both the parameters pand the patch experts.\\nUnder the probabilistic point of view, the solution of ( 5)i s\\nequivalent to maximize the a posteriori probability (MAP)\\nof the deformable model parameters p:\\np(\\np|{li=1}n\\ni=1,I)\\n∝p((p))n∏\\ni=1p(li=1|xi,I),(6)\\nwhere, li∈{1,−1}is a discrete random variable indicating\\nwhether the ithlandmark is aligned or misaligned, p(p)\\nis the prior probability of the deformable parameters p,\\nandp(li=1|xi,I)is the probability of a landmark being\\naligned at a particular pixel location xi, which is quantified\\nfrom the response maps created by patch. Therefore, the\\nlast term in ( 6) represents the joint probability of the patch\\nexpert response maps.\\nThe MAP problem is solved using a optimization strategy\\ndesigned specifically for CLNF fitting called non-uniformJIntellRobotSyst(2019)96:267–281 273\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 6}),\n",
       "  Document(page_content='regularized landmark mean shift (NU-RLMS) [ 47], which\\nuses two step process. The first step evaluates each of\\nthe patch experts around the current landmark using a\\nGaussian Kernel Density Estimator (KDE). The second step\\niteratively updates the model parameters to maximize ( 6).\\nThe NU-RLMS uses expectation maximization algo-\\nrithm, where the E-step involves evaluating the posterior\\nprobability over the candidates, and the M-step finds the\\nparameter updated through the mean shift vector v.T h e\\nmean shift vector points in the direction where the feature\\npoint should go, but the motion is restricted by the statisti-\\ncal shape model and the R(p). This interpretation leads to\\nthe new update function:\\nargmin\\n/Delta1p{\\n∥J/Delta1 p−v∥2\\nW+r∥p+/Delta1p∥2\\n˜/Lambda1−1}\\n, (7)\\nwhere ris a regularization term, Jis the Jacobian, which\\ndescribe how the landmarks location are changing based\\non the infinitesimal changes of the parameters p,˜/Lambda1−1=\\ndiag([0;0;0;0;0;0;λ−1\\n1;...;λ−1\\nm]),a n d Wallows for\\nweighting of mean-shift vectors. Non-linear least squares\\nleads to the following update rule:\\n/Delta1p=−(\\nJTWJ+r/Lambda1−1)(\\nr/Lambda1−1p−JTWv)\\n.( 8 )\\nTo construct W, the performance of patch experts on training\\ndata is used.\\n5.3DataFusion\\nThe fusion of the local results for the head pose estimation\\nis done applying a consensus over the rotation algorithm\\n[53]. This algorithm consists of calculating the weighted\\naverage pose between each camera estimation and its\\nimmediate sensors’ estimation neighbors using the axis-\\nangle representation. The local pose is penalized by two\\nweights: the alignment confidence of landmarks detection\\nprocedure and the Mahalanobis distances between the head\\npose and a neutral pose.\\n5.4FieldofView(FoV)andVisualfocusofAttention\\n(VFOA)\\nThe VFOA estimation model is implemented as a dynamic\\nBayesian network through a Hidden Markov Model\\n(HMM). The model assumes a specific set of child’s\\nattention attractors or targets F. The estimation process\\ndecodes the sequence of child’s head poses Ht=\\n(Hyaw\\nt,Hpitch\\nt)∈R2in terms of VFOA states Ft∈F\\nat time t[54]. The probability distribution of the head\\nposes in reference to a given VFOA target is represented\\nby a Gaussian distribution, whereas the transitions amongthese targets are represented by the transition matrix A.T h e\\nHMM equations can then be written as follows:\\nP(H t|Ft=f,μh\\nt)=N(Ht|μh\\nt(f ), /Sigma1 H(f )) (9)\\np(Ft=f|Ft−1=ˆf)=Afˆf. (10)\\nThe Gaussian covariances is defined manually to reflect\\ntarget sizes and head pose estimation variability. Moreover,\\nthe Gaussian means corresponding to each specific target\\nμh\\ntis calculated through a gaze model that sets this\\nparameter as a fixed linear combination of the target\\ndirection and the head reference direction [ 55]:\\nμh\\nt(f )=α⋆μ t(f )+(12−α) ⋆ R t, (11)\\nwhere ⋆denotes the component wise product 1 2=(1,1),\\nα=(αyaw,αpitch)=(0.7,0.5)are adjustable constants\\nthat describe the fraction of the gaze shift that corresponds\\nto the child’s head rotation, μt∈(R2)Kis the directions\\nof the given K targets, and Rt∈R2represents the\\nreference direction, which is the average head pose over\\na time window WR. The above assumption describes the\\nbody orientation behavior of any child who tends to orient\\nhimself/herself towards the set of gaze targets to make more\\ncomfortable to rotate his/her head towards different targets\\n[55].\\nRt=1\\nWRt∑\\ni=t−WRHi. (12)\\nFinally, for the estimation of the VFOA sequence a classic\\nViterbi algorithm of HMM is implemented [ 54].\\n6CaseStudy\\nFor the case study, the vision system is composed of\\nthree Kinect V2 sensors. Each sensor is connected to a\\nworkstation equipped with a processor of Intel Core i5\\nfamily and a GeForce GTX GPU board (two workstation\\nwith GTX960 board, and one workstation with GTX580\\nboard). All workstation are connected through a local area\\nnetwork synchronized using the NTP protocol.6The sensors\\nwere intrinsically and extrinsically calibrated through a\\nconventional calibration process using a standard black-\\nwhite chessboard.7\\n6.1In-clinicSetup\\nA multidisciplinary team of psychologists, doctors and\\nengineers developed a case study using a psychology room\\nequipped with a unidirectional mirror to perform behavioral\\n6Network Time Protocol Homepage, http://www.ntp.org .\\n7Tools for using the Kinect One (Kinect V2) in ROS, https://github.\\ncom/code-iai/iai kinect2 .JIntellRobotSyst(2019)96:267–281 274\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 7}),\n",
       "  Document(page_content='Fig.4 Representation of the\\ninterventions room of in-clinic\\nsetup\\nobservation appropriately. The room was prepared with a\\ntable and three chairs: one for the child, another for the\\ncaregiver and a third one for the therapist. The robot was\\nplaced on the table, and the following toys, a helicopter,\\na truck and a train, were attached to room’s walls. The\\nRGBD sensors were located close to the walls, and no\\nadditional camera was placed on the robot or the table, so\\nas not to attract the child’s attention. A representation of the\\ninterventions room of in-clinic setup is shown in Fig. 4.\\n6.2InterventionProtocol\\nIn this work, a technology-based system was used as a\\ntool in various stages of the ASD diagnostic process.\\nThe framework can be implemented to extract differentbehavioral features to be assessed, e.g., eye contact,\\nstereotyped movements of the head, concentration and\\nexcessive interest in objects or events. However, for the\\nscope of this research, a specific clinical setup intervention\\nto assess Joint Attention (JA) behaviors is presented. The\\nintervention aims to evaluate the capacity of JA; which can\\nbe divided into three classes: initiation of joint attention\\n(IJA), responding to joint attention bids (RJA), and initiation\\nof request behavior (IRB) [ 6]. The therapist guides the\\nintervention all the time and leverages the robot device as\\nan alternative channel of communication with the child,\\nfor the above, both the specialist and the robot remained\\nin the room during the intervention. The children were\\naccompanied throughout the session by a caregiver who\\nwas oriented not to help the child in the execution of the\\nFig.5 The child’s nonverbal\\ncues elicited by the CRI, to look\\ntowards the therapist, towards\\nthe robot, point and self\\nocclusion\\nJIntellRobotSyst(2019)96:267–281 275\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 8}),\n",
       "  Document(page_content='Fig.6 Performance of the\\nchild’s face analysis pipeline for\\nthe case study. Face detection\\nand recognition, landmarks\\ndetection, head pose and eye\\ngaze estimation were executed\\ntasks. The exercise developed aimed to direct the attention\\nof the child towards objects located in the room through\\nstimuli, such as, look at, point and speak. The stimuli were\\ngenerated first only by the therapist and later just by the\\nrobot.\\n6.3Subjects\\nThree children without confirmed ASD diagnosis, but with\\nevidence of risk factors, and three typically developing\\n(TD) children as the control group participated in the\\nexperiments. All volunteers participated with their parent’s\\nconsent, which were five boys (3 ASD, 2 TD) and\\none TD girl, between 36 months to 48 months. Each\\nvolunteer participated in one single session. The goal was to\\nanalyze the based-line of the child’s behavior and establish\\ndifferences in the behavioral reaction between TD and ASDchildren for stimuli generated through CRI and leverage the\\nnovelty effect raised by the robot mediator.\\n7ResultsandDiscussion\\nThe child’s nonverbal cues elicited by the CRI can be\\no b s e r v e di nF i g . 5. Some examples of children’s behavior\\ntagged to perform the behavioral coding are shown in the\\nsix pictures. The tagged behaviors were: to look towards\\nan object, towards the robot, and towards the therapist, to\\npoint and, to respond to a prompt of both mediators and self\\nocclusion. Typical occlusion problem, as occlusion by hair,\\nhands and the robot were detected.\\nThe performance of video processing in the proof of\\nconcept session is reported in Fig. 6. In the case study\\nsessions, the child’s face detection and recognition, the\\nFig.7 Evolution over time of\\nthe child’s head/neck rotation\\n(yaw rotation) for a TD group\\nJIntellRobotSyst(2019)96:267–281 276\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 9}),\n",
       "  Document(page_content='Fig.8 Evolution over time of the child’s head/neck rotation (yaw rotation) for a TD volunteer and VFOA estimation results\\nlandmarks detection, head pose and eye gaze estimation for\\ndifferent viewpoints are shown in Fig. 3. The recognition\\nprocess was able to detect all faces in the session\\nsuccessfully in most cases.\\nThe child’s head pose was captured throughout the\\nsession and analyzed automatically to estimate the evolution\\nover time of child’s head and the VFOA. Along the\\nsession, the child’s neck right/left rotation movement was\\npredominant (Yaw axis), while the neck flexion/extension\\n(Pitch axis) and neck R/L lateral flexion movements (Roll\\naxis) remained approximately constant. The Yaw rotation of\\nthe TD children group is reported in Fig. 7. The vertical light\\nblue stripe indicates the intervention period with therapist-\\nmediator, and the vertical light green stripe represents\\nthe period with robot-mediator. The continuous blue line\\nrepresents the raw data recorded, and the continuous red line\\ndescribes the average data trend. From the observation of the\\nthree plot, the TD children started the intervention looking\\ntowards the robot, evidently, the robot was a naturalistic\\nattention attractor. Subsequently, when the therapist begins\\nthe protocol explaining the tasks, the children attention\\nshifts towards the therapist. The children remained this\\nbehavior until that the therapist introduced the robot-\\nmediator. In this transition, the children’s behaviors, such as,RJA and IJA toward the therapist were observed. Once the\\ntherapist changed the mediation with the robot, the children\\nturned his/her attention to the robot and the objects in the\\nroom.\\nA more detailed analysis of one of the TD volunteers is\\nshown in Fig. 8. The plot (A) shows the overall intervention\\nsession; the plot (B) and plot (C) are a zoom of the period\\nwith therapist and robot mediator, respectively. The colors\\nconvention in the three plots of Fig. 8describes the results\\ngenerated by the automated estimation of VFOA. From\\nthese scenarios, some essential aspects already emerge. In\\nthe therapist-mediator interval, the child responded to JA\\ntask using only one repetition for all prompt level. The\\nchild’s behavior of RJA was according to the protocol,\\ni.e., the child looked towards the therapist to wait for\\ninstructions, rapidly the child searched in the target, and\\nnext looked again toward the therapist (Color sequence:\\nlight blue - yellow - light blue - orange - light blue - red).\\nThis behavior was the same for all prompts. In contrast,\\nwith the robot-mediator, the child did not look toward\\nthe robot among indications at consecutive targets (Color\\nsequence: light green - yellow - orange - red - orange -\\nyellow). The above happened because, in the protocol, both\\nmediators executed the instructions in the same order, andJIntellRobotSyst(2019)96:267–281 277\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 10}),\n",
       "  Document(page_content='Fig.9 Evolution over time of the child’s head/neck rotation (yaw rotation) for a ASD group\\nthe child memorized the commands and the object’s position\\nuntil the robot mediator interval. This fact did not affect\\nthe intervention’s aim, as the robot mediator succeeded to\\nelicit the child’s behaviors of RJA and IJA. In addition,\\nas highlighted in the plot (A) in Fig. 8, when the session\\nfinalized and the robot mediator said goodbye, again, RJA\\nand IJA behaviors were perceived. The pictures (a-d) show\\nthese events: first the child said goodbye towards the robot,\\nthen, he looked the therapist to confirm that the session\\nended and looked again towards the robot, finally the child\\ntook the robot’s hand.\\nFrom the analysis of the three TD volunteers, the same\\nreported behaviors were perceived. However, the analysis\\nof the children in the ASD group showed different behavior\\npatterns concerning comfort, visual contact and novelty\\nstimulus effect during the sessions. The evolution over time\\nof the child’s head/neck rotation (yaw rotation) for an ASD\\ngroup is shown in Fig. 9. On the one hand, the three children\\nin the ASD group maintained more visual contact with\\nthe robot compared to the therapist and exhibited more\\ninterest in the robot platform compared to the TD children.\\nHowever, the performance of the children in the activities\\nof JA did not improve significantly when the robot executed\\nthe prompt. On the other hand, the clinicians manifested that\\nin all cases the first visual contact toward them occurred\\nin the instant that the robot entered the scene and startedinteracting, i.e., the ONO mediation elicited behaviors of\\nIJA towards the therapist. In addition, the CwASD exhibited\\nless discomfort regarding the session, from the first moment\\nwhen the robot initiated mediation in the room and, in some\\ncases, when showed appearance of verbal and non-verbal\\npro-social behaviors. These facts did not arise with the TD\\nchildren, because the first visual contact with the therapist\\noccurred when they entered the room. Additionally, TD\\nchildren showed the ability to divide the attention between\\nthe robot and the therapist from the beginning to the end\\nof the intervention, exhibiting comfort in every moment.\\nThe behavior modulation of CwASD is observed in Fig. 9.\\nBefore the period with robot-mediator the children exhibited\\ndiscomfort (unstable movements of their head), and after\\nof this period, the head movement tended to be more\\nstable.\\nThe novelty of a robot-mediator at diagnostic session\\ncan be analyzed as an additional stimulus of the CRI.\\nAccordingly, in this case study the children of the ASD\\ngroup showed more behavior modification (attention and\\ncomfort) produced by the robot interaction at the beginning\\nof the CRI, remaining until the end of the session. On the\\nother hand, the children of the TD group responded to the\\nnovelty effect of the robot mediator from the time the child\\nentered the room and saw the robot, until the beginning of\\nthe therapist presentation. For the above, despite the noveltyJIntellRobotSyst(2019)96:267–281 278\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 11}),\n",
       "  Document(page_content='of the stimuli effect, these did not seem to affect the social\\ninteraction between the TD children and the therapist, and\\nin contrast, these stimuli seemed to enhance the CwASD\\nsocial interaction with the therapist along the intervention.\\nThese results are impressive, since they show the\\npotential of CRI intervention to systematically elicit\\ndifferences between the pattern of behavior on TD and ASD\\nchildren. We identified RJA and IJA toward the therapist at\\nthe beginning of the intervention, at the transition between\\ntherapist to robot mediator, and at the end for all TD\\nchildren. In contrast, we only identified IJA towards the\\ntherapist in the transition between mediators, for ASD\\nchildren. This fact shows a clear difference of behavior\\npattern between CwASD and TD children, which can be\\nanalyzed using a JA task protocol. In fact, these pattern\\ndifferences can be used as evidence to improve the ASD\\ndiagnosis.\\n8Conclusions\\nThis work presented a Robot-Assisted tool to assist and\\nenhance the traditional practice of ASD diagnosis. The\\ndesigned framework combines a vision system with the\\nautomated analysis of nonverbal cues in addition to a\\nrobotic platform; both developed upon open source projects.\\nThis research contributes to the state-of-the-art with an\\ninnovative flexible and scalable architecture capable to\\nautomatically register events of joint attention and patterns\\nof visual contact before and after of a robot-based mediation\\nas well as the pattern of behavior related to comfort or\\ndiscomfort along the ASD intervention.\\nIn addition, an artificial vision pipeline based on a multi-\\ncamera approach was proposed. The vision system performs\\nface detection, recognition and tracking, landmark detection\\nand tracking, head pose, gaze and estimation of visual focus\\nof attention was proposed, with its performance considered\\nsuitable for use into conventional ASD intervention. At\\nleast one camera captured the child’s face in each sample\\nframe. Furthermore, the feedback information about the\\nchild’s performance was successfully used to modulate the\\nsupervised behavior of ONO, improving the performance of\\nthe CRI and the visual attention of the children. Regarding\\nthe VFOA estimation, the algorithm was able to estimate\\nthe target into the FoV in different situations recurrently.\\nAlso, the robot was able to react according to the estimation.\\nHowever, the algorithm only failed when occlusion by the\\nchild’s hands is generated. On the other hand, the occlusion\\nby the therapist and the robot was compensated using the\\nmulti-camera approach. The child’s face recognition system\\nshowed to be imperative to analyze the child’s behavior in\\nthe clinical setup implemented in this work, which required\\nthe caregiver’s attention in the room.Despite the limited number of children of this study,\\npreliminary results of this case study showed the feasibility\\nof identifying and quantify differences in the patterns of\\nbehavior of TD children and CwASD elicited by the CRI\\nintervention. Through the proof of concept, it is evidenced\\nhere the system ability to improve the traditional tools used\\nin ASD diagnosis. As future works, it is recommended a\\nstudy to replicate the protocol proposed in this paper with\\nten CwASD and ten TD children. Another suggestion is to\\nquantify other kinds of behaviors in addition to that assessed\\nin this paper, such as verbal utterance patterns, physical\\nand emotional engagement, object or event preferences and\\ngather more evidence to improve the assistance to therapists\\nin ASD diagnosis processes.\\nAcknowledgements This work was supported by the Google Latin\\nAmerica Research Awards (LARA) program. The first author scholar-\\nship was supported in part by the Coordenac ¸˜ao de Aperfeic ¸oamento de\\nPessoal de N ´ıvel Superior - Brasil (CAPES) - Finance Code 001.\\nDisclosurestatement No potential conflict of interest was reported by\\nthe authors.\\nReferences\\n1. Belpaeme, T., Baxter, P.E., de Greeff, J., Kennedy, J., Read, R.,\\nLooije, R., Neerincx, M., Baroni, I., Zelati, M.C.: Child-Robot\\ninteraction: perspectives and challenges. In: 5th International\\nConference, ICSR 2013, pp. 452–459. Springer International\\nPublishing, Bristol (2013)\\n2. Diehl, J.J., Schmitt, L.M., Villano, M., Crowell, C.R.: The clinical\\nuse of robots for individuals with autism spectrum disorders: A\\ncritical review. Res. Autism Spectr. Disord. 6(1), 249–262 (2012)\\n3. Scassellati, B., Admoni, H., Maja, M.: Robots for use in autism\\nresearch. Annu. Rev. Biomed. Eng. 14(1), 275–294 (2012)\\n4. Pennisi, P., Tonacci, A., Tartarisco, G., Billeci, L., Ruta, L.,\\nGangemi, S., Pioggia, G.: Autism and social robotics: A\\nsystematic review (2016)\\n5. American Psychiatric Association: DSM-5 diagnostic classifica-\\ntion. In: Diagnostic and Statistical Manual of Mental Disorders.\\nAmerican Psychiatric Association, 5 (2013)\\n6. Eggebrecht, A.T., Elison, J.T., Feczko, E., Todorov, A., Wolff, J.J.,\\nKandala, S., Adams, C.M., Snyder, A.Z., Lewis, J.D., Estes, A.M.,\\nZwaigenbaum, L., Botteron, K.N., McKinstry, R.C., Constantino,\\nJ.N., Evans, A., Hazlett, H.C., Dager, S., Paterson, S.J., Schultz,\\nR.T., Styner, M.A., Gerig, G., Das, S., Kostopoulos, P., Schlaggar,\\nB.L., Petersen, S.E., Piven, J., Pruett, J.R.: Joint attention and brain\\nfunctional connectivity in infants and toddlers. Cerebral Cortex\\n27(3), 1709–1720 (2017)\\n7. Steiner, A.M., Goldsmith, T.R., Snow, A.V ., Chawarska, K.:\\nDisorders in infants and toddlers. J. Autism Dev. Disord. 42(6),\\n1183–1196 (2012)\\n8. Belpaeme, T., Baxter, P.E., Read, R., Wood, R., Cuay ´ahuitl, H.,\\nKiefer, B., Racioppa, S., Kruijff-Korbayov ´a, I., Athanasopoulos,\\nG., Enescu, V ., Looije, R., Neerincx, M., Demiris, Y ., Ros-\\nEspinoza, R., Beck, A., Canamero, L., Hiolle, A., Lewis, M.,\\nBaroni, I., Nalin, M., Cosi, P., Paci, G., Tesser, F., Sommavilla, G.,\\nHumbert, R.: Multimodal child-robot interaction: building social\\nbonds. Journal of Human-Robot Interaction 1(2), 33–53 (2012)JIntellRobotSyst(2019)96:267–281 279\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 12}),\n",
       "  Document(page_content='9. Vanderborght, B., Simut, R., Saldien, J., Pop, C., Rusu, A.S.,\\nPintea, S., Lefeber, D., David, D.O.: Using the social robot probo\\nas a social story telling agent for children with ASD. Interact. Stud.\\n13(3), 348–372 (2012)\\n10. Warren, Z.E., Zheng, Z., Swanson, A.R., Bekele, E., Zhang,\\nL., Crittendon, J.A., Weitlauf, A.F., Sarkar, N.: Can robotic\\ninteraction improve joint attention skills? J. Autism Dev. Disord.\\n45(11), 3726–3734 (2015)\\n11. Wood, L.J., Dautenhahn, K., Lehmann, H., Robins, B., Rainer,\\nA., Syrdal, D.S.: Robot-mediated interviews: Do robots pos-\\nsess advantages over human interviewers when talking to chil-\\ndren with special needs? Lecture Notes in Computer Sci-\\nence (including subseries Lecture Notes in Artificial Intelli-\\ngence and Lecture Notes in Bioinformatics) 8239 LNAI , 54–63\\n(2013)\\n12. Feil-Seifer, D., Mataric, M.J.: b3IA A control architecture for\\nautonomous robot-assisted behavior intervention for children\\nwith Autism Spectrum Disorders. In: ROMAN 2008 The 17th\\nIEEE International Symposium on Robot and Human Interactive\\nCommunication, pp. 328–333 (2008)\\n13. Leo, M., Del Coco, M., Carcagn ´ı, P., Distante, C., Bernava,\\nM., Pioggia, G., Palestra, G.: Automatic emotion recognition in\\nRobot-Children interaction for ASD treatment. In: Proceedings\\nof the IEEE International Conference on Computer Vision, 2015-\\nFebru(c), pp. 537–545 (2015)\\n14. Esteban, P.G., Baxter, P.E., Belpaeme, T., Billing, E., Cai, H.,\\nCao, H.-L., Coeckelbergh, M., Costescu, C., David, D., De Beir,\\nA., Fang, Y ., Ju, Z., Kennedy, J., Liu, H., Mazel, A., Pandey,\\nA., Richardson, K., Senft, E., Thill, S., Van De Perre, G.,\\nVanderborght, B., Vernon, D., Hui, Y ., Ziemke, T.: How to build\\na supervised autonomous system for Robot-Enhanced therapy\\nfor children with autism spectrum disorder. Paladyn Journal of\\nBehavioral Robotics 8(1), 18–38 (2017)\\n15. Pour, A.G., Taheri, A., Alemi, M., Ali, M.: Human–Robot\\nfacial expression reciprocal interaction platform: case studies\\non children with autism. Int. J. Soc. Robot. 10(2), 179–198\\n(2018)\\n16. Feng, Y ., Jia, Q., Wei, W.: A control architecture of Robot-\\nAssisted intervention for children with autism spectrum disorders.\\nJ. Robot. 2018 , 12 (2018)\\n17. Bekele, E., Crittendon, J.A., Swanson, A., Sarkar, N., Warren,\\nZ.E.: Pilot clinical application of an adaptive robotic system for\\nyoung children with autism. Autism: The International Journal of\\nResearch and Practice 18(5), 598–608 (2014)\\n18. Huijnen, C.A.G.J., Lexis, M.A.S., Jansens, R., de Witte, L.P.:\\nMapping robots to therapy and educational objectives for children\\nwith autism spectrum disorder. J. Autism Dev. Disord. 46(6),\\n2100–2114 (2016)\\n19. Aresti-Bartolome, N., Begonya, G.-Z.: Technologies as support\\ntools for persons with autistic spectrum disorder: s systematic\\nreview. Int. J. Environ. Res. Public Health 11(8), 7767–7802\\n(2014)\\n20. Boucenna, S., Narzisi, A., Tilmont, E., Muratori, F., Pioggia, G.,\\nCohen, D., Mohamed, C.: Interactive technologies for autistic\\nchildren: a review. Cogn. Comput. 6(4), 722–740 (2014)\\n21. Grynszpan, O., Patrice, L., Weiss, T., Perez-Diaz, F., Gal, E.:\\nInnovative technology-based interventions for autism spectrum\\ndisorders: a meta-analysis. Autism 18(4), 346–361 (2014)\\n22. Rehg, J.M., Rozga, A., Abowd, G.D., Goodwin, M.S.: Behavioral\\nimaging and autism. IEEE Pervasive Comput. 13(2), 84–87, 4\\n(2014)\\n23. Cabibihan, J.J., Javed, H., Ang, M., Aljunied, S.M.: Why robots?\\na survey on the roles and benefits of social robots in the\\ntherapy of children with autism. Int. J. Soc. Robot. 5(4), 593–618\\n(2013)24. Sartorato, F., Przybylowski, L., Sarko, D.K.: Improving therapeu-\\ntic outcomes in autism spectrum disorders: enhancing social com-\\nmunication and sensory processing through the use of interactive\\nrobots. J. Psychiatr. Res. 90, 1–11 (2017)\\n25. Chong, E., Chanda, K., Ye, Z., Southerland, A., Ruiz, N., Jones,\\nR.M., Rozga, A., Rehg, J.M.: Detecting gaze towards eyes in\\nnatural social interactions and its use in child assessment. Proc.\\nACM Interact. Mob. Wearable Ubiquitous Technol. 1(3), 43:1–\\n43:20 (2017)\\n26. Ness, S.L., Manyakov, N.V ., Bangerter, A., Lewin, D., Jagannatha,\\nS., Boice, M., Skalkin, A., Dawson, G., Janvier, Y .M., Goodwin,\\nM.S., Hendren, R., Leventhal, B., Shic, F., Cioccia, W., Gahan,\\nP.: JAKE® Multimodal data capture system: Insights from an\\nobservational study of autism spectrum disorder. Frontiers in\\nNeuroscience 11(SEP) (2017)\\n27. Rehg, J.M., Abowd, G.D., Rozga, A., Romero, M., Clements,\\nM.A., Sclaroff, S., Essa, I., Ousley, O.Y ., Li, Y ., Kim, C., Rao,\\nH., Kim, J.C., Lo Presti, L., Zhang, J., Lantsman, D., Bidwell,\\nJ., Ye, Z.: Decoding children’s social behavior. In: 2013 IEEE\\nConference on Computer Vision and Pattern Recognition, pp.\\n3414–3421 (2013)\\n28. Adamo, F., Palestra, G., Crifaci, G., Pennisi, P., Pioggia, G.,\\nRuta, L., Leo, M., Distante, C., Cazzato, D.: Non-intrusive\\nand calibration free visual exploration analysis in children\\nwith autism spectrum disorder. In: Computational Vision and\\nMedical Image Processing V - Proceedings of 5th Eccomas\\nThematic Conference on Computational Vision and Medical\\nImage Processing, VipIMAGE 2015, pp .201–208 (2016)\\n29. Michaud, F., Salter, T., Duquette, A., Mercier, H., Lauria, M.,\\nLarouche, H., Larose, F.: Assistive technologies and Child-Robot\\ninteraction. American Association for Artificial Intelligence ii(3),\\n8–9 (2007)\\n30. Duquette, A., Michaud, F., Mercier, H.: Exploring the use of\\na mobile robot as an imitation agent with children with low-\\nfunctioning autism. Auton. Robot. 24(2), 147–157 (2008)\\n31. Simut, R.E., Vanderfaeillie, J., Peca, A., Van de Perre, G., Bram,\\nV .: Children with autism spectrum disorders make a fruit salad\\nwith probo, the social robot: an interaction study. J. Autism Dev.\\nDisord. 46(1), 113–126 (2016)\\n32. Bekele, E., Lahiri, U., Swanson, A.R., Crittendon, J.A., Warren,\\nZ.E., Nilanjan, S.: A step towards developing adaptive robot-\\nmediated intervention architecture (ARIA) for children with\\nautism. IEEE Trans. Neural Syst. Rehabil. Eng. 21(2), 289–299\\n(2013)\\n33. Zheng, Z., Zhang, L., Bekele, E., Swanson, A., Crittendon, J.A.,\\nWarren, Z.E., Sarkar, N.: Impact of robot-mediated interaction\\nsystem on joint attention skills for children with autism. In: IEEE\\nInternational Conference on Rehabilitation Robotics (2013)\\n34. Anzalone, S.M., Tilmont, E., Boucenna, S., Xavier, J., Jouen,\\nA.L., Bodeau, N., Maharatna, K., Chetouani, M., Cohen, D.: How\\nchildren with autism spectrum disorder behave and explore the\\n4-dimensional (spatial 3D + time) environment during a joint\\nattention induction task with a robot. Res. Autism Spectr. Disord.\\n8(7), 814–826 (2014)\\n35. Chevalier, P., Martin, J.C., Isableu, B., Bazile, C., Iacob,\\nD.O., Adriana, T.: Joint attention using human-robot interaction:\\nimpact of sensory preferences of children with autism. In: 25th\\nIEEE International Symposium on Robot and Human Interactive\\nCommunication, RO-MAN 2016, pp. 849–854 (2016)\\n36. Lemaignan, S., Garcia, F., Jacq, A., Dillenbourg, P.: From real-\\ntime attention assessment to “with-me-ness” in human-robot\\ninteraction. In: ACM/IEEE International Conference on Human-\\nRobot Interaction, 2016-April, pp. 157–164 (2016)\\n37. Del Coco, M., Leo, M., Carcagni, P., Fama, F., Spadaro, L.,\\nRuta, L., Pioggia, G., Distante, C.: Study of mechanisms ofJIntellRobotSyst(2019)96:267–281 280\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 13}),\n",
       "  Document(page_content='social interaction stimulation in autism spectrum disorder by\\nassisted humanoid robot. IEEE Transactions on Cognitive and\\nDevelopmental Systems 8920 (c), 1–1 (2017)\\n38. Palestra, G., Varni, G., Chetouani, M., Esposito, F.: A multimodal\\nand multilevel system for robotics treatment of autism in children.\\nIn: Proceedings of the International Workshop on Social Learning\\nand Multimodal Interaction for Designing Artificial Agents - DAA\\n’16, pp. 1–6. ACM Press, New York (2016)\\n39. Quigley, M., Gerkey, B., Conley, K., Faust, J., Foote, T., Leibs,\\nJ., Berger, E., Wheeler, R., Ng, A.: ROS : an open-source robot\\noperating system. In: ICRA workshop on open source software,\\nnumber 3.2, pp. 5 (2009)\\n40. Vandevelde, C., Saldien, J., Ciocci, C., Vanderborght, B.: The\\nuse of social robot ono in robot assisted therapy. In: International\\nConference on Social Robotics, Proceedings, m (2013)\\n41. Dautenhahn, K.: A paradigm shift in artificial intelligence: why\\nsocial intelligence matters in the design and development of robots\\nwith human-like intelligence. 50 Years of Artificial Intelligence,\\npp. 288–302 (2007)\\n42. Ekman, P., Friesen, W.: Facial Action Coding System. Consulting\\nPsychologists Press (1978)\\n43. King, D.E.: Dlib-ml: a machine learning toolkit. J. Mach. Learn.\\nRes. 10, 1755–1758 (2009)\\n44. Baltru ˇsaitis, T., Robinson, P., Morency, L.-P.: OpenFace: an open\\nsource facial behavior analysis toolkit. IEEE Winter Conference\\non Applications of Computer Vision (2016)\\n45. King, D.E.: Max-Margin Object Detection. 1 (2015)\\n46. He, K., Zhang, X., Ren, S., Jian, S.: Deep residual learning for\\nimage recognition. In: 2016 IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), pp. 770–778. IEEE, 6\\n47. Baltru ˇsaitis, T., Robinson, P., Morency, L.P.: Constrained local\\nneural fields for robust facial landmark detection in the wild. In:\\nProceedings of the IEEE International Conference on Computer\\nVision, pp. 354–361 (2013)\\n48. Cristinacce, D., Cootes, T.F.: Feature detection and tracking with\\nconstrained local models. In: Proceedings of the British Machine\\nVision Conference 2006, pp. 1–95 (2006)\\n49. Saragih, J.M., Lucey, S., Cohn, J.F.: Deformable model fitting\\nby regularized landmark mean-shift. Int. J. Comput. Vis. 91(2),\\n200–215 (2011)\\n50. Baltru ˇsaitis, T., Robinson, P., Morency, L.P.: 3D constrained local\\nmodel for rigid and non-rigid facial tracking. In: Proceedings of\\nthe IEEE Computer Society Conference on Computer Vision and\\nPattern Recognition, pp. 2610–2617 (2012)\\n51. Belhumeur, P.N., Jacobs, D.W., Kriegman, D.J., Neeraj, K.:\\nLocalizing parts of faces using a consensus of exemplars. IEEE\\nTrans. Pattern Anal. Mach. Intell. 35(12), 2930–2940 (2013)\\n52. Le, V ., Brandt, J., Lin, Z., Bourdev, L., Huang, T.S.: Interactive\\nFacial Feature Localization, pp. 679–692. Springer, Berlin (2012)\\n53. Jorstad, A., Dementhon, D., Jeng Wang, I., Burlina, P.: Distributed\\nconsensus on camera pose. IEEE Trans. Image Process. 19(9),\\n2396–2407 (2010)\\n54. Ba, S.O., Odobez, J.-M.: Multi-Person visual focus of attention\\nfrom head pose and meeting contextual cues. IEEE Trans. Pattern\\nAnal. Mach. Intell. 33(August), 1–16 (2008)55. Sheikhi, S., Jean-Marc, O.: Combining dynamic head pose-\\ngaze mapping with the robot conversational state for attention\\nrecognition in human-robot interactions. Pattern Recogn. Lett. 66,\\n81–90 (2015)\\nPublisher’s Note Springer Nature remains neutral with regard to\\njurisdictional claims in published maps and institutional affiliations.\\nAndr´esA.Ram´ ırez-Duque received his bachelor’s degree in Mecha-\\ntronics Engineering from the Universidad Nacional de Colombia,\\nBogot ´a, Colombia, in 2009, and his Industrial Automation Master\\ndegree from the Universidad Nacional de Colombia, Bogot ´a, Colom-\\nbia, in 2011. He is currently working toward a Ph.D. degree in the\\nAssistive Technology Center, Federal University of Esp ´ırito Santo,\\nVit´oria, Brazil. He won a Google Latin America Research Award 2017.\\nHis current research interests include Child-Robot interaction, cloud\\nparallel computing, high performance computing, smart environments\\nand serious games applied to Children with development impairments.\\nAnselmo Frizera-Neto received his bachelor’s degree in Electrical\\nEngineering (2006) from the Federal University of Esp ´ırito Santo\\n(UFES) in Brazil and his doctorate in Electronics (2010) at the\\nUniversity of Alcal ´a, Spain. From 2006 to 2010 he was a researcher of\\nthe Bioengineering Group of the Consejo Superior de Investigaciones\\nCient ´ıficas (Spain) where he carried out research related to his\\ndoctoral thesis. He is currently a permanent professor and adjunct\\ncoordinator of the Graduate Program in Electrical Engineering at\\nUFES. He has authored or co-authored more than 250 papers in\\nscientific journals, books and conferences in the fields of electrical\\nand biomedical engineering. He has conducted or co-directed master’s\\nand doctoral theses in research institutions from Brazil, Argentina,\\nItaly and Portugal. His research is aimed at rehabilitation robotics, the\\ndevelopment of advanced strategies of human-robot interaction and the\\nconception of sensors and measurement technologies with applications\\nin different fields of electrical and biomedical engineering. Along with\\nAndr ´es Ram ´ırez-Duque, he won a Google Latin America Research\\nAward 2017.\\nTeodiano Freire Bastos received his B.Sc. degree in Electrical\\nEngineering from Universidade Federal do Esp ´ırito Santo (Vit ´oria,\\nBrazil) in 1987, his Specialist degree in Automation degree from\\nInstituto de Autom ´atica Industrial (Madrid, Spain) in 1989, and\\nhis Ph.D. degree in Physical Science (Electricity and Electronics)\\nfrom Universidad Complutense de Madrid (Spain) in 1994. He made\\ntwo postdocs, one at the University of Alcal ´a (Spain, 2005) and\\nanother at RMIT University (Australia, 2012). He is currently a\\nfull professor at Universidade Federal do Esp ´ırito Santo (Vit ´oria,\\nBrazil), teaching and doing research at the Postgraduate Program of\\nElectrical Enginneering, Postgraduate Program of Biotechnology and\\nRENORBIO Ph.D. Program. His current research interests are signal\\nprocessing, rehabilitation robotics and assistive technology for people\\nwith disabilitiesJIntellRobotSyst(2019)96:267–281 281\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 14}),\n",
       "  Document(page_content='1.\\n2.\\n3.\\n4.\\n5.\\n6.Terms and Conditions \\nSpringer Nature journal content, brought to you courtesy of Springer Nature Customer Service Center GmbH (“Springer Nature”). \\nSpringer Nature supports a reasonable amount of sharing of  research papers by authors, subscribers and authorised users (“Users”), for small-\\nscale personal, non-commercial use provided that all copyright, trade and service marks and other proprietary notices are maintained. By\\naccessing, sharing, receiving or otherwise using the Springer Nature journal content you agree to these terms of use (“Terms”). For these\\npurposes, Springer Nature considers academic use (by researchers and students) to be non-commercial. \\nThese Terms are supplementary and will apply in addition to any applicable website terms and conditions, a relevant site licence or a personal\\nsubscription. These Terms will prevail over any conflict or ambiguity with regards to the relevant terms, a site licence or a personal subscription\\n(to the extent of the conflict or ambiguity only). For Creative Commons-licensed articles, the terms of the Creative Commons license used will\\napply. \\nWe collect and use personal data to provide access to the Springer Nature journal content. We may also use these personal data internally within\\nResearchGate and Springer Nature and as agreed share it, in an anonymised way, for purposes of tracking, analysis and reporting. We will not\\notherwise disclose your personal data outside the ResearchGate or the Springer Nature group of companies unless we have your permission as\\ndetailed in the Privacy Policy. \\nWhile Users may use the Springer Nature journal content for small scale, personal non-commercial use, it is important to note that Users may\\nnot:  \\nuse such content for the purpose of providing other users with access on a regular or large scale basis or as a means to circumvent access\\ncontrol;\\nuse such content where to do so would be considered a criminal or statutory offence in any jurisdiction, or gives rise to civil liability, or is\\notherwise unlawful;\\nfalsely or misleadingly imply or suggest endorsement, approval , sponsorship, or association unless explicitly agreed to by Springer Nature in\\nwriting;\\nuse bots or other automated methods to access the content or redirect messages\\noverride any security feature or exclusionary protocol; or\\nshare the content in order to create substitute for Springer Nature products or services or a systematic database of Springer Nature journal\\ncontent. \\nIn line with the restriction against commercial use, Springer Nature does not permit the creation of a product or service that creates revenue,\\nroyalties, rent or income from our content or its inclusion as part of a paid for service or for other commercial gain. Springer Nature journal\\ncontent cannot be used for inter-library loans and librarians may not upload Springer Nature journal content on a large scale into their, or any\\nother, institutional repository. \\nThese terms of use are reviewed regularly and may be amended at any time. Springer Nature is not obligated to publish any information or\\ncontent on this website and may remove it or features or functionality at our sole discretion, at any time with or without notice. Springer Nature\\nmay revoke this licence to you at any time and remove access to any copies of the Springer Nature journal content which have been saved. \\nTo the fullest extent permitted by law, Springer Nature makes no warranties, representations or guarantees to Users, either express or implied\\nwith respect to the Springer nature journal content and all parties disclaim and waive any implied warranties or warranties imposed by law,\\nincluding merchantability or fitness for any particular purpose. \\nPlease note that these rights do not automatically extend to content, data or other material published by Springer Nature that may be licensed\\nfrom third parties. \\nIf you would like to use or distribute our Springer Nature journal content to a wider audience or on a regular basis or in any other manner not\\nexpressly permitted by these Terms, please contact Springer Nature at  \\nonlineservice@springernature.com ', metadata={'source': 'papers/1_Ramırez-Duque_.pdf', 'page': 15})],\n",
       " [Document(page_content='Ouss et al. Translational Psychiatry           (2020) 10:54 \\nhttps://doi.org/10.1038/s41398-020-0743-8 Translational Psychiatry\\nARTICLE Open Access\\nBehavior and interaction imaging at 9 months of\\nage predict autism/intellectual disability in high-riskinfants with West syndrome\\nLisa Ouss1,G i u s e p p eP a l e s t r a2, Catherine Saint-Georges2,3, Marluce Leitgel Gille1, Mohamed Afshar4,\\nHugues Pellerin2, Kevin Bailly2, Mohamed Chetouani2, Laurence Robel1,B e r n a r dG o l s e1, Rima Nabbout5,\\nIsabelle Desguerre5, Mariana Guergova-Kuras4and David Cohen2,3\\nAbstract\\nAutomated behavior analysis are promising tools to overcome current assessment limitations in psychiatry. At\\n9 months of age, we recorded 32 infants with West syndrome (WS) and 19 typically developing (TD) controls during astandardized mother –infant interaction. We computed infant hand movements (HM), speech turn taking of both\\npartners (vocalization, pause, silences, overlap) and motherese. Then, we assessed whether multimodal social signals\\nand interactional synchrony at 9 months could predict outcomes (autism spectrum disorder (ASD) and intellectualdisability (ID)) of infants with WS at 4 years. At follow-up, 10 infants developed ASD/ID (WS +). The best machine\\nlearning reached 76.47% accuracy classifying WS vs. TD and 81.25% accuracy classifying WS +vs. WS−. The 10 best\\nfeatures to distinguish WS +and WS −included a combination of infant vocalizations and HM features combined with\\nsynchrony vocalization features. These data indicate that behavioral and interaction imaging was able to predict ASD/ID in high-risk children with WS.\\nIntroduction\\nBehavior and interaction imaging is a promising domain\\nof affective computing to expl ore psychiatric conditions1–3.\\nRegarding child psychiatry, many researchers haveattempted to identify reliable indicators of neurodevelop-mental disorders (NDD) in high-risk populations (e.g., sib-\\nlings of children with autism) during the ﬁrst year of life to\\nrecommend early interventions\\n4,5. However, social signals\\nand any alterations of them are very dif ﬁcult to identify at\\nsuch a young age6. In addition, exploring the quality and\\ndynamics of early interactions is a complex endeavor. Itusually requires (i) the perception and integration of mul-timodal social signals and (ii) an understanding of how twointeractive partners synchronize and proceed in turn\\ntaking\\n7,8.\\nAffective computing offers the possibility to simulta-\\nneously analyze the interaction of several partners whileconsidering the multimodal nature and dynamics of socialsignals and behaviors\\n9. To date, few seminal studies have\\nattempted to apply social signal processing to\\nmother –infant interactions with or without a speci ﬁc\\ncondition, and these studies have focused on speech turns(e.g., Jaffe et al.\\n10), motherese11, head movements12, hand\\nmovements13, movement kinematics2, and facial\\nexpressions3.\\nHere, we focused on West syndrome (WS), a rare epi-\\nleptic encephalopathy with early onset (before age 1 year)\\nand a high risk of NDD outcomes, including one-third of\\nWS children showing later autism spectrum disorder(ASD) and/or intellectual disability (ID). We recruited 32infants with WS and 19 typically developing (TD) controlsto participate in a standardized early mother –infant\\n© The Author(s) 2020\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 Internat ional License, which permits use, sharing, adaptation, distribution and reproduction\\nin any medium or format, as long as you give appropriate credit to the origina l author(s) and the source, provide a li n kt ot h eC r e a t i v eC ommons license, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article ’s Creative Commons license, unless indicated otherwise in a credit line to the material. If\\nmaterial is not included in the article ’s Creative Commons license and your intended use is not permitted by sta tutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright hol der. To view a copy of this license, visit http://creativecommons .org/licenses/by/4.0/ .Correspondence: Lisa Ouss ( lisa.ouss@aphp.fr ) or David Cohen\\n(david.cohen@aphp.fr )\\n1Service de Psychiatrie de l ’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres,\\n75015 Paris, France\\n2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne\\nUniversité, 4 Place Jussieu, 75252 Paris Cedex, FranceFull list of author information is available at the end of the article1234567890():,;1234567890():,;\\n1234567890():,;\\n1234567890():,;', metadata={'source': 'papers/22_Ouss_ASD.pdf', 'page': 0}),\n",
       "  Document(page_content='interaction protocol and followed infants with WS to\\nassess outcomes at 4 years of age. We aim to explorewhether multimodal social signals and interpersonal\\nsynchrony of infant –mother interactions at 9 months\\ncould predict outcomes.\\nMaterials and methods\\nDesign, participants, and clinical measures\\nWe performed a prospective follow-up study of infants\\nwith WS14. The Institutional Review Board ( Comité de\\nProtection des Personnes from the Groupe-Hospitalier\\nNecker Enfants Malades ) approved the study, and both\\nparents gave written informed consent after they receivedverbal and written information on the study. They wereasked to participate to a follow-up study to assess out-come of WS taking into account development, earlyinteraction, genetics and response to pharmacologicaltreatment\\n14. The study was conducted from November\\n2004 to March 2010 in the Neuro-pediatrics Department\\nCenter for Rare Epilepsia of Necker Enfants-Malades\\nHospital, Paris. Of the 41 patients screened during thestudy period, we enrolled all but two cases ( N=39) with\\nWS. Seven patients dropped out before the age of 3leading to a sample of 32 patients with detailed follow-updata. Typical developing infants ( N=19) were recruited\\nfrom Maternal and Infant Prevention institutions, inpediatric consultations, or by proxy.To assess neurodevelopmental outcomes, we focused on\\nID and ASD. ID was assessed through the Brunet-Lézine\\nDevelopmental Examination , performed for all children at\\nthe age of 3 years. The Brunet-Lézine Developmental\\nExamination estimates a developmental quotient (DQ)based upon normative data available for 3-year-oldFrench toddlers\\n15. The diagnosis of autism was based\\nupon several measurements and an expert assessmentthat was blind to other variables: (i) At 3 years of age, allparents completed the Autism Diagnostic Interview-\\nRevised (ADI-R) to assess autism signs by dimensions\\nand developmental delay\\n16. (ii) At 2 and 3 years of age, all\\npatients were assessed with the Children ’s Autism Rating\\nScale (CARS)17. (iii) An expert clinician (LR) who was\\nblind to child history assessed autism and ID from 20-minvideotapes of child/mother play at 2 years of age. Finally,diagnoses of ASD and/or ID at age 4 were based upon aconsensus approach using direct assessment of the childby a clinician with expertise in autism (LO) as well as by\\nclinical information from the CARS, ADI-R, and DQ.\\nVideo recordings\\nInfant –mother interactions were assessed between 9\\nand 12 months of age during a play session (Fig. 1). Two\\nsynchronized cameras (face and pro ﬁle; Fig. S1A) recor-\\nded the movements in two dimensions while the infantwas sitting in a baby chair. Audio interactions were also\\nAround 9 months emordnys tseWn erldihc\\n(N=32)lacipyTg nipoleved\\ncontrols (N=19)Lab recording of \\ninfant-mother\\ninterac/g415on\\nMotherInfant\\nFree interac/g415on\\nInterac/g415on\\nwith the giraﬀe\\nMother singing3 sequences\\nof interac/g415on\\nWS with\\nID/ASD (N=10)WS without\\nID/ASD (N=22)Typical developing\\ncontrols (N=19)At 4 years\\nMachine \\nlearning\\nclassiﬁca/g415on\\nWest \\nsyndrome\\nvs.TDWS with\\nID/ASD\\nvs. WS \\nwithout\\nID/ASD Audio features extrac/g415on\\nInfant : typical vocaliza/g415on, \\natypical vocaliza/g415on, pause\\nMother : vocaliza/g415on, \\nmotherese, pause\\nSynchrony : overlap, silence, \\ninfant synchrony ra/g415o\\nVideo features extrac/g415on\\nInfant’s hand movement: \\nvelocity, accelera/g415on, \\ncurvature, spa/g415al, pause\\nFig. 1 Pipeline of our machine learning approach to classify WS vs. TD.Ouss et al. Translational Psychiatry           (2020) 10:54 Page 2 of 7', metadata={'source': 'papers/22_Ouss_ASD.pdf', 'page': 1}),\n",
       "  Document(page_content='recorded. The standardized situation encompassed three\\nsequences of 3 min: (sequence 1) free play after instruct-ing the mother to interact “as usual ”without any toy;\\n(sequence 2) free play using the help of a toy (Sophie the\\ngiraffe); (sequence 3) mother singing to her baby. Due tothe position of the baby chair on the ﬂoor and the\\nmother ’s seated position, the mother was positioned\\nslightly higher in all of the recordings. The mother ’s\\nindicated position was on the left of the child as shown onthe picture, but exceptions were sometimes observedduring the recordings. For infant hand movement (HM)\\nfeatures, 1 min was extracted from each 3-min video and\\nall recordings, according to two criteria: the child ’s hands\\nshould be visible for at least part of the sequence (e.g., themother is not leaning on the child), and the minuterepresented the greatest amount of interaction betweenthe mother and the child. For audio and speech turn-taking computing, we only used the 3-min audiorecording of sequence 1.\\nVision computing (Fig. S1B, vision computing panel)\\nTo process infant hand movements (HM), we used the\\nmethods developed in Ouss et al.13. Here, we summarize\\nthe successive steps to calculate HM features. In step 1(hand trajectory extraction and data processing), the two-dimensional coordinates of the hand were extracted fromeach of the video recordings by tracking a wristband on\\nthe right hand (yellow in Fig. S1A, video-audio recording\\npanel). The tracking framework comprised three steps:prediction, observation, and estimation as proposed inref.\\n18. As the hand motion was highly nonlinear, we\\ndeveloped an approach using a bootstrap-based particleﬁlter with a ﬁrst-order model to address abrupt changes in\\ndirection and speed\\n19,20. To address hand occlusion, we\\nimplemented an approach combining tracking with\\ndetection by adding a boolean variable to the state vector\\nassociated with each particle18.\\nEach extracted trajectory consisted of 1500 pairs of x\\nandycoordinates (25 frames per second, generating 1500\\npairs of coordinates in the 60 s; see Fig. S1 left panel,vision computing). The frames where the hand was notvisible were clearly indicated in each trajectory as missingcoordinates for these time points. To account for differ-\\nences in the camera zoom parameters, the trajectories\\nobtained were normalized using a ﬁxed reference system\\npresent in the settings of each video recording. The nor-malization was performed on all trajectories, and 95% ofthe normalization factors ranged between 0.8 and 1.22with a few outlier trajectories that required greater cor-rection. Forty-one percent of the trajectories required <5%correction. Although the recordings between the two\\ncameras were synchronized and in principle allowed 3D\\nreconstruction of the trajectory, the accumulation ofmissing data prevented such reconstruction. However, 2Dmotion capture with appropriately de ﬁned movement\\ndescriptors can be powerful for detecting clinically rele-vant changes\\n21, thereby justifying the independent ana-\\nlysis of the 2D-trajectory videos (see Fig. S1B, vision\\ncomputing, 2d panel on the left).\\nIn step 2, the descriptors of the HM were calculated\\nfrom the planar trajectories (Fig. S1B, table shown in thevision computing panel). Descriptors covered thosealready reported in the literature as important in char-acterizing infants ’HM\\n21. (1) To describe the space\\nexplored by the hand, we calculated the maximum dis-\\ntance observed on the two axes (xRange, yRange) and the\\nstandard deviation of the X and Y coordinates observedduring the 60 s (xSd, ySd). We also calculated the max-imum distance between any two points of the trajectoryusing the FarthestPair java library ( http://algs4.cs.\\nprinceton.edu/code/ ) (Fig. S1B, vision computing panel,\\nred line in the third panel from the left). (2) To evaluateHM dynamics, we calculated the velocity and accelera-\\ntion. (3) Also related to HM dynamics, we calculated HM\\npauses de ﬁned as part of the trajectory in which the\\nvelocity was lower than a speci ﬁc threshold for a mini-\\nmum duration of 4 s. (4) Finally, the curvature of thetrajectories was calculated using a standard de ﬁnition of\\nthe curvature ( κ) of plane curves in Cartesian coordinates\\nasγ(t)=(x(t),y(t)). The curvature calculated at each point\\nof the trajectory is presented in the right panel of Fig. S1B\\n(video computing), where the ﬁrst 1.2 s of the trajectory\\nare plotted and the associated calculated curvatures ateach point (and respective time, indicated on the axis) arepresented as columns.\\nAudio computing (Fig. S1C, audio computing)\\nWe extracted two types of audio social signals from the\\naudio channel of the mother –infant interaction: speech\\nturn taking (STT) and motherese. For STT extraction, we\\nfollowed the methods developed by Weisman et al.22and\\nBourvis et al.23(Fig. S1, audio computing). First, we used\\nELAN to segment the infants ’and mothers ’speech turns\\nand annotate the dialog acts. Mothers ’audio interactions\\nwere categorized as mother vocalization (meaningfulvocalizations, laugh, singing, animal sounds) or othernoise (clap hands, snap ﬁngers or snap the tongue, mouth\\nnoise, etc.). Similarly, infants ’audio production was\\ndeﬁned as infant vocalization (babbling vocalizations,\\nlaugh, and cry) or atypical vocalization (other noise suchas “rale ”). The infant ’s and mother ’s utterances were\\nlabeled by two annotators (blind to group status). Cohen ’s\\nkappa between the two annotators was calculated for eachdyad, each task and each item of the grid. For all items, thekappa values were between 0.82 and 1.\\nFrom the annotation, we extracted all the speech turns\\nof the infant and the mother. A speech turn is a con-tinuous stream of speech with <150 ms of silence. WeOuss et al. Translational Psychiatry           (2020) 10:54 Page 3 of 7', metadata={'source': 'papers/22_Ouss_ASD.pdf', 'page': 2}),\n",
       "  Document(page_content='obtained a list of triples: speaker label (infant or mother),\\nstart time, and duration of speech turn. From these triples,we also deduced the start time and duration of the time\\nsegments when the mother or the infant were not\\nspeaking (pauses). Therefore, we extracted M other\\nVocalizations ;Mother Other Noise ;Infant Vocalizations ;\\nInfant Atypical Vocalizations ;Mother Pauses ;Infant\\nPauses . We also extracted three dyadic features: (1)\\nSilence deﬁned as sequences of time during which neither\\nparticipant was speaking for more than 150 ms; (2)Overlap Ratio deﬁned as the duration of vocalization\\noverlaps between mothers and infants divided by the\\nduration of the total interaction. This ratio measures theproportion of interactional time in which both partici-pants were simultaneously vocalizing; (3) Infant Syn-\\nchrony Ratio deﬁned as the number of infants ’responses\\nto their mother ’s vocalization within a time limit of 3 s\\ndivided by the number of mother vocalizations during thetime paradigm. The 3-s window was based on the avail-\\nable literature on synchrony\\n7,24.\\nFrom the mother vocalizations, we also computed\\naffective speech analysis, as previous work has shown thatmotherese may shape parent-infant interactions\\n25. The\\nsegments of mother vocalizations were analyzed using acomputerized classi ﬁer for categorization as “motherese ”\\nor “non-motherese/other speech ”initially developed to\\nanalyze home movies\\n11. The system exploits the fusion of\\ntwo classi ﬁers, namely, segmental and suprasegmental26.\\nConsequently, the utterances are characterized by bothsegmental (Mel frequency cepstrum coef ﬁcients) and\\nsuprasegmental/prosodics (e.g., statistics with regard tofundamental frequency, energy, and duration) features.The detector used the GMM (Gaussian mixture model)classi ﬁer for both segmental and suprasegmental features\\n(M, number of Gaussians for the GMM Classi ﬁer:M=12\\nand 15, respectively, and λ=weighting coef ﬁcient used in\\nthe equation fusion: λ=0.4). For the purpose of the\\ncurrent study, we explored the performance of ourmotherese classi ﬁer in French mothers. We analyzed\\n200 sequences from French mothers (100 motherese vs.100 other speech) that were blindly validated by twopsycholinguists. We calculated the Intraclass correlation(ICC) between the two raters (the expert and the algo-\\nrithm) and found a good and very signi ﬁcant ICC (ICC =\\n0.79 (95% CI: 0.59 –0.90), p< 0.001). This level of predic-\\ntion made it suitable for further analysis of the entiredata set.\\nBased on this automatic detection of motherese, we\\ncreated two subclasses for mother vocalizations: mother-ese vs. non-motherese. Two variables were derived:Motherese Ratio (duration of motherese vocalization/\\nduration of interaction) and Non-motherese Ratio (dura-\\ntion of non-motherese vocalization/duration of interac-tion). We also derived two synchrony ratios: SynchronyMotherese Ratio and Synchrony Non-motherese Ratio ,\\nwhich re ﬂect the ratio of time during which the infant\\nvocalizes in response to his/her mother motherese and\\nother speech (non-motherese).\\nPrediction of the outcome using machine learning\\nThe pipeline of our approach is shown in Fig. 1. First, a\\ndata quality analysis was performed to ensure the validityof the data. As expected, all data were available for audioanalysis. However, a substantial proportion of the datawere discarded due to video recording or vision com-\\nputing issues. We ﬁnally kept 18 video recordings for the\\nWS and 17 videos for the TD groups. Second, given thenumber of features (21 infant HM for each camera andeach sequence; 16 STT) compared with the data set (32WS and 19 TD), we reduced our data using principalcomponent analysis (PCA). Third, we tested severalalgorithms to classify WS vs. TD based on the whole dataset available for both vision and audio computing features\\n(leave one out) (Table S1). The best algorithm was deci-\\nsion stump\\n27. All results presented here are based on the\\nclassi ﬁcation with a decision stump algorithm. We also\\nanalyzed WS with ID/ASD (WS +) vs. WS without ID/\\nASD (WS −). For each classi ﬁcation, we also extracted a\\nconfusion matrix and explored which individual featurescontributed the most to a given classi ﬁcation using\\nPearson correlations.\\nResults\\nTable S2 summarizes the demographic and clinical\\ncharacteristics of children with WS. At follow-up, 10infants out of 32 children with WS developed ASD/ID(WS+). Eight children had ASD and ID, whereas 2 had\\nonly ID. As expected, all variables related to ASD and IDwere signi ﬁcantly different in WS +compared with WS −.\\nFigure 2a summarizes the best classi ﬁcation models\\nusing the decision stump algorithm (leave one out). Asshown, multimodal classi ﬁcation outperformed unimodal\\nclassi ﬁcation to distinguish WS and TD. Therefore, we\\nonly used the multimodal approach to classify WS +vs.\\nWS−. The best model reached 76.47% accuracy classify-\\ning WS vs. TD and 81.25% accuracy classifying WS +vs.\\nWS−based on multimodal features extracted during early\\ninteractions. Interestingly, the confusion matrices (Fig.\\n2b) show that when classifying WS vs. TD, all errors came\\nfrom TD being misclassi ﬁed as WS ( N=12); when clas-\\nsifying WS +vs. WS−, most errors came from WS +being\\nmisclassi ﬁed as WS −(N=5).\\nTable 1lists the best features for each multimodal\\nclassi ﬁcation based on the Pearson correlation values. The\\nbest features to distinguish WS and TD included four\\ninfant HM features, 1 mother audio feature. In contrast,\\nthe best features to distinguish WS +and WS −included a\\ncombination of infant vocalization features ( N=2),Ouss et al. Translational Psychiatry           (2020) 10:54 Page 4 of 7', metadata={'source': 'papers/22_Ouss_ASD.pdf', 'page': 3}),\n",
       "  Document(page_content='0102030405060708090\\nMu/g415modal Video AudioWest vs. TD (N=51)\\nWest with ID/ASD vs. \\nWest with out ID/ASD \\n(N=32)Classiﬁed\\nas West TD\\nWest 32 0\\nTD 12 7\\nClassiﬁed\\nas West with\\nID/ASDWest with\\noutID/ASD\\nWest with\\nID/ASD5 5\\nWest with\\noutID/ASD1 21Machine learning classiﬁca/g415on\\nDecision stump (leave one out)a\\nConfusion\\nmatricesb\\nFig. 2 Machine learning classi ﬁcation of WS vs. TD and WS +vs. WS−based on uni- and multimodal features extracted during early infant –mother\\ninteraction.\\nTable 1 Best features for classi ﬁcation (based on signi ﬁcant Pearson ’s correlation between feature and class).\\nFeature characteristics Pearson rp -value\\nWest vs. Typical developing\\nRatio of all maternal audio intervention during free interaction Audio, mother 0.35 0.012\\nTotal number of infant HM pauses (side view camera) during free interaction Video, infant 0.34 0.014\\nTotal number of infant HM pauses (side view camera) when the mother is singing Video, infant 0.32 0.023\\nVertical amplitude of the giraffe (front view camera) Video, infant −0.30 0.032\\nMovement acceleration max (side view camera) during free interaction Video, infant 0.29 0.034\\nWest with ASD/ID vs. West without ASD/ID\\nTotal number of all infant vocalization during free interaction Audio, infant −0.56 <0.001\\nSynchrony ratio (infant response to mother) Audio, synchrony −0.55 <0.001\\nRatio of all infant vocalization during free interaction Audio, infant −0.55 0.001\\nMotherese synchrony ratio (infant response to motherese) Audio, synchrony −0.54 0.002\\nNon-motherese synchrony ratio (infant response to non-motherese) Audio, synchrony −0.48 0.005\\nHM acceleration SD (front view camera) during the giraffe interaction Video, infant −0.46 0.008\\nHM acceleration max (side view camera) during the giraffe interaction Video, infant −0.45 0.01\\nHM velocity SD (front view camera) during the giraffe interaction Video, infant −0.43 0.014\\nCurvature max (side view camera) during the giraffe interaction Video, infant −0.37 0.039\\nRelative time spent motionless (pause) (front view camera) during free interaction Video, infant 0.36 0.04\\nHMhand movement, ASD autism spectrum disorder, IDintellectual disability, SDstandard deviation.Ouss et al. Translational Psychiatry           (2020) 10:54 Page 5 of 7', metadata={'source': 'papers/22_Ouss_ASD.pdf', 'page': 4}),\n",
       "  Document(page_content='synchrony vocalization features ( N=3) and infant HM\\nfeatures ( N=5), the last of which showed lower correla-\\ntion scores.\\nDiscussion\\nTo the best of our knowledge, this is the ﬁrst study to\\napply multimodal social signal processing tomother –infant interactions in the context of WS. Com-\\nbining speech turns and infant HM during aninfant –mother interaction at 9 months signi ﬁcantly pre-\\ndicted the development of ASD or severe to moderate ID\\nat 4 years of age in the high-risk children with WS.\\nConfusion matrices showed that the classi ﬁcation errors\\nwere not random, enhancing the interest of the compu-tational method proposed here. In addition, the bestcontributing features for the performed classi ﬁcations\\ndiffered when classifying WS vs. TD and WS +vs. WS −.\\nInfant HMs were the most signi ﬁcant features to distin-\\nguish WS versus TD, probably re ﬂecting the motor\\nimpact due to acute WS encephalopathy. For classifying\\nWS+vs. WS −, the contribution of infant audio features\\nand synchrony features became much more relevantcombined with several HM features.\\nWe believe that the importance of synchrony and\\nreciprocity during early interactions is in line withrecent studies that have investigated the risk of ASD orNDD during the ﬁrst year of life from home movies\\n(e.g., refs.\\n11,24), from prospective follow-up of high-risk\\ni n f a n t ss u c ha ss i b l i n g s( e . g . ,r e f s .4,28) or infants with\\nWS (e.g., ref.14), and from prospective studies assessing\\ntools to screen risk for autism (e.g., ref.29). In the ﬁeld of\\nASD, synchrony, reciprocity, parental sensitivity, and\\nemotional engagement are now proposed as targets ofearly interventions\\n30, which could prevent early inter-\\nactive vicious circl es. Parents of at-risk infants try to\\ncompensate for the lack of interactivity of their child by\\nmodifying their stimulation and therefore sometimesreinforcing the dysfunctional interactions\\n24.E a r l y\\nidenti ﬁcation of these interactive targets is especially\\nuseful among babies with neurological comorbiditiesbecause delays in developmental milestones andimpairments in early social interactions are not suf ﬁ-\\ncient to predict ASD.\\nSimilarly, we believe that the importance of HM in\\ndistinguishing WS vs. TD on one hand, and WS +vs.\\nWS−on the other hand, is also in line with the studies\\nthat investigated the importance of non-social behaviorsfor investigating the risk of ASD or NDD during the ﬁrst\\nyear of life. For example, studying home movies, Purpuraet al. found more bilateral HM and ﬁnger movements in\\ninfants who will later develop ASD\\n31. Similarly, several\\nprospective follow-up studies of high-risk siblings32–35or\\nretrospective studies on home movies36,37reported spe-\\nciﬁc motor atypical repertoire in infants with ASD.In ASD, early social signals have previously been\\nassessed with automatized and computational procedures,focusing on eye tracking at early stages\\n38–40, vocal pro-\\nductions41, analysis of acoustics of ﬁrst utterances or cry\\nepisodes42, but none was done in an interactive setting.\\nOur study proposed a paradigm shift from the assessmentof infant behavior to dyadic assessment of interactions, aspreviously achieved in retrospective approaches usinghome movies\\n24. The aim is not to implement studies of\\nsocial signal processing in routine clinical work but ratherto decompose clinical intuitions and signs and validate the\\nmost relevant cues of these clinical features. From clinical\\nwork, back to clinics, social signal processing is a rigorousstep to help clinicians better identify and assess earlytargets of interventions.\\nGiven the exploratory nature of both our approach and\\nmethod, our results should be interpreted with cautiontaking into account strengths (prospective follow-up,automatized multimodal social signal processing, and\\necological standardized assessment) and limitations.\\nThese limitations include (1) the overall sample sizeknowing that WS is a rare disease; (2) the high rate ofmissing data during video recording due to the ecologicalconditions of the infant –mother interaction (mothers\\ninterposing between the camera and the infant); the ﬁnal\\nsample size of WS +(N=10) that limited the power of\\nmachine learning methods.\\nWe conclude that the method proposed here combining\\nmultimodal automatized assessment of social signal pro-cessing during early interaction with infants at risk forNDD is a promising tool to decipher clinical features thatremain dif ﬁcult to identify and assess. In the context of\\nWS, we showed that such a method we proposed to label‘behavioral and interaction imaging ’was able to sig-\\nniﬁcantly predict the development of ASD or ID at 4 years\\nof age in high-risk children who had WS and were\\nassessed at 9 months of age.\\nAcknowledgements\\nThe authors thank all of the patients and families who participated in thisstudy. The study was funded by the EADS foundation (PILE), by the Agence\\nNationale de la Recherche (ANR-12-SAMA-006-1) and the Groupement de\\nRecherche en Psychiatrie (GDR-3557). It was partially performed in the Labex\\nSMART (ANR-11-LABX-65), which is supported by French state funds andmanaged by the ANR in the Investissements d ’Avenir program under reference\\nANR-11-IDEX-0004-02. The sponsors had no involvement in the study design,\\ndata analysis, or interpretation of the results.\\nAuthor details\\n1Service de Psychiatrie de l ’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres,\\n75015 Paris, France.2Institut des Systèmes Intelligents et de Robotique, CNRS,\\nUMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France.\\n3Département de Psychiatrie de l ’Enfant et de l ’Adolescent, AP-HP, Hôpital\\nPitié-Salpêtrière, 47-83, Boulevard de l ’Hôpital, 75651 Paris, Cedex 13, France.\\n4Ariana Pharmaceuticals, Research Department, Paris, France.5Service de\\nNeuropédiatrie, AP-HP, Hôpital Necker, 136, Rue de Vaugirard, 75015 Paris,FranceOuss et al. Translational Psychiatry           (2020) 10:54 Page 6 of 7', metadata={'source': 'papers/22_Ouss_ASD.pdf', 'page': 5}),\n",
       "  Document(page_content='Conﬂict of interest\\nThe authors declare that they have no con ﬂict of interest.\\nPublisher ’s note\\nSpringer Nature remains neutral with regard to jurisdictional claims in\\npublished maps and institutional af ﬁliations.\\nSupplementary Information accompanies this paper at ( https://doi.org/\\n10.1038/s41398-020-0743-8 ).\\nReceived: 7 December 2019 Revised: 13 January 2020 Accepted: 16 January\\n2020\\nReferences\\n1. Spodenkiewicz, M. et al. Distinguish se lf- and hetero-perceived stress through\\nbehavioral imaging and ph ysiological features. Prog. Neuropsychopharmacol.\\nBiol. Psychiatry 82,1 0 7–114 (2018).\\n2. Leclere, C. et al. Interaction and behaviour imaging: a novel method to\\nmeasure mother-infant interaction using video 3D reconstruction. Transl.\\nPsychiatry 6, e816 (2016).\\n3 . M e s s i n g e r ,D .S . ,M a h o o r ,M .H . ,C h o w ,S .M .&C o h n ,J .F .A u t o m a t e dm e a -\\nsurement of facial expression in infant-mother interaction: a pilot study.Infancy 14,2 8 5–305 (2009).\\n4. Wan, M. W. et al. Parent-infant interact ion in infant siblings at risk of autism. Res\\nDev. Disabil. 33,9 2 4–932 (2012).\\n5 . R o g e r s ,S .J .e ta l .A u t i s mt r e a t m e n ti nt h e ﬁrst year of life: a pilot study of\\ninfant start, a parent-implemented intervention for symptomatic infants. J.\\nAutism Dev. Disord. 44,2 9 8 1 –2995 (2014).\\n6. Zwaigenbaum, L., Bryson, S. & Garon, N. Early identi ﬁcation of autism spectrum\\ndisorders. Behav. Brain Res 251, 133–146 (2013).\\n7. Feldman, R. Parent-infant synchrony and the construction of shared timing;\\nphysiological precursors, developmental outcomes, and risk conditions. J.\\nChild Psychol. Psychiatry 48,3 2 9–354 (2007).\\n8. Delaherche, E. et al. Interpersonal synchrony: a survey of evaluation methods\\nacross disciplines. IEEE Trans. Affect Comput 3,3 4 9–365 (2012).\\n9. Vinciarelli, A., Pantic, M. & Bourlard, H. Social signal processing: survey of an\\nemerging domain. Image Vis. Comput 27,1 7 4 3 –1759 (2009).\\n10. Jaffe, J., Beebe, B., Feldstein, S., Crown, C. L. & Jasnow, M. D. Rhythms of\\ndialogue in infancy: coordina ted timing in development. Monogr. Soc. Res\\nChild Dev. 66,1–132 (2001).\\n11. Cohen, D. et al. Do parentese prosody and fathers ’involvement in interacting\\nfacilitate social interaction in in fants who later develop autism? PLoS ONE 8,\\ne61402 (2013).\\n12. Hammal, Z., Cohn, J. F. & Messinger, D. S. Head movement dynamics during\\nplay and perturbed mother-infant interaction. IEEE Trans. Affect Comput. 6,\\n361–370 (2015).\\n13. Ouss, L. et al. Developmental trajectories of hand movements in typical infants\\nand those at risk of developmental disorders: an observational study ofkinematics during the ﬁ\\nrst year of life. Front Psychol. 9,8 3( 2 0 1 8 ) .\\n14. Ouss, L. et al. Taking into account infant ’s engagement and emotion during\\nearly interactions may help to determine the risk of autism or intellectual\\ndisability in infants with West syndrome. Eur. Child Adolesc. Psychiatry 23,\\n143–149 (2014).\\n15. Josse, D. Le manuel BLR-C, “Brunet-Lézine Révisé: Echelle de Developpement\\nPsychomoteur de la Premiere Enfance (EAP, Paris, 1997).\\n16. Lord, C., Rutter, M. & Le Couteur, A. Autism diagnostic interview-revised: a\\nrevised version of a diagnostic interview for caregivers of individuals withpossible pervasive dev elopmental disorders. J. Autism Dev. Disord. 24,6 5 9–685\\n(1994).\\n17. Schopler, E., Reichler, R. J., DeVellis , R. F. & Daly, K. Toward objective classi ﬁ-\\ncation of childhood autism: Childhood Autism Rating Scale (CARS). J. Autism\\nDev. Disord. 10,9 1–103 (1980).\\n18. Czyz, J., Ristic, B. & Macq, B. A color-based particle ﬁlter for joint detection and\\ntracking of multiple objects. in Proceedings (ICASSP ‘05) IEEE InternationalConference on Acoustics, Speech, and Signal Processing (IEEE, Philadelphia, PA,\\n2005).\\n19. Hue, C. Méthodes Séquentielles de Monte Carlo pour le Filtrage non Linéaire\\nMulti-Objets dans un Environnement Bruité. Applications au Pistage Multi-Cibleset à la Trajectographie d ’E n t i t é sd a n sd e sS é q u e n c e sd ’Images 2D .P h DT h e s i s ,\\nUniversité de Rennes I, Rennes, France (2003).\\n20. Isard, M. & Blake, A. Condensation —conditional density propagation for visual\\ntracking. Int J. Comput Vis. 29,5–28 (1998).\\n2 1 . M a r c r o f t ,C . ,K h a n ,A . ,E m b l e t o n ,N .D . ,T r e n e l l ,M .&P l o t z ,T .M o v e m e n t\\nrecognition technology as a method of assessing spontaneous generalmovements in high risk infants. Front Neurol. 5, 284 (2014).\\n22. Weisman, O. et al. Dynamics of non-verbal vocalizations and hormones during\\nfather-infant interaction. IEEE Trans. Affect Comput 7,3 3 7–345 (2016).\\n23. Bourvis, N. et al. Pre-linguistic infan ts employ complex communicative loops\\nto engage mothers in social exchanges and repair interaction ruptures. R. Soc.\\nOpen Sci. 5, 170274 (2018).\\n24. Saint-Georges, C. et al. Do parents recognize autistic deviant behavior long\\nbefore diagnosis? Taking into account interaction using computational\\nmethods. PLoS ONE 6, e22393 (2011).\\n25. Saint-Georges, C. et al. Motherese in interaction: at the cross-road of emotion\\nand cognition? (A systematic review). PLoS ONE 8, e78103 (2013).\\n26. Mahdhaoui, A. et al. Computerized home video detection for motherese may\\nhelp to study impaired interaction between infants who become autistic and\\ntheir parents. Int J. Methods Psychiatr. Res. 20,e 6–e18 (2011).\\n27. Iba, W. & Langley, P. Induction of one-level decision trees. in Machine Learning:\\nProceedings of the Ninth International Workshop (eds Sleeman, D. & Edwards, P.)\\n233–240 (Morgan Kaufmann, San Mateo, CA, 1992).\\n28. Wan, M. W. et al. Quality of interaction between at-risk infants and caregiver at\\n12–15 months is associated with 3-year autism outcome. J. Child Psychol.\\nPsychiatry 54,7 6 3–771 (2013).\\n29. Olliac, B. et al. Infant and dyadic as sessment in early community-based\\nscreening for autism spectrum disorder with the PREAUT grid. PLoS ONE 12,\\ne0188831 (2017).\\n30. Green, J. et al. Parent-mediated interve ntion versus no intervention for infants\\nat high risk of autism: a parallel, single-blind, randomised trial. Lancet Psychiatry\\n2,1 3 3–140 (2015).\\n31. Purpura, G. et al. Bilateral patterns of repetitive movements in 6- to 12-\\nmonth-old infants with autism spectrum disorders. Front Psychol. 8, e1168\\n(2017).\\n3 2 . L o h ,A .e ta l .S t e r e o t y p e dm o t o rb e h a v i ors associated with autism in high-risk\\ninfants: a pilot videotape an alysis of a sibling sample. J. Autism Dev. Disord. 37,\\n25–36 (2007).\\n33. Morgan, L., Wetherby, A. M. & Barber, A. Repetitive and stereotyped move-\\nments in children with autism spectru m disorders late in the second year of\\nlife.J. Child Psychol. Psychiatry 49,8 2 6–837 (2008).\\n34. Elison, J. T. et al. Repetitive beh avior in 12-month-olds later classi ﬁed with\\nautism spectrum disorder. J. Am. Acad. Child Adolesc. Psychiatry 53,1 2 1 6 –1224\\n(2014).\\n35. Wolff, J. J. et al. Longitudinal patterns of repetitive behavior in toddlers with\\nautism. J. Child Psychol. Psychiatry 55,9 4 5–953 (2014).\\n36. Phagava, H. et al. General movements in infants with autism spectrum dis-\\norders. Georgian Med. N. 156,1 0 0–105 (2008).\\n3 7 . L i b e r t u s ,K . ,S h e p e r d ,K .A . ,R o s s ,S .W .&L a n d a ,R .J .L i m i t e d ﬁne motor and\\ngrasping skills in 6-month-old infants at high risk for autism. Child Dev. 85,\\n2218–2231 (2014).\\n38. Bedford, R. et al. Precursors to social and communication dif ﬁculties in infants\\nat-risk for autism: gaze following and attentional engagement. J. Autism Dev.\\nDisord. 42, 2208 –2218 (2012).\\n3 9 . E l s a b b a g h ,M .e ta l .W h a ty o us e ei sw h a ty o ug e t :c o n t e x t u a lm o d u l a t i o no f\\nface scanning in typical and atypical development. Soc. Cogn. Affect Neurosci.\\n9,5 3 8–543 (2014).\\n4 0 . J o n e s ,W .&K l i n ,A .A t t e n t i o nt oe y e s is present but in decline in 2-6-month-\\nold infants later diagnosed with autism. Nature 504,4 2 7–431 (2013).\\n4 1 . P a u l ,R . ,F u e r s t ,Y . ,R a m s a y ,G . ,C h a w a r s k a ,K .&K l i n ,A .O u to ft h em o u t h so f\\nbabes: vocal production in infan t siblings of children with ASD. J. Child Psychol.\\nPsychiatry 52,5 8 8–598 (2011).\\n4 2 . S h e i n k o p f ,S .J . ,I v e r s o n ,J .M . ,R i n a l d i ,M .L .&L e s t e r ,B .M .A t y p i c a lc r ya c o u s t i c s\\nin 6-month-old infants at risk for autism spectrum disorder. Autism Res. 5,\\n331–339 (2012).Ouss et al. Translational Psychiatry           (2020) 10:54 Page 7 of 7', metadata={'source': 'papers/22_Ouss_ASD.pdf', 'page': 6})],\n",
       " [Document(page_content='Research and Applications\\nMachine learning approach for early detection of autism\\nby combining questionnaire and home video screening\\nHalim Abbas,1Ford Garberson,1Eric Glover,2and Dennis P Wall1,3,4\\n1Cognoa Inc., Palo Alto, CA, USA www.linkedin.com/in/halimabbas,2eric_g@ericglover.com,3Department of Pediatrics, Stanford\\nUniversity, Stanford, CA, USA,4Department of Biomedical Data Science, Stanford University, Stanford, CA, USA\\nCorrespondence to: Cognoa Inc., Palo Alto, CA, USA; halim@cognoa.com\\nReceived 19 September 2017; Revised 16 March 2018; Editorial Decision 25 March 2018; Accepted 2 April 2018\\nABSTRACT\\nBackground: Existing screening tools for early detection of autism are expensive, cumbersome, time- intensive,\\nand sometimes fall short in predictive value. In this work, we sought to apply Machine Learning (ML) to goldstandard clinical data obtained across thousands of children at-risk for autism spectrum disorder to create alow-cost, quick, and easy to apply autism screening tool.\\nMethods: Two algorithms are trained to identify autism, one based on short, structured parent-reported ques-\\ntionnaires and the other on tagging key behaviors from short, semi-structured home videos of children. A com-bination algorithm is then used to combine the results into a single assessment of higher accuracy. To over-come the scarcity, sparsity, and imbalance of training data, we apply novel feature selection, featureengineering, and feature encoding techniques. We allow for inconclusive determination where appropriate inorder to boost screening accuracy when conclusive. The performance is then validated in a controlled clinicalstudy.\\nResults: A multi-center clinical study of n¼162 children is performed to ascertain the performance of these\\nalgorithms and their combination. We demonstrate a signiﬁcant accuracy improvement over standard screen-ing tools in measurements of AUC, sensitivity, and speciﬁcity.\\nConclusion: These ﬁndings suggest that a mobile, machine learning process is a reliable method for detection\\nof autism outside of clinical settings. A variety of confounding factors in the clinical analysis are discussed alongwith the solutions engineered into the algorithms. Final results are statistically limited and will beneﬁt from fu-ture clinical studies to extend the sample size.\\nKey words : supervised machine learning, autism spectrum disorder, diagnostic techniques and procedures, mobile applications\\nINTRODUCTION\\nDiagnosis within the first few years of life dramatically improves the\\noutlook of children with autism, as it allows for treatment while the\\nchild’s brain is still rapidly developing.1,2Unfortunately, autism is\\ntypically not diagnosed earlier than age 4 in the United States, withapproximately 27% of cases remaining undiagnosed at age 8.\\n3This\\ndelay in diagnosis is driven primarily by a lack of effective screening\\ntools and a shortage of specialists to evaluate at-risk children. Theuse of higher accuracy screening tools to prioritize children to be\\nseen by specialists is therefore essential.Most autism screeners in use today are based on questions for\\nthe parent or the medical practitioner, that produce results by com-paring summed answer scores to predetermined thresholds. Notableexamples are the Modified Checklist for Autism in Toddlers, Re-vised (M-CHAT),\\n4a checklist-based screening tool for autism that is\\nintended to be administered during developmental screenings forchildren between the ages of 16 and 30 months, and the Child Be-havior Checklist (CBCL).\\n5Both are parent-completed screening\\ntools. For both instruments, responses to each question are summedwith each question given equal weighting, and if the total is above apre-determined threshold the child is considered to be at high risk of\\nVCThe Author(s) 2018. Published by Oxford University Press on behalf of the American Medical Informatics Association.\\nAll rights reserved. For permissions, please email: journals.permissions@oup.com\\n1000Journal of the American Medical Informatics Association , 25(8), 2018, 1000–1007\\ndoi: 10.1093/jamia/ocy039\\nAdvance Access Publication Date: 7 May 2018\\nResearch and Applications', metadata={'source': 'papers/Abbas_2018.pdf', 'page': 0}),\n",
       "  Document(page_content='autism. In the case of CBCL there are multiple scales based upon dif-\\nferent sets of questions corresponding to different conditions. The“Autism Spectrum Problems” scale of CBCL is used when comparingits performance to the performances of our algorithms in this paper.\\nIn this paper, we present two new machine learning screeners that\\nare reliable, cost-effective, short enough to be completed in minutes,and achieve higher accuracy than existing screeners on the same agespan as existing screeners. One is based on a short questionnaire about\\nthe child, which is answered by the parent. The other is based on iden-\\ntification of specific behaviors by trained analysts after watching twoor three short videos of the child within their natural environmentthat are captured by parents using a mobile device.\\nThe parent questionnaire screener keys on behavioral patterns\\nsimilar to those probed by a standard autism diagnostic instrument,the Autism Diagnostic Interview – Revised (ADI-R).\\n6This clinical\\ntool consists of an interview of the parent with 93 multi-part ques-tions with multiple choice and numeric responses which are deliv-ered by a trained professional in a clinical setting. While thisinstrument is considered a gold-standard, and gives consistentresults across examiners, the cost and time to administer it can beprohibitive in a primary care setting. In this paper, we present ourapproach to using clinical ADI-R instrument data to create ascreener based on a short questionnaire presented directly to parentswithout supervision.\\nThe video screener keys on behavioral patterns similar to those\\nprobed in another diagnostic tool, the Autism Diagnostic Observa-tion Schedule (ADOS).\\n7ADOS is widely considered a gold standard\\nand is one of the most common behavioral instruments used to aidin the diagnosis of autism.\\n8It consists of an interactive and struc-\\ntured examination of the child by trained clinicians in a tightly con-trolled setting. ADOS is a multi-modular diagnostic instrument,with different modules for subjects at different levels of cognitive de-velopment. In this paper, we present our approach to mining ADOSclinical records, with a focus on younger developmental age, to cre-ate a video-based screener that relies on an analyst evaluating shortvideos of children filmed by their parents at home.\\nThe use of behavioral patterns commonly probed in ADI-R and\\nADOS scoresheets as inputs to train autism screening classifiers wasintroduced, studied, and clinically validated in previous work.\\n9–12\\nThere are several new aspects in this paper. First, the algorithms de-\\ntailed in the present study have been designed to be more accurate\\nand more robust against confounding biases between training and\\napplication data. Next, this paper focuses considerable attention onthe impact of confounding factors on machine learning algorithmsin this context. Examples of these confounding biases will be dis-cussed below and highlighted in Table 2 . Labeled data usually origi-\\nnates from tightly controlled clinical environments and is, hence,clean but sparse, unbalanced, and of a different context to the dataavailable when applying the screening techniques in a less formal en-vironment. This paper also presents a combination between thealgorithms for a more powerful single screener. Lastly, this papergeneralizes the algorithms to be non-binary, sometimes resulting inan “inconclusive” determination when presented with data frommore challenging cases. This allows higher screening accuracy forthose children who do receive a conclusive screening, while still pre-senting a clinically actionable inconclusive outcome in the morechallenging cases.\\nThese classifiers of this paper were applied to screen children in\\na clinical study using the Cognoa\\n13App. To date, Cognoa has been\\nused by over 250 000 parents in the US and internationally. The ma-jority of Cognoa users are parents of young children between 18 and30 months. The clinical study consisted of 162 at-risk children who\\nhad undergone full clinical examination and received a clinical diag-nosis at a center specialized in neurodevelopmental disorders.\\nMETHODS\\nIt is not feasible to amass large training sets of children who havebeen evaluated by the mobile screeners and who also have received aprofessional medical diagnosis. Our approach is to start with histor-ical medical instrument records of previously diagnosed subjects,and use those as training data for screeners that will rely on informa-tion acquired outside the clinical setting. Expected performance deg-radation from applying the algorithms into a less controlled settingwould result in inaccurate screeners if conventional machine learn-ing methods were used. Much of this paper outlines the details ofcreative machine learning methods designed to overcome this chal-lenge and create reliable screeners in this setting.\\nTraining data were compiled from multiple repositories of\\nADOS and ADI-R score-sheets of children between 18 and 84months of age including Boston Autism Consortium, Autism Ge-\\nnetic Resource Exchange, Autism Treatment Network, Simons Sim-\\nplex Collection, and Vanderbilt Medical Center. Since suchrepositories are highly imbalanced with very few non-autisticpatients, the controls across the datasets were supplemented withbalancing data obtained by conducting ADI-R interviews by atrained clinician on a random sample of children deemed at low riskfor autism from Cognoa’s user base. For both algorithms a smallerset of optimal features was selected using methods that will be dis-cussed below. Details about the final selected features are given intheSupplementary Material .\\nThe clinical validation sample consists of 230 children who pre-\\nsented to one of three autism centers in the United States between 18and 72 months of age. All participants were referred through theclinics’ typical referral program process, and only those withEnglish-speaking parents were considered for the study. The threeclinical centers were approved on a multisite IRB (project number2202803). Every child received an ADOS as well as standard screen-ers like M-CHAT and CBCL as appropriate, and a diagnosis was ul-timately ascertained by a licensed health care provider. For 162 ofthose children, the parents also used their mobile devices to com-plete the short parental questionnaire and submit the short videosrequired for the screeners discussed in this paper. The sample break-down by age group and diagnosis for both the training and clinicalvalidation datasets is shown in Table 1 .\\nApproach\\nWe trained two independent ML classifiers and combined their out-\\nputs into a single screening assessment. The parent questionnaireclassifier was trained using data from historical item-level ADI-Rscore-sheets with labels corresponding to established clinical diagno-ses. The video classifier was trained using ADOS instrument score-sheets and diagnostic labels. In each case, progressive sampling wasused to verify sufficient training volume as detailed in the Supple-\\nmentary Materials . Multiple machine learning algorithms were eval-\\nuated including ensemble techniques on the training data. A numberof algorithms performed well. Random Forests were chosen becauseof robustness against overfitting.\\nADI-R and ADOS instruments are designed to be administered by\\ntrained professionals in highly standardized clinical settings and typi-cally take hours. In contrast, our screening methods are deliberatelyJournal of the American Medical Informatics Association , 2018, Vol. 25, No. 8 1001', metadata={'source': 'papers/Abbas_2018.pdf', 'page': 1}),\n",
       "  Document(page_content='designed to be administered at home by parents without expert super-\\nvision, and to take only minutes to complete. This change of environ-ment causes significant data degradation and biases resulting in anexpected loss of screening accuracy. For each classifier, we presentmindful adjustments to ML methodology to mitigate these issues.These biases and efforts to mitigate them are discussed below.\\nDifferences Between Training and Application\\nEnvironments\\nThe screeners are trained on historical patient records that corre-\\nspond to controlled, lengthy clinical examinations, but applied viaweb or mobile app aimed at unsupervised parents at home. Table 2\\ndetails the various mechanisms by which confounding biases mayconsequently creep into the application data. Note that inaccuraciesintroduced by such biases cannot be probed by cross- validation orsimilar analysis of the training data alone.\\nHyperparameter Optimization\\nFor each parental questionnaire and video model that will be dis-cussed below, model hyperparameters were tuned with a boot-strapped grid search. In all cases, class labels were used to stratifythe folds, and (age, label) pairs were used to weight-balance the sam-ples. More details can be found in the Supplementary Materials .Parent Questionnaire\\nMultiple model variants representing incremental improvements\\nover a generic ML classification approach are discussed below.\\nGeneric ML Baseline Variant\\nA random forest was trained over the ADI-R instrument data. Each\\nof the instrument’s 155 data columns was treated as a categoricalvariable and one-hot encoded. The subject’s age and gender were in-cluded as features as well. Of the resulting set of features, the top 20were selected using feature-importance ranking in the decisionforest.\\nRobust Feature Selection Variant\\nDue to the small size and sparsity of the training dataset, generic fea-ture selection was not robust, and the selected features (along withthe performance of the resulting model) fluctuated from run to rundue to the stochastic nature of the learner’s underlying bagging ap-proach. Many ADI-R questions are highly correlated, leading tomultiple competing sets of feature selection choices that were seem-ingly equally powerful during training, but which had different per-formance characteristics when the underlying sampling bias wasexposed via full bootstrapped cross-validation. This resulted in awide performance range of the variant of the Generic ML baselinemethod as shown in Table 3 .Table 1. Dataset Breakdown by Age Group and Condition Type for Each of the Sources of Training Data and for the Clinical Validation\\nSample. The Negative Class Label Includes Normally Developing (i.e. neurotypical) Children as Well as Children with Developmental Delaysand Conditions other than Autism\\nNumber of samples\\nAge Condition Classification type Questionnaire Video Clinical validation\\n(years) training training\\n<4 Autism þ 414 1445 84\\n<4 Other condition /C0 133 231 18\\n<4 Neurotypical /C0 74 308 3\\n/C214 Autism þ 1885 1865 37\\n/C214 Other condition /C0 154 133 11\\n/C214 Neurotypical /C0 26 277 9\\nTable 2. Differences Between Training and Application Environments. These Differences are Expected to Cause Bias that Cannot be Cap-\\ntured by Cross-validation Studies\\nAspect Training Setting Application Setting\\nSource ADI-R and ADOS instrument administered\\nby trained professionals during clinicaleval-uationsShort parent questionnaires displayed on smartphone, and behavior tagging by\\nana-lysts after observing two or three 1-minute home videos uploaded byparents\\nProctor Highly trained medical professionals Parents answering the questionnaires are un-trained, and the analysts evaluating\\nthe home videos are only minimally trained. As a result, their answers may not\\nbe as consistent, objective, or reliable\\nSetting Clinic setting with highly standardized and\\nsemi-structured interactionsAt home. Not possible to recreate the structured clinical environment, resulting in\\nan undesired variability of the output signals. Subjects might also behave differ-\\nently at the clinic than at home, further amplifying the bias\\nDuration The ADI-R can take up to 4 hours to com-\\nplete; The ADOS can take up to 45\\nminutes of direct observation by trainedprofessionalsUnder 10 minutes to complete the parent questionnaire, and a few minutes of\\nhome video. As a result, some symptoms and behavioral patterns might be pre-\\nsent but not observed. Also causes big uncertainty about the severity and fre-quency of observed symptoms\\nQuestionnaires Sophisticated language involving psycholog-\\nical concepts, terms, and subtleties unfa-\\nmiliar to nonexpertsSimpliﬁed questions and answer choices result in less nuanced, noisier inputs1002 Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8', metadata={'source': 'papers/Abbas_2018.pdf', 'page': 2}),\n",
       "  Document(page_content='Robust feature selection overcame that limitation using a two-\\nstep approach. First, a 100-count bootstrapped feature selectionwas run, with a weight balanced 90% random sample selected ineach iteration. The top 20 features were selected each time, and arank-invariant tally was kept for the number of times each featuremade it to a top-20 list. Next, the top 30 features in the tally werekept as candidates and all other features were discarded. A finalfeature-selection run was used to pick the best subset of these can-didate features. This approach was found to be more robust to sta-tistical fluctuations, usually selecting the same set of features whenrun multiple times. A minimal subset of maximally performant fea-tures was chosen and locked for clinical validation, totaling 17 fea-tures for the young children and 21 features for the old. Detailsabout these selected features are available in the Supplementary\\nMaterial .\\nAge Silo Variant\\nThis variant built upon the improvements of the robust feature selec-\\ntion method, by exploiting of the dichotomy between pre-phrasal\\nand fully-phrasal language capability in at-risk children. Languagedevelopment is significant in this domain as it is known to affect thenature in which autism presents, and consequently the kinds of be-havioral clues to look for in order to screen for it.\\nThis variant achieved better performance by training separate\\nclassifiers for children in the younger and older age groups ofTable 1 . The age dichotomy of <4,/C214 was chosen to serve as the\\nbest proxy for language ability. Feature selection, model parameter-tuning, and cross-validation were run independently for each agegroup classifier. Before siloing by age group, the classifier was lim-ited to selecting features that work well across children of both de-velopmental stages. Siloing enabled the classifiers to specialize onfeatures that are most developmentally appropriate within each agegroup.Severity-level Feature Encoding Variant\\nBuilding upon the method including age siloing above, this variant\\nachieved better performance by replacing one-hot feature encodingwith a more context-appropriate technique. One-hot encoding doesnot distinguish between values that correspond to increasing levels of\\nseverity of a behavioral symptom, and values that do not convey a clearconcept of severity. This is especially troublesome since a typical ADI-R instrument question includes answer choices from both types of val-ues. For example, ADI-R question 37, which focuses on the child’s ten-dency to confuse and mix up pronouns, allows for answer codes 0, 1,2, 3, 7, 8, and 9. Among those choices, 0 through 3 denote increasingdegrees of severity in pronominal confusion, while 7 denotes any othertype of pronominal confusion not cove red in 0-3 regardless of severity.\\nCodes 8 and 9 denote the non-applicability of the question (for exam-ple, to a child still incapable of phrasal speech) or the lack of an answer(for example, if the question was skipped) respectively. When codingthe answers to such questions, generic one-hot encoding would allowfor non-symptomatic answer codes to be selected as screening featuresbased on phantom correlations present in the dataset.\\nSeverity-level encoding converts all answer codes that do not con-\\nvey a relevant semantic concept to a common value, thereby reducing\\nthe chance of useless feature selection, and reducing the number of fea-tures to choose from. In addition, severity-level encoding condenses thesignal according to increasing ran ges of severity. For example, the\\nencoding of ADI-R question 37 would map its responses to new fea-tures with 1s in the following cases (all other new features would bezero): (0 !“¼0,” 1!“1,” 2![“1,” “2”], 3 ![“1,” “2,” “3”], 7 !\\n“¼7,” 8, 9 !None ). This more closely resembles the way medical prac-\\ntitioners interpret such answer choi ces, and helps alleviate the problem\\nof sparsity over each of the one-hot encoded features in the dataset.\\nAggregate Features Variant\\nBuilding upon the method including severity level encoding above,this variant achieved better performance by incorporating aggregateTable 3. Performance of Increasingly Effective Classiﬁer Variants Based on the Training Data for the Parent Questionnaire. Results in the\\nTop Table are Based on Cross-validated Training Performance. Results in the BottomTable (which are only available for variants using theoptimally selected features) are Based on Actual Clinical Results\\nAUC Sensitivity Specificity\\nAll ages <4 years >¼4 years All ages <4 years >¼4 years All ages <4 years >¼4 years\\nTraining scenario\\nGeneric ML baseline 0.932 to\\n0.9500.928 to\\n0.9530.928 to\\n0.9530.976 to\\n0.9820.975 to\\n0.9840.975 to\\n0.9840.628 to\\n0.6450.625 to\\n0.6480.625 to\\n0.648\\nRobust feature selection\\nvariant0.958 0.958 0.958 0.982 0.982 0.982 0.624 0.624 0.624\\nAge silo variant 0.953 0.939 0.961 0.962 0.939 0.977 0.777 0.774 0.779\\nSeverity-level feature\\nencoding variant0.965 0.950 0.974 0.962 0.912 0.993 0.748 0.833 0.692\\nAggregate features variant 0.972 0.987 0.963 0.992 0.988 0.994 0.754 0.894 0.661\\nWith inconclusive\\nallowance [up to 25\\\\%]0.991 0.997 0.983 1.000 1.000 1.000 0.939 0.977 0.881\\nApplication scenario\\nAge silo variant 0.62 0.68 0.54 0.65 0.62 0.52 0.48 0.46 0.24\\nSeverity-level feature\\nencoding variant0.67 0.69 0.64 0.64 0.62 0.58 0.48 0.46 0.33\\nAggregate features variant 0.68 0.73 0.68 0.68 0.69 0.65 0.57 0.62 0.48\\nWith inconclusive\\nallowance [up to 25\\\\%]0.72 0.72 0.73 0.70 0.72 0.67 0.67 0.71 0.53Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8 1003', metadata={'source': 'papers/Abbas_2018.pdf', 'page': 3}),\n",
       "  Document(page_content='features such as the minimum, maximum, and average severity level,\\nas well as number of answer choices by severity level across thequestions corresponding to the 20 selected features. These new fea-tures were especially helpful due to the sparse, shallow, and wide na-ture of the training set, whereupon any semantically meaningfulcondensation of the signal can be useful to the trained classifier.\\nInconclusive Results Variant\\nChildren with more complex symptom presentation are known topose challenges to developmental screening. These children oftenscreen as false positives or false negatives, resulting in an overalldegradation of screening accuracy that is observed by all standardmethods and has become acceptable in the industry. Given that ourlow-cost instruments do not rely on sophisticated observations todifferentiate complex symptom cases, our approach was to avoidassessing them altogether, and to try instead to spot and label themas “inconclusive.”\\nBuilding upon the method including feature engineering, two\\nmethods to implement this strategy were devised. The first was totrain a binary classifier with a continuous output score, then replacethe cutoff threshold with a cutoff range, with values within the cut-off range considered inconclusive. A grid search was used to deter-mine the optimal cutoff range representing a tradeoff betweeninconclusive determination rate and accuracy over conclusive sub-jects. The second approach was to train and cross-validate a simplebinary classifier, label the correctly and incorrectly predicted sam-ples as conclusive or inconclusive respectively, and then build a sec-\\nond classifier to predict whether a subject would be incorrectly\\nclassified by the first classifier. At runtime, the second classifier wasused to spot and label inconclusives. The conclusives were sent forclassification by a third, binary classifier trained over the conclusivesamples only. Both methods for labeling inconclusive results yieldedsimilar performance. Therefore, the simpler method of using athreshold range in the machine learning output was used to reportinconclusive results for this paper.\\nThe inconclusive rate is a configurable model parameter that\\ncontrols the tradeoff between coverage and accuracy. Throughoutthis paper, the inconclusive rate for this variant was set to 25%.\\nVideo\\nThe second of our two-method approach to autism screening is anML classifier that uses input answers about the presence and severityof target behaviors among subjects. This information was providedby an analyst upon viewing two or three 1-minute home videos ofchildren in semi-structured settings that are taken by parents ontheir mobile phones. The classifier was trained on item-level data\\nfrom two of the ADOS modules (module 1: preverbal, module 2:\\nphrased speech) and corresponding clinical diagnosis.\\nTwo decision forest ML classifiers were trained corresponding to\\neach ADOS module. For each classifier, 10 questions were selectedusing the same robust feature selection method, and the same allow-ance for inconclusive outcomes was made as for the parental ques-tionnaire classifier. Each model was independently parameter-tunedwith a bootstrapped grid search. Class labels were used to stratifythe cross-validation folds, and (age, label) pairs were used to weight-balance the samples.\\nProblems related to the change of environment from training to\\napplication are especially significant in the case of video screeningbecause ADOS involves a 45 minute direct observation of the childby experts, whereas our screening was based on unsupervised shorthome videos. Specifically, we expect the likelihood of inconclusive\\nor unobserved behaviors and symptoms to be much higher in the ap-plication than in the training data, and the assessed level of severityor frequency of observed symptoms to be less reliable in the applica-tion than in the training data. The following improvements weredesigned to help overcome these limitations.\\nPresence of Behavior Encoding\\nTo minimize potential bias from a video analyst misreading the se-verity of a symptom in a short cell phone video, this encodingscheme improves feature reliability at the expense of feature infor-mation content by collapsing all severity gradations of a questioninto one binary value representing the presence vs absence of the be-havior or symptom in question. Importantly, a value of 1 denotesthe presence of behavior, regardless of whether the behavior is indic-ative of autism or of normalcy. This rule ensures that a value of 1corresponds to a reliable observation, whereas a 0 does not necessar-ily indicate the absence of a symptom but possibly the failure to ob-serve the symptom within the short window of observation.\\nMissing Value Injection to Balance the Nonpresence of Features for\\nthe Video Screener Training DataWhile collapsing severity gradations into a single category over-comes noisy severity assessment, it does not help with the problemof a symptom not present or unnoticeable in a short home video.For this reason, it is important that the learning algorithm treat avalue of 1 as semantically meaningful, and a value of 0 as inconse-\\nquential. To this end, we augmented the training set with duplicate\\nsamples that had some feature values flipped from 1 to 0. The injec-tion of 0s was randomly performed with probabilities such that thesample-weighted ratio of positive to negative samples for which thevalue of any particular feature is 0 is about 50%. Such ratios ensurethat the trees in a random forest will be much less likely to drawconclusions from the absence of a feature.\\nCombination\\nIt is desirable to combine the questionnaire and video screeners toachieve higher accuracy. However, the needed overlapping trainingset was not available. Instead, the clinical validation dataset itselfwas used to train the combination model.\\nThe numerical responses of each of the parent questionnaire and\\nvideo classifiers were combined using L2-regularized logistic regres-sion, which has the advantage of reducing the concern of overfitting,particularly given the logistic model has only three free parameters.Bootstrapping and cross -validation studies showed that any overfit-ting that may be present from this procedure is not detectable withinstatistical limitations. Since each of the individual methods wassiloed by age, separate combination algorithms were trained per agegroup silo. For each combination algorithm, optimal inconclusiveoutput criteria were chosen using the logistic regression response,using the same techniques as for the parental questionnaire andvideo classifiers. The performance characteristics of the overallscreening process compared to standard alternative screeners are\\nshown below.\\nRESULTS\\nParent Questionnaire Performance on Training Data\\nBootstrapped cross-validation performance metrics for the optimally\\nparameter-tuned version of each of the variants of the parental1004 Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8', metadata={'source': 'papers/Abbas_2018.pdf', 'page': 4}),\n",
       "  Document(page_content='questionnaire are reported in the top of Table 3 . The results for\\nbaseline variant are reported as a range rather than a single value,\\nbecause the unreliability of generic feature selection leads to differ-ent sets of features selected from run to run, with varying perfor-mance results.\\nParents of children included in the clinical study answered short,\\nage-appropriate questions chosen using the robust feature selectionmethod discussed above. The clinical performance metrics for each ofthe classification variants that build upon that feature selection schemea r es h o w ni nt h eb o t t o mo f Table 3 . The difference in performance be-\\ntween the training and validation datasets is driven by the differencesthat are emphasized in Table 2 . See below and the results of Table 4\\nfor a discussion of the statistical significance of these results.\\nROC curves in Figure 1 show how our parent questionnaire clas-\\nsification approach outperforms some of the established screeningtools like MCHAT and CBCL on the clinical sample. Since clinicalcenters are usually interested in screening tools with a high sensitiv-ity, we have drawn shaded regions between 70% and 90% sensitiv-ity to aid the eye.\\nCombination Screening Performance on Clinical Data\\nROC curves in Figure 2 show how combining the questionnaire and\\nvideo classifiers into a single assessment further boosted perfor-mance on the clinical study sample. When up to 25% of the mostchallenging cases are allowed to be determined, inconclusive the per-formance on the remaining cases is shown in Figure 3 . Note that the\\nROC curves in these figures for M-CHAT contain only younger chil-dren (mostly under four years of age) due to the fact that this instru-\\nment is not intended for older children. A same-sample comparison\\nbetween M-CHAT and the ML screeners can be seen in the agebinned figures ( Figures 4 and5).\\nResults for Young Children\\nYoung children are of particular interest given the desire to identify\\nautism as early as possible. Results restricted to only children lessthan four years old are shown in Figures 4 and5.Statistical Significance\\nFor the training data, sample sizes are large enough that statistical\\nlimitations are minimal. However, results reported for the clinicaldata have significant statistical limitations. In this section we com-pare the performance of the screening algorithms on the clinicaldata that we have discussed in this paper: (1) the questionnaire-based algorithm of,\\n13(2) M-CHAT, (3) CBCL, (4) the\\nquestionnaire-based algorithm of this paper, and (5) the combined\\nquestionnaire plus video algorithm of this paper. Direct comparisonsin performance between many of these algorithms are reportedalong with statistical significances in Table 4 .\\nDISCUSSION\\nWe have introduced a novel machine learning algorithm based on a\\nparental questionnaire and another based on short home videosrecorded by parents and scored by a minimally trained analyst. Wehave discussed pitfalls such as data sparsity, and the mixed ordinaland categorical nature of the questions in our training data. Wehave also identified several important confounding factors that arisefrom differences between the training and application settings of the\\nalgorithms. We have shown novel feature encoding, feature selec-\\ntion, and feature aggregation techniques to address these challenges,and have quantified their benefits. We have shown the benefits ofallowing some subjects with lower certainty output from the algo-rithms to be classified as inconclusive. We have also shown the bene-fits of combining the results of the two algorithms into a singledetermination.\\nBy specializing the machine learning models on a dichotomy of\\nage groups, we found that the screener for younger children capital-ized on non-verbal behavioral features such as eye contact, gestures,and facial expressions, while the screener for older children focusedmore on verbal communication and interactions with other children.For more details please refer to the Supplementary Material .\\nThe methods and resulting improvements shown in this paper\\nare expected to translate well into other clinical science applicationsTable 4. Performance Comparisons Between Various Algorithms on Clinical Data\\nBase model Model from this paper AUC improvement Mean recall improvement\\n2012 publication Questionnaire 0.07, [ /C00.03, 0.17] 0.1, [0.02, 0.17]\\nM-CHAT Questionnaire 0.01, [ /C00.11, 0.12] 0.06, [ /C00.04, 0.17]\\nCBCL Questionnaire 0.06, [ /C00.04, 0.17] 0.11, [0.03, 0.2]\\n2012 publication Questionnaire & video 0.16, [0.07, 0.25] 0.12, [0.04, 0.2]\\nM-CHAT Questionnaire & video 0.08, [ /C00.03, 0.19] 0.1, [ /C00.01, 0.21]\\nCBCL Questionnaire & video 0.15, [0.04, 0.26] 0.14, [0.04, 0.24]\\n2012 publication Questionnaire þinconclusive 0.16, [0.02, 0.28] 0.09, [ /C00.02, 0.2]\\nM-CHAT Questionnaire þinconclusive /C00.01, [/C00.39, 0.31] 0.08, [ /C00.18, 0.29]\\nCBCL Questionnaire þinconclusive 0.15, [0.01, 0.29] 0.11, [ /C00.02, 0.24]\\n2012 publication Questionnaire & video þinconclusive 0.21, [0.1, 0.32] 0.19, [0.1, 0.28]\\nM-CHAT Questionnaire & video þinconclusive 0.09, [ /C00.05, 0.23] 0.15, [0.04, 0.27]\\nCBCL Questionnaire & video þinconclusive 0.2, [0.09, 0.32] 0.2, [0.09, 0.31]\\nQuestionnaire Questionnaire & video 0.09, [0.02, 0.15] 0.03, [ /C00.04, 0.09]\\nQuestionnaire Questionnaire þinconclusive 0.09, [ /C00.01, 0.17] /C00.0, [/C00.09, 0.08]\\nQuestionnaire Questionnaire & video þinconclusive 0.14, [0.06, 0.23] 0.09, [0.01, 0.17]\\nQ. and video Questionnaire & video þinconclusive 0.06, [0.01, 0.11] 0.06, [0.0, 0.13]\\nEach row evaluates the improvement of one of the algorithms from this paper over a “Base model” algorithm for the AUC metric, and for the average between\\nthe autism and the non-autism recalls at a response threshold point that achieves approximately 80% sensitivity. Negative values would represent a w orsening of\\nperformance for a given algorithm compared to the base model. Both average values of the improvements and [5%, 95%] conﬁdence intervals are reported. Algo-\\nrithms that are labeled “inconclusive” allow up to 25% of the most difﬁcult samples to be discarded from the metric evaluation. Note that the M-CHAT ins tru-\\nment is intended for use on younger children. Therefore, older children were excluded when preforming comparisons to M-CHAT in this table.Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8 1005', metadata={'source': 'papers/Abbas_2018.pdf', 'page': 5}),\n",
       "  Document(page_content='Figure 1. ROC curves on the clinical sample for various questionnaire based autism screening techniques, ordered from the least to most sophisticated. Note th at\\nunlike Figures 2 through 3and4, 168 children are included in this sample (six children did not have videos available).\\nFigure 2. ROC curves on the clinical sample for the questionnaire and the video based algorithms, separately and in combination. The established screening too ls\\nMCHAT and CBCL are included as baselines.\\nFigure 3. ROC curves on the clinical sample for the questionnaire and the video based algorithms, separately and in combination. Inconclusive determination i s\\nallowed for up to 25% of the cases. The established screening tools MCHAT and CBCL are included as baselines.\\nFigure 4. ROC curves on the clinical results for children under four years of age, for the questionnaire and the video based algorithms, as well as the combinatio n.\\nComparisons with the established (nonmachine learning) screening tools MCHAT and CBCL are also shown.1006 Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8', metadata={'source': 'papers/Abbas_2018.pdf', 'page': 6}),\n",
       "  Document(page_content='including screening for cognitive conditions such as dementia for the\\nelderly and physical conditions such as concussions in adults. Fur-ther, we expect that these methods would apply well to any othersurvey based domain in which the application context is differentfrom the training context.\\nSignificant further improvements may be possible. Initial studies\\nhave identified probable improvements to the machine learningmethodology as well as improved methods for handling the biasesbetween the training data and application settings. A new clinicaltrial with larger sample sizes is underway that will make it possibleto validate new improvements resulting from these studies as well asto improve confidence in the high performance of our algorithms.\\nCONCLUSION\\nMachine learning can play a very important role in improving the ef-fectiveness of behavioral health screeners. We have achieved a sig-nificant improvement over established screening tools for autism inchildren as demonstrated in a multi-center clinical trial. We havealso shown some important pitfalls when applying machine learningin this domain, and quantified the benefit of applying proper solu-\\ntions to address them.\\nFUNDING\\nThis research received no specific grant from any funding agency in the pub-\\nlic, commercial or not-for-profit sectors.\\nCOMPETING INTERESTS\\nAll authors are affiliated with Cognoa Inc. in an employment and/or\\nadvisory capacity.\\nCONTRIBUTORS\\nAll listed authors contributed to the study design as well as the draft-ing and revisions of the paper. All authors approve of the final ver-sion of the paper to be published and agree to be accountable for allaspects of the work.SUPPLEMENTARY MATERIAL\\nSupplementary material is available at Journal of the American\\nMedical Informatics Association online.\\nREFERENCES\\n1. Durkin MS, Maenner MJ, Meaney FJ. Socioeconomic inequality in the\\nprevalence of autism spectrum disorder: evidence from a U.S. cross-\\nsectional study. PLoS One 2010; 5 (7): e11551.\\n2. Christensen DL, Baio J, Braun KV, et al. Prevalence and characteristics of\\nautism spectrum disorder among children aged 8 years—autism and devel-\\nopmental disabilities monitoring network, 11 sites, United States, 2012.\\nMMWR Surveill Summ 2016; 65 (3): 1–23.\\n3. Zwaigenbaum L, Bryson S, Lord C, et al . Clinical assessment and\\nmanagement of toddlers with suspected autism spectrum disorder:insights from studies of high-risk infants. Pediatrics 2009; 123 (5):\\n1383–91.\\n4. BernierMao RA, Yen J. Diagnosing autism spectrum disorders in primary\\ncare. Practitioner 2011; 255 (1745): 27–30.\\n5. Achenbach TM, Rescorla LA. Manual for the ASEBA School-Age Forms\\n& Proﬁles . Burlington, VT: University of Vermont, Research Center for\\nChildren, Youth, & Families. 2001.\\n6. Lord C, Rutter M, Le Couteur A. Autism diagnostic interview-revised: a\\nrevised version of a diagnostic interview for caregivers of individuals with\\npossible pervasive developmental disorders. J Autism Dev Disord 1994;\\n24 (5): 659–85.\\n7. Lord C, Rutter M, Goode S, et al. Autism diagnostic observation schedule:\\na standardized observation of communicative and social behavior. J Au-\\ntism Dev Disord 1989; 19 (2): 185–212.\\n8. Lord C, Petkova E, Hus V, et al. A multisite study of the clinical diagnosis\\nof different autism spectrum disorders. Arch Gen Psychiatry 2012; 69 (3):\\n306–13.\\n9. Wall DP, Dally R, Luyster R, et al. Use of artiﬁcial intelligence to shorten\\nthe behavioral diagnosis of autism. PLoS One 2012; 7 (8): e43855.\\n10. Duda M, Kosmicki JA, Wall DP, et al . Testing the accuracy of an\\nobservation-based classiﬁer for rapid detection of autism risk. Transl Psy-\\nchiatry 2014; 4 (8): e424.\\n11. DudaDaniels MJ, Wall DP. Clinical evaluation of a novel and mobile au-\\ntism risk assessment. J Autism Dev Disord 2016; 46 (6): 1953–1961.\\n12. Fusaro VA, Daniels J, Duda M, et al. The potential of accelerating early\\ndetection of autism through content analysis of youtube videos. PLoS One\\n2014; 16;9 (4): e93533.\\n13. Cognoa, Inc. Palo Alto: CA. https://www.cognoa.com/.\\nFigure 5. ROC curves on the clinical results for children under four years of age, for the questionnaire and the video based algorithms, as well as the combinatio n,\\nrestricted to the children who were not determined to have an inconclusive outcome (tuned to have at most 25% allowed to be inconclusive). Comparisons with\\nthe established (nonmachine learning) screening tools MCHAT and CBCL are also shown.Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8 1007', metadata={'source': 'papers/Abbas_2018.pdf', 'page': 7})],\n",
       " [Document(page_content='1 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreportsMulti-modular Ai Approach to \\nStreamline Autism Diagnosis in \\nYoung children\\nHalim Abbas  1, ford Garberson  1, Stuart Liu-Mayo  1, eric Glover1* & Dennis p . Wall  2\\nAutism has become a pressing healthcare challenge. the instruments used to aid diagnosis are time \\nand labor expensive and require trained clinicians to administer, leading to long wait times for at-risk \\nchildren. We present a multi-modular, machine learning-based assessment of autism comprising three \\ncomplementary modules for a unified outcome of diagnostic-grade reliability: A 4-minute, parent-report questionnaire delivered via a mobile app, a list of key behaviors identified from 2-minute, semi-structured home videos of children, and a 2-minute questionnaire presented to the clinician at the time of clinical assessment. We demonstrate the assessment reliability in a blinded, multi-site clinical \\nstudy on children 18-72 months of age (n = 375) in the United States. It outperforms baseline screeners \\nadministered to children by 0.35 (90% CI: 0.26 to 0.43) in AUC and 0.69 (90% CI: 0.58 to 0.81) in \\nspecificity when operating at 90% sensitivity. Compared to the baseline screeners evaluated on children less than 48 months of age, our assessment outperforms the most accurate by 0.18 (90% CI: 0.08 to 0.29 at 90%) in AUC and 0.30 (90% CI: 0.11 to 0.50) in specificity when operating at 90% sensitivity.\\nIdiopathic forms of Autism Spectrum Disorder (ASD)\\xa0have no known biological cause and may correspond to \\nmultiple conditions with similar symptoms. The incidence of ASD has increased in recent years, and it impacts 1 \\nin 59 children according to the latest studies\\n1. ASD is diagnosed from clinical observations according to standard \\ncriteria2 relating to the child’s social and behavioral symptoms. Autism is said to be on a spectrum due to the \\nvaried severities of symptoms, ranging from relatively mild social impairment to debilitating intellectual disabil-ities, inabilities to change routines and severe sensory reactions\\n2. Approximately 25–50%3 of autistic children are \\nnon-verbal and\\xa0have severe symptoms.\\nNotably, diagnosis within the first few years of life dramatically improves the outlook of children with autism, \\nas it allows for treatment during a key window of developmental plasticity4,5. Unfortunately, the latest studies \\nshow that although 85% of parents of children with autism reported developmental concerns about their chil-dren by 36 months of age, the median age of diagnosis in the United States is 52 months\\n1. The complexity of the \\ndiagnostic procedures and the shortage of trained specialists can result in children with ASD not\\xa0getting a diag-nosis\\xa0early enough to receive behavioral\\xa0therapies during the time when they are most effective.\\nDiagnosing autism in the United States generally takes two steps: developmental screening followed by com-\\nprehensive diagnostic evaluation if screened positive\\n6. Screening instruments typically use\\xa0questionnaires that \\nare answered by a parent, teacher or clinician7,8. They are\\xa0generally easy and inexpensive to administer and can \\nbe useful to flag some at-risk children, however, they are not always\\xa0accurate enough to help inform a diagnosis9. \\nStandard autism screeners can also have a high false positive rate, leading to unnecessary referrals and healthcare costs\\n10. Comprehensive diagnostic evaluation instruments, on the other hand, are more accurate but require long \\nand expensive\\xa0interactions with highly trained clinicians11,12.\\nIn this paper, we present improvements to\\xa0two previously published13 automated autism assessment modules \\nunderlying the Cognoa14 software. The first module is based on a brief questionnaire about the child presented \\ndirectly to parents without supervision. The second module is based on lightly\\xa0trained analysts evaluating short videos of children within their natural environment that are captured by parents using a mobile device. We also present a new, third module that is intended to be completed in a primary care setting such as a pediatrician’s office during a clinic visit. The third module is based upon a questionnaire that is answered by a clinician after \\nexamining the child and talking to the parent. We demonstrate that these three modules are as fast and easy to \\n1Cognoa Inc., Palo Alto, CA, USA. 2Departments of Pediatrics, Biomedical Data Science and Psychiatry and \\nBehavioral Sciences, Stanford University, Stanford, CA, USA. *email: eri_g@ericglover.comopen', metadata={'source': 'papers/Abbas_2020.pdf', 'page': 0}),\n",
       "  Document(page_content='2 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/administer as most of the typical screening instruments, yet their combined assessment accuracy is shown in this \\nwork to be significantly higher, such that they may be used to aid in diagnosis of autism.\\nWe present our approach to selecting maximally predictive features for each of the modules. Both the parent \\nand the clinician questionnaire modules key on behavioral patterns similar to those probed by a standard autism diagnostic instrument, the Autism Diagnostic Interview - Revised (ADI-R)\\n11. ADI-R is administered by a trained \\nclinician, and typically gives consistent results across examiners. But its 93 point questionnaire often spanning 2.5 hours of the interviewer and parent’s time makes it largely impractical for the primary care setting\\n15. The \\nvideo assessment module keys on behavioral patterns similar to those probed in another diagnostic instrument, the Autism Diagnostic Observation Schedule (ADOS)\\n12. ADOS is a multi-modular diagnostic instrument, with \\ndifferent modules for subjects at different levels of cognitive development. It is widely considered a gold standard and is one of the most common behavioral instruments used to aid in the diagnosis of autism\\n16. It consists of an \\ninteractive and structured examination of the child by trained clinicians in a tightly controlled setting.\\nFor validation, the three modules are applied to assess children in a clinical study using the Cognoa14 software. \\nTo-date, Cognoa has been used by over 300,000 parents in the US and internationally. The majority of Cognoa users are parents of young children between 18 and 48 months. The clinical study underlying the validation results discussed in the results section consists of a total of 375 at-risk children who had undergone full clinical examination and received a clinical diagnosis at a center specialized in neurodevelopmental disorders\\n17. The \\noutputs of the assessment modules are compared to those of three screening instruments. The Modified Checklist for Autism in Toddlers, Revised (M-CHAT-R)\\n7 is a parent-completed questionnaire for autism that is intended \\nto be administered during developmental screenings for children between the ages of 16 and 30 months and is commonly used as an autism screening instrument. The Social Responsiveness Scale - Second Edition (SRS) is another standard ASD screener that is based upon a questionnaire filled out by an examiner\\n18–20. The SRS has a \\npreschool form intended for children of ages 30 months to 54 months, and a school age form intended for chil-dren of ages 48 months through 18 years of age. We use SRS “total score” scale as a baseline autism assessment. The Child Behavior Checklist (CBCL)\\n8 is a parent-completed questionnaire that provides risk assessments in \\nmany categories. We use the “ Autism Spectrum Problems” scale of CBCL for comparison. In all cases, the answers to the questions comprising the screeners are coded, then the codes are summed and the sum compared against a threshold to determine whether the child is at risk.\\nMethods\\nWe base our approach on de-identified\\xa0historical patient records. We collect medical instrument score sheet data pertaining to children tested for suspicion of autism, and process those into training sets for the predictive models underlying each of our three autism assessment modules.\\nSince we apply said predictive models in a significantly different setting than the clinics where the correspond-\\ning training data were generated, we expect a consequential performance degradation resulting in unacceptable \\ndiagnostic accuracy if conventional machine learning methods are used\\n13. To counteract that effect, we apply \\ncustom machine learning techniques as detailed in this section, building upon previous experimental work13. The \\nnew techniques discussed below are empirical post-hoc feature selection, training data noise injection, and an \\noverfitting-resilient probabilistic combination of module outcomes.\\nData. Training data were compiled from multiple repositories of de-identified\\xa0ADOS and ADI-R score sheets \\nof children between 18 and 84 months of age including Boston Autism Consortium, Autism Genetic Resource Exchange, Autism Treatment Network, Simons Simplex Collection, and Vanderbilt Medical Center. To counteract class imbalance, the sample set negative class was supplemented with 59 low risk children random-sampled from Cognoa’s user-base, and ADI-R was administered on those additional controls.\\nThe diagnostic accuracy of our modules was measured using data from a multi-site blinded clinical validation \\nstudy (reviewed and approved by Western IRB project number 2202803)\\n17. The study was performed in 2016 and \\n2017 at three tertiary care centers in the United States. Informed consent was obtained from guardians of each child, and all relevant regulations and guidelines were followed. Children enrolled in the study were 18 to 72 months of age, of English-speaking households, and were all referred through the typical referral process for sus-picion of autism. Every child was measured using autism assessment instruments (such as ADOS, M-CHAT-R, \\nand/or CBCL) as appropriate for his or her age. Diagnosis was ultimately ascertained by a licensed health care \\nprovider. Prior to the clinical assessment, parents used the Cognoa mobile app to complete the parent question-naire and video assessment modules, and starting in 2017, a clinician also completed the Cognoa clinician ques-tionnaire. The clinicians were blinded to the results of the assessment rendered by Cognoa. More details on the steps of the clinical study are shown in Fig.\\xa0 1.\\nThe enrollment process in 2016 yielded 162 validation samples, which were used to validate the parent ques-\\ntionnaire and video modules. This same clinical enrollment cohort was used as validation dataset in our previous publication on the subject\\n13. Given the learnings from this dataset, and prior to the extension of the study in 2017, \\nseveral improvements were made to the algorithms including tuning of model thresholds, training combination modules, and performing feature selection for the clinician module which was newly introduced in 2017. The enrollment process in 2017 yielded 213 additional validation participants, bringing the total N to 375 samples over the course of the two years.\\nThe sample breakdown by cohort, age group, and diagnosis for all data used for training and validation is \\nshown in Table\\xa0 1. In both the training and the validation datasets, the majority of the “Not autism” class label is \\ncomposed mostly of children who are diagnosed with an alternate developmental delay (e.g.,\\xa0ADHD or speech and\\xa0language disorder). Since these conditions share many symptoms with autism, this is a particularly challeng-ing sample for classification\\xa0. Only seven of the children in the validation sample are neurotypical, suggesting that this sample will be harder to perform correct classifications on than in the general population.', metadata={'source': 'papers/Abbas_2020.pdf', 'page': 1}),\n",
       "  Document(page_content='3 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/Algorithm methodology.  In this section we explain important aspects of our machine learning methodol-\\nogy that are common to the classifiers underlying each of our three assessment modules.\\nTraining procedure.  Classifier training, feature selection, and optimization were done separately for children \\nunder four years of age and four years of age and over. The parent questionnaire and clinician questionnaire clas-\\nsifiers make predictions based off of answers to questions that probe similar concepts to the ADI-R questionnaire. They were trained using the answers to questions from historical item-level ADI-R score sheets with labels corre-sponding to established clinical diagnoses. The video module makes predictions based off of answers to questions that probe similar concepts to the ADOS instrument, as recorded by video analysts. It was trained using ADOS instrument score sheets and diagnostic labels. Progressive sampling was used to verify sufficient training volume as detailed in the supplementary materials. Gradient boosted decision trees were used for all three modules as they consistently performed better than other options that were considered such as neural networks, support \\nvector machines, and logistic regression. For all models, hyper-parameters were tuned with a bootstrapped grid \\nsearch. In all cases, true class labels (ASD or non-ASD) were used to stratify the folds, and (age, label) pairs were used to weight-balance the samples. More details can be found in the supplementary materials.\\nIn all cases, the machine learning models were trained using historical patient records that correspond to con-\\ntrolled clinical examinations, but focused on application in non-clinical\\xa0settings aimed for brevity, ease-of-use, and/or unsupervised parent usage at home. These differences introduce biases which can be significant enough to ruin the performance of an algorithm if not properly addressed, and which cannot be probed by cross validation. See the supplementary material for further details. New strategies to address these biases are discussed below that result in big improvements in accuracy compared to previous work\\n13.\\nFigure 1. Detailed steps performed during the clinical study described in this document.\\nAge (years) ConditionNumber of samples\\nParent/Clinician \\nmodule trainingVideo module trainingClinical validation 2016Clinical validation 2017\\n<4 Autism 414 1445 75 91\\n<4 Not autism 207 539 20 30\\n≥4 Autism 1885 1865 46 60\\n≥4 Not autism 180 410 21 32\\nTable 1. Dataset breakdown by age group and condition for each of the sources of training data and for the \\nclinical validation sample. Machine learning model training was stratified by age group. Clinical validation 2016 \\nand 2017 samples are used together to evaluate performance of the parent and video modules in this paper, while the clinician module was only available for the clinical 2017 dataset.', metadata={'source': 'papers/Abbas_2020.pdf', 'page': 2}),\n",
       "  Document(page_content='4 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/Inconclusive outcomes.  Each of the three modules predicts one of three assessment outcomes: Positive, negative, \\nand inconclusive. As outlined in Fig.\\xa0 2, support for inconclusive determination is incorporated using a process \\nthat involves three separate machine learning training runs. The first model is trained to make predictions that are \\nused to label the samples in the training data that are the most likely to be misclassified. A second model is then trained using these labels to predict the likelihood of any new samples being misclassified. Finally, only samples \\nthat are likely to be classified correctly are used to train a final, binary autism classifier. Only the latter two models \\nare used at prediction time: the one to identify and filter out samples that should be labeled “inconclusive” , and the other to make a binary prediction of whether the child is autistic in those which are likely to be correctly labeled. More details about how these models are trained are\\xa0available in the supplementary material.\\nparental module.  Initial feature selection . The parental questionnaire probes for a minimal set of highly \\nrelevant child behavioral patterns that are maximally predictive of autism in combination. Care is taken to phrase the questions and answers such that the most reliable signal can be input from everyday parents undertaking the questionnaire via mobile app without clinical assistance.\\nTo that effect, a custom feature selection method is devised involving robust bootstrap-driven backwards \\nsubtraction, the details of which are discussed in a previous publication\\n13. Out of an initial set of 93 questions \\nunder consideration, this produces an optimal set of 17 novel\\xa0questions for children less than four years old, and 21 questions for children four and older.\\nEmpirical post-hoc feature selection refinement. Following the conclusion of the 2016 clinical validation study \\nenrollment, we studied differences in the distribution of answers to each question between the training data and \\nthe validation data that was collected in the 2016 clinical study. While some questions had quite good agree-ment, others show a strong bias towards higher (or lower) severity answer choices in the clinical data than in the training data. Questions for which the mean absolute severity difference was statistically greater than three standard errors (averaged over the autism and the non-autism samples) are rejected. This requirement results in \\nthe exclusion of 4 out of the 17 questions in the younger cohort, and 8 out of 21 questions in the older cohort, and \\nthe models are re-trained (with new hyper-parameter tuning) on the reduced feature set. This further refinement of the selected features minimizes the significant biases due to differences between the training and application environment. See the supplementary material for more details on these differences.\\nThis feature refinement leads to a larger boost in performance compared with\\n13 than any other improvement. \\nThe size of the performance improvement is validated on the held-out sample of children collected during 2017, where the new models show a statistically equivalent increase in performance compared with the 2016 sample.\\nVideo module. The video assessment module consists of a parent upload of 2 or 3 mobile videos, each 1 to \\n2 minutes in length, of the child during play or meal time at home. The underlying algorithm produces autism assessments based upon the responses of at least three minimally-trained analysts who watch the videos and then respond to a behavioral questionnaire.\\nThe data available for training the video module’s classification model are taken from ADOS sessions admin-\\nistered by clinicians in standardized clinical settings. Gradient boosted decision trees are trained keying off of the features identified in the analysis of\\xa0ADOS records. The questionnaires that the video analysts answer are then created to probe for similar behavioral features as those observed in the training data. A challenge of this method-ology is that the module must make predictions in the face of missing features that are not observable in the short videos uploaded by the parents. The video analysts are allowed to skip any questions if not answerable based on \\nFigure 2. An illustration of the methodology for training diagnostic assessment algorithms capable of \\noutputting one of three possible outcomes: “positive” , “negative” , or “inconclusive” . The first binary classifier is only used to assist in training and never at runtime. It is trained to predict binary “autism” vs “not autism” , and these labels are then compared with the true ASD results to label which samples are incorrectly classified. The samples with their “correct” and “incorrect” labels are\\xa0used to train the classifiers at runtime. A “indeterminate” \\nclassifier is trained to predict which samples will have their ASD diagnosis misclassified, which serves as a filter \\nto identify “inconclusive” cases at runtime, while only the predicted “correct” samples are used to train the final binary ASD diagnosis classifier.', metadata={'source': 'papers/Abbas_2020.pdf', 'page': 3}),\n",
       "  Document(page_content='5 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/the posted videos. On average, analysts skipped questions 15% of the time, with big variations among particular \\nquestions. This effect, combined with the large discrepancy in the observation time window from the original clinical examination to the brief home-video version, would result in a big assessment-accuracy degradation unless steps are taken to correct for the bias and variance .\\nWe tackle this problem by introducing bias and variance to the training data in a manner designed to make it statis-\\ntically similar to the video analyst answers on which the assessments will be run. The data from the 2016 clinical study\\n13 \\nis used to develop this methodology, and the performance of the algorithm on the data from the children enrolled in 2017 is used to validate the generalizability of the improvements. Most children who participated in the clinical study are also administered a full ADOS, which provided paired ADOS and video data that we use to determine what\\xa0noise patterns to add. Using these paired data, we construct a probability map for each question-response set describing the \\nways video analysts are likely to respond for a given “true” ADOS response. We then use the mapping as a stochastic \\ntransform to build a new training data set that can be thought of as the results of a hypothetical experiment in which the technicians watch parent-supplied video of the children in the training data and respond accordingly.\\nThe addition of simulated “setting noise” to the classifier training data leads to a larger boost in performance \\ncompared with\\n13 than any other improvement13. Additionally, the optimal parameters for the resulting decision \\ntree models favor larger tree depth. This is as expected, since the new models are expected to make determina -\\ntions as to which features are reliable when present, as well as which features to fall back on when the best features are missing.\\nclinician module.  We introduce a module to screen for autism using questionnaire responses from a cli-\\nnician. A pediatrician might answer these questions during a regular checkup. The questions for the clinician were selected in a similar manner as used for the Parental Module (see the supplementary material for details). Responses from both the parent and the clinician are used in a machine learning module in the same manner as described for the parental questionnaire above. Some key behaviors are probed via questions directed at both the \\nparent and the clinician, but the clinician questions are more nuanced and allow for more subtle answer choices. \\nIn cases where the parent and the clinician give contradictory answers to the same question, the clinician’s answer overrides that of the parent. The clinician module was introduced to the clinical validation study beginning in 2017. Its results are therefore based on a smaller sample size than those of the other modules.\\nFeature selection. In order to create a brief clinician questionnaire appropriate for the primary care setting, \\nmultiple lists of candidate questions are each compiled and ordered using different strategies. The lists are then \\nintersected and prioritized, then the top features in the intersection set are shortlisted. The first list of candidate \\nquestions is prepared by considering those questions from the original medical instruments that had been excluded from the parental questionnaire because they were deemed too difficult for a parent to answer reliably. This list is ranked by feature importance values as measured and ranked by a dedicated offline machine learning training and cross validation run in the same manner as performed for initial parental module feature selection. The second list is prepared from the parental questionnaire questions by simulating the effects of parents over or underestimating answer severities on children with machine learning responses near a decision threshold. Children in the training data for whom the model response was between [0 and 0.1] above the ASD-vs-non ASD decision threshold had their question severities dropped one at a time by one severity value, while children who were between [0 and 0.1] \\nbelow the decision threshold had their question severities raised by one severity category. The questions in this list \\nare then ranked based upon the average size of the resulting shift in model responses. The procedure is repeated for children in the training data between [0.1 and 0.3] above or below the decision threshold. In each case the top 7 questions are selected (with significant overlap). This results to a total of 9 candidate questions for young chil-dren and 10 for older children. The third list is prepared by consulting domain experts for an assessment of the likelihood of each candidate question to benefit from a clinician’s input as a complement to the parent’s input. This method is conducted separately for each of the two age-silo groups, and results in an overall clinician questionnaire of 13 questions for children 18 through 47 month old, and 15 questions for children 4 to six years old.\\nModule combination. Due to limitations on available training data, it is not possible to train a single \\ncombined model that uses the input features from each of the parental, video, and clinician modules. Instead, responses from the modules are each considered to be a probability and combined mathematically\\n21 using the \\nequation:\\n=Σ ∗Σ−− −rI RI I () () (1) combTT 11 1\\nWhere rcomb is the result of the combination, I is a vector of 1s, R is a vector of responses for each module to be \\ncombined, and Σ  is the covariance matrix of the response residuals compared to the true diagnosis. The “training” \\nof the combination module consists of calculating the values of Σ to use in this equation, which is done using the \\nresponses of each module on data from the clinical study. For each child, the Σ  values in the rcomb equation were \\ncalculated with that child excluded. This process is similar to leave-one-out cross validation, and ensures that the results reported for our combination procedure do not suffer from overfitting.\\nSince Eq. (1) produces only a single model response, the determination of “inconclusive” outcomes proceeds \\nin a different manner than for the individual assessment modules. Both a lower and an upper threshold are \\napplied on the combined response. Children with a response less than both thresholds are considered to be \\nnon-ASD, children with a response in between the two thresholds are considered to be inconclusive, and children with a response greater than both thresholds are considered to have ASD. As in the single model cases, the two thresholds can be tuned independently to optimize the sensitivity, specificity, and model coverage.', metadata={'source': 'papers/Abbas_2020.pdf', 'page': 4}),\n",
       "  Document(page_content='6 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/Results\\nEach of the individual Cognoa assessment modules, their combinations, as well as 3 baselines based on \\ncommonly-used autism screening instruments (CBCL, M-CHAT-R, and SRS) are evaluated on the data collected during a blinded clinical study. When the inconclusive determination feature is turned off and all samples are required to be assessed conclusively, the Cognoa assessment modules achieve ROC AUC up to 0.83 and sensi-tivity and specificity up to 80% and 75% respectively. Turning on the inconclusive determination feature with an \\nallowance of up to 30% inconclusive outcomes results in an accuracy improvement over the conclusive samples, \\nwith AUC up to 0.92 with sensitivity and specificity up to 90% and 83% respectively. This performance is shown to be a statistically significant improvement over each of the baselines used for comparison.\\nROC curves in Fig.\\xa0 3 show how the parent module performs individually, as well as in combination with the \\nvideo and clinician modules at a 30% inconclusive rate allowance. Figure\\xa0 4 shows a similar comparison with all \\nthe variants consistently restricted to children under four years of age. ROC curves corresponding to the assess-ment modules with the inconclusive allowance turned off can be found in the supplementary material.\\nStatistical model performance comparisons between assessment modules and baselines are shown in Table\\xa0 2. \\nFor each comparison, the subset of children for whom both screeners were administered are selected (n  in the \\ntable), and 10,000 bootstrapping experiments are run where n children are selected with replacement. The average \\nand [5%, 95%] confidence interval improvements in AUC and the specificity between the screeners are evaluated across all bootstrapping experiments. In the case of specificity the calculation of the improvement is performed using thresholds that are set to achieve 90% sensitivity. \\nTable\\xa0 2 shows that Cognoa modules show an improvement of at least 0.26 in AUC and at least 0.52 in spec-\\nificity compared with the CBCL and SRS-2 screeners at 95% confidence level. Due to the fact that M-CHAT-R screener is only evaluated on younger children the statistical uncertainty in the comparison is larger, however, it also shows an improvement of at least 0.08 in AUC and 0.11 in specificity at 95% confidence level. In these comparisons we allow Cognoa assessment modules to decide to hold aside up to 30% of the hardest cases as \\ninconclusive. The same comparisons when we force the classification on\\xa0all of the hardest cases can be found in \\nTable 3 of the supplementary material.\\nFigure 3. ROC curves on the clinical sample for the parent, video, and clinician modules, separately and in \\ncombination. Inconclusive determination is allowed for up to 30% of the cases. The established screening tools M-CHAT-R, SRS-2 and CBCL are compared as baselines. The ROC curve for the M-CHAT-R baseline instrument only includes children under four years of age since M-CHAT-R is not applicable for older children.\\nFigure 4. ROC curves on kids \\xa0 <\\xa04 years of age in the clinical sample for the parent, video, and clinician \\nmodules, separately and in combination. Inconclusive determination is allowed for up to 30% of the cases. The established screening tools M-CHAT-R, SRS-2 and CBCL are compared as baselines.', metadata={'source': 'papers/Abbas_2020.pdf', 'page': 5}),\n",
       "  Document(page_content='7 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/time to completion comparisons.  A random sample of 529 Cognoa users was used in order to measure \\ntime to completion of each of the Cognoa autism assessment modules. The median time to completion of the \\nparent module was just under 4 minutes. The median time to completion of the clinician module was 1.2 minutes. The median time per video analysts to score a videos was 20 minutes. More details can be found in the supple-mentary material. The results indicate that the parent and clinician modules can be completed in as little time as \\nmost established autism screeners and in some cases much faster, while achieving significantly higher accuracy. \\nThe time required for a video analyst to score a video is more lengthy, however, the turnaround time is faster than for an ADOS administration\\n12 and can be performed by minimally trained analysts as opposed to certified \\nclinical practitioners.\\nDiscussion\\nWe presented a multi-modular assessment consisting of three machine learning modules for the identification of autism via mobile App as well as an evaluation of their performance and time-to-completion in a blinded clinical study. The assessment modules outperform conventional autism screeners, as shown in Table\\xa0 2 and Fig.\\xa0 3. The \\naccuracy of the combined assessment is similar to that of gold-standard instruments such as ADOS and ADI-R\\n22, \\nwithout requiring hours of time from certified clinical practitioners. This suggests the potential for the Cognoa assessment to be useful as an autism diagnostic. The high performance of these modules benefits from the use of the techniques described in this paper to identify and set aside up to 30% of the most challenging samples as inconclusive. The supplementary material of this paper shows that we outperform conventional autism screeners \\nwithout this technique as well.\\nImportant open questions remain. First, in all cases in this paper, the assessment modules were validated on \\nchildren who had been preselected as having high risk of autism. Children that are pre-selected in this way tend to \\nhave autism-like characteristics regardless of their true diagnosis, increasing the challenge of distinguishing true ASD cases. These modules are expected to perform better on a general population sample of children. Further work is needed to verify this hypothesis by conducting clinical studies on children from the general population. Second, the clinician module newly presented in this work appears promising, but so far it has only been applied in a secondary-care setting. Further testing in primary care clinics is needed to validate accuracy in that setting. In addition, two wider avenues of exploration are interesting as further steps. First, while these assessment mod-\\nules have been shown to be effective at identifying the presence or absence of autism, our goal is to extend them \\nto identify the severity of the condition (if present) as well. Second, the techniques presented in this paper could potentially be used to build algorithms for other child behavioral conditions than autism, as well as behavioral conditions affecting adults and seniors.\\nReceived: 12 March 2019; Accepted: 20 February 2020;\\nPublished: xx xx xxxxAge Group Baseline Screener Assessment Module ΔAUC, [0.05, 0.95] C.I.ΔSpecificity at 90% \\nsensitivity, [0.05, 0.95] C.I. n\\nAll ages CBCL Parent 0.17, [0.10, 0.23] 0.21, [0.13, 0.30] 370\\nAll ages SRS-2 Parent 0.20, [0.12, 0.28] 0.21, [0.12, 0.31] 307\\nAll ages CBCL Parent, Video 0.29, [0.22, 0.36] 0.41, [0.30, 0.52] 363\\nAll ages SRS-2 Parent, Video 0.32, [0.24, 0.40] 0.41, [0.30, 0.52] 302\\nAll ages CBCL Parent, Video, and Clinician 0.35, [0.26, 0.43] 0.69, [0.58, 0.81] 200\\nAll ages SRS-2 Parent, Video, and Clinician 0.42, [0.33, 0.50] 0.65, [0.52, 0.78] 175\\n1.5 to 3 y.o. M-CHAT-R Parent −0.01, [−0.10, 0.07] 0.09, [−0.02, 0.21] 209\\n1.5 to 3 y.o. CBCL Parent 0.12, [0.03, 0.22] 0.20, [0.07, 0.34] 214\\n1.5 to 3 y.o. SRS-2 Parent 0.15, [0.03, 0.27] 0.24, [0.12, 0.38] 161\\n1.5 to 3 y.o. M-CHAT-R Parent, Video 0.14, [0.06, 0.22] 0.20, [0.07, 0.34] 204\\n1.5 to 3 y.o. CBCL Parent, Video 0.28, [0.18, 0.38] 0.33, [0.18, 0.49] 209\\n1.5 to 3 y.o. SRS-2 Parent, Video 0.31, [0.20, 0.42] 0.37, [0.21, 0.55] 157\\n1.5 to 3 y.o. M-CHAT-R Parent, Video, and Clinician 0.18, [0.08, 0.29] 0.30, [0.11, 0.50] 107\\n1.5 to 3 y.o. CBCL Parent, Video, and Clinician 0.34, [0.22, 0.45] 0.46, [0.25, 0.67] 111\\n1.5 to 3 y.o. SRS-2 Parent, Video, and Clinician 0.40, [0.27, 0.53] 0.50, [0.28, 0.73] 91\\n1.5 to 3 y.o. CBCL Parent 0.21, [0.11, 0.30] 0.23, [0.11, 0.35] 156\\n1.5 to 3 y.o. SRS-2 Parent 0.25, [0.15, 0.36] 0.18, [0.06, 0.31] 146\\n4 to 6 y.o. CBCL Parent, Video 0.30, [0.20, 0.39] 0.49, [0.34, 0.64] 154\\n4 to 6 y.o. SRS-2 Parent, Video 0.33, [0.22, 0.44] 0.44, [0.29, 0.59] 145\\n4 to 6 y.o. CBCL Parent, Video, and Clinician 0.35, [0.23, 0.47] 0.93, [0.83, 1.00] 89\\n4 to 6 y.o. SRS-2 Parent, Video, and Clinician 0.43, [0.30, 0.55] 0.79, [0.66, 0.91] 84\\nTable 2. Statistical tests of performance improvements between models in this paper and standard baseline \\nscreening models. ΔAUC tells us the increase in AUC found in the screeners of this paper across bootstrapping \\nexperiments. ΔSpecificity tells us the increase in the specificity in the bootstrapping experiments at a threshold \\ndesigned to achieve 90% sensitivity. Each Δ calculation shows the average value of the improvement along with the [0.05,\\xa00.95] confidence interval.\\n', metadata={'source': 'papers/Abbas_2020.pdf', 'page': 6}),\n",
       "  Document(page_content='8 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/References\\n 1. Baio, J. et al . Prevalence of autism spectrum disorder among children aged 8 years— autism and developmental disabilities \\nmonitoring network, 11 sites, united states, 2014. MMWR Surveill Summ  67(No. SS-6), 1–23, https://doi.org/10.15585/mmwr.\\nss6706a1 (2018).\\n 2.  Association., A. P . & Association., A. P . Diagnostic and statistical manual of mental disorders : DSM-5 (American Psychiatric \\nAssociation Arlington, V A, 2013), 5th ed. edn.\\n 3.  Patten, E., Ausderau, K. K., Watson, L. R. & Baranek, G. T. Sensory response patterns in nonverbal children with asd. Autism Res. Treat .2013, https://doi.org/10.1155/2013/436286 (2013).\\n 4. Dawson, G. & Bernier, R. A quarter century of progress on the early detection and treatment of autism spectrum disorder. Dev. \\nPsychopathol.  25, 1455–1472, https://doi.org/10.1017/S0954579413000710 (2013).\\n 5.  Dawson, G. et al . Randomized, controlled trial of an intervention for toddlers with autism: The early start denver model. Pediatrics  \\n125, e17–e23 https://doi.org/10.1542/peds.2009-0958, http://pediatrics.aappublications.org/content/125/1/e17.full.pdf (2010).\\n 6.  Gordon-Lipkin, E., Foster, J. & Peacock, G. Whittling down the wait time exploring models to minimize the delay from initial \\nconcern to diagnosis and treatment of autism spectrum disorder. Pediatr. clinics North Am. 63, 851–859 https://doi.org/10.1016/j.\\npcl.2016.06.007 (2016). Exported from https://app.dimensions.ai on 2018/10/19.\\n 7. Bernier, R., Mao, A. & Y en, J. Diagnosing autism spectrum disorders in primary care. Practitioner  255(1745), 27–30 (2011).\\n 8.  Achenbach, T. & Rescorla, L. Manual for the ASEBA preschool forms & profiles. Univ. Vermont, Res. Cent. for Child. Youth & Fam. (2000).\\n 9.  Norris, M. & Lecavalier, L. Screening accuracy of level 2 autism spectrum disorder rating scales: A review of selected instruments. Autism , 14, 263–284, https://doi.org/10.1177/1362361309348071 (2010). PMID: 20591956,\\n 10.  Charman, T. & Gotham, K. Measurement issues: Screening and diagnostic instruments for autism spectrum disorders - lessons \\nfrom research and practise. Child Adolesc. Mental Heal.18, 52–63, https://doi.org/10.1111/j.1475-3588.2012.00664.x.\\n 11. Lord, C., Rutter, M. & Le Couteur, A. Autism diagnostic interview-revised: a revised version of a diagnostic interview for caregivers \\nof individuals with possible pervasive developmental disorders. J. Autism Dev. Disord. 24, 659–685 (1994).\\n 12. Lord, C. et al. Autism diagnostic observation schedule: a standardized observation of communicative and social behavior. J Autism \\nDev Disord  19, 185–212 (1989).\\n 13.  Abbas, H., Garberson, F., Glover, E. & Wall, D. P . Machine learning approach for early detection of autism by combining \\nquestionnaire and home video screening. J. Am. Med. Informatics Assoc. ocy039, 10.1093_jamia_ocy039/1/ocy039 (2018).\\n 14.  Cognoa, Inc. 2390 El Camino Real St 220, Palo Alto, CA 94306 https://www.cognoa.com/.\\n 15.  Wall, D. P ., Dally, R. L., Luyster, R., Jung, J.-Y . & DeLuca, T. F. Use of artificial intelligence to shorten the behavioral diagnosis of autism. PLoS One, https://doi.org/10.1371/journal.pone.0043855. (2012).\\n 16. Lord, C. et al. A multisite study of the clinical diagnosis of different autism spectrum disorders. Arch. Gen. Psychiatry 69, 306–313 \\n(2012).\\n 17.  Kanne, S., Arnstein Carpenter, L. & Warren, Z. Screening in toddlers and preschoolers at risk for autism spectrum disorder: \\nEvaluating a novel mobile-health screening tool. Autism Res . (2017).\\n 18. Moody, E. J. et al . Screening for autism with the srs and scq: Variations across demographic, developmental and behavioral factors \\nin preschool children. J. Autism Dev. Disord. 47, 3550–3561 (2017).\\n 19. Aldridge, F. J., Gibbs, V . M., Schmidhofer, K. & Williams, M. Investigating the Clinical Usefulness of the Social Responsiveness Scale (SRS) in a Tertiary Level, Autism Spectrum Disorder Specific Assessment Clinic. Journal of Autism and Developmental Disorders  \\n42(8), 294–300 (2012).\\n 20. Schanding, G. T., Nowell, K. P . & Goin-Kochel, R. P . Utility of the Social Communication Questionnaire-Current and Social Responsiveness Scale as Teacher-Report Screening Tools for Autism Spectrum Disorders. Journal of Autism and Developmental \\nDisorders  42(8), 1705–1716 (2012).\\n 21. Jacobs, R. A. Methods for combining experts’ probability assessments. Neural Computation  7, 867–888, https://doi.org/10.1162/\\nneco.1995.7.5.867 (1995).\\n 22. Falkmer, T., Anderson, K., Falkmer, M. & Horlin, C. Diagnostic procedures in autism spectrum disorders: a systematic literature \\nreview. Eur. Child & Adolesc. Psychiatry 22, 329–340, https://doi.org/10.1007/s00787-013-0375-0 (2013).\\nAuthor contributions\\nHalim Abbas, Ford Garberson, and Stuart Liu-Mayo were each involved in all aspects of model development, \\noptimization, training, and validation of the models for each of the modules, as well as the writing of this paper. Eric Glover and Dennis Wall provided advice on the development of the algorithms. Halim Abbas, Ford Garberson, Stuart Liu-Mayo and Dennis Wall co-wrote the manuscript.\\ncompeting interests\\nAll authors are affiliated with Cognoa Inc. in an employment and/or advisory capacity.\\nAdditional information\\nSupplementary information is available for this paper at https://doi.org/10.1038/s41598-020-61213-w .\\nCorrespondence and requests for materials should be addressed to E.G.\\nReprints and permissions information is available at www.nature.com/reprints.Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \\ninstitutional affiliations.\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \\nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-ative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \\nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/. © The Author(s) 2020', metadata={'source': 'papers/Abbas_2020.pdf', 'page': 7})],\n",
       " [Document(page_content='RESEA RCH ARTICL E\\nEarly screening ofautism spectrum disorder\\nusing cryfeatures\\nAida Khozaei ID\\n1,Hadi Moradi ID\\n1,2*,Reshad Hosseini ID\\n1,Hamidreza Pouretemad3,\\nBahareh Eskandari3\\n1School ofElectrical andComput erEnginee ring, Univers ityofTehran, Tehran, Iran, 2Intelligent System s\\nResearc hInstitute, SKKU, Suwon, South Korea, 3Department ofPsychology, Shahid Behesh tiUniversity,\\nTehran, Iran\\n*moradih @ut.ac.ir\\nAbstract\\nTheincrease inthenumber ofchildren withautism andtheimportance ofearly autism inter-\\nvention hasprompted researchers toperform automatic andearly autism screening. Conse-\\nquently, inthepresent paper, acry-based screening approach forchildren withAutism\\nSpectrum Disorder (ASD) isintroduced which would provide both early andautomatic\\nscreening. During thestudy, werealized thatASD specific features arenotnecessarily\\nobservable inallchildren withASD andinallinstances collected from each child. Therefore,\\nweproposed anewclassification approach tobeabletodetermine such features andtheir\\ncorresponding instances. Totesttheproposed approach asetofdata relating tochildren\\nbetween 18to53months which hadbeen recorded using high-quality voice recording\\ndevices andtypical smartphones atvarious locations such ashomes anddaycares was\\nstudied. Then, after preprocessing, theapproach wasused totrain aclassifier, using data\\nfor10boys withASD and10Typically Developed (TD) boys. Thetrained classifier was\\ntested onthedata of14boys and7girlswithASD and14TDboys and7TDgirls. Thesensi-\\ntivity, specificity, andprecision oftheproposed approach forboys were 85.71%, 100%, and\\n92.85%, respectively. These measures were 71.42%, 100%, and85.71% forgirls, respec-\\ntively. Itwasshown thattheproposed approach outperforms thecommon classification\\nmethods. Furthermore, itdemonstrated better results than thestudies which used voice fea-\\ntures forscreening ASD. Topilotthepracticality oftheproposed approach forearly autism\\nscreening, thetrained classifier wastested on57participants between 10to18months.\\nThese 57participants consisted of28boys and29girlsandtheresults were veryencourag-\\ningfortheuseoftheapproach inearly ASD screening.\\nIntroduction\\nChildren with Autism Spectrum Disorder (ASD) aredefined bytheir abnormal orimpaired\\ndevelopment insocial interaction and communication, aswell asrestricted and repetitive\\nbehaviors, interests, oractivities [1].The rapid growth ofASD inthepast 20years hasinspired\\nmany research efforts toward thediagnosis and rehabilitation ofASD [2–5]. Inthefield of\\nPLOS ONE\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 1/21a1111111111\\na1111111111\\na1111111111\\na1111111111\\na1111111111\\nOPEN ACCESS\\nCitation: Khozaei A,Moradi H,Hosseini R,\\nPouretem adH,Eskandari B(2020) Early screening\\nofautism spectrum disorder using cryfeatures.\\nPLoS ONE15(12): e0241690. https://do i.org/\\n10.1371/ journal.pone. 0241690\\nEditor: Zhishun Wang, Columbia University,\\nUNITED STATES\\nReceived: November 26,2019\\nAccepted: October 19,2020\\nPublished: December 10,2020\\nPeer Review History: PLOS recognize sthe\\nbenefits oftranspar ency inthepeer review\\nprocess; therefore, weenable thepublication of\\nallofthecontent ofpeer review andauthor\\nresponse salongside final, published articles. The\\neditorial history ofthisarticle isavailable here:\\nhttps://doi.o rg/10.1371/jo urnal.pone.0 241690\\nCopyright: ©2020 Khozaei etal.Thisisanopen\\naccess article distributed under theterms ofthe\\nCreative Commons Attribution License, which\\npermits unrestricte duse,distribu tion,and\\nreproduction inanymedium, provided theoriginal\\nauthor andsource arecredited.\\nData Availabilit yStatement: Theoriginal and\\ncleaned voices andtheirextracted features (the\\ndataset)inthisresearch andtheimplementation\\ncodes oftheproposed method aredeposited inthe\\nfollowing repositories: CodeOcean 10.24433/ CO.', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 0}),\n",
       "  Document(page_content='diagnosis, there areseveral well-established manual methods todiagnose children over 18\\nmonths [6].However, thepractical average ageofdiagnosis isover 3years duetothelack of\\nknowledge about ASD and thelack ofexpertise fordiagnosing autism [7,8].Itisoftheutmost\\nimportance tohave early diagnosis/screening inorder toprovide early intervention which is\\nmore effective atthefirst fewyears oflifethan later on[7,9–11]. Itisshown that early inter-\\nvention improves thedevelopmental performance inchildren with ASD [12]. Ithasalso been\\nreported that early interventions would becost saving forfamilies and thetreatment service\\nsystems [13, 14]. Consequently, there aretwomain questions: 1)canautism bescreened earlier\\nthan 18months toreduce thetypical diagnosis orintervention ageand 2)isitpossible to\\nemploy intelligent methods forthescreening ofautism toeliminate thewidespread need for\\nexperts? Itshould bementioned that ourgoal wastoanswer these questions with respect to\\nscreening allchildren who may nothave clear symptoms. The screened children should go\\nthrough adiagnosis procedure toacquire confirmation and/or becautiously worked with.\\nFortunately, there arestudies intheliterature showing that theageofdiagnosis canbe\\nlower than 18months. Forexample, Thabtah and Peebles [15] reviewed several questionnaire-\\nbased approaches that may beable toscreen ASD above 6months ofage. However, those\\napproaches, likeAutism Diagnostic Interview-Revised (ADI-R) [16] and Autism Diagnostic\\nObservation Schedule (ADOS) [17] which have been clinically proven tobeeffective and ade-\\nquate, aretime-consuming instruments [15] and need trained practitioners tousethem. To\\nreduce thedependency onthehuman expertise needed inusing such questionnaires [8],sev-\\neralstudies proposed machine learning methods toclassify children with ASD [18, 19]using\\nquestionnaires. Their goal wastoautomate theprocess and/or find anoptimum subset of\\nquestions orfeatures. Forinstance, Abbas etal.[20] proposed amulti-modular assessment sys-\\ntem combined ofthree modules, aparent questionnaire, aclinician questionnaire, and avideo\\nassessment module. Although theauthors used machine learning toautomate and improve\\nclassification process, theneed forhuman involvement stillexists inorder toanswer questions\\norassess videos.\\nOntheother hand, Emerson et.alshowed that fMRI [21] canbeused topredict thediagno-\\nsisofautism attheageof2inhigh-risk 6-month-old infants. Denisova and Zhao [22] used\\nmovement data from rs-fMRI from 1–2month-old infants topredict future atypical develop-\\nmental trajectories asbiological features. Furthermore, Bosl, Tager-Flusberg, and Nelson [23]\\nsuggested that useful biomarkers canbeextracted from EEG signals forearly detection of\\nautism. Blood-based markers [24, 25]and prenatal immune markers [26] were also proposed\\ntodiagnose ASD that canbeused right after birth. Although these approaches suggest new\\ndirections towards early ASD diagnosis/screening, they arecostly, require expertness and dedi-\\ncated equipment, which would limit their usage. Furthermore, these methods arestillinthe\\nearly stages ofresearch and require further approval. Finally, approaches which involve meth-\\nodssuch asfMRI orEEG, aredifficult touseonchildren, especially onchildren with autism\\nwho may have trouble following instructions appropriately [27], have atypical behaviors [28],\\norhave excessive head movements [29, 30].\\nThere arestudies that used vocalization-based analysis toscreen children with autism. For\\ninstance, Brisson etal.[31] showed differences invoice features between children with ASD\\nand Typically Developing (TD) children. Several studies, like[32], used speech-related features\\nforthescreening ofchildren older than 2.Toreach thegoal ofearly ASD screening, vocaliza-\\ntions ofinfants under 2years ofagehave been investigated [33–35]. Santos etal.[33] used\\nvocalizations, such asbabbling, toscreen ASD children attheageof18months. They collected\\ndata from 23and 20ASD and TDchildren, respectively. They reported high accuracy of\\naround 97% which canbeduetothefactthat they used k-fold cross-validation without consid-\\nering subject-wise hold outinorder tohave unseen subjects inthetestfold [36]. Oller etal.\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 2/210622770.v1 Harvard Datave rse(Contain sonlyarar\\nfileofsounds): 10.7910/DVN/L STBQW\\nFunding: HMreceived asmall fund forcollecting\\ndataandfordiagnosing thesubjects. Grant\\nnumber 123Cognitive Sciences andTechnology\\nCouncil ofIrancogc.ir Thefunders hadnorolein\\nstudy design, datacollection andanalysis, decision\\ntopublish, orpreparation ofthemanuscript.\\nCompeting interests :Theauthors have declared\\nthatnocompeting interests exist.', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 1}),\n",
       "  Document(page_content='[34] proposed another vocalization-based classification method inwhich they included age\\nand excluded crying. They applied themethod on106TDchildren and 77children with ASD\\nbetween 16to48months and reached 86% accuracy. Pokorny etal.[35] extracted eGeMAPS\\nparameter set[37], which includes 88acoustic parameters, in10month oldchildren. This set\\nconsists ofstatistics calculated for25frequency-related, energy-related, and spectral low-level\\ndescriptors. They reached a75% accuracy onapopulation of10TDchildren and 10children\\nwith ASD.\\nEsposito, Hiroi, and Scattoni [38] showed that crying isapromising biomarker forthe\\nscreening ofASD children. Sheinkopf etal.[39] and Orlandi etal.[40] have shown that there\\naredifferences inthecryofchildren with ASD compared toTDchildren. Tothebest ofour\\nknowledge, ourown group’s preliminary study [41] wastheonly research that hasused cry\\nsounds forthescreening ofchildren with ASD. Weused adataset of5children with ASD and\\n4TDchildren older than twoyears. The accuracy oftheproposed method is96.17% using k-\\nfold cross validation without considering subject-wise hold out, which isashortcoming ofthis\\nstudy. Inother words, ithasbeen overfitted totheavailable data and may failtocorrectly clas-\\nsifynew samples. So,athorough examination using anunseen testsetoncryfeatures isneces-\\nsary toevaluate theresults. Itshould benoted that thedata from ourprevious study [41] could\\nnotbeused inthestudy presented inthispaper duetothedifferences indata collection\\nprocedures.\\nInalltheabove studies, itwasassumed that thespecific sound features, distinguishing chil-\\ndren with ASD from TDchildren, arecommon among alltheASD cases. However, thismay\\nnotbethecase forallthefeatures. Forinstance, tiptoe walking, which isoneoftherepetitive\\nbehaviors ofchildren with ASD, appears inapproximately 25% ofthese children [42]. Conse-\\nquently, inthecurrent study, wepropose anew cry-based approach forscreening children\\nwith ASD. Our screening approach makes useoftheassumption that alldiscriminative charac-\\nteristics ofautism may notappear inallASD children. This assumption isincontrast with the\\nassumption putforward intheordinary instance-based machine learning methods, which\\nassumes that allinstances ofaclass include alldiscriminative features needed forclassification.\\nInourproposed method, atfirst, discriminable instances ofcries, which exist insubsets ofchil-\\ndren with ASD, arefound. Then ituses these instances toselect features todistinguish between\\nthese ASD instances from TDinstances. Itshould bementioned that thefinal selected features,\\ninthisstudy, arecommon among oursetofchildren with ASD between 18to53months of\\nage. These selected features support theexperiential knowledge ofourexperts stating that the\\nvariations inthecries ofchildren with ASD aremore than TDchildren. This approach isdif-\\nferent from theother approaches that either used adataset ofchildren with aspecific age[33,\\n35]orused ageinformation forclassification [34]. The proposed approach hasbeen imple-\\nmented and tested on62participants. The results show theeffectiveness oftheapproach with\\nrespect toaccuracy, sensitivity, and specificity.\\nMethod\\nSince thisstudy wasperformed onhuman subjects, first, itwasapproved bytheethics commit-\\nteeatShahid Beheshti University ofMedical Sciences and Health Services. Alltheparents of\\ntheparticipants were informed about thestudy and signed anagreement form before being\\nincluded inthestudy.\\nParticipants\\nThere were 62participants aged between 18and 53months, who were divided into two\\ngroups, i.e.31ASD and 31TDwith 24boys and 7girls ineach group. Since weexpected to\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 3/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 2}),\n",
       "  Document(page_content='have different vocalization characteristics forboys and girls, thetraining setwasassembled of\\nonly boys, including 10TD, and 10ASD. Inother words, wewanted toeliminate thegender\\neffects onthefeature extraction and model training. Unfortunately, duetothelower number\\nofgirls with ASD intherealworld, notenough data forgirls with ASD could becollected.\\nNonetheless, themodel wasalso tested onthegirls toseehow itwould generalize even on\\nthem.\\nThe inclusion criteria oftheASD participants were: a)being very recently diagnosed with\\nASD based onDSM-5 with noother neurodevelopmental, mental, and intellectual disorder, b)\\nhaving noother known medical orgenetic conditions, orenvironmental factors, and c)not\\nhaving received anytreatment ormedication, orhaving received treatment inlessthan a\\nmonth. There were only twogirls who didnotfallinto these criteria since they hadbeen diag-\\nnosed more than ayear before. The participants’ average language development atthetime of\\nparticipation, which wasassessed based on[43–46], wasequal tochildren between 6to12\\nmonths old. The autism diagnosis procedure started with theGilliam Autism Rating Scale-Sec-\\nond Edition (GARS-2) questionnaire [47] which wasanswered bytheparents. Then the\\nparents were interviewed, based onDSM-5, while theparticipants were evaluated and\\nobserved bytwochild clinical psychologists with Ph.D. degrees. Inaddition, thediagnosis of\\nASD wasseparately confirmed byatleast onechild psychiatrist inadifferent setting. Itshould\\nbenoted that ADOS, which isavery common diagnostic tool isnotadministered widely in\\nIran since there isnoofficial translation ofADOS inFarsi. TDchildren were selected from\\nthose inanagerange similar totheASD participants from volunteer families from their\\nhomes and health centers. They hadnoevidence orofficial diagnosis ofanyneurological or\\npsychological disorder atthetime ofrecording their voices. The children with ASD were older\\nthan 20months with themean, standard deviation, and range of35.6, 8.8,and 33months\\nrespectively. The TDchildren were younger than 51months with themean, standard devia-\\ntion, and range ofabout 30.8, 10.3, and 33months respectively. Itshould bementioned that\\nthediagnosis ofthechildren under 3years wasmainly based onexperts’ evaluation, notthe\\nGARS score. Furthermore, allTDparticipants under 3years ofagehadafollow upstudy when\\nthey passed theageof3,tomake sure theinitial TDassignment wascorrect orstillvalid. Todo\\nso,weused asetofexpert-selected questions based on[48] toassess them through interviews\\nwith parents.\\nTables 1and 2show thedetails oftheparticipants onthetraining and testsets, respectively.\\nIneach table, thenumber ofvoice instances from each participant and thetotal duration ofall\\nitsinstances inseconds areshown incolumns 3and 4,respectively. The recording device cate-\\ngory, i.e.ahigh-quality recorder (HQR) and typical cellphones (CP), isgiven inthedevice cat-\\negory column. The next twocolumns include GARS-2 scores and thelanguage developmental\\nmilestone oftheparticipants with ASD atthetime oftherecording. Insixcases, there were no\\nGARS score available atthetime ofstudy, demonstrated byND(No Data). The column\\nlabeled as‘Place’ shows thelocation oftherecording which canbeinhomes (H), autism cen-\\nters(C1, C2,and C3), and health centers (C4, C5,and C6). There wasatotal number of359\\nsamples forallchildren. 53.44% ofthesamples were from ASD participants and 46.56% were\\nfrom TDparticipants.\\nTwo groups of10TDand 10ASD children were selected fortraining theclassifiers such\\nthat twogroups were asbalanced aspossible with respect toageand therecording device.\\nThus, each child intheTDgroup hadacorresponding child intheASD group around the\\nsame age. Asaresult ofthisdata balancing, weobtained training participants with anage\\nbetween 20and 51months. The mean ages inthetraining setwere 32.7 and 35.2 months for\\nASD and TDparticipants, respectively. The standard deviations are9and 9.9months with the\\nrange of25and 30months forASD and TDparticipants, respectively.\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 4/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 3}),\n",
       "  Document(page_content='Although thisapproach wastrained andtested onchildren older than 18months, wetested the\\nproposed approach on57participants between 10to18months toinvestigate how itworks onchil-\\ndren under 18months. These 57participants consisted of28boys and29girls with themean age\\nof15.2 forboth andstandard deviations of2.8and2.9respectively. Allthese participants were eval-\\nuated atalater date attheageof3orolder, bythesame follow-up procedure, using ourexpert-\\nselected questionnaire. Atthetime ofinitial voice collection, 55ofthese participants hadnoevident\\nordiagnosed disorder. Two ofthem were referred toourexperts duetothepositive results of\\nscreening using ourmethod. The diagnosis orconcerns about thetwomentioned participants, as\\nwell astheparticipants with anyevidence ofhaving abnormality inthedevelopmental milestones\\nduring thefollow-up procedure aresummarized inTable 3.The summary ofdisorders given inthe\\nlastcolumn ofTable 3isbased ontheparental interviews andourexperts’ evaluation. Unfortu-\\nnately, Child5, Child6, andChild7’s parents didnotcooperate inobtaining expert evaluation.\\nData collection and preprocessing\\nAsmentioned earlier, thedata wasrecorded using high-quality devices and typical smart-\\nphones. The high-quality devices were aUX560 Sony voice recorder and aSony UX512F voice\\nrecorder. Tousetypical smartphones, avoice-recording and archiving application wasdevel-\\noped and used onvarious types ofsmartphones. Allvoices, through theapplication orthe\\nhigh-quality recorders, were recorded inwav format, 16bits, and with thesampling rate of\\n44.1 kHz. The reason forusing various devices wastoavoid biasing oftheapproach toaspe-\\ncific device. Similarly, theplace ofrecording wasnotrestricted tooneplace inorder tomake\\ntheresults applicable toallplaces.\\nThe parents and trained voice collectors were asked torecord thevoices inaquiet environ-\\nment. Furthermore, they were asked tokeep therecorders orsmartphones about 25cmfrom\\ntheparticipants’ mouth. Despite theproposed tworecommendations ,there were recordedTable 1.The training setdata ofparticipants .\\nID Age (month) #ofinstances Total duration(s ec) Device GARS score Language milesto ne(month) Place Reason forcryingASDASD1 20 9 7.8 CP 104 0–6 C1 Annoyed/U ncomfort able\\nASD2 24 3 1.5 HQR 83 0–6 C2 Unwillin g\\nASD3 26 5 2.1 HQR 120 0–6 C1 Annoyed/U ncomfort able\\nASD4 28 13 9.1 HQR 121 0–6 C2 Annoyed/U ncomfort able\\nASD5 29 14 26 HQR 89 6–12 C2 Unwillin g/Complain ing\\nASD6 31 4 2.4 HQR 87 0–6 C2 Unwillin g/Complain ing\\nASD7 36 11 11 HQR 87 6–12 C2 Unwillin g/Complain ing\\nASD8 43 2 0.7 CP ND ND C2 Unwillin g\\nASD9 45 3 2.6 CP 72 6–12 C2 Complaini ng\\nASD10 45 4 3.4 CP ND ND H SleepyTDTD1 21 11 14 HQR NA NA H Complaini ng\\nTD2 24 12 12 HQR NA NA C4 Scared/U nwilling\\nTD3 26 2 2.3 HQR NA NA C5 Unwillin g\\nTD4 28 6 13 CP NA NA C5 Scared/U nwilling\\nTD5 36 3 2.6 CP NA NA H Unwillin g/Complain ing\\nTD6 38 3 1.5 HQR NA NA C6 Complaini ng\\nTD7 41 3 2.4 HQR NA NA H Unwillin g\\nTD8 43 3 2.2 CP NA NA H Sleepy\\nTD9 44 2 1.2 CP NA NA H Complaini ng\\nTD10 51 2 1.7 CP NA NA H Complaini ng\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t001\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 5/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 4}),\n",
       "  Document(page_content='voices where therecommendations were notfollowed and didnothave therequired quality.\\nConsequently, those recordings were eliminated from thestudy. Also, allthecrysounds which\\nwere duetopain, hadbeen removed from thestudy since they were similar between theTD\\nand ASD groups.Table 2.The test setinformation .\\nID Age (month) #ofinstances Total duration(S) Device GARS score Language milestone (months) Place Reason forcryingASD\\nBoysASD11 28 12 7.2 HQR 102 0–6 C2 Unwilling/ Uncomfort able\\nASD12 30 18 17.1 HQR ND ND C3 Separation from mother\\nASD13 30 3 2.9 CP ND ND H Unwilling/Sl eepy\\nASD14 31 5 2.3 HQR 73 0–6 C2/HSeparation from mother/Hungr iness\\nASD15 33 3 2.5 HQR 91 0–6 C2 Unwillin g\\nASD16 33 2 2.5 HQR 104 0–6 C1 Annoyed/U ncomfort able\\nASD17 34 1 0.6 HQR 91 0–6 C2 Unwilling/ Complaining\\nASD18 35 2 1.7 HQR 81 ND C1 Annoyed/U ncomfort able\\nASD19 37 1 0.6 HQR 94 12–18 C2 Unwilling/ Complaining\\nASD20 40 19 14 HQR 91 0–6 C1 Annoyed\\nASD21 45 1 0.3 HQR 81 6–12 C2 Unwilling/ Complaining\\nASD22 48 2 1.6 HQR 100 6–12 C2 Annoyed/C omplaining\\nASD23 52 6 3.1 HQR 113 12–18 C2 Unwilling/ Complaining\\nASD24 53 7 5.2 HQR 78 6–12 C1 Annoyed/U ncomfort ableGirlsASD25 25 12 14 HQR 85 0–6 C2 Unwilling/ Complaining\\nASD26 26 5 2 CP 102 0–6 C1 Scared\\nASD27 31 3 1.7 HQR 94 0–6 C2 Unwilling/ Complaining\\nASD28 32 2 1.3 HQR 100 0–6 C2 Unwilling/ Complaining\\nASD29 41 8 3 HQR 102 0–6 C2 Unwilling/ Complaining\\nASD30 45 2 1.2 CP ND ND H Thirsty\\nASD31 49 7 12 CP ND ND H Unwilling/ ComplainingTD\\nBoysTD11 18 4 2 HQR NA NA C4 Scared\\nTD12 18 7 5.1 HQR NA NA C4 Scared/Un willing\\nTD13 19 7 4.2 HQR NA NA C5 Unwillin g\\nTD14 20 9 8 HQR NA NA C5 Unwilling/ Complaining\\nTD15 21 4 1.2 HQR NA NA H Complaini ng\\nTD16 24 3 2.7 HQR NA NA C5 Scared /Unwillin g\\nTD17 24 2 1.5 HQR NA NA C5 Scared/Un willing\\nTD18 24 6 5.1 HQR NA NA C4 Unwilling/ Complaining\\nTD19 24 4 2.4 HQR NA NA C5 Unwilling/ Complaining\\nTD20 24 5 4.2 HQR NA NA C5 Unwilling/ Complaining\\nTD21 29 11 10 HQR NA NA H Unwilling/ Complaining\\nTD22 30 4 2 HQR NA NA C5 Scared/Un willing\\nTD23 30 4 2 CP NA NA H Unwillin g\\nTD24 43 12 11 HQR NA NA H Complaini ngGirlsTD25 24 5 6 HQR NA NA C4 Unwilling/ Complaining\\nTD26 25 2 4.4 HQR NA NA C5 Scared\\nTD27 29 5 5 HQR NA NA C5 Scared\\nTD28 33 2 2.1 CP NA NA H Complaini ng\\nTD29 45 16 11 HQR NA NA H Unwilling/ Complaining\\nTD30 50 6 7 HQR NA NA H Complaini ng\\nTD31 51 2 0.7 CP NA NA H Unwillin g\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t002\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 6/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 5}),\n",
       "  Document(page_content='After data collection, there wasapreprocessing phase inwhich only pure crying parts ofthe\\nrecordings, with noother types ofvocalization, were selected. Toexplain more, theparts ofcry\\nsounds which were accompanied byscreaming, saying words/other vocalizations, orthat\\noccurred with closed/non-empty mouth were eliminated. Allsegmentations and eliminations\\nwere done manually using Sound Forge Pro11.0. From theselected cries, thebeginning and\\ntheend, which contained voice rises and fades, were removed inorder tojustkeep thesteady\\nparts ofthecries; thisprevents having toomuch variation inthevoice which canlead to\\nunsuitable statistics. Also, theuvular/guttural parts ofthecries were removed. The reason for\\nthiswasthat webelieve these parts distort thefeature values ofthesteady parts ofavoice. Each\\nremaining continuous segment ofthecries wasconsidered and used asasample (instance) in\\nthisstudy. Finally, since thebasic voice features were extracted from 20milliseconds frames\\n[50], togenerate statistical features ofthebasic features, theminimum length ofthecryseg-\\nments were setto15frames, i.e.300milliseconds. Thus, anycrysamples below 300millisec-\\nonds were eliminated from thestudy. Inthisstudy, thefinal prepared samples were between\\n320milliseconds to3seconds.\\nFeature extraction\\nPrevious studies working onvoice features fordiscriminating ASD children used different setsof\\nfeatures. These methods share several common features likeF0,i.e.thefundamental frequency\\nofavoice, andMel-Frequency Cepstral Coefficients (MFCC), i.e.coefficients which represent\\ntheshort-term power spectrum ofasound [51]. F0hasbeen oneofthemost common features\\nused [31,32,39].However, since ageisanimportant factor affecting F0[52], thisfeature isuseful\\nwhen participants have asimilar age. Ontheother hand, MFCC coefficients andseveral related\\nstatistical values have been reported tobeuseful features inseveral studies [35,41,53].Consider-\\ningtheuseful features reported inprevious studies andthespecifications ofthecurrent study,\\nseveral features were selected tobeused inthiswork that areexplained inthefollowing.\\nInthisstudy, each instance wasdivided into 20milliseconds frames, toextract basic voice\\nfeatures. Weused several features proposed byMotlagh, Moradi, and Pouretemad [41] and by\\nBelalca ´zar-Bolaños etal.[54]. The features used byMotlagh, Moradi, and Pouretemad [41]\\ninclude certain statistics likemean and covariance oftheframe-wise basic features, such as\\nMFCC coefficients, over avoice segment. They also used themean and variance offrame-wise\\ntemporal derivative [55, 56]ofthebasic features. The frame-wise temporal derivative meansTable 3.The participants with anabnormali tyinthefollow-up.\\nID Gender Age (inmonths) Disorder\\natrecording time atfollowing -up time\\nChild1 M 11 11 Developm ental delaya,signs ofgenetic diseases\\nChild2 M 17 17 UNDDb\\nChild3 M 12 40 ASDb\\nChild4 M 12 36 Sensory processi ngdisorderc,several ADHD symptomsb\\nChild5 M 18 40 Languag edelay\\nChild6 M 15 46 Development aldelay symptoms\\nChild7 M 12 43 Development aldelay symptoms\\nUNDD, Unspecif iedNeurodev elopmental Disorder .\\naClinical observa tion byourexpert based on[48].\\nbClinical observation byourexpert based on[1]\\ncClinical observa tion byourexpert based on[49].\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t003\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 7/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 6}),\n",
       "  Document(page_content='thedifference between twoconsecutive frames, which inasense istherate ofchange ofafea-\\nture value inoneframe step. Wemodified thespectral flatness features byincluding therange\\nof125–250 Hzbeside the250–500 Hzrange. This range wasadded tocover awider frequency\\nrange than thenormal children frequency range, which showed tobenecessary intheprocess\\noffeature extraction and selection. Each range isdivided into 4octaves and thespectral flatness\\niscomputed forthose octaves.\\nWeremoved alluninformative and noisy features ofthesetwhich areexplained inthefol-\\nlowing. The mean offrame-wise temporal derivative ofthebasic features isremoved because it\\nisnotameaningful feature and isequal totaking thedifference between thevalue ofthelast\\nand thefirst frames. There aremeans ofthefeatures related totheenergy, such astheaudio\\npower, total loudness, SONE, and thefirst coefficient ofMFCC, that were removed tomake\\ntheclassifier independent oftheloudness/power inchildren’s voices. Zero crossing rate (ZCR)\\nwasomitted too, duetoitsdependency onthenoise intheenvironment.\\nThe second setoffeatures used inthisstudy wasfrom Belalca ´zar-Bolaños etal.[54] because\\nithasphonation features, likejitter andshimmer. Jitter andshimmer, which have been reported\\ntobediscriminative forASD, arelinked toperceptions ofbreathiness, hoarseness, andrough-\\nness [57]. Other features used from Belalca ´zar-Bolaños etal.[54] include glottal features related\\ntovocal quality andtheclosing velocity ofthevocal folds [33]. The mean oflogarithmic energy\\nfeature wasomitted forthesame reason asother energy-related features. Asummary ofthefea-\\ntures, added toorremoved from thesetsby[41] and[54], ispresented inTable 4.\\nThe proposed subset instance classifier\\nToexplain theproposed classifier, itwasassumed that there isatarget group ofparticipants\\nthat wewant todistinguish from therestoftheparticipants, called therest. Furthermore, each\\nparticipant inthetarget group may have several instances that may beused todistinguish the\\ntarget group from therest. Fig1Ashows asituation inwhich allinstances ofallparticipants of\\nthetarget group aredifferentiable using common classifiers that wecallWhole SetInstance\\n(WSI) classifiers. Inthisfigure, thecircles represent ourtarget group andthetriangles represent\\ntherest. The color coding isused todifferentiate between theinstances ofeach participant in\\neach group. Incontrast tothesituation inFig1A,inFig1Bthetarget group cannot easily bedis-\\ntinguished from therest. Insuch asituation, there areinstances oftwoparticipants inthetarget\\ngroup, i.e.theredandbrown circles that arenoteasily separable from theinstances intherest\\n(Case 1).Furthermore, there isaparticipant with noinstances, i.e.theorange circles, easily\\nTable 4.The features and statisti cswhich were added orremoved tothetwo feature sets.\\nFeature removing /adding Reason\\nSecond\\nsetlogarithm icenergy Mean statistic isremoved Classificat iondependen cyonloudness/powe rofcries\\nFirst set Audio power\\nTotal loudness\\nSONE\\nFirst MFCC coefficient\\nZCR The basic feature isremoved The feature’s depende ncyonenvironm ental noise\\nAllbasic features\\napplicab lemean offrame-wise tempora lderivative ofthebasic\\nfeatures isremovedNomeaning forthefeature\\nMFCC Coefficients of14–24 areadded Having higher-order coefficients forvocal cords information as\\nwell asvocal tract\\nSpectral flatness Arange of125–250 Hzisadded Coverin gthelow-frequ ency range ofhuman voice\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t004\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 8/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 7}),\n",
       "  Document(page_content='separable from therest(Case 2).Anexample ofCase 1istiptoe walking inchildren with ASD,\\nwhich iscommon inabout 25% ofthese children [42] who doitmost ofthetime. Anexample\\nofCase 2ischildren with ASD who donottiptoe walk. Inother words, there arechildren with\\nASD who cannot bedistinguished from TDchildren using thetiptoe walking behavior factor.\\nApplying anyWSI classifier may failforthedata type shown inFig1B.Consequently, we\\nproposed SubSet Instance (SSI) classifier that first finds differentiable instances and then trains\\naclassifier onthese instances. Asanexample, theproposed SSIclassifier first tries tofind the\\ncircles ontheleftofthelineinFig1B,using aclustering method. Then, ituses these circles, as\\nexclusiveinstances having aspecific feature common inasubset ofthetarget group, totrain a\\nclassifier separating asubset ofthetarget group.\\nThe steps ofcommon WSI classifiers areshown inFig2A.The steps ofourproposed SSI\\nclassifier areshown inFig2B.IntheSSIclassification approach, after thefeature extraction\\nand clustering steps, foreach cluster, aclassifier istrained toseparate itsexclusive instances\\nfrom theinstances oftherestoftheparticipants. Inthetesting phase, anyparticipant with only\\noneinstance classified inthetarget group (positive instance), isclassified asatarget group’s\\nparticipant. The pseudo-code fortheproposed approach isgiven inAlgorithms 1and 2.\\nAlgorithm 1.Training SSIclassifiers\\nT:set ofall target group instances\\nR:set ofall the rest instances\\nF:set ofall classifiers\\nρ:threshold for the number ofsamples inacluster\\ns:the number ofminimum samples needed inacluster tobeable to\\ntrain aclassifier for it\\nCj:The jthcluster\\nn:number ofclusters\\nF=;\\n1:While9j|Cj| >ρ;while there isacluster bigger than athreshold\\norn=1\\n2: n=n+1;increase the number ofclusters\\n3: Cluster the T+Rinto nclusters Cj,j=1,...,n\\n4: EC={Cj�T}; the set ofclusters ofonly exclusive instances,\\ni.e. exclusive clusters\\nFig1.Two different hypothetic altypes oftwo-dimens ional data ofthetarget group and therest. The instances shown bythewarm-colo redcircles and\\nthecool-color edtriangles areforthetarget group and therest, respective ly.Allinstances belonging toaparticipa nthave thesame color. In(a),allthetarget\\ngroup participan ts’instances aredistinguis hable using aclassifier. In(b),only some instances ofthetarget group participant sareseparable from theother\\ninstances byaclassifier.\\nhttps://d oi.org/10.1371/j ournal.pon e.0241690.g0 01\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 9/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 8}),\n",
       "  Document(page_content='5: IfEC6¼;;check ifthere isany exclusive cluster\\n6: For all CjinECwith |Cj| >s\\n7: Train aclassifier using positive labels c�Cjand negative\\nlabels r�R\\n8: Add the classifier toF\\n9:T¼T\\x00P\\nCj�ECCj;remove the instances ofthe exclusive clusters from\\ntarget group instances\\n10: n=1;set 1tore-start clustering intwo groups onthe remain-\\ning instances\\nAlgorithm 2.Testing SSIclassifiers\\nF:set oftrained classifiers\\nA:set ofsubject instances\\n1:For all instances aofA\\n2: P={a2A|9f, classifies aaspositive instance}\\n3: IfP6¼;\\n4: The participant isfrom the target group\\n5: Else\\n6: The participant isfrom the rest\\nIntheproposed training algorithm oftheSSIapproach, thegoal istofind clusters contain-\\ningtheASD instances only. Then aclassifier istrained using theinstances ofthese clusters and\\nFig2.Anoverall view ofWSI and SSImethods. (a)InWSI method, after feature extraction, aclassifier istrained onallinstances and\\nmajor itypooling (MP) isusually used inthetesting phase. Inthisstudy Best-chan cethreshold Pooling (BP), which isathreshold -based\\npooling with thethreshold giving thebest accuracy onthetestset,isalso used togive thebest chance toWSI classifier. (b)Inthe\\npropos edSSIclassifier, after feature extraction, clustering isapplied tofind and select exclusive instances containing instances ofthe\\ntarget group participa ntsonly. Then classifiers aretrained using exclusive instances, andaparticipant isclassified inthetarget group in\\nthetesting phase ifanyclassifier detects apositive instance forit.\\nhttps:// doi.org/10.137 1/journal.pone. 0241690.g002\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 10/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 9}),\n",
       "  Document(page_content='added toalistofalltrained classifiers (lines 7and 8ofAlgorithm 1).Asshown intheloop of\\nthealgorithm, starting atline1,thedata isclustered starting with twoclusters. Then thenum-\\nberofclusters isincreased until acluster, containing only thetarget group instances, emerges.\\nThe exclusive instances insuch acluster areremoved from thesetofalltarget group’s\\ninstances, and theloop isrestarted. Before restarting theloop, ifthenumber ofinstances in\\nthiscluster ismore than athreshold, anew classifier using these instances istrained and this\\nclassifier isadded tothesetofalltrained classifiers. The loop stops when thenumber ofsam-\\nples ineach cluster islessthan athreshold.\\nFortesting theparticipants, using thetrained classifiers, alltheinstances ofeach participant\\nareclassified onebyoneusing allthetrained classifiers (line 2ofAlgorithm 2).Asubject would\\nbeclassified inthetarget group ifatleast oneofitsinstances isclassified inthetarget group at\\nleast byoneoftheclassifiers (lines 3and4ofAlgorithm 2).Otherwise, ifthere isnoinstance\\nclassified among thetarget group, theparticipant isclassified astherest(lines 5and6).\\nDetails oftheimplementations\\nThe classifiers were implemented inPython using scikit-learn library.\\nWSI classifiers. Wehave tested several common WSI classifiers, butwereport only the\\nresult ofSVM with RBF kernel and with nofeature selection, which gives thebest average\\naccuracy. Itshould benoted that several feature selection approaches, likeL1-SVM and back-\\nward elimination, were tested butthey only reduced theaccuracy. Weused group 5-fold cross-\\nvalidation fortuning hyper-parameters. Group K-fold means that allinstances ofeach partici-\\npant areplaced inonly oneofthefolds. This prevents having thesame participant’s instances\\ninthetrain and validation folds simultaneously. Ineach fold, there were twoASD and twoTD\\nparticipants. Itshould bementioned that before applying thealgorithms, webalanced the\\nnumber ofinstances ofthetwogroups using upsampling.\\nTwo approaches were exploited tocombine thedecisions ondifferent samples ofapartici-\\npant intheWSI approach. The first approach wasmajority pooling which classifies apartici-\\npant asASD, ifthenumber ofinstances classified asASD aremore than 50percent ofall\\ninstances. The second approach wasthreshold-based pooling which issimilar tothefirst\\napproach except that athreshold other than 50isused.\\nSSIclassifiers. Before applying thealgorithm, webalanced thenumber ofinstances ofthe\\ntwogroups byupsampling. The threshold fortheminimum number ofsamples, needed ina\\ncluster, tobeable totrain aclassifier issetto10.Itshould bementioned that agglomerative\\nclustering and decision tree arethemethods used forclustering and classification parts of\\nAlgorithm 1,respectively.\\nTraining theSSIclassifiers. After running Algorithm 1onourdata, twoexclusive clus-\\nterswith enough instances, i.e.atleast 10instances inourstudy, were found. Then twoclassifi-\\nerswere trained corresponding toeach cluster. One ofthese exclusive clusters had11\\ninstances from 4ASD participants (Table 1).These 11instances consisted of6outof9\\ninstances ofASD1, 2outof4instances ofASD10, 1outof2instances ofASD8, and 2outof4\\ninstances ofASD6. Asexplained inthealgorithm, foreach cluster, adecision tree classifier was\\ntrained using theASD instances inthecluster versus allTDinstances. Interestingly, only one\\nfeature wasenough todiscriminate instances inthecluster from allTDinstances. Among\\nthose features that candiscriminate thecluster’s instances, weselected theVariance ofFrame-\\nwise Temporal Derivative (VFTD) ofthe7thMFCC coefficient asthefeature which candis-\\ncriminate more ASD participants from thesetofallparticipants with asimple threshold. The\\nclassifier obtained bysetting athreshold based onthisfeature wasthefirst classifier. This fea-\\nture supports ourexpert’s report regarding thehigher variations inthecrysounds ofASD\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 11/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 10}),\n",
       "  Document(page_content='children than TDchildren. From 10ASD children, 8ofthem canbediscriminated using this\\nfeature. Foreach participant, thenumber ofinstances found bythisclassifier isshown inthe\\n2ndcolumn ofTable 5.\\nAfter excluding theASD samples from thefirst classifier, thesecond classifier wastrained\\nbased onthesecond exclusive cluster. This cluster included allinstances ofparticipant ASD4.\\nThe only feature used forclassifying thiscluster wasVFTD ofthe6thSONE coefficient. SONE\\nisaunit ofloudness which isasubjective perception ofsound pressure [58]. Having higher\\nVFTD ofthe6thSONE coefficient confirms theexperiential knowledge ofourexperts men-\\ntioned before. Among alltheASD participants, eight hadinstances with VFTD ofthe6th\\nSONE higher than athreshold (Shown inthe3rdcolumn ofTable 5).The results ofclassifica-\\ntion based onthese twofeatures aredepicted inFig3.Asmentioned intheproposed method\\nsection, theparticipants with atleast oneinstance classified into thiscluster would beconsid-\\nered asaparticipant with ASD.\\nResults\\nInthispart, theperformance ofourproposed SSIclassifier against acommon WSI classifier is\\nevaluated onourtestsetofASD and TDparticipants. Each participant hasmultiple instances\\nwhich arecleaned using thecriteria explained inthedata collection and preprocessing section.\\nThe participants who hadatleast oneaccepted instance were used inthetraining and testing\\nphases, which areshown inTables 1and 2.\\nThe output oftheSSIapproach wastwoclassifiers, each ofthem works bysetting athresh-\\noldbased onafeature. The number ofinstances ofASD participants inthetraining set,cor-\\nrectly detected bythefirst and thesecond classifiers, areshown inthesecond and third\\ncolumns ofTable 5,respectively. Ontheother hand, thebest-resulting classifier fortheWSI\\napproach wasRadial Basis Function-Support Vector Machine (RBF-SVM) [59].\\nThe classification results onthetestsetfordifferent classifiers areshown inTable 6.The\\nportion ofeach participant’s instances, correctly classified byeach classifier, iswritten asa\\npercentage under thename oftheclassifier. The decision made bytheWSI and SSIclassifi-\\nersforeach participant isshown byASD orTD. Toclassify each subject using theWSI clas-\\nsifier, theMajority Pooling (MP) and theBest-chance threshold Pooling (BP) approaches\\nwere used. BPisathreshold-based pooling with thethreshold giving thebest accuracy on\\nthetestsetformale participants. Fortheboys, MPhasspecificity, sensitivity, and precision\\nequal to100%, 35.71%, and 67.85%, respectively. Ontheother hand, BPleads tospecificity,\\nsensitivity, and precision equal to85.71%, 71.42%, and 78.57%, respectively. The threshold\\nTable 5.The number ofinstanc esofeach participant inthetraining setthat areclassified asASD using each\\ntrained SSIclassifier.\\nID First SSIclassifier Second SSIclassifier\\nASD1 8 3\\nASD2 1 2\\nASD3 3 1\\nASD4 10 9\\nASD5 0 0\\nASD6 1 3\\nASD7 1 0\\nASD8 1 2\\nASD9 0 1\\nASD10 2 4\\nhttps://d oi.org/10.1371/j ournal.pon e.0241690.t00 5\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 12/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 11}),\n",
       "  Document(page_content='forBPwas setto20% that means if20% ofinstances ofaparticipant were classified asASD\\ninstance, theparticipant was classified ashaving ASD. The results ofthepercentage of\\ninstances correctly classified bythetwo classifiers intheSSIapproach areshown asC1(the\\nfirst SSIclassifier) and C2(the second SSIclassifier) inTable 6.The aggregated result ofthe\\ndecisions byC1and C2makes thefinal decision oftheSSIclassifier which isshown inthe\\ndecision column, under theSSIclassification section. The achieved specificity, sensitivity,\\nand precision using theproposed method fortheboys are100%, 85.71%, and 92.85%,\\nrespectively.\\nTofurther show theapplicability oftheproposed approach togirls, weapplied theboys’\\ntrained classifiers onthetestsetofthegirls. The results areshown inthelastrow ofTable 6\\nwhich show that theMPapproach hasspecificity, sensitivity, and precision equal to100%,\\n71.42%, and 85.71%, respectively. Furthermore, theBPapproach gives specificity, sensitivity,\\nand precision allequal to85.71%, respectively. The results oftheproposed SSIclassifier is\\n100% specificity, 71.42% sensitivity, and 85.71% precision.\\nAtwo-dimensional scatter plot ofthetwofeatures, used inC1and C2classifiers, areshown\\ninFig4.Ascanbeseen inthisfigure, theinstances ofaparticipant with ASD arescattered in\\nthearea containing instances ofboth TDand ASD participants. Nevertheless, there are\\ninstances forthisparticipant uniquely distinguishable using theselected twofeatures.\\nWecompared theresults ofourproposed method with that oftheonly method available\\nintheliterature which was trained using only cryfeatures [41] based onourdata. The\\nresults (Table 7)show thesuperiority ofourmethod, compared tothepreviously proposed\\nmethod.\\nInvestigating thetrained classifier onparticipants under 18months\\nThe SSIclassifier which was trained using thetraining setinTable 1was also tested onthe\\ndata ofchildren younger than 18months. From 57participants under 18months, two boys\\n(Child1 and Child2 inTable 3)were classified asASD bythementioned trained classifier.\\nThese participants were referred toourexperts fordiagnosis. These two were suspected of\\nhaving neurodevelopmental problems. Allother boys were classified asTD. However,\\namong them, Child3 was diagnosed with ASD attheageof2.Also, Child4 showed\\nFig3.Two classifiers trained onthetwo exclusive clusters found during theSSIclassifier training phase. (a)The Variance ofFrame-w iseTemporal Derivati ve\\n(VFTD) ofthe7thMFCC coefficient separates 27instances of8ASD subjects from allTDinstances ofthetraining set.(b)VFTD ofthe6thSONE coefficient\\nseparates 17instances of7ASD participants from allTDinstances ofthetraining set.\\nhttps://d oi.org/10.1371/j ournal.pon e.0241690.g0 03\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 13/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 12}),\n",
       "  Document(page_content='symptoms ofhaving ADHD and sensory processing disorder attheageof3.Three other\\nchildren had symptoms which suggested that they arenotTDchildren. Two ofthegirls\\nwho were 18months oldwere classified asASD, using thetrained classifier. The other girls\\nwere classified asTD. The results oftesting thetrained SSIclassifier onthis data setare\\nsummarized inTable 8.\\nThe original and cleaned voices and their extracted features (the data set)inthisresearch\\nand theimplementation codes oftheproposed method aredeposited inthefollowing\\nrepositories:\\nCodeOcean\\n10.24433/CO.0622 770.v1\\nHarvard Dataverse (Contains only ararfileofsounds):\\n10.7910/DVN/LSTBQ WTable 6.The results ofclassifier sontheinstances ofeach participa ntinthetest set.\\nTDchildren Children with ASD\\nID Portion ofinstances classified asTDasapercentag eand the\\ndecisionID Portion ofinstances classified asASD asapercentage and the\\ndecision\\nWSI classificat ion SSIclassificat ion WSI classificat ion SSIclassificat ion\\nSVM Dec. C1 C2 Dec. SVM Dec. C1 C2 Dec.\\nMP BP MP BPBoysTD11 100 TD TD 100 100 TD ASD11 50 ASD ASD 17 50 ASD\\nTD12 100 TD TD 100 100 TD ASD12 33 TD ASD 11 28 ASD\\nTD13 100 TD TD 100 100 TD ASD13 33 TD ASD 33 0 ASD\\nTD14 100 TD TD 100 100 TD ASD14 20 TD ASD 20 20 ASD\\nTD15 100 TD TD 100 100 TD ASD15 0 TD TD 0 40 ASD\\nTD16 100 TD TD 100 100 TD ASD16 50 ASD ASD 100 0 ASD\\nTD17 100 TD TD 100 100 TD ASD17 0 TD TD 0 100 ASD\\nTD18 83 TD TD 100 100 TD ASD18 50 ASD ASD 50 50 ASD\\nTD19 100 TD TD 100 100 TD ASD19 0 TD TD 0 0 TD\\nTD20 80 TD ASD 100 100 TD ASD20 42 TD ASD 42 16 ASD\\nTD21 100 TD TD 100 100 TD ASD21 100 ASD ASD 0 0 TD\\nTD22 100 TD TD 100 100 TD ASD22 0 TD TD 0 50 ASD\\nTD23 75 TD ASD 100 100 TD ASD23 33 TD ASD 33 17 ASD\\nTD24 92 TD TD 100 100 TD ASD24 86 ASD ASD 86 86 ASD\\nAcc. % 100 85.71 100 35.71 71.42 85.71GirlsTD25 100 TD TD 100 100 TD ASD25 42 TD ASD 17 0 ASD\\nTD26 100 TD TD 100 100 TD ASD26 60 ASD ASD 60 20 ASD\\nTD27 100 TD TD 100 100 TD ASD27 50 ASD ASD 0 0 TD\\nTD28 100 TD TD 100 100 TD ASD28 100 ASD ASD 0 50 ASD\\nTD29 100 TD TD 100 100 TD ASD29 62 ASD ASD 50 50 ASD\\nTD30 67 TD ASD 100 100 TD ASD30 100 ASD ASD 50 50 ASD\\nTD31 100 TD TD 100 100 TD ASD31 0 TD TD 0 0 TD\\nAcc. % 100 85.71 100 71.42 85.71 71.42\\nEach classifier result onaparticipant ’sinstances isreported asapercentage.\\nDec., Decision; MP, Majority Pooling; BC,Best-chance threshold Pooling; C1,Classifier1; C2,Classifier2 ;Acc., Accuracy.\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t006\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 14/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 13}),\n",
       "  Document(page_content='Discussion andconclusion\\nInthispaper, wepresented anovel cry-based screening method todistinguish between chil-\\ndren with autism and typically developing children. Intheproposed method, groups ofchil-\\ndren with autism who have specific features intheir crysounds canbedetermined. This\\nmethod isbased onanew classification approach called SubSet Instance (SSI) classifier. An\\nappealing property oftheproposed SSIclassifier, inthecase ofvoice-based autism screening,\\nisitshigh specificity such that anormal child canbedetected with noerror. Weapplied the\\nproposed method onagroup ofparticipants consisting of24boys with ASD between 20and\\nFig4.Instances ofseveral ASD and TDparticipa ntsscattered inthespace oftwo features given bytheproposed SSImethod. The\\ninstances ofachosen ASD participant areillustrated ingreen toshow that aparticipant may have instances inthearea common with TD\\ninstances besides those twoareas separated bytheselected thresholds asASD. The mention edASD participant (with green instances) is\\ntagged asASD, duetohaving atleast oneinstance with thegreater value than atleast oneofthethresho ldsonthetwofeatures.\\nhttps:// doi.org/10.1371 /journal.pone. 0241690.g004\\nTable 7.Compariso noftheresults onthetest setusing thetwo methods; SSIapproac hand abaseline approach.\\nSensitivity Specificity PrecisionBoysSSI 85.71% 100% 92.85%\\nBaseline 50.58% 81% 65%GirlsSSI 71.42% 100% 85.71%\\nBaseline 21% 86.48% 53%\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t007\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 15/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 14}),\n",
       "  Document(page_content='53months ofageand 24TDboys between 18and 51months ofage. The twofeatures, found\\ninthisstudy, were used totrain aclassifier on10boys with ASD and 10TDboys. Then, the\\nclassifier wasused todistinguish 14boys with ASD from 14TDboys, reaching 92.8% accuracy.\\nDue tothefactthat girls arelesslikely tohave autism and consequently, itisharder tocollect\\nenough data from girls than boys, thenumber ofgirls with ASD wasnotsufficient totrain a\\nseparate classifier forthisgender. Itshould benoted that wetested thetrained system on7\\ngirls with ASD and 7TDgirls. Itwasseen that thetrained classifier canscreen girls with 7%\\nlower accuracy than boys ofthetestset.Inother words, itseems that gender differences should\\nbeconsidered inthetraining ofthesystem. Intesting thedata from participants under 18\\nmonths, oneTDgirlwasclassified asASD which wasnotthecase foranyTDchildren ofthe\\nmale counterparts. This result also confirms theaforementioned point about thegender effect.\\nHowever, infuture work, wewould trytocollect more data ongirls tobeable totrain asystem\\ntoaccurately screen girls. Furthermore, wewould also trytotrain asingle classifier forboys\\nand girls todetermine whether itcanbeused forboth ofthem.\\nItshould bementioned that ourtraining and testdata were completely separate, tomake\\nthetrained model more general. The features found inthisstudy areapplicable intheage\\nrange ofourparticipants from 18to53months. This isincontrast toother approaches that\\neither used adataset ofchildren with aspecific age[33, 35]orused ageinformation forclassifi-\\ncation [34]. Due totheageinvariant features found inthisstudy, itcanbeclaimed that there\\naremarkers inthevoices ofchildren with ASD that aresustained atleast inarange ofages.\\nThe twodiscriminative features, found inthisstudy, were acoefficient ofMFCC andaSONE\\ncoefficient. MFCC andSONE arerelated tothepower spectrum ofaspeech signal. SONE mea-\\nsures loudness inspecific Bark bands [56]. Ontheother hand, MFCC, which istheinverse DFT\\noflog-spectrum intheMel scale, isrelated tothetimbre ofthevoice [60]. Therefore, MFCC and\\nSONE canbeinterpreted toberelated tothetimbre andloudness ofatone. Furthermore, based\\nonthefeedback from ourexperts, there isunpredictability inthecrying sound ofchildren with\\nautism which isnotthecase forTDchildren. Consequently, weused thevariance oftemporal\\ndifference asafeature suitable forscreening children with autism. This isduetothefactthat ifa\\nsignal isconstant orchanges linearly over time, thevariance oftemporal difference iszero.\\nTherefore, thevariance oftemporal difference canbeseen astheamount ofambiguity orunpre-\\ndictability ofasound. Ontheother hand, theheightened variability inthetwofeatures, found in\\nthisstudy, forchildren with ASD issignificant duetothereports from other studies [22,61]\\nwhich shows increased biological signals variability inchildren with ASD andinfants athigh\\nriskforautism incomparison with TDchildren. These features arestatistical features ofthecry\\ninstances that hold constant, atleast, across anagerange studied inthisresearch.\\nTothebest ofourknowledge, [34] and [35] were theonly studies onscreening children\\nwith autism using voice features onchildren younger than 2years ofage. Our proposed\\nmethod hashigher precision than these two, i.e.6%more than [34] and 17% more than [35],\\nusing only cryfeatures. The useofcryfeatures assuitable biomarkers forautism screening\\nmatches theclaims in[38].Table 8.Classificati onoftheparticipa ntsunder 18months using our trained SSIclassifier.\\nBoys Girls\\nASD TD OthersaASD TD Othersa\\nClassified asASD 0 0 2 0 1 0\\nClassified asTD 1 22 4 0 27 0\\naOther developm ental ormental disorders\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t008\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 16/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 15}),\n",
       "  Document(page_content='Inthepresent study only children with ASD and TDchildren were tested. Other develop-\\nmental disorders orhealth issues were nottested toseehow children with such disorders\\nwould beclassified using theproposed method which candecrease thespecificity of100%.\\nHowever, thisapproach isproposed tobeused asascreening tool and thefinal diagnosis\\nshould bedone under experts’ supervision. So,thisapproach canbeapplied asageneral\\nscreener ofautism spectrum disorder.\\nThe trained classifier wasalso tested on57participants between 10to18months ofage. The\\nclassifier screened twoboys from therest, i.e.Child1 andChild2 (Table 3).Child1 showed evi-\\ndences ofgenetic disease andwasdiagnosed with developmental delay andChild2 received\\nUNDD classification byourexperts. This suggests that a)thesystem canbeused forchildren\\nunder 2years ofage, andb)itmay beable todistinguish other neurodevelopmental disorders.\\nOntheother hand, there were 5boys, i.e.Child3 toChild7 (Table 3),who hadnoevidence of\\nmental ordevelopmental disorders atthetime oftheir recording. Atthesame time, ourapproach\\ndidnotdistinguish them aschildren with ASD either. However, when they were older than 3\\nyears, they showed symptoms ofneurodevelopmental disorders. Out ofthese children, wecould\\nmanage tocollect new recordings from Child3 andChild4 that were classified aschildren with\\nASD using ourapproach. Unfortunately, Child5, Child6, andChild7 didnotcooperate and\\ncould notbeevaluated byanexpert tovalidate theresults ofourexpert-selected questionnaire.\\nFurthermore, theparents refused tocooperate send ustheir children’s recent crysounds.\\nThe result ofstudying these 57children under theageof18months may suggest that: a)\\nthere could besymptoms inthecrying sounds ofchildren with neurodevelopmental disorders\\nunder 18months (Child1 and Child2), b)theapproach may notbeable toscreen aparticipant\\nwith neurodevelopmental disorders under theageof18months duetothepossibility that: 1)\\ntheparticipant wasamong those children with neurodevelopmental disorders who donot\\nhave ourproposed specific features intheir crying sounds, 2)theparticipant’s recorded cry\\nsamples didnotinclude ourspecific features, and/or 3)neurodevelopmental disorders and\\ntheir features hadnotbeen developed inthechild atthetime ofinitial recording. The reason\\nbehind notclassifying Child3 and Child4, aschildren with ASD under theageof18,could be\\nb.2orb.3.Toclearly determine anyreason behind thisphenomena, afurther investigation is\\nneeded.\\nWebelieve that thisapproach canbeused toperform early autism screening under 18\\nmonths ofage. Thus, inthefuture, weneed tocollect data and testtheapproach onmore data\\nofchildren under 18months tovalidate these results with more confidence.\\nWehave tofurther check theproposed approach and theextracted features onother neuro-\\ndevelopmental disorders, such asADHD, toevaluate thecapability oftheapproach todistin-\\nguish thechildren with these disorders from TDchildren.\\nFurthermore, without comparing thecrysounds ofchildren with ASD tothose without\\nASD butanother disorder, wedonotreally know ifthese findings arespecific toautism orto\\ngeneral atypical brain developments. Thus, weshould collect crysounds ofchildren with other\\nneurodevelopmental disorders and compare voices ofchildren with ASD tovoices ofchildren\\nwith other neurodevelopmental disorders toseeifthese features would beable toseparate\\nthem ornot.\\nIthasbeen demonstrated that crying consists ofintricate motor activities [62]. Ontheother\\nhand, ithasbeen shown that children with ASD have problems inthemotor domain and in\\ncoordination oftheir motor capabilities with other modalities [63]. Consequently, itispossible\\nthat theextracted features inthecrying sounds ofchildren with ASD come from thisdefi-\\nciency/problem inthemotor domain which requires further investigations.\\nFinally, automating thepreprocessing part isatechnical issue that should beaddressed ifit\\nisdeemed necessary that thecry-based screening befully automated. This isimportant since\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 17/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 16}),\n",
       "  Document(page_content='such ascreening system canbedeployed insystems such asAmazon Alexa [64] toautomati-\\ncally screen problematic crysounds.\\nAcknowledgmen ts\\nWewould liketothank theCenter forTreatment ofAutism Disorder (CTAD) and itsmem-\\nbers forsupporting thisstudy. Wewould also liketothank allthefamilies who helped this\\nresearch bytaking thetime tocollect thecrysounds oftheir children. The authors would also\\nliketoexpress their gratitude toProf. H.Sameti from Sharif University ofTechnology forhis\\nvaluable and constructive feedbacks onthedata collection and voice processing.\\nAuthor Contributions\\nConceptualization: Aida Khozaei, Hadi Moradi, Reshad Hosseini, Hamidreza Pouretemad,\\nBahareh Eskandari.\\nData curation: Aida Khozaei.\\nFormal analysis: Aida Khozaei.\\nFunding acquisition: Hadi Moradi.\\nInvestigation: Hadi Moradi.\\nMethodology: Aida Khozaei.\\nProject administration: Hadi Moradi.\\nSoftware: Aida Khozaei.\\nSupervision: Hadi Moradi.\\nValidation: Aida Khozaei.\\nVisualization: Aida Khozaei.\\nWriting –original draft: Aida Khozaei, Hadi Moradi, Reshad Hosseini.\\nWriting –review &editing: Aida Khozaei, Hadi Moradi, Reshad Hosseini, Hamidreza Poure-\\ntemad, Bahareh Eskandari.\\nReferences\\n1.American Psychiatric Association .Diagnostic andstatistical manual ofmental disorders (DSM-5®).\\nAmerican Psychiatric Pub; 2013.\\n2.Chen JL,Sung C,PiS.Vocational rehabilitatio nservice pattern sandoutcomes forindividuals with\\nautism ofdifferent ages. JAutism DevDisord. 2015; 45(9):3015 –29. https://doi.or g/10.100 7/s10803-\\n015-2465- yPMID: 25982310\\n3.Fakhoury M.Autistic spectrum disorders: Areview ofclinical features, theories anddiagnosis .IntJDev\\nNeurosci. 2015; 43:70–7. https://doi.o rg/10.1016/j.ij devneu.2 015.04.003 PMID: 25862937\\n4.Constantino JN,Charma nT.Diagnosis ofautism spectrum disorder: reconciling thesyndrome, its\\ndiverse origins, andvariation inexpression .Lancet Neurol. 2016; 15(3):279– 91.https://do i.org/10.\\n1016/S1474- 4422(15)0 0151-9 PMID: 264977 71\\n5.Caldero niS,Billeci L,Narzisi A,Brambilla P,Retico A,Muratori F.Rehabilitat iveintervent ionsandbrain\\nplasticity inautism spectrum disorde rs:focus onMRI-bas edstudies. Front Neurosci. 2016; 10:139.\\nhttps://doi.or g/10.338 9/fnins.201 6.00139 PMID: 270657 95\\n6.Brentani H,Paula CSd, Bordini D,Rolim D,Sato F,Portolese J,etal.Autism spectrum disorders :an\\noverview ondiagnos isandtreatment. Braz JPsychiatry. 2013; 35:S62 –S72. https://doi.or g/10.159 0/\\n1516-4446- 2013-S104 PMID: 241421 29\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 18/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 17}),\n",
       "  Document(page_content='7.Mandell DS,Novak MM, Zubritsky CD.Factors associat edwithageofdiagnosis among children with\\nautism spectrum disorde rs.Pediatrics. 2005; 116(6):148 0–6. https://doi. org/10.1542/p eds.2005 -0185\\nPMID: 163221 74\\n8.Thabtah F,Peebles D.Anewmachine learning model based oninduction ofrules forautism detection.\\nHealth Inform J.2019. https://doi. org/10.1177/1 46045821 8824711 PMID: 30693818\\n9.Volkmar F,Cook EH,Pomeroy J,Realmuto G,Tanguay P.Practice parame tersfortheassessme nt\\nandtreatment ofchildren, adolescen ts,andadults withautism andother pervasive develop mental dis-\\norders. JAmAcad Child Adolesc Psychiatry. 1999; 38(12, Supplem ent):32S–54 S.https://d oi.org/10.\\n1016/S0890- 8567(99)8 0003-3\\n10. Campbell M,Schopler E,Cueva JE,Hallin A.Treatmen tofautistic disorder. JAmAcad Child Adolesc\\nPsychiatry. 1996; 35(2):134– 43.https://doi.or g/10.109 7/00004583- 1996020 00-00005 PMID: 8720622\\n11. Zachor DA,Itzchak EB.Treatment approach, autism severity andinterven tionoutcomes inyoung chil-\\ndren. ResAutism Spectr Disord. 2010; 4(3):425–3 2.\\n12. Boyd BA,Hume K,McBee MT,Alessand riM,Gutierrez A,Johnson L,etal.Comparative Efficacy of\\nLEAP, TEACCH andNon-Model -Specific Special Education Program sforPreschoolers withAutism\\nSpectrum Disorders. JAutism DevDisord. 2014; 44(2):366– 80.https://doi.or g/10.1007/ s10803-013-\\n1877-9 PMID: 238126 61\\n13. Jacobson JW,Mulick JA,Green G.Cost–be nefitestima tesforearly intensive behavio ralintervention for\\nyoung children withautism —general model andsingle state case. Behav Interv. 1998; 13(4):20 1–26.\\n14. Jacobson JW,Mulick JA.System andCost Research Issues inTreatmen tsforPeople withAutistic Dis-\\norders. JAutism DevDisord. 2000; 30(6):585– 93.https://doi.or g/10.102 3/a:100569141 1255 PMID:\\n11261469\\n15. Thabtah F,Peebles D.Early Autism Screening: AComprehen siveReview. IntJEnviro nResPublic\\nHealth. 2019; 16(18):350 2.https:// doi.org/10.33 90/ijerph16 183502 PMID: 31546906\\n16. Rutter M,LeCouteur A,Lord C.ADI-R: Autism Diagnostic Interview-R evised. LosAngeles ,CA:West-\\nernPsychologi calServices; 2003.\\n17. Lord C.,RisiS.,Lambrech tL.,Cook E.H.Jr.,Leventha lB.L.,Lavore Di,etal.Theautism diagnostic\\nobservatio nschedule- generic: astandard measure ofsocial andcommu nication deficits associated\\nwiththespectrum ofautism. JAutism DevDisord. 2000; 30(3), 205–223. PMID: 110554 57\\n18. Levy S,Duda M,Haber N,Wall DP.Sparsifyi ngmachine learning models identify stable subsets ofpre-\\ndictive features forbehavio raldetection ofautism. MolAutism. 2017; 8(1):65. https://doi.or g/10.1186/\\ns13229-017 -0180-6 PMID: 29270283\\n19. Ku¨pper C,Stroth S,Wolff N,Hauck F,Kliewer N,Schad-Ha nsjosten T,etal.Identifying predictive fea-\\ntures ofautism spectrum disorders inaclinical sample ofadolescen tsandadults using machine learn-\\ning.SciRep. 2020; 10(1):4805 .https://doi.or g/10.1038/ s41598-020- 61607-w PMID: 32188882\\n20. Abbas H,Garberson F,Liu-Mayo S,Glover E,Wall DP.Multi-mod ularAIApproach toStreaml ine\\nAutism Diagnosis inYoung Children .Scientific Reports. 2020; 10(1):5014 .https://do i.org/10.1038 /\\ns41598-020 -61213-w PMID: 3219340 6\\n21. Emerson RW, Adams C,Nishino T,Hazlett HC,Wolff JJ,Zwaigenba umL,etal.Functional neuroim ag-\\ningofhigh-risk 6-month- oldinfants predicts adiagnos isofautism at24months ofage. SciTransl Med.\\n2017; 9(393):eaa g2882. https://doi.or g/10.1126 /scitranslmed.a ag2882 PMID: 28592562\\n22. Denisova K,Zhao G.Inflexible neurobiolo gical signatures precede atypical developme ntininfants at\\nhighriskforautism. SciRep. 2017; 7(1):11285 .https://doi.or g/10.103 8/s41598 -017-0902 8-0PMID:\\n28900155\\n23. Bosl WJ,Tager-Flus berg H,Nelson CA.EEG analytics forearly detection ofautism spectrum disorder:\\nadata-driven approac h.SciRep. 2018; 8(1):6828. https://d oi.org/10.103 8/s41598 -018-2431 8-xPMID:\\n29717196\\n24. Momeni N,Bergquist J,Brudin L,Behnia F,Sivberg B,Joghatae iM,etal.Anovel blood-based bio-\\nmarker fordetection ofautism spectrum disorders. Transl Psychiatry. 2012; 2(3):e91. https://doi.or g/10.\\n1038/tp.2 012.19 PMID: 228328 56\\n25. Glatt SJ,Tsuang MT,Winn M,Chandler SD,Collins M,Lopez L,etal.Blood-bas edgene expression\\nsignatures ofinfants andtoddlers withautism. JAmAcad Child Adolesc Psychiatry .2012; 51(9):934-\\n944.e2. https:/ /doi.org/10.10 16/j.jaa c.2012.07.0 07PMID: 22917206\\n26. Croen LA,Brauns chweig D,Haapanen L,Yoshida CK,Fireman B,Grether JK,etal.Maternal mid-preg-\\nnancy autoantibodie stofetalbrain protein: theearly markers forautism study. BiolPsychiatry .2008; 64\\n(7):583–8. https://doi.o rg/10.1016/j.b iopsych. 2008.05.006 PMID: 185716 28\\n27. Greene DJ,Black KJ,Schlagg arBL.Considera tions forMRIstudy design andimplementa tioninpediat-\\nricandclinical populations. DevCogn Neurosci. 2016; 18:101–112. https:// doi.org/10.10 16/j.dcn.20 15.\\n12.005 PMID: 26754461\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 19/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 18}),\n",
       "  Document(page_content='28. Webb SJ,Bernier R,Henderson HA,Johnson MH,Jones EJ,Lerner MD,etal.Guideline sandbest\\npractices forelectroph ysiological data collection, analysis andreporting inautism. JAutism DevDisord.\\n2015; 45(2):425– 243. https://doi.or g/10.1007/ s10803-013- 1916-6 PMID: 2397514 5\\n29. Engelhard tLE,RoeMA,Juranek J,DeMa sterD,Harden KP,Tucker-D robEM,etal.Children ’shead\\nmotion during fMRI tasks isheritable andstable over time. DevCogn Neurosci. 2017; 25:58–6 8.https://\\ndoi.org/10.10 16/j.dcn.20 17.01.011 PMID: 282230 34\\n30. Denisova K.Ageattenuate snoise andincreases symmetry ofhead movem entsduring sleep resting-\\nstate fMRI inhealthy neonates, infants, andtoddlers .Infant Behav Dev. 2019; 57:1013 17.https://doi.\\norg/10.1016/ j.infbeh.2019.0 3.008 PMID: 31102945\\n31. Brisson J,Martel K,Serres J,Sirois S,Adrien JL.Acoustic analysis oforalproducti onsofinfants later\\ndiagnos edwithautism andtheir mother. Infant Ment Health J.2014; 35(3):285– 95.https://doi.or g/10.\\n1002/imh j.21442 PMID: 257984 82\\n32. Nakai Y,Takigu chiT,Matsui G,Yamaoka N,Takada S.Detectin gabnormal word utterances in\\nchildren withautism spectrum disorders: machine-l earning-base dvoice analysis versus speech\\ntherapis ts.Percept MotSkills. 2017; 124(5):96 1–73. https://doi .org/10.1177/0 0315125 17716855\\nPMID: 2864992 3\\n33. Santos JF,Brosh N,FalkTH,Zwaigenba umL,Bryson SE,Roberts W,etal.Very early detection of\\nautism spectrum disorde rsbased onacoustic analysis ofpre-verbal vocalization sof18-mon tholdtod-\\ndlers. In:Proceedings oftheInternational Conferen ceonAcoustics ,Speech andSignal Processin g;\\n2013; Vancou ver,BC,Canada: IEEE. 2013. Doi:10.1109/ICA SSP.2013.6 639134\\n34. Oller D,Niyogi P,Gray S,Richards J,Gilkerson J,XuD,etal.Automate dvocal analysis ofnaturalis tic\\nrecordings from children withautism, language delay, andtypical developme nt.Proc NatlAcad Sci.\\n2010; 107(30):13 354–9. https://doi.or g/10.1073 /pnas.10038 82107 PMID: 20643944\\n35. Pokorny FB,Schuller BW,Marschik PB,Brueckner R,Nystro ¨mP,Cummins N,etal.Earlier Identifica-\\ntionofChildren withAutism Spectru mDisorder: AnAutomatic Vocalisati on-Based Approach. In:Pro-\\nceeding softheINTERS PEECH 2017; 2017; Stockholm ,Sweden: ISCA. 2017. Doi:10.2143 7/\\nInterspee ch.2017-1007\\n36. Little MA,Varoquaux G,Saeb S,Lonini L,Jayaraman A,Mohr DC,etal.Using andundersta nding\\ncross-valida tionstrategie s.Perspe ctives onSaeb etal.Gigascie nce.2017; 6(5):1–6. https://doi.or g/10.\\n1093/gigas cience/gix02 0PMID: 283279 89\\n37. Eyben F,Scherer KR,Schulle rBW,Sundberg J,Andre ´E,Busso C,etal.TheGeneva minimalis tic\\nacoustic parame terset(GeMAPS )forvoice research andaffective comput ing.IEEE Trans Affect Com-\\nput.2015; 7(2):190–2 02.\\n38. Esposito G,Hiroi N,Scattoni ML.Cry,Baby, Cry:Expressio nofDistres sAsaBiomarke randModulato r\\ninAutism Spectrum Disorder. IntJNeuropsyc hopharmac ol.2017; 20(6):498– 503. https:// doi.org/10.\\n1093/ijnp/py x014 PMID: 28204487\\n39. Sheinkopf SJ,Iverson JM,Rinaldi ML,Lester BM.Atypical CryAcoustics in6-Month-Ol dInfants atRisk\\nforAutism Spectrum Disorder. Autism Res. 2012; 5(5):331–9 .https://doi.or g/10.100 2/aur.1244 PMID:\\n22890558\\n40. Orlandi S,Manfredi C,Bocchi L,Scattoni ML,editors. Automatic newborn cryanalysis: anon-invas ive\\ntooltohelpautism early diagno sis.In:Proceedings oftheAnnual International Conferen ceoftheIEEE\\nEngineeri nginMedicine andBiology Society; 2012; SanDiego, CA,USA: IEEE. 2012. Doi:10.1109 /\\nEMBC.2012 .634658 3\\n41. Motlagh SHRE, Moradi H,Pouretemad H,editors. Using general sound descriptor sforearly autism\\ndetection: 2013 9thAsian Control Conferen ce(ASCC) Control; 2013; Istanbul, Turkey: IEEE. 2013.\\nDoi:10.1109/ASC C.2013.660 6386\\n42. Barrow WJ,Jaworski M,Accardo PJ.Persisten ttoewalking inautism. JChild Neurol. 2011; 26(5):619–\\n21.https://doi. org/10.1177/0 8830738 10385344 PMID: 21285033\\n43. Paul R,Norbury CF.Langua gedisorders from infancy throug hadolescence :Elsevier; 2012.\\n44. Jalilevand N,Ebrahim ipour M.Pronoun acquisition inFarsi-spe aking children from 12to36months. J\\nChild Lang Acquis Dev. 2013; 1(1):1–9.\\n45. Goldstein S,Ozonoff S.Assessme ntofautism spectrum disorder: Guilford Publica tions; 2018.\\n46. Lund NJ,Duchan JF.Assessing childre n’slanguage innaturalis ticcontexts: Prentice Hall; 1993.\\n47. Gilliam JE.Gilliam autism rating scale: GARS 2:Pro-ed; 2006.\\n48. Berk L.Developm entthrough thelifespan: Pearson Educatio nIndia; 2010.\\n49. Three ZT.Diagnostic classific ation ofmental health anddevelop mental disorde rsofinfancy andearly\\nchildhood: Revised edition (DC: 0-3R). Washingto n,DC: Zero ToThree Press; 2005.\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 20/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 19}),\n",
       "  Document(page_content='50. Paliwal KK,Lyons JG,Wo´jcicki KK,editors. Preference for20–40 mswindow duration inspeech analy-\\nsis.In:Proceedings ofthe4thInterna tional Conference onSignal Processin gandCommunic ation Sys-\\ntems; 2010: Gold Coast, QLD, Australia: IEEE. Doi:10.1109 /ICSPCS.201 0.57097 70\\n51. Molau S,PitzM,Schluter R,NeyH.Comput ingMel-fre quency cepstral coefficient sonthepower spec-\\ntrum. In:Proceedings oftheInternational Conferen ceonAcoustic s,Speech ,andSignal Processin g\\nProceedings (CatNo01C H37221); 2001; SaltLake City, UT,USA: IEEE. Doi:10.1109 /ICASSP. 2001.\\n940770\\n52. Esposito G,Venuti P.Developme ntalchanges inthefundamen talfrequency (f0)ofinfants’ cries: a\\nstudy ofchildren withAutism Spectrum Disorder. Early Child DevCare. 2010; 180(8):109 3–102.\\n53. Marchi E,Schuller B,Baron-Cohen S,Golan O,Bo¨lteS,Arora P,etal.Typicality andemotion inthe\\nvoice ofchildren withautism spectrum condition :Evidence across three langua ges.In:Proceedings of\\ntheINTERS PEECH 2015; 2015; Dresden, Germany :ISCA. 2015. p.115–119. Available from: https://\\nwww.isca -speech.o rg.\\n54. Belalca ´zar-Bolañ osE.A., Orozco-Arr oyave J.R., Vargas-Bo nillaJ.F., Haderle inT.,No¨thE.Glottal Flow\\nPatterns Analyses forParkinson’s Disease Detection: Acoustic andNonlinear Approaches .In:Sojka\\nP.,Hora´kA.,Kopeček I.,Pala K.,editors. Text, Speech, andDialogue :Proceedings ofthe19th Interna -\\ntional Conferen ceonText, Speech ,andDialogue ;2016 Sep12–16; Brno, Czech Republic .Cham:\\nSpringer; 2016. Doi:10.1007/978- 3-319-455 10-5_46\\n55. Rabiner LR,Schafer RW. Introduction todigital speech processing .Found andtrends insignal process.\\n2007; 1(1):1–194 .\\n56. Peeters G.Alarge setofaudio features forsound descriptio n(similarity andclassif ication) intheCUI-\\nDADO project. CUIDAD OISTProjRep. 2004; 54(0):1–25 .\\n57. Bone D,LeeC-C, Black MP,Williams ME,LeeS,Levitt P,etal.Thepsychologist asaninterlocut orin\\nautism spectrum diorder assessmen t:Insights from astudy ofspontane ousprosody. JSpeech Lang\\nHear Res. 2014; 57(4):1162 –77. https://doi. org/10.1044/2 014_JS LHR-S-13 -0062 PMID: 24686340\\n58. Ha¨nsler E,Schmidt G.Speech andaudio processing inadverse environm ents: Springer Science &\\nBusiness Media; 2008. https://doi.or g/10.105 5/s-2008-106 5331 PMID: 18473287\\n59. Theodoridis S,Koutroumbas K.Pattern recognition: Elsevier ;2003.\\n60. LiTL,Chan AB.Genre classification andtheinvariance ofMFCC feature stokeyandtempo. In:Lee\\nKT.,TsaiWH., LiaoHY.M., Chen T.,Hsieh JW., Tseng CC., editors. Advances inMultimedia Modeling:\\nProceedings ofthe17th Interna tional MultiMedia Modeling Conferen ce;2011 Jan5–7; Taipei, Taiwan.\\nBerlin, Heidelberg: Springer; 2011. Doi:10.1007 /978-3-642 -17832-0_ 30\\n61. Takahashi T,Yoshimur aY,Hiraishi H,Hasegawa C,Munesue T,Higashida H,etal.Enhanc edbrain\\nsignal variabi lityinchildren withautism spectrum disorder during early childhoo d.Hum Brain Mapp.\\n2016; 37(3):1038 –50. https://doi.or g/10.1002/ hbm.23089 PMID: 26859309\\n62. Lester BM,Boukydis CZ.Infant crying: Theoret icalandresearch perspectiv es:Springer; 1985.\\n63. MacDon aldM,Lord C,Ulrich D.Therelations hipofmotor skills andadaptive behavio rskills inyoung\\nchildren withautism spectrum disorders .ResAutism Spectr Disord. 2013; 7(11):1383 –90. https:// doi.\\norg/10.1016/ j.rasd.2013.07 .020 PMID: 25774214\\n64. HoyMB.Alexa, Siri,Cortana, andMore: AnIntroducti ontoVoice Assistant s.Med RefServ Q.2018; 37\\n(1):81–8. https://doi.or g/10.108 0/02763869. 2018.1404391 PMID: 293279 88\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 21/21', metadata={'source': 'papers/Asd_Cry_patterns.pdf', 'page': 20})],\n",
       " [Document(page_content='RESEARCH ARTICLE\\nDigital Behavioral Phenotyping Detects Atypical Pattern of Facial\\nExpression in Toddlers with Autism\\nKimberly L. H. Carpenter , Jordan Hahemi, Kathleen Campbell, Steven J. Lippmann, Jeffrey P. Baker,\\nHelen L. Egger, Steven Espinosa, Saritha Vermeer, Guillermo Sapiro, and Geraldine Dawson\\nCommonly used screening tools for autism spectrum disorder (ASD) generally rely on subjective caregiver questionnaires.\\nWhile behavioral observation is more objective, it is also expensive, time-consuming, and requires signi ﬁcant expertise to\\nperform. As such, there remains a critical need to develop feasible, scalable, and reliable tools that can characterize ASD\\nrisk behaviors. This study assessed the utility of a tablet-based behavioral assessment for eliciting and detecting one type\\nof risk behavior, namely, patterns of facial expression, in 104 toddlers (ASD N= 22) and evaluated whether such patterns\\ndifferentiated toddlers with and without ASD. The assessment consisted of the child sitting on his/her caregiver ’s lap and\\nwatching brief movies shown on a smart tablet while the embedded camera recorded the child ’s facial expressions. Com-\\nputer vision analysis (CVA) automatically detected and tracked facial landmarks, which were used to estimate head posi-tion and facial expressions (Positive, Neutral, All Other). Using CVA, speci ﬁc points throughout the movies were\\nidenti ﬁed that reliably differentiate between children with and without ASD based on their patterns of facial movement\\nand expressions (area under the curves for individual movies ranging from 0.62 to 0.73). During these instances, children\\nwith ASD more frequently displayed Neutral expressions compared to children without ASD, who had more All Other\\nexpressions. The frequency of All Other expressions was driven by non-ASD children more often displaying raised eye-brows and an open mouth, characteristic of engagement/interest. Preliminary results suggest computational coding of\\nfacial movements and expressions via a tablet-based assessment can detect differences in affective expression, one of the\\nearly, core features of ASD. Autism Res 2020, 00: 1 –12.© 2020 International Society for Autism Research and Wiley\\nPeriodicals LLC\\nLay Summary: This study tested the use of a tablet in the behavioral assessment of young children with autism. Children\\nwatched a series of developmentally appropriate movies and their facial expressions were recorded using the camera\\nembedded in the tablet. Results suggest that computational assessments of facial expressions may be useful in early detec-\\ntion of symptoms of autism.\\nKeywords: autism; risk behaviors; facial expressions; computer vision; early detection\\nIntroduction\\nAutism spectrum disorder (ASD) can be reliably diagnosed\\nas early as 24 months old and the risk signs can be\\ndetected as early as 6 –12 months old [Dawson &\\nBernier, 2013; Luyster et al., 2009]. Despite this, the aver-\\nage age of diagnosis in the United States remains around\\n4 years of age [Christensen et al., 2016]. While there is\\nmixed evidence for the stability of autism traits over early\\nchildhood [Bieleninik et al., 2017; Waizbard-Bartov\\net al., 2020], the delay in diagnosis can still impact timely\\nintervention during a critical window of development. Inresponse to this, in 2007 the American Academy of Pedi-\\natrics published guidelines supporting the need for all\\nchildren to be screened for ASD between 18- and24-months of age as part of their well-child visits [Myers,Johnson, & Council on Children With Disabilities, 2007].Current screening typically relies on caregiver report,such as the Modi ﬁed Checklist of ASD in Toddlers —\\nRevised with Follow-up (M-CHAT-R/F) (Robins\\net al., 2014). Evidence suggests that a two-tiered screen-\\ning approach, including direct observational assessmentof the child, improves the positive predictive value ofM-CHAT screening by 48% [Khowaja, Robins, &\\nFrom the Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Dur-\\nham, North Carolina, USA (K.L.H.C., J.H., K.C., H.L.E., S.E., S.V., G.D.); Department of Electrical and Computer Engineering, Duke University, Dur ham,\\nNorth Carolina, USA (J.H., S.E.); Department of Pediatrics, University of Utah, Salt Lake City, Utah, USA (K.C.); Department of Population Health Sc i-\\nences, Duke University School of Medicine, Durham, North Carolina, USA (S.J.L.); Department of Pediatrics, Duke University School of Medicine, Dur -\\nham, North Carolina, USA (J.P.B.); NYU Langone Child Study Center, New York University, New York, New York, USA (H.L.E.); Departments of\\nBiomedical Engineering Computer Science, and Mathematics, Duke University, Durham, North Carolina, USA (G.S.); Duke Institute for Brain Sciences ,\\nDuke University, Durham, North Carolina, USA (G.D.)\\nReceived April 7, 2020; accepted for publication August 24, 2020Address for correspondence and reprints: Kimberly L. H. Carpenter, Duke Center for Autism and Brain Development, Department of Psychiatry and\\nBehavioral Sciences, Duke University School of Medicine, 2608 Erwin Rd #300, Durham, NC 27705. E-mail: kimberly.carpenter@duke.edu\\nPublished online 00 Month 2020 in Wiley Online Library (wileyonlinelibrary.com)DOI: 10.1002/aur.2391© 2020 International Society for Autism Research and Wiley Periodicals LLC\\nINSAR Autism Research 000: 1 –12, 2020 1\\n', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 0}),\n",
       "  Document(page_content='Adamson, 2017] and may reduce ethnic/racial disparities\\nin general screening [Guthrie et al., 2019]. Current toolsfor observational assessment of ASD signs in infants and\\ntoddlers, such as the Autism Observation Scale for Infants\\n(AOSI) and Autism Diagnostic Observation Schedule(ADOS), take substantial time and training to administer,resulting in a shortage of quali ﬁed diagnosticians to per-\\nform these observational assessments. As such, there\\nremains a critical need to develop feasible, scalable, andreliable tools that can characterize ASD risk behaviors\\nand identify those children who are most in need of\\nfollow-up by an ASD specialist. In an effort to address thiscritical need, we have embarked on a program of research\\nusing computer vision analysis (CVA) to develop tools for\\ndigitally phenotyping early emerging risk behaviors forASD [Dawson & Sapiro, 2019]. If successful, such digital\\nscreening tools have the opportunity to help existing\\npractitioners reach more children and assist in triagingboundary cases for review by specialists.\\nO n eo ft h ee a r l ye m e r g i n gs i g n so fA S Di sa\\ntendency to more often display a neutral facial expres-sion. This pattern is evident in the quality of facial\\nexpressions and in sharing emotional expressions\\nwith others [Adrien et al., 1993; Baranek, 1999;S. Clifford & Dissanayake, 2009; S. Clifford,\\nYoung, & Williamson, 2007; S. M. Clifford &\\nDissanayake, 2008; Maestro et al., 2002; Osterling, Daw-son, & Munson, 2002; Werner, Dawson, Osterling, &\\nDinno, 2000]. A restricted range of emotional expres-\\nsion and its integration with eye gaze (e.g., during socialreferencing) have been found to differentiate children\\nwith ASD from typically developing children, as well\\nas those who have other developmental delays, asearly as 12 months of age [Adrien et al., 1991;\\nS. Clifford et al., 2007; Filliter et al., 2015; Gangi,\\nIbanez, & Messinger, 2014; Nich ols, Ibanez, Foss-Feig, &\\nStone, 2014]. While core features of ASD vary by age,\\ncognitive ability, and language, one of the most stable\\nsymptoms from early childhood through adolescencesis increased frequency of neutral expression [Bal, Kim,\\nFok, & Lord, 2019]. As such, differences in facial affect\\nmay show utility in assessing early risk for ASD.\\nA recent meta-analysis of facial expression production\\nin autism found that individuals with ASD display facial\\nexpressions less often than non-ASD participants andthat, when they did display facial expressions, the expres-\\nsions occurred for shorter durations and were of different\\nquality than non-ASD individuals (Trevisan, Hoskyn, &Birmingham, 2018). Decreases in the frequency of both\\nemotional facial expressions and the sharing of those\\nexpressions with others has been demonstrated acrossnaturalistic interactions [Bieberich & Morgan, 2004;\\nCzapinski & Bryson, 2003; Dawson, Hill, Spencer,\\nGalpert, & Watson, 1990; Mcgee, Feldman, &Chernin, 1991; Snow, Hertzig, & Shapiro, 1987; Tantam,Holmes, & Cordess, 1993], in lab-based assessments such\\nas during the ADOS [Owada et al., 2018] or the AOSI[Filliter et al., 2015] and in response to emotion-eliciting\\nvideos [Trevisan, Bowering, & Birmingham, 2016]. Fur-\\nthermore, higher frequency of neutral expressions corre-lates with social impairment in children with ASD\\n[Owada et al., 2018] and differentiates them from chil-\\ndren with other delays [Bieberich & Morgan, 2004;Yirmiya, Kasari, Sigman, & Mundy, 1989]. As such, fre-\\nquency and duration of facial affect is a promising early\\nrisk marker for young children with autism.\\nPrevious research on atypical facial expressions in chil-\\ndren with ASD has relied on hand coding of facial expres-\\nsions, which is time intensive and often requiressigniﬁcant training [Bieberich & Morgan, 2004; S. Clifford\\net al., 2007; Dawson et al., 1990; Gangi et al., 2014;\\nMcgee et al., 1991; Nichols et al., 2014; Snowet al., 1987]. This approach is not scalable for use in gen-\\neral ASD risk screening or as a behavioral biomarker or\\noutcome assessment for use in large clinical trials. Assuch, the ﬁeld has moved toward automating the coding\\nof facial expressions. In one of the earliest studies of this\\napproach, Guha and colleagues demonstrated that chil-dren with ASD have atypical facial expressions when\\nmimicking others. However, their technology required\\nthe children to wear markers on their face for data cap-ture [Guha, Yang, Grossman, & Narayanan, 2018; Guha\\net al., 2015], which is both invasive and not scalable.\\nMore recently, several groups have applied non-invasiveCVA technology to measuring affect in older children\\nand adults with ASD within the laboratory setting\\n[Capriola-Hall et al., 2019; Owada et al., 2018; Samadet al., 2018]. This represents an important move toward\\nscalability as CVA approaches do not rely on the presence\\nof physical markers on the face to extract emotion infor-mation. Rather, CVA relies on videos of the individual in\\nwhich features around speci ﬁc regions on a face (e.g., the\\nmouth and eyes) are extracted. Notably, these featuresmirror those used by the manually rated facial affect cod-\\ning system (FACS) [Ekman, 1997]. Both our earlier work\\n[Hashemi et al., 2018] and that of others [Capriola-Hallet al., 2019] have shown good concordance between\\nhuman coding and CVA rating of facial emotions. Fur-\\nthermore, previous research in adults has demonstratedthat CVA can detect neutral facial expressions more reli-\\nably than human coders [Lewinski, 2015].\\nBuilding on previous work applying CVA in laboratory\\nsettings, we have developed a portable tablet-based tech-\\nnology that uses the embedded camera and automatic\\nCVA to code ASD risk behaviors in <10 min across a rangeof non-laboratory settings (e.g., pediatric clinics, at home,\\netc.). We developed a series of movies designed to capture\\nchildren ’s attention, elicit emotion in response to novel\\nand interesting events, and assess the toddler ’s ability to\\nsustain attention and share it with others. By embedding\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 2', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 1}),\n",
       "  Document(page_content='these movies in a fully automated system on a cost-\\neffective tablet whereby the elicited behaviors, in thiscase the frequency of different patterns of facial affect,\\nare automatically encoded with CVA, we aim to create a\\ntool that is objective, ef ﬁcient, and accessible. The cur-\\nrent analysis focuses on preliminary results supporting\\nthe utility of this tablet-based assessment for the detec-\\ntion of facial movement and affect in young children andthe use of facial affect to differentiate children with and\\nwithout ASD. Though facial affect is the focus of the cur-\\nrent analysis, the ultimate goal is to combine informationacross autism risk features collected through the current\\ndigital screening tool [e.g., delayed response to name as\\ndescribed in Campbell et al., 2019], to develop a risk scorebased on multiple behaviors [Dawson & Sapiro, 2019].\\nThis information could then be combined with addi-\\ntional measures of risk to enhance screening for ASD.\\nMethods\\nParticipants\\nParticipants were 104 children 16 –31 months of age\\n(Table 1). Children were recruited at their pediatric pri-\\nmary care visit by a research assistant embedded within\\nthe clinic or via referral from their physician, as well asthrough community advertisement ( N= 4 in the non-\\nASD group and N= 15 in the ASD group). For children\\nrecruited within the pediatric clinics, recruitmentoccurred at the 18- or 24-month well-child visit at the\\nsame time as they received standard screening for ASD\\nwith the M-CHAT-R/F. A total of 76% of the participantsrecruited in the clinic by a research assistant chose to par-\\nticipate. Of the participants who chose not to participate,\\n11% indicated that they were not interested in the study,whereas the remainder declined due to not having\\nenough time, having another child to take care of, want-ing to discuss with their partner, or their child was\\nalready too distressed after the physician visit. All chil-\\ndren who enrolled in the study found the procedureengaging enough that they were able to provide adequate\\ndata for analysis. Because the administration is very brief\\nand non-demanding, data loss was not a signi ﬁcant\\nproblem.\\nExclusionary criteria included known vision or hearing\\ndeﬁcits, lack of exposure to English at home, or insuf ﬁ-\\ncient English language skills for caregiver ’s informed con-\\nsent. Twenty-two children were diagnosed with ASD. The\\nnon-ASD comparison group ( N= 82) was comprised of\\n74 typically developing children and 8 children with a\\nnon-ASD delay, which was de ﬁned by a diagnosis of lan-\\nguage delay or developmental delay of clinical signi ﬁ-\\ncance suf ﬁcient to qualify for speech or developmental\\ntherapy as recorded in the electronic medical record. All\\ncaregivers/legal guardians gave written informed consent,and the study protocol was approved by the Duke Uni-\\nversity Health System IRB.\\nChildren recruited from the pediatric primary care\\nsettings received screening with a digital version of the\\nM-CHAT-R/F as part of a quality improvement study\\nongoing in the clinic [Campbell et al., 2017]. Participantsrecruited from the community received ASD screening\\nwith the digital M-CHAT-R/F prior to the tablet assess-\\nment. As part of their participation in the study, childrenwho either failed the M-CHAT-R/F or for whom there was\\ncaregiver or physician concern about possible ASD under-\\nwent diagnostic testing using the ADOS-Toddler (ADOS-T) [Luyster et al., 2009] conducted by a licensed psycholo-\\ngist or research-reliable examiner supervised by a licensed\\nTable 1. Sample Demographics\\nTypically developing ( N= 74; 71%) Non-ASD delay ( N= 8; 8%) ASD ( N= 22; 21%)\\nAge\\nMonths [mean (SD)] 21.7 (3.8) 23.9 (3.7) 26.2 (4.1)\\nSex\\nFemale 31 (42) 3 (38) 5 (23)\\nMale 43 (58) 5 (62) 17 (77)\\nEthnicity/race\\nAfrican American 10 (14) 1 (13) 3 (14)\\nCaucasian 46 (62) 2 (25) 10 (45)Hispanic 1 (1) 0 (0) 1 (4)\\nOther/unknown 17 (23) 5 (62) 8 (37)\\nInsurance\\na\\nMedicaid 11 (15) 1 (14) 6 (67)\\nNon-Medicaid 60 (85) 6 (86) 3 (33)\\nMCHAT resultb\\nPositive 1 (1) 0 (0) 18 (82)Negative 73 (99) 8 (100) 4 (18)\\naInsurance status was unknown for 17 (16%) of participants in this study.\\nbChildren for whom the MCHAT was negative but received and ASD diagnosis were referred for assessment due to concerns by either the parent or the\\nchild ’s physician.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 3', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 2}),\n",
       "  Document(page_content='psychologist. The mean ADOS-T score was 18.00\\n(SD = 4.67). A subset of the ASD children ( N= 13) also\\nreceived the Mullen Scales of Early Learning\\n(Mullen, 1995). The mean IQ based on the Early Learning\\nComposite Score for this subgroup was 63.58(SD = 25.95). None of the children in the non-ASD com-\\nparison group was administered the ADOS or Mullen.\\nChildren ’s demographic information was extracted\\nfrom the child ’s medical record or self-reported by the\\ncaregiver at their study visit. Children in the ASD group\\nwere, on average, 4 months younger than the compari-son group ( t[102] = 4.64, P< 0.0001). Furthermore, as\\nwould be expected, there was a higher proportion of\\nmales in the ASD group than in the comparison group,though the difference was not statistically signi ﬁcant ( χ\\n2\\n[1, 104] = 2.60, P= 0.11). There were no differences in the\\nproportion of racial/ethnic minority children betweenthe two groups ( χ\\n2[1, 104] = 1.20, P= 0.27). When\\nlooking only at the children for which Medicaid status\\nwas known, there was no difference in the proportion ofchildren on Medicaid in the ASD and the non-ASD group\\n(χ\\n2[1, 87] = 1.82, P= 0.18).\\nStimuli and Procedure\\nA series of developmentally appropriate brief movies\\ndesigned to elicit affect and engage the child ’s attention\\nwere shown on a tablet while the child sat on a care-\\ngiver’s lap. The tablet was placed on a stand approxi-\\nmately 3 ft away from the child to prevent the child fromtouching the screen as depicted in previous publications\\n[Campbell et al., 2019; Dawson et al., 2018; Hashemi\\net al., 2015; Hashemi et al., 2018]. Movies consisted ofcascading bubbles (2 ×30 sec), a mechanical bunny\\n(66 sec), animal puppets interacting with each other\\n(68 sec), and a split screen showing a woman singingnursery rhymes on one side and dynamic, noise-making\\ntoys on the other side (60 sec; Fig. 1). These movies\\nincluded stimuli that have been used in previous studiesof ASD symptomatology [Murias et al., 2018], as well as\\ndeveloped speci ﬁcally for the current tablet-based tech-\\nnology to elicit autism symptoms, based on Dawsonet al. [2004], Jones, Dawson, Kelly, Estes, and\\nWebb [2017], Jones et al. [2016], and Luyster et al. [2009].At three points during the movies, the examiner located\\nbehind the child called the child ’s name.\\nPrior to the administration of the app, caregivers were\\nclearly instructed not to direct their child ’s attention or\\nin any way try to in ﬂuence the child ’s behavior during\\nthe assessment. Furthermore, if the caregiver began to tryto direct their child ’s attention, the examiner in the room\\nimmediately asked the caregiver to refrain from doing\\nso. If the caregiver persisted, this was noted on our valid-ity form and the administration would have been consid-\\nered invalid. Researchers stopped the task for one\\ncomparison participant due to crying. Researchersrestarted the task for three participants with ASD due to\\ndifﬁculty remaining in view of the tablet ’s camera for\\nmore than half of the ﬁrst video stimulus. If other family\\nmembers were present during the well-child visit, they\\nwere asked to stand behind the caregiver and child so as\\nto not distract the child during the assessment. Addition-ally, for children assessed during a well-child visit,\\nresearch assistants were instructed to collect data prior to\\nany planned shots or blood draws.\\nComputer Vision Analysis\\nThe frontal camera in the tablet recorded video through-\\nout the experiment at 1280 ×720 resolution and\\n30 frames per second. The CVA algorithm [Hashemi\\net al., 2018] ﬁrst automatically detected and tracked\\n49 facial landmarks on the child ’s face [De la Torre\\net al., 2015]. Head positions relative to the camera were\\nestimated by computing the optimal rotation parametersbetween the detected landmarks and a 3D canonical face\\nmodel [Fischler & Bolles, 1981]. A “not visible ”tag was\\nassigned to frames where the face was not detected or theface exhibited drastic yaw (>45\\n/C14from center). We\\nacknowledge that the current method used only indicates\\nwhether the child is oriented toward the stimulus anddoes not track eye movements. For each “visible ”frame,\\nthe probability of expressing three standard categories of\\nfacial expressions, Positive, Neutral (i.e., no active facial\\nFigure 1. Example of movie stimuli. Developmentally appropriate movies consisted of cascading bubbles, a mechanical bunny, animal\\npuppets interacting with each other, and a split screen showing a woman singing nursery rhymes on one side and dynamic, noise-makingtoys on the other side.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 4', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 3}),\n",
       "  Document(page_content='action unit), or Other (all other expressions), was\\nassigned [Hashemi et al., 2018]. The model for automaticfacial expression is an extension of the pose-invariant\\nand cross-modal dictionary learning approach originally\\ndescribed in Hashemi et al. [2015]. During training, thedictionary representation is setup to map facial informa-\\ntion between 2D and 3D modalities and is then able to\\ninfer discriminative facial information for facial expres-sions recognition even when only 2D facial images are\\navailable at deployment. For training, data from Bing-\\nhamton University 3D Facial Expression database [Yin,Wei, Sun, Wang, & Rosato, 2006] were used, along with\\nsynthesized faces images with varying poses [see Hashemi\\net al., 2015 for synthesis details]. Extracted image featuresand distances between a subset of facial landmarks were\\nused as facial features to learn the robust dictionary.\\nLastly, using the inferred discriminative 3D and frontal2D facial features, a multiclass support vector machine\\n[Chang & Lin, 2011] was trained to classify the different\\nfacial expressions.\\nIn recent years, there has been progress on\\nautomatic facial expression analysis of both children\\nand toddlers [Dys & Malti, 2016; Gadea, Aliño,Espert, & Salvador, 2015; Haines et al., 2019;\\nLoBue & Thrasher, 2014; Messinger, Mahoor, Chow,\\n& Cohn, 2009]. In addition to this, we have previouslyvalidated our CVA algorithm against expert human rater\\ncoding of facial affect in a subsample of 99 video record-\\nings across 33 participants (ASD = 15, non-ASD = 18).This represents 20% of the non-ASD sample and a mat-\\nched group from the ASD sample. The selection of partici-\\npants for this previously published validation study wasbased on age distribution to ensure representation across\\nthe range of ages for both non-ASD and ASD groups. This\\nprevious work showed strong concordance between CVA-and human-rated coding of facial emotion in this data\\nset, with high precision, recall, and F1 scores of 0.89,\\n0.90, and 0.89, respectively [Hashemi et al., 2018].\\nStatistical Approach\\nFor each video frame, the CVA algorithm produces a\\nprobability value for each expression (Positive, Neutral,\\nOther). We calculated the mean of the probability values\\nfor each of the three expression types within non-overlapping 90-frame (3 sec) intervals, excluding frames\\nwhen the face was not visible. A 3-sec window was\\nselected as it provided us with a continuous distributionof the emotion probabilities, while still being within the\\n0.5–4-sec window of a macroexpression (Ekman, 2003).\\nAdditionally, for each of the name call events weremoved the time window starting at cue for the name\\ncall prompt through the point where 75% of the audible\\nname calls actually occurred, plus 150 frames (5 sec). Thiswindow was selected based on our previous study[Campbell et al., 2019] showing that orienting tended to\\noccur within a few seconds after a name call. We calcu-lated the proportion of frames the child was not attend-\\ning to the movie stimulus, based on the “visible ”and\\n“not visible ”tags described above, within each 90-frame\\ninterval, excluding name call periods. Thus, for each\\nchild, we generated four variables (mean probabilities of\\nPositive, Neutral, or Other; and proportion of frames notattending) for each 90-frame interval within each of the\\nﬁve movies.\\nTo evaluate differences between ASD and non-ASD\\nchildren at regular intervals throughout the assessment,\\nweﬁt a series of bivariate logistic regressions to obtain\\nthe odds ratios for the associations between the meanexpression probability or attending proportion during a\\ngiven interval, parameterized as increments of 10%\\npoints, and ASD diagnosis. We then ﬁt a series of multi-\\nvariable logistic regression models, separately for each\\nmovie and variable, which included parameters for each\\nof the 3-sec intervals within the movie to predict ASDdiagnosis. Given the large number of intervals relative to\\nthe small sample size, we used a Least Absolute Shrinkage\\nand Selection Operator (LASSO) penalized regressionapproach [Tibshirani, 1996] to select a parsimonious set\\nof parameters representing the intervals within each\\nmovie and expression type that were most predictive ofASD diagnosis.\\nFor each of the ﬁve movies, we then combined the\\nLASSO-selected interval parameters into a full logisticmodel. When more than one expression parameter was\\nselected for a given interval, we selected the one with the\\nstronger odds ratio estimate. Analyses were conductedwith and without age as a covariate. Since the small study\\nsize precluded having separate training and validation\\nsets, we used leave-one-out cross-validation to assessmodel performance. Receiver-operating characteristics\\n(ROC) curves were plotted and the c-statistic for the area\\nunder the ROC curve was calculated for each movie.\\nResults\\nFigure 2 depicts the odds ratio analysis using the“Rhymes and Toys ”movie as one illustrative example. As\\nshown by the variability in the odds ratio estimates, some\\nparts of the movies elicited strongly differential responsesin certain patterns of expression (blue window, Fig. 2),\\nwhile in other sections, there were not substantial differ-\\nences between the two groups (green window, Fig. 2).Overlaid on the plot are the odds ratios and con ﬁdence\\nbands for the interval parameters selected by the\\nexpression-speci ﬁc LASSO models. These selected param-\\neters were then used in the movie-level logistic models\\nfor which we calculated classi ﬁcation metrics.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 5', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 4}),\n",
       "  Document(page_content='Figure 3 compares the ROC curves for the ﬁveﬁnal\\nmovie-level logistic models after leave-one-out cross-\\nvalidation. ROC curves analyses were performed for eachvideo individually. The model for the “Rhymes ”movie\\nyielded the strongest predictive ability, with an area\\nunder the curve (AUC) of 0.73 (95% con ﬁdence interval\\n[CI] 0.59 –0.86), followed by the “Puppets ”(AUC = 0.67;\\n95% CI 0.53 –0.80) and the “Bunny ”(AUC = 0.66; 95%CI 0.51 –0.82) videos. Finally, the two “Bubbles ”movies\\nthat bookend the stimulus sets were the least predictive,\\nwith AUCs of 0.62 (95% CI 0.49 –0.74) and 0.64 (95% CI\\n0.51–0.76), respectively. Because there was a signi ﬁcant\\ndifference in age between the ASD and non-ASD com-\\nparison groups, we ran a second set of ROC analyses\\nwhere age was included as a covariate, shown in Table 2.Results remained signi ﬁcant after including the age\\ncovariate.\\nGiven the preponderance for the Other emotional cate-\\ngory in our non-ASD comparison group, we explored:\\n(a) what speci ﬁc facial movements are driving this cate-\\ngory of Other expressions and (b) how it differs from theNeutral expression category. We focused on analyzing\\nmovements of facial landmarks and head pose angles and\\nhow they differ between the facial expression categories.Our CVA algorithm aligns the facial landmarks of the\\nchild to a canonical face model through an af ﬁne trans-\\nformation, which normalizes the landmark locationsacross all video frames to a common space. This normali-\\nzation process is commonly used across CVA tasks related\\nto facial analysis as it allows one to analyze/comparelandmark locations across different frames or partici-\\npants. With this alignment step, we were able to quantify\\nthe distances between the eye corners and the corners ofthe eyebrows, the vertical distance between the inner lip\\npoints, and the vertical distance between the outer lip\\npoints (Fig. 4, right). To interpret features that differenti-ated the Neutral from the Other facial expression cate-\\ngory, we assessed differences between these facial\\nlandmark distances of a given child when they were pre-dominately expressing Neutral versus Other facial expres-\\nsions. We also included yaw and pitch head pose angles\\nsince they may play a role in the alignment process.\\nFigure 2. Time series of odds ratios for the association between the mean expression probability or proportion attending and ASD\\ndiagnosis. Using the “Rhymes ”movie as one illustrative example, lines depict the odds of meeting criteria for ASD (OR > 1) or being in\\nthe non-ASD comparison group (OR < 1) for each of the outcomes of interest for each 3 sec time bin across the movie. Points with errorbars are intervals that were selected by the LASSO regression models and included in the ﬁnal logistic model. The blue window depicts a\\nsegment of the movie where there were differential emotional responses between the ASD and non-ASD children. The green window\\ndepicts a segment of the movie in which there was no difference in emotional responses between the groups.\\nFigure 3. Receiver-operating characteristics (ROC) curves. ROC\\ncurves were calculated for predictive ability of expression-speci ﬁc\\nLASSO selected interval parameters for facial expressions andattention to stimulus for each movie independently.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 6', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 5}),\n",
       "  Document(page_content='We focused in on three stimuli in which participants\\nexhibited high probabilities of Other expressions,\\nnamely, the ﬁrst Bubbles, Puppets, and Rhymes and Toys\\nvideos. Out of the 104 participants, all exhibited frames\\nwhere both Neutral and Other facial expressions were\\ndominant (probability of expression over 60%). A Wilsonsigned-ranks test, reported as (median difference,\\nPvalue), indicated that within individual participants for\\neach diagnostic group, the median differences betweenthe distances of Other versus Neutral facial expressions\\nwere signi ﬁcantly higher for inner right eyebrows (non-\\nASD: diff = 2.5, P< 8.1e-10; ASD: diff = 4.0, P< 1.1e-4),\\ninner left eyebrows (non-ASD: diff = 2.4, P< 1.5e-9; ASD:\\ndiff = 3.6, P< 1.4e-4), outer right eyebrows (non-ASD:\\ndiff = 1.2, P< 1.0e-5; ASD: diff = 2.3, P< 1.6e-4), outer left\\neyebrows (non-ASD: diff = 0.9, P< 3.4e-6; ASD: diff = 1.4,\\nP< 1.4e-3), and mouth height (non-ASD: diff = 1.5,\\nP< 1.0e-3), as well as for pitch head pose angles (non-\\nASD: diff = 2.8, P< 8.7e-10; ASD: diff = 4.4, P< 1.2e-3);\\nbut not for eye heights, lips parting, nor yaw head poseangles.\\nDiscussion\\nThe present study evaluated an application administeredon a tablet that was comprised of carefully designed\\nmovies that elicited affective expressions combined with\\nCVA of recorded behavioral responses to identify patternsof facial movement and emotional expression that differ-\\nentiate toddlers with ASD from those without ASD. We\\ndemonstrated that the movies elicited a range of affectivefacial expressions in both groups. Furthermore, using\\nCVA we found children with ASD were more likely to dis-\\nplay a neutral expression than children without ASDwhen watching this series of videos, and the patterns of\\nfacial expressions elicited during speci ﬁc parts of the\\nmovies differed between the two groups. We believe thisﬁnding has strong face validity that rests on both\\nresearch and clinical observations of a restricted range of\\nfacial expression in children with autism. Furthermore,this replicates a previous ﬁnding of our group reporting\\nincreased frequency of neutral expression in\\nyoung children who screened positive on the M-CHAT[Egger et al., 2018]. Together, these preliminary resultssupport the use of engaging brief movies shown on a\\ncost-effective tablet, combined with automated CVAbehavioral coding, as an objective and feasible tool for\\nmeasuring an early emerging symptom of ASD, namely,\\nincreased frequency of neutral facial expressions.\\nWhile the predictive power of emotional expression in\\nsome of the videos varied, all but one represent a medium\\neffect, equivalent to Cohen ’sd= 0.5 or greater [Rice &\\nHarris, 2005]. Overall, the best predictor from our battery\\nof videos is the “Rhymes ”video, which had an AUC with\\na large effect size (equivalent to d> 0.8). While this may\\nsuggest that presenting the “Rhymes ”video alone is suf ﬁ-\\ncient for differentiating between the ASD and non-ASD\\ngroups, we caution readers from coming to this conclu-sion for two reasons: First, it is possible that, had we had\\na larger sample, the other videos would have had a larger\\neffect. Second, we anticipate that there will be variabilityin the ASD group with regard to which features a single\\nchild will express and different videos may be better\\nsuited to elicit different features in any given individual.As such, we believe that it is important to understand\\nhow each independent feature, in this case facial affect,\\nperforms across the different videos so that we can beingto build better predictive models from combinations of\\nfeatures [e.g., facial affect and postural sway as described\\nin Dawson et al., 2018].\\nTo further understand the difference of facial expres-\\nsion in the non-ASD group as compared to our ASD sam-\\nple, we explored the facial landmarks differentiatingbetween the Other facial expression category that domi-\\nnated the non-ASD control group versus the Neutral\\nfacial expression, which was more common in the ASDgroup. Through this analysis, we identi ﬁed the features of\\nraised eyebrows and open mouth to play a role in dis-\\ncriminating between the Other vs. Neutral categories.This facial pattern is consistent with an engaged/inter-\\nested look displayed when a child is actively watching, as\\ndescribed in young children by Sullivan and Lewis [2003].It is interesting to note a raised pitch angle was also statis-\\ntically signi ﬁcant. Since the median difference of this\\nangle between the two facial expression is small (3.2\\n/C14),\\nthis may be a natural movement of raising one ’s\\neyebrows.\\nOur results need to be considered in light of several\\nlimitations. First, the CVA models of facial expressions\\nused in the current study were trained on adult faces\\n[Hashemi et al., 2018]. Despite this, our previous ﬁndings\\nwith young children demonstrate good concordance\\nbetween human and CVA coding on the designation of\\nfacial expressions [Hashemi et al., 2018]. Furthermore,the Other facial expression category includes all non-\\npositive or negative expressions. As such, even though\\nwe were able to determine the predominant feature driv-ing those expressions was the raised eyebrows, which is\\nin line with our observations from watching the movies,Table 2. Comparison of ASD Versus Non-ASD: Area Under the\\nCurve (AUC) Analyses\\nAUC without covariates AUC with age in the model\\nBubbles 1 0.62 0.75\\nBunny 0.66 0.81\\nPuppets 0.67 0.78\\nRhymes 0.73 0.83Bubbles 2 0.64 0.79\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 7', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 6}),\n",
       "  Document(page_content='it is possible that there are a combination of facial expres-\\nsions in the non-ASD group driving this designation.\\nFuture studies will need to train on the engaged/inter-ested facial expression speci ﬁcally and test the robustness\\nof this ﬁnding. Additionally, though we have previously\\ndemonstrated good reliability between our CVA algo-rithms and human coding of emotions [Hashemi\\net al., 2015], future validation of our CVA analysis of\\nemotional facial expressions in larger datasets is currentlyunderway. Second, using the LASSO statistical approach\\nmeans our model may not select all features that have dif-\\nferentiating information. However, we selected thisapproach because it minimizes over ﬁtting the model.\\nThird, our sample size was relatively small and we do not\\nhave separate training and testing samples. To accountfor this, we applied cross-validation on the ROC curves\\neven though this decreased the performance metrics of\\nthe model. This suggests that our ROC results are poten-tially conservative. Finally, our comparison group con-\\ntains both children with typical development and\\nchildren with non-ASD developmental delays, a factorthat can be viewed as both a weakness and a strength.\\nPrevious research has demonstrated that increased fre-\\nquency of neutral expressions does differentiate childrenwith ASD from those with other developmental delays\\n[Bieberich & Morgan, 2004; Yirmiya et al., 1989]. How-\\never, due to the small sample size of children with non-ASD developmental delays, we were unable to directly\\ntest this in our data. Furthermore, because only a subset\\nof the sample received an assessment of cognitive ability,it is possible that there were additional children in the\\nnon-ASD comparison group that also had a developmen-\\ntal delay that was undetected. Ongoing research in a pro-spective, longitudinal study with larger samples is\\nunderway to further parse the ability of our CVA tools to\\ndifferentiate between children with ASD, children with anon-ASD developmental delay and/or attention de ﬁcit\\nhyperactivity disorder, and typically developing children.\\nWhile a difference in facial expression is one core fea-\\nture of ASD, the heterogeneity in ASD means we do not\\nexpect all children with ASD to display this sign of ASD.\\nAs such, our next step is to combine the current resultswith other measures of autism risk assessed through the\\ncurrent digital screening tool, including response to\\nname [Campbell et al., 2019], postural sway [Dawsonet al., 2018], and differential vocalizations [Tenenbaum\\net al., 2020], among other features, to develop a risk score\\nbased on multiple behaviors [Dawson & Sapiro, 2019].Since no one child is expected to display every risk behav-\\nior, a goal is to determine thresholds based on the total\\nnumber of behaviors, regardless of which combination ofbehaviors, to asses for risk. This is similar to what is done\\nin commonly used screening and diagnostic tools, such\\nas the M-CHAT [Robins et al., 2014], Autism Diagnostic\\nFigure 4. Analysis of Other Facial Expression. The 4 panels on the left depict heat maps of aligned landmarks across ASD and non-ASD\\nparticipants when they were exhibiting Neutral and other facial expressions (the color bar indicates proportion of frames where land-marks were displayed in a given image location). The single panel on the right is an example of the landmark distances explored.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 8', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 7}),\n",
       "  Document(page_content='Interview [Lord, Rutter, & Le Couteur, 1994], and ADOS\\n[Gotham, Risi, Pickles, & Lord, 2007; Lord et al., 2000].\\nIn summary, we evaluated an integrated, objective tool\\nfor the elicitation and measurement of facial movements\\nand expressions in toddlers with and without ASD. Thecurrent study adds to a body of research supporting digi-\\ntal behavioral phenotyping as a viable method for\\nassessing autism risk behaviors. Our goal is to furtherdevelop and validate this tool so that it can eventually be\\nused within the context of current standard of care to\\nenhance autism screening in pediatric populations.\\nAcknowledgments\\nFunding for this work was provided by NIH R01-MH120093 (Sapiro, Dawson PIs), NIH RO1-MH121329\\n(Dawson, Sapiro PIs), NICHD P50HD093074 (Dawson,\\nKollins, PIs), Simons Foundation (Sapiro, Dawson, PIs),Duke Department of Psychiatry and Behavioral Sciences\\nPRIDe award (Dawson, PI), Duke Education and Human\\nDevelopment Initiative, Duke-Coulter Translational Part-nership Grant Program, National Science Foundation, a\\nStylli Translational Neuroscience Award, and the Depart-\\nment of Defense. Some of the stimuli used for the movieswere created by Geraldine Dawson, Michael Murias, and\\nSara Webb at the University of Washington. This work\\nwould not have been possible without the help of Eliza-beth Glenn, Elizabeth Adler, and Samuel Marsan. We also\\ngratefully acknowledge the participation of the children\\nand families in this study. Finally, we could not havecompleted this study without the assistance and collabo-\\nration of Duke pediatric primary care providers.\\nConﬂict of interests\\nGuillermo Sapiro has received basic research gifts from\\nAmazon, Google, Cisco, and Microsoft and is a consul-tant for Apple and Volvo. Geraldine Dawson is on the Sci-\\nentiﬁc Advisory Boards of Janssen Research and\\nDevelopment, Akili, Inc., LabCorp, Inc., Tris Pharma, andRoche Pharmaceutical Company, a consultant for Apple,\\nInc, Gerson Lehrman Group, Guidepoint, Inc., Teva Phar-\\nmaceuticals, and Axial Ventures, has received grantfunding from Janssen Research and Development, and is\\nCEO of DASIO, LLC (with Guillermo Sapiro). Dawson\\nreceives royalties from Guilford Press, Springer, andOxford University Press. Dawson, Sapiro, Carpenter,\\nHashemi, Campbell, Espinosa, Baker, and Egger helped\\ndevelop aspects of the technology that is being used inthe study. The technology has been licensed and Daw-\\nson, Sapiro, Carpenter, Hashemi, Espinosa, Baker, Egger,\\nand Duke University have bene ﬁtedﬁnancially.References\\nAdrien, J. L., Faure, M., Perrot, A., Hameury, L., Garreau, B.,\\nBarthelemy, C., & Sauvage, D. (1991). Autism and familyhome movies: Preliminary ﬁndings. Journal of Autism and\\nDevelopmental Disorders, 21(1), 43 –49.\\nAdrien, J. L., Lenoir, P., Martineau, J., Perrot, A., Hameury, L.,\\nLarmande, C., & Sauvage, D. (1993). Blind ratings of earlysymptoms of autism based upon family home movies. Jour-\\nnal of the American Academy of Child and Adolescent Psy-\\nchiatry, 32(3), 617 –626. https://doi.org/10.1097/00004583-\\n199305000-00019\\nBal, V. H., Kim, S. H., Fok, M., & Lord, C. (2019). Autism spec-\\ntrum disorder symptoms from ages 2 to 19 years: Implicationsfor diagnosing adolescents and young adults. AutismResearch, 12(1), 89 –99. https://doi.org/10.1002/aur.2004\\nBaranek, G. T. (1999). Autism during infancy: a retrospective\\nvideo analysis of sensory-motor and social behaviors at 9 –12\\nmonths of age. Journal of Autism and Developmental Disor-ders, 29(3), 213 –224.\\nBieberich, A. A., & Morgan, S. B. (2004). Self-regulation and\\naffective expression during play in children with autism orDown syndrome: A short-term longitudinal study. Journal ofAutism and Developmental Disorders, 34(4), 439 –448.\\nhttps://doi.org/10.1023/b:jadd.0000037420.16169.28\\nBieleninik, Ł., Posserud, M.-B., Geretsegger, M., Thompson, G.,\\nElefant, C., & Gold, C. (2017). Tracing the temporal stabilityof autism spectrum diagnosis and severity as measured by theAutism Diagnostic Observation Schedule: A systematic reviewand meta-analysis. PLoS One, 12(9), e0183160 –e0183160.\\nhttps://doi.org/10.1371/journal.pone.0183160\\nCampbell, K., Carpenter, K. L., Hashemi, J., Espinosa, S.,\\nMarsan, S., Borg, J. S., …Dawson, G. (2019). Computer vision\\nanalysis captures atypical attention in toddlers with autism.Autism, 23(3), 619 –628. https://doi.org/10.1177/\\n1362361318766247\\nCampbell, K., Carpenter, K. L. H., Espinosa, S., Hashemi, J.,\\nQiu, Q., Tepper, M., …Dawson, G. (2017). Use of a Digital\\nModi ﬁed Checklist for Autism in Toddlers —Revised with\\nfollow-up to improve quality of screening for autism. TheJournal of Pediatrics, 183, 133 –139.e1. https://doi.org/10.\\n1016/j.jpeds.2017.01.021\\nCapriola-Hall, N. N., Wieckowski, A. T., Swain, D., Tech, V.,\\nAly, S., Youssef, A., …White, S. W. (2019). Group differences\\nin facial emotion expression in autism: Evidence for the util-ity of machine classi ﬁcation. Behavior Therapy, 50(4),\\n828–838. https://doi.org/10.1016/j.beth.2018.12.004\\nChang, C.-C., & Lin, C.-J. (2011). LIBSVM: A library for support\\nvector machines. ACM Transactions on Intelligent Systemsand Technology (TIST), 2(3), 1 –27.\\nChristensen, D. L., Baio, J., Braun, K. V., Bilder, D., Charles, J.,\\nConstantino, J. N., …Yeargin-Allsopp, M. (2016). Prevalence\\nand characteristics of autism spectrum disorder among chil-dren aged 8 years —Autism and Developmental Disabilities\\nMonitoring Network, 11 Sites, United States, 2012. MMWRSurveillance Summaries, 65(SS-3), 1 –23.\\nClifford, S., & Dissanayake, C. (2009). Dyadic and triadic behav-\\niours in infancy as precursors to later social responsiveness inyoung children with autistic disorder. Journal of Autism and\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 9', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 8}),\n",
       "  Document(page_content='Developmental Disorders, 39(10), 1369 –1380. https://doi.\\norg/10.1007/s10803-009-0748-x\\nClifford, S., Young, R., & Williamson, P. (2007). Assessing the\\nearly characteristics of autistic disorder using video analysis.Journal of Autism and Developmental Disorders, 37(2),301–313. https://doi.org/10.1007/s10803-006-0160-8\\nClifford, S. M., & Dissanayake, C. (2008). The early development\\nof joint attention in infants with autistic disorder using homevideo observations and parental interview. Journal of Autismand Developmental Disorders, 38(5), 791 –805. https://doi.\\norg/10.1007/s10803-007-0444-7\\nCzapinski, P., & Bryson, S. (2003). Reduced facial muscle move-\\nments in autism: Evidence for dysfunction in the neuromus-\\ncular pathway? Brain and Cognition, 51(2), 177 –179.\\nDawson, G., & Bernier, R. (2013). A quarter century of progress\\non the early detection and treatment of autism spectrumdisorder. Development and Psychopathology, 25(4 Pt 2),1455–1472. https://doi.org/10.1017/S0954579413000710\\nDawson, G., Campbell, K., Hashemi, J., Lippmann, S. J.,\\nSmith, V., Carpenter, K., …Sapiro, G. (2018). Atypical pos-\\ntural control can be detected via computer vision analysis intoddlers with autism spectrum disorder. Scienti ﬁc Reports, 8\\n(1), 17008. https://doi.org/10.1038/s41598-018-35215-8\\nDawson, G., Hill, D., Spencer, A., Galpert, L., & Watson, L.\\n(1990). Affective exchanges between young autistic-childrenand their mothers. Journal of Abnormal Child Psychology, 18(3), 335 –345. https://doi.org/10.1007/Bf00916569\\nDawson, G., & Sapiro, G. (2019). Potential for digital behavioral\\nmeasurement tools to transform the detection and diagnosisof autism spectrum disorder. JAMA Pediatrics, 173(4),305–306. https://doi.org/10.1001/jamapediatrics.2018.5269\\nDawson, G., Toth, K., Abbott, R., Osterling, J., Munson, J.,\\nEstes, A., & Liaw, J. (2004). Early social attention impairmentsin autism: social orienting, joint attention, and attention todistress. Developmental Psychology, 40(2), 271 –283. https://\\ndoi.org/10.1037/0012-1649.40.2.271\\nDe la Torre, F., Chu, W. S., Xiong, X., Vicente, F., Ding, X., &\\nCohn, J. (2015). IntraFace. IEEE International Conference on\\nAutomatic Face Gesture Recognition Workshops . (pp. 1 –8).\\nLjubljana, Slovenia: IEEE. https://doi.org/10.1109/fg.2015.7163082\\nDys, S. P., & Malti, T. (2016). It ’s a two-way street: Automatic\\nand controlled processes in children ’s emotional responses\\nto moral transgressions. Journal of Experimental Child Psy-chology, 152, 31 –40. https://doi.org/10.1016/j.jecp.2016.\\n06.011\\nEgger, H. L., Dawson, G., Hashemi, J., Carpenter, K. L. H.,\\nEspinosa, S., Campbell, K., …Sapiro, G. (2018). Automatic\\nemotion and attention analysis of young children at home: AResearchKit autism feasibility study. NPJ Digital Medicine, 1(1), 20. https://doi.org/10.1038/s41746-018-0024-6\\nEkman, P. (2003). Emotions revealed (2nd ed.). New York, NY:\\nTimes Books.\\nEkman, R. (1997). What the face reveals: Basic and applied stud-\\nies of spontaneous expression using the Facial Action CodingSystem (FACS). New York, NY: Oxford University Press.\\nFilliter, J. H., Longard, J., Lawrence, M. A., Zwaigenbaum, L.,\\nBrian, J., Garon, N., …Bryson, S. E. (2015). Positive affect in\\ninfant siblings of children diagnosed with autism spectrumdisorder. Journal of Abnormal Child Psychology, 43(3),\\n567–575. https://doi.org/10.1007/s10802-014-9921-6\\nFischler, M. A., & Bolles, R. C. (1981). Random sample consen-\\nsus: A paradigm for model ﬁtting with applications to image\\nanalysis and automated cartography. Communications of theAssociation for Computing Machinery, 24(6), 381 –395.\\nhttps://doi.org/10.1145/358669.358692\\nGadea, M., Aliño, M., Espert, R., & Salvador, A. (2015). Deceit\\nand facial expression in children: The enabling role of the“poker face ”child and the dependent personality of the\\ndetector. Frontiers in Psychology, 6, 1089 –1089. https://doi.\\norg/10.3389/fpsyg.2015.01089\\nGangi, D. N., Ibanez, L. V., & Messinger, D. S. (2014). Joint atten-\\ntion initiation with and without positive affect: Risk group\\ndifferences and associations with ASD symptoms. Journal ofAutism and Developmental Disorders, 44(6), 1414 –1424.\\nhttps://doi.org/10.1007/s10803-013-2002-9\\nGotham, K., Risi, S., Pickles, A., & Lord, C. (2007). The Autism\\nDiagnostic Observation Schedule: Revised algorithms forimproved diagnostic validity. Journal of Autism and Develop-mental Disorders, 37(4), 613 –627. https://doi.org/10.1007/\\ns10803-006-0280-1\\nGuha, T., Yang, Z., Grossman, R. B., & Narayanan, S. S. (2018).\\nA computational study of expressive facial dynamics inchildren with autism. IEEE Transactions on AffectiveComputing, 9(1), 14 –20. https://doi.org/10.1109/taffc.2016.\\n2578316\\nGuha, T., Yang, Z., Ramakrishna, A., Grossman, R. B., Darren, H.,\\nLee, S., & Narayanan, S. S. (2015). On quantifying facialexpression-related atypicality of children with autism spec-trum disorder. Proceedings of IEEE International Conferenceon Acoustics, Speech, and Signal Processing, 2015, 803 –807.\\nhttps://doi.org/10.1109/icassp.2015.7178080\\nGuthrie, W., Wallis, K., Bennett, A., Brooks, E., Dudley, J.,\\nGerdes, M., …Miller, J. S. (2019). Accuracy of autism screen-\\ning in a large pediatric network. Pediatrics, 144(4),e20183963. https://doi.org/10.1542/peds.2018-3963\\nHaines, N., Bell, Z., Crowell, S., Hahn, H., Kamara, D.,\\nMcDonough-Caplan, H., …Beauchaine, T. P. (2019). Using\\nautomated computer vision and machine learning to codefacial expressions of affect and arousal: Implications for emo-tion dysregulation research. Development and Psychopathol-ogy, 31(3), 871 –886. https://doi.org/10.1017/S09545794\\n19000312\\nHashemi, J., Campbell, K., Carpenter, K., Harris, A., Qiu, Q.,\\nTepper, M., …Calderbank, R. (2015). A scalable app for mea-\\nsuring autism risk behaviors in young children: A technical validity\\nand feasibility study . Paper presented at the Proceedings of the\\n5th EAI International Conference on Wireless Mobile Com-munication and Healthcare, Dublin, Ireland.\\nHashemi, J., Dawson, G., Carpenter, K. L. H., Campbell, K.,\\nQiu, Q., Espinosa, S., …Sapiro, G. (2018). Computer vision\\nanalysis for quanti ﬁcation of autism risk behaviors. IEEE\\nTransactions on Affective Computing, 1 –1. https://doi.org/\\n10.1109/taffc.2018.2868196\\nJones, E. J., Dawson, G., Kelly, J., Estes, A., & Webb, S. J. (2017).\\nParent-delivered early intervention in infants at risk for ASD:Effects on electrophysiological and habituation measures ofsocial attention. Autism Research, 10(5), 961 –972.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 10', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 9}),\n",
       "  Document(page_content='Jones, E. J., Venema, K., Earl, R., Lowy, R., Barnes, K., Estes, A., …\\nWebb, S. (2016). Reduced engagement with social stimuli in6-month-old infants with later autism spectrum disorder: Alongitudinal prospective study of infants at high familial risk.Journal of Neurodevelopmental Disorders, 8(1), 7.\\nKhowaja, M., Robins, D. L., & Adamson, L. B. (2017). Utilizing\\ntwo-tiered screening for early detection of autism spectrumdisorder. Autism, 22, 881 –890. https://doi.org/10.1177/\\n1362361317712649\\nLewinski, P. (2015). Automated facial coding software outper-\\nforms people in recognizing neutral faces as neutral fromstandardized datasets. Frontiers in Psychology, 6, 1386.https://doi.org/10.3389/fpsyg.2015.01386\\nLoBue, V., & Thrasher, C. (2014). The Child Affective Facial\\nExpression (CAFE) set: Validity and reliability from untrainedadults. Frontiers in Psychology, 5, 1532. https://doi.org/10.3389/fpsyg.2014.01532\\nLord, C., Risi, S., Lambrecht, L., Cook, E. H., Jr., Leventhal, B. L.,\\nDiLavore, P. C., …Rutter, M. (2000). The autism diagnostic\\nobservation schedule-generic: A standard measure of socialand communication de ﬁcits associated with the spectrum of\\nautism. Journal of Autism and Developmental Disorders, 30(3), 205 –223.\\nLord, C., Rutter, M., & Le Couteur, A. (1994). Autism Diagnostic\\nInterview-Revised: A revised version of a diagnostic interviewfor caregivers of individuals with possible pervasive develop-mental disorders. Journal of Autism and Developmental Dis-orders, 24(5), 659 –685.\\nLuyster, R., Gotham, K., Guthrie, W., Cof ﬁng, M., Petrak, R.,\\nPierce, K., …Lord, C. (2009). The Autism Diagnostic Observa-\\ntion Schedule-toddler module: A new module of astandardized diagnostic measure for autism spectrum disor-ders. Journal of Autism and Developmental Disorders, 39(9),1305–1320. https://doi.org/10.1007/s10803-009-0746-z\\nMaestro, S., Muratori, F., Cavallaro, M. C., Pei, F., Stern, D.,\\nGolse, B., & Palacio-Espasa, F. (2002). Attentional skills duringtheﬁrst 6 months of age in autism spectrum disorder. Journal\\nof the American Academy of Child and Adolescent Psychia-try, 41(10), 1239 –1245. https://doi.org/10.1097/00004583-\\n200210000-00014\\nMcgee, G. G., Feldman, R. S., & Chernin, L. (1991). A compari-\\nson of emotional facial display by children with autism andtypical preschoolers. Journal of Early Intervention, 15(3),237–245. https://doi.org/10.1177/105381519101500303\\nMessinger, D. S., Mahoor, M. H., Chow, S. M., & Cohn, J. F.\\n(2009). Automated measurement of facial expression in\\ninfant-mother interaction: A pilot study. Infancy, 14(3),\\n285–305. https://doi.org/10.1080/15250000902839963\\nMullen, E. M. (1995). Mullen scales of early learning. Circle\\nPines, MN: American Guidance Service Inc.\\nMurias, M., Major, S., Compton, S., Buttinger, J., Sun, J. M.,\\nKurtzberg, J., & Dawson, G. (2018). Electrophysiological bio-markers predict clinical improvement in an open-label trialassessing ef ﬁcacy of autologous umbilical cord blood for treat-\\nment of autism. Stem Cells Translational Medicine, 7(11),783–791. https://doi.org/10.1002/sctm.18-0090\\nMyers, S. M., Johnson, C. P., & Council on Children With Dis-\\nabilities. (2007). Management of children with autismspectrum disorders. Pediatrics, 120(5), 1162 –1182. https://\\ndoi.org/10.1542/peds.2007-2362\\nNichols, C. M., Ibanez, L. V., Foss-Feig, J. H., & Stone, W. L.\\n(2014). Social smiling and its components in high-risk infantsiblings without later ASD symptomatology. Journal ofAutism and Developmental Disorders, 44(4), 894 –902.\\nhttps://doi.org/10.1007/s10803-013-1944-2\\nOsterling, J. A., Dawson, G., & Munson, J. A. (2002). Early recog-\\nnition of 1-year-old infants with autism spectrum disorderversus mental retardation. Development and Psychopathol-ogy, 14(2), 239 –251.\\nOwada, K., Kojima, M., Yassin, W., Kuroda, M., Kawakubo, Y.,\\nKuwabara, H., …Yamasue, H. (2018). Computer-analyzed\\nfacial expression as a surrogate marker for autism spectrum\\nsocial core symptoms. PLoS One, 13(1), e0190442. https://doi.org/10.1371/journal.pone.0190442\\nRice, M. E., & Harris, G. T. (2005). Comparing effect sizes in\\nfollow-up studies: ROC area, Cohen ’sd, and r. Law and\\nHuman Behavior, 29(5), 615 –620. https://doi.org/10.1007/\\ns10979-005-6832-7\\nRobins, D. L., Casagrande, K., Barton, M., Chen, C. M., Dumont-\\nMathieu, T., & Fein, D. (2014). Validation of the modi ﬁed\\nchecklist for autism in toddlers, revised with follow-up (M-CHAT-R/F). Pediatrics, 133(1), 37 –\\n45. https://doi.org/10.\\n1542/peds.2013-1813\\nSamad, M. D., Diawara, N., Bobzien, J. L., Harrington, J. W.,\\nWitherow, M. A., & Iftekharuddin, K. M. (2018). Afeasibility study of autism behavioral markers in spontaneousfacial, visual, and hand movement response data. IEEETransactions on Neural Systems and Rehabilitation Engineer-ing, 26(2), 353 –361. https://doi.org/10.1109/tnsre.2017.\\n2768482\\nSnow, M. E., Hertzig, M. E., & Shapiro, T. (1987). Expression\\nof emotion in young autistic children. Journal of theAmerican Academy of Child and Adolescent Psychiatry, 26(6), 836 –838. https://doi.org/10.1097/00004583-198726060-\\n00006\\nSullivan, M. W., & Lewis, M. (2003). Emotional expressions of\\nyoung infants and children —A practitioner ’s primer. Infants\\nand Young Children, 16(2), 120 –142. https://doi.org/10.\\n1097/00001163-200304000-00005\\nTantam, D., Holmes, D., & Cordess, C. (1993). Nonverbal expres-\\nsion in autism of Asperger type. Journal of Autism and Devel-opmental Disorders, 23(1), 111 –133.\\nTenenbaum, E. J., Carpenter, K. L. H., Sabatos-DeVito, M.,\\nHashemi, J., Vermeer, S., Sapiro, G., & Dawson, G. (2020). A\\nsix-minute measure of vocalizations in toddlers with autism\\nspectrum disorder. Autism Research, 13(8), 1373 –1382.\\nhttps://doi.org/10.1002/aur.2293\\nTibshirani, R. (1996). Regression shrinkage and selection via the\\nlasso. Journal of the Royal Statistical Society: Series B(Methodological), 58(1), 267 –288. https://doi.org/10.1111/j.\\n2517-6161.1996.tb02080.x\\nTrevisan, D. A., Bowering, M., & Birmingham, E. (2016).\\nAlexithymia, but not autism spectrum disorder, may berelated to the production of emotional facial expressions.Molecular Autism, 7, 46. https://doi.org/10.1186/s13229-016-0108-6\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 11', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 10}),\n",
       "  Document(page_content='Trevisan, D. A., Hoskyn, M., & Birmingham, E. (2018). Facial\\nexpression production in autism: A meta-analysis. AutismResearch, 11(12), 1586 –1601. https://doi.org/10.1002/aur.2037\\nWaizbard-Bartov, E., Ferrer, E., Young, G. S., Heath, B., Rogers, S.,\\nWu Nordahl, C., …Amaral, D. G. (2020). Trajectories of\\nautism symptom severity change during early childhood.Journal of Autism and Developmental Disorders. https://doi.org/10.1007/s10803-020-04526-z\\nWerner, E., Dawson, G., Osterling, J., & Dinno, N. (2000). Brief\\nreport: Recognition of autism spectrum disorder before oneyear of age: A retrospective study based on home videotapes.Journal of Autism and Developmental Disorders, 30(2),\\n157–162.\\nYin, L., Wei, X., Sun, Y., Wang, J., & Rosato, M. J. (2006). A3 D\\nfacial expression database for facial behavior research . Paper pres-\\nented at the 7th International Conference on automatic faceand gesture recognition (FGR06), University of Southampton,Southampton, UK.\\nYirmiya, N., Kasari, C., Sigman, M., & Mundy, P. (1989). Facial\\nexpressions of affect in autistic, mentally retarded and normalchildren. Journal of Child Psychology and Psychiatry, 30(5),725–735.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 12', metadata={'source': 'papers/carpenter2020 (1).pdf', 'page': 11})],\n",
       " [Document(page_content='1\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8www.nature.com/scientificreportsAtypical postural control can be \\ndetected via computer vision \\nanalysis in toddlers with autism \\nspectrum disorder\\nGeraldine Dawson   1, Kathleen Campbell2, Jordan Hashemi1,3, Steven J. Lippman n  4, \\nValerie Smith4, Kimberly Carpente r1, Helen Egger5, Steven Espinosa3, Saritha Vermeer1, \\nJeffrey\\xa0 Baker6 & Guillermo Sapiro3,7\\nEvidence\\xa0 suggests\\xa0 that\\xa0differences\\xa0 in\\xa0motor\\xa0function\\xa0 are\\xa0an\\xa0early\\xa0feature\\xa0of\\xa0autism\\xa0spectrum\\xa0 disorder\\xa0\\n(ASD).\\xa0One\\xa0aspect\\xa0of\\xa0motor\\xa0ability\\xa0that\\xa0develops\\xa0 during\\xa0childhood\\xa0 is\\xa0postural\\xa0 control,\\xa0reflected\\xa0 in\\xa0the\\xa0\\nability to maintain a steady head and body position without excessive sway. Observational studies have \\ndocumented\\xa0 differences\\xa0 in\\xa0postural\\xa0 control\\xa0in\\xa0older\\xa0children\\xa0with\\xa0ASD.\\xa0The\\xa0present\\xa0study\\xa0used\\xa0computer\\xa0\\nvision\\xa0analysis\\xa0to\\xa0assess\\xa0midline\\xa0head\\xa0postural\\xa0 control,\\xa0as\\xa0reflected\\xa0 in\\xa0the\\xa0rate\\xa0of\\xa0spontaneous\\xa0 head\\xa0\\nmovements\\xa0 during\\xa0states\\xa0of\\xa0active\\xa0attention,\\xa0 in\\xa0104\\xa0toddlers\\xa0 between\\xa0 16–31\\xa0months\\xa0of\\xa0age\\xa0(Mean\\xa0 =\\xa022\\xa0\\nmonths),\\xa0 22\\xa0of\\xa0whom\\xa0were\\xa0diagnosed\\xa0 with\\xa0ASD.\\xa0Time-series\\xa0 data\\xa0revealed\\xa0 robust\\xa0group\\xa0differences\\xa0\\nin\\xa0the\\xa0rate\\xa0of\\xa0head\\xa0movements\\xa0 while\\xa0the\\xa0toddlers\\xa0 watched\\xa0 movies\\xa0depicting\\xa0 social\\xa0and\\xa0nonsocial\\xa0\\nstimuli.\\xa0Toddlers\\xa0 with\\xa0ASD\\xa0exhibited\\xa0 a\\xa0significantly\\xa0 higher\\xa0rate\\xa0of\\xa0head\\xa0movement\\xa0 as\\xa0compared\\xa0 to\\xa0\\nnon-ASD\\xa0 toddlers,\\xa0 suggesting\\xa0 difficulties\\xa0 in\\xa0maintaining\\xa0 midline\\xa0position\\xa0of\\xa0the\\xa0head\\xa0while\\xa0engaging\\xa0\\nattentional\\xa0 systems.\\xa0 The\\xa0use\\xa0of\\xa0digital\\xa0phenotyping\\xa0 approaches,\\xa0 such\\xa0as\\xa0computer\\xa0 vision\\xa0analysis,\\xa0\\nto\\xa0quantify\\xa0 variation\\xa0 in\\xa0early\\xa0motor\\xa0behaviors\\xa0 will\\xa0allow\\xa0for\\xa0more\\xa0precise,\\xa0objective,\\xa0 and\\xa0quantitative\\xa0\\ncharacterization\\xa0 of\\xa0early\\xa0motor\\xa0signatures\\xa0 and\\xa0potentially\\xa0 provide\\xa0new\\xa0automated\\xa0 methods\\xa0 for\\xa0early\\xa0\\nautism\\xa0risk\\xa0identification.\\nAlthough the core symptoms of autism spectrum disorder (ASD) are defined by atypical patterns of social inter -\\naction and the presence of stereotyped and repetitive behaviors and interests, evidence suggests that differences \\nin motor function are also an important early feature of autism. Motor delays could contribute to early hall-mark autism symptoms, including difficulties in orienting to name involving the eyes and head turns, coordinat-ing head and limb movements involved in gaze following and other joint attention behavior, such as pointing. Teitelbaum et al.\\n1 found that atypical movements (e.g. shape of mouth, patterns of lying, righting, sitting) were \\npresent by 4–6 months of age in infants later diagnosed with ASD. Another study of videotapes taken of infants 12–21 weeks of age detected lower levels of positional symmetry among infants later diagnosed with ASD\\n2 sug-\\ngesting atypical development of cerebellar pathways that control balance and symmetry. Six-month-old infants who later were diagnosed with ASD tend to exhibit head lag when pulled to sit, reflecting early differences in motor development\\n3. A study of home videos taken between birth and six months of age found that some infants \\nwho were later diagnosed with ASD showed postural stiffness, slumped posture, and/or head lag4. Other motor \\nsymptoms observed in infants later diagnosed with ASD include fluctuating muscle tone5 and oral-motor abnor -\\nmalities, such as insufficient opening of the mouth in anticipation of the approaching spoon during feeding6. \\nLongitudinal research with very low birth weight infants revealed that infants who are later diagnosed with ASD had poorer ability in maintaining midline position of the head at 9–20 weeks of age\\n7. The authors used visual \\n1Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke \\nUniversity, Durham, North Carolina, USA. 2University of Utah, Salt Lake City, Utah, USA. 3Department of Electrical \\nand Computer Engineering, Duke University, Durham, North Carolina, USA. 4Department of Population Health \\nSciences, Duke University, Durham, North Carolina, USA. 5NYU Langone Child Study Center, New York University, \\nNew York, New York, USA. 6Department of Pediatrics, Duke University, Durham, NC, USA. 7Departments of \\nBiomedical Engineering, Computer Science, and Mathematics, Duke University, Durham, NC, USA. Correspondence \\nand requests for materials should be addressed to G.D. (email: geraldine.dawson@duke.edu )Received: 26 July 2018\\nAccepted: 31 October 2018\\nPublished online: 19 November 2018OPENCorrected: Author Correction', metadata={'source': 'papers/Dawson.pdf', 'page': 0}),\n",
       "  Document(page_content='www.nature.com/scientificreports/2\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8inspection to classify head position in each video frame to yield a measure of midline head position and number \\nof changes in position.\\nThe development of postural control is an index of neuromuscular reactions to the motion of body mass in \\norder to retain stability. Previous studies have documented the developmental progression of the ability to main-tain an upright posture that is accompanied by decreases in postural sway\\n8. Several studies with older children \\nwith ASD have documented deficiencies in postural control, reflected in the presence of postural sway, which is accentuated when children with ASD are viewing arousing stimuli, including complex multi-sensory and social stimuli\\n9–11. Less is known about the presence of postural sway in young children with ASD.\\nStudies of motor and other behaviors in young children have typically relied on subjective and labor-intensive \\nhuman coding to rate and measure behavior. The recent use of digital phenotyping approaches, such as computer vision analysis (CV A) of videotaped recordings of behavior, has allowed for automated, precise and quantitative measurement of subtle, dynamic differences in motor behavior. We reported previously on a result using CV A to \\nmore precisely measure toddlers’ orienting response to a name call, noting that, compared to toddlers without \\nASD, toddlers with ASD oriented less frequently; when they did orient, their head turn was a full second slower, on average\\n12. Such differences in motor speed would likely not be detected with the naked eye during a typical clinical \\nevaluation. Anzulewicz et al .13 used smart tablet computers with touch-sensitive screens and embedded inertial \\nmovement sensors to record movement kinematics and gesture forces in 3–6-year-old children with and without ASD. Children with ASD used greater force and faster and larger gesture kinematics. Machine learning analysis of the children’s motor patterns classified the children with ASD with a high level of accuracy. In another study using automated methods, differences in head movement dynamics were found between 2.5–6.5-year-old children \\nwith and without ASD while they watched movies of social and nonsocial stimuli. Children with ASD showed \\nmore frequent head turning, especially while watching social stimuli\\n14. The authors suggested that the children \\nwith ASD might be using head movement to modulate their arousal while watching social stimuli. Wu et al.15  \\nused electromagnetic sensors to analyze continuous movements at millisecond time scale in older children with ASD versus typical development. They applied a triangular smoothing algorithm to the 3D positional raw move-ment data that preserved the local speed fluctuations. They found that individuals with ASD exhibited signifi-cantly more “sensorimotor noise” when compared to individuals with typical development.\\nThe present study used CV A to characterize head movements that didn’t involve spontaneous or volitional \\norienting or turning away from the stimuli. Rather, we were interested in subtler midline head movements that are more likely related to postural stability. The study compared the behaviors of toddlers with ASD versus those without ASD while the children watched a series of dynamic movies involving different types of stimuli, includ-ing stimuli of both a social and nonsocial nature. While the children watched the movies, their head movements were automatically detected and tracked using landmarks on the participant’s face. The goal of this analysis was to quantify the rate of spontaneous head movements and to determine whether there were differences in this motor feature between young children with and without ASD.\\nMethods\\nParticipants. Participants were 104 children between 16–31 months of age (Mean = 22 months). \\nExclusionary criteria included known vision or hearing deficits, lack of exposure to English at home and/or car -\\negivers who did not speak and read English sufficiently for informed consent. Twenty-two of the children had autism spectrum disorder. The non-ASD comparison group was comprised of 96 typically developing children and 8 children with language delay or developmental delay of clinical significance sufficient to qualify for speech \\nor developmental therapy. Participants in the comparison group had a mean age of 21.91 months (SD = 3.78) and \\nthose in the ASD group had a mean age of 26.19 months (SD = 4.07). Ethnic/racial composition of the ASD and \\ncomparison groups, respectively, was 59% and 45% white, 13% and 14% African American, 6% and 5% Asian, \\nand 22% and 36% multi-racial/other. Percent males was 77% in the ASD group and 59% in the comparison group.\\nParticipants were recruited from primary care pediatric clinics by a research assistant, referral from their phy-\\nsician, and by community advertisement. All caregivers/legal guardians of participants gave written, informed consent, and the study protocol was approved by the Duke University Health System Institutional Review Board. Methods were carried out in accordance with institutional, state, and federal guidelines and regulation.\\nDiagnostic Assessments. Diagnostic evaluations to confirm ASD were based on the Autism Diagnostic \\nObservation Scale-Toddler (ADOS-T), which were conducted by a licensed psychologist or trained research-reliable examiner overseen by a licensed psychologist\\n16. The mean ADOS-T score was 18.81 (SD = 4.20). \\nThe mean IQ based on the Mullen Scales of Early Learning Composite Score for the ASD group was 63.58 (SD = 25.95). Developmental and/or language delay was determined based on the Mullen Scales (> 1 SD below \\nthe mean in overall learning composite or receptive/expressive language).\\nStimuli. A series of stimuli, comprised of brief movies, were shown on a smart tablet while the child sat on \\na caregiver’s lap. The tablet was placed on a stand approximately 3 feet away from the child to prevent the child from touching the screen. The stimuli consisted of a series of brief developmentally-appropriate movies designed to elicit positive affect and engage the child’s attention. The movies consisted of cascading bubbles, a mechanical bunny, animal puppets interacting with each other, and a split screen showing on one side a woman singing \\nnursery rhymes and on the other side dynamic, noise-making toys. The lengths of the movies were 30 seconds \\n(Bubbles), 60 seconds (Rhyme), and ∼ 70 seconds (Bunny and Puppets). Each movie was shown once except for \\nBubbles which was shown at the beginning and end of the series. The entire series of movies lasted 5 minutes. \\nExamples of the stimuli and experimental setup are presented in Fig.\\xa0 1 and described in two previous publi-\\ncations\\n17. Examples of clips from the movies are provided in the Supplementary Material. During three of the \\nmovies, the examiner, standing behind the child, called the child’s name. A failure to orient to name is an early ', metadata={'source': 'papers/Dawson.pdf', 'page': 1}),\n",
       "  Document(page_content='www.nature.com/scientificreports/3\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8symptom of autism, and results of our analysis of the orienting results have previously been published12. However, \\nall segments when children looked away from the movie, including to orient to name, as well as all 5 second seg-\\nments post the name-call stimulus, were automatically removed from the present analyses. Specifically, in order to \\nremove any influence on head movement due to the child orienting when his or her name was called, we removed the time window starting at cue for the name call prompt (a subtle icon used to prompt the examiner to call the \\nname) through the point where 75% of the audible name calls actually occurred, plus 150 frames (5 seconds). \\nSince previous studies have shown that orienting tends to occur within a few seconds after a name call, this elim-\\ninated segments influenced by the name call.\\nParents were asked to attempt to keep the child seated in their lap, but to allow the child to get off their lap if \\nthe child became too distressed to stay seated. Researchers stopped the task for 1 child due to crying. Researchers restarted the task for three participants due to noncompliance.\\nComputer Vision Analysis. The frontal camera in the tablet recorded video of the child’s face throughout \\nthe experiment at 1280 × 720 spatial resolution and 30 frames per second. The fully automatic CV A algorithm \\ndetects and tracks 49 facial landmarks on the child’s face (see Fig.\\xa0 1)18 and estimates head pose angles relative to \\nthe camera by computing the optimal rotation parameters between the detected landmarks and a 3D canonical face model\\n19. For each video frame the algorithm outputted 2D positional coordinates of the facial landmarks and \\n3 head pose angles: yaw (left-right), pitch (up-down), and roll (tilting left-right). The yaw head pose angle was used to determine the frames when the child was engaged with the movie stimuli, where frames exhibiting a yaw \\npose with a magnitude less than 20° were considered as the child being engaged.\\nFollowing the work of\\n17, to quantify head movement when the child is engaged (less than 20° yaw), per-frame \\npixel-wise displacements of 3 central facial landmarks were computed and normalized with respect to the child’s \\neye width, thus head movement was measured as a (normalized) proportion of the child’s eye width per frame. The pixel-wise displacements of the central facial landmarks are dependent on the child’s distance to the camera in the tablet. Although the tablet was placed approximately 3 feet away from the child at the start of the experi-\\nment, the child is free to move throughout the experiment, thus affecting the magnitude of landmark displace-\\nments (when the child is near to the camera the pixel displacements are larger than if the child did the same movement but farther away from the camera). Normalizing the displacements with respect to the eye-width diminishes this distance to camera dependency. More formally, the head movement between frame n and n-1 is \\ndefined as the average Euclidean displacements of the central nose, left inner eye, and right inner eye landmarks (see Fig.\\xa0 1) normalized by a ±  second windowed-average, centered around frame n , of the Euclidean distances \\nbetween the inner left and right eye landmarks,\\n−\\n−+d\\nw,nn\\nnn1,\\n15,1 5\\nFigure 1. iPad movie task and facial landmark detection: (A) Two examples of facial landmark points detected \\nby CV A and estimated head pose (indicated by the three arrows). The landmarks colored in red are the inner left, inner right, and central nose landmarks that are used for head movement computation. The left example depicts landmarks and head pose of a participant engaged in the movie stimuli; while in the right example, the participant is looking away. Both states are automatically detected. (B) Example frames from movie stimuli. \\nEach row displays a frame from corresponding movie stimuli show in the columns (going from left-to-right): \\nBubbles (30 seconds, two repetitions), Bunny (66 seconds), Rhymes (60 seconds), and Puppet show (68 seconds).', metadata={'source': 'papers/Dawson.pdf', 'page': 2}),\n",
       "  Document(page_content='www.nature.com/scientificreports/4\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8where −dnn1, is the average landmark displacement of the three central landmarks between frame n  and n-1 , \\nand −+wnn15, 15 is the average Euclidean distance between the left and right eye landmarks when the child is \\nengaged between a half-second (15 frames) before and after frame n.\\nResults evaluating the validity of the CV A methods, which rely on landmark identification and tracking on the \\nface, have been previously published. One study demonstrated high reliability between the automatic methods \\nand the expert human rater of head movements, with agreement between the computer and expert clinical rater \\noccurring 92.5% of the time with interrater reliability based on Cohen’s kappa  = 0.7520. A second study compared \\nthe automatic classification based on landmarks to human coders for head movement, demonstrated inter-rater \\nreliability based on intraclass correlation coefficient (ICC) = 0.8917. Other papers report high reliability between \\nCV A and human coding for head turning in response to name (ICC = 0.84)12 and positive affective expression \\n(happy; ICC = 0.90 and 0.89 for ASD and non-ASD toddlers)21.\\nThe original dataset consisted of frame-by-frame measurements of head movement, with observations for \\neach 1/30th of a second. Groups interested in direct use of the data can do so via collaboration with the authors \\ndue to privacy and consent considerations as well as backend designs, and the data will be stored in a separate partition at Duke\\xa0University. In order to prepare the data for statistical analysis, we first aggregated the move-ment measurements by calculating the head movement rate, defined as the moving sum of the cumulative frame-by-frame movement measurements for each 10 frame period (representing 1/3\\nrd of a second). If any indi-\\nvidual frames within a 10-frame set were set to missing, such as when the facial landmarks were not visible or during the name-call period, the moving sum was also set to missing. Outliers were addressed by Winsorizing to the 95\\nth percentile prior to aggregation.\\nAll statistical analyses were performed separately for each of the movie stimuli. To visualize the time series, we \\ncalculated and plotted the median head movement rate as well as the 1st and 3rd quartiles at each 1/3 second time \\ninterval for both ASD and non-ASD children.\\nUnadjusted and adjusted rate ratios for the association between ASD diagnosis and the rate of head move-\\nment in each 1/3 second time interval were estimated using a generalized linear mixed log-gamma regression \\nmodel. Adjusted estimates controlled for ethnicity/race (white; other), age (in months), and sex (male; female). To account for potential within-subject correlations due to repeated measurement, we included a random intercept for each participant.\\nResults\\nThe time series data depicting the rate of head movement, defined as the distance traveled per 1/3 second (10 \\nvideoframes), for the ASD and non-ASD groups are shown in Fig.\\xa0 2.\\nBased on a generalized linear mixed regression model with a log link and gamma distribution (adjusting for \\nethnicity/race, age, and sex), significant associations between diagnostic group (ASD versus non-ASD) and rate of head movement were found during all movies except for the Bubbles 2, the last movie. For Bubbles 2, the shorter \\nduration might have affected power to detect a result as there was nevertheless a trend toward a group difference \\nin the same direction as all other movies. Results of the analysis are shown in Table\\xa0 1.\\nRobust group differences in the rate of head movement were evident during 4 out of 5 of the movies. For \\nexample, the rate of head movement among participants with ASD was 2.22 times that of non-ASD participants during the Bunny movie, after adjusting for age, ethnicity/race, and sex (95% Confidence Interval 1.60, 3.07). The rate ratio was higher for all movies that had animated and more complex stimuli (Bunny, Puppets, Rhymes and Toys), as compared to the less complex Bubbles videos.\\nAlthough the LD/DD group was too small to conduct independent analyses of that group, as a sensitivity anal-\\nysis, the 8 patients with LD/DD were from the main regression model and re-estimated the associations, as shown \\nin Table\\xa0 2. Overall, the results are consistent with those reported in the main analysis; in fact, the associations are \\nslightly stronger when the LD/DD group is removed from the non-ASD group.\\nDiscussion\\nThe present study adds to a large and growing body of literature indicating that differences in early motor devel-\\nopment are an important feature of ASD. We found highly significant differences in postural control, reflected in differences in the rate of spontaneous movement of the head between toddlers with ASD versus those without \\nASD. Using an automated, objective approach, we analyzed data comprised of video-frame-level measurements \\nof head movements with observation for each 1/30\\nth of a second and created 10-frame moving sums to capture \\nmovement. Time-series data revealed group differences in the rate of head movement across all movies repre-senting a wide range of stimuli, such as bubbles, a hopping bunny, and a woman singing a nursery rhyme paired with dynamic toys. An increase in the rate of head movement observed in young children with ASD during states of engaged attention might indicate underlying differences in the ability to maintain midline postural con-\\ntrol and/or atypical engagement of attentional systems in young toddlers with ASD. These movements were not \\ndefined by spontaneous looking away from the stimulus, as was reported by Martin et al .\\n14. Rather, they were \\ncharacterized by a failure to keep the head in a still midline position while viewing the movie. This is distinct from the feature studied by Martin et al ., which was characterized by greater yaw angular displacement and \\ngreater yaw and roll angular velocity, which was primarily present during the presentation of social stimuli and might reflect sensory modulation. The movements we describe in this paper may be similar to those described \\nin previous studies of postural sway in older children with ASD, as well as school aged children with attention \\ndeficit hyperactivity disorder (ADHD) by Heiser et al .\\n22. Heiser et al . used infrared motion analysis to record \\nhead movements during a continuous performance task and found that boys with ADHD moved their head 2.3 times as far as typically-developing boys performing the same task. In a study of siblings of children with ASD, Reiersen and colleagues\\n23 found that siblings who have impaired motor coordination, features of attention deficit \\nhyperactivity disorder (ADHD), or both are much more likely to have ASD than are other siblings. They suggest ', metadata={'source': 'papers/Dawson.pdf', 'page': 3}),\n",
       "  Document(page_content='www.nature.com/scientificreports/5\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8that identification of nonspecific traits that can amplify risk for ASD, such as attention and motor differences, \\ncould allow for earlier identification and targeted therapy that modify these traits and potentially reduce later risk for ASD.\\nDelays and differences in sensorimotor development have been noted across the lifespan in individuals with \\nASD from early infancy through adulthood\\n24. For example, Lim et al . showed that postural sway and attention \\ndemands of postural control were larger in adults with ASD than in typically developed adults25. Morris et al . \\nfound that adults with ASD did not use visual information to control standing posture, in contrast to adults without ASD\\n26. Brain imaging studies suggest that atypical motor function in autism may be related to increased \\nFigure 2. Time series of head movement rate, measured as the distance traveled per 1/3 seconds (10 video \\nframes), by ASD diagnosis. Solid lines are the median values at each time point. Bands represent the first and third quartiles at each time point. Blank sections represent name calls, which were removed from this analysis.\\nMovieUnadjusted Adjusted\\nRate Ratio (95% \\nConfidence Interval) for ASD vs non-ASD P-valueRate Ratio (95% Confidence Interval) for ASD vs non-ASD P-value\\nVideo Bubbles 1 1.46 (1.09, 1.97) 0.011 1.53 (1.10, 2.12) 0.012\\nVideo Bunny 2.13 (1.60, 2.85) <0.0001 2.22 (1.60, 3.07) <0.0001\\nVideo Puppets 2.08 (1.50, 2.88) <0.0001 2.30 (1.60, 3.31) <0.0001\\nVideo Rhymes and Toys 2.37 (1.77, 3.16) <0.0001 2.45 (1.78, 3.39) <0.0001\\nVideo Bubbles 2 1.52 (1.08, 2.14) 0.018 1.43 (0.97, 2.10) 0.070\\nTable 1. Unadjusted and adjusted rate ratios for the associations between diagnostic group and rate of head \\nmovement.', metadata={'source': 'papers/Dawson.pdf', 'page': 4}),\n",
       "  Document(page_content='www.nature.com/scientificreports/6\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8sensitivity to proprioceptive error and a decreased sensitivity to visual error, aspects of motor learning dependent \\non the cerebellum27. Atypical presentation of motor functions of the cerebellum has been noted in children with \\nASD as young as 14 months of age. Esposito et al . identified significant differences in gait pattern, reflected in \\npostural asymmetry, in toddlers with ASD as compared to those without ASD28.\\nThe sample of toddlers with ASD was recruited from primary pediatric care where children suspected of \\nhaving autism were then evaluated using gold-standard diagnostic methods. Although this method of recruit-ment increases the likelihood of obtaining a more representative population-based sample, it also results in a comparison group of toddlers without ASD that is much larger than the ASD sample. Because the sample of ASD \\ntoddlers in this study was relatively small, it will be important to replicate these findings with a larger group of \\nchildren. A larger sample would also provide the statistical power to examine whether differences in postural control exist based on individual characteristics of children with ASD, such as age, sex, and co-morbid intellectual disability\\xa0and/or ADHD.\\nPrevious analyses of motor differences associated with ASD often have required labor-intensive coding of \\npatterns of behavior that are recognizable by the naked eye. Moreover, such studies typically use a “top down” approach in which specific behaviors of interest are defined and then rated by more than one person (for relia-bility assessments). The use of digital phenotyping offers multiple advantages over previous methods that rely on human coding, namely, the ability to automatically and objectively measure dynamic features of behavior on a \\nspatiotemporal scale that is not easily perceptible to the naked eye. Because digital approaches are scalable, they \\nalso allow for collection of larger data sets that can be analyzed using machine learning. We anticipate that the use of digital phenotyping will reveal a number of objective biomarkers, such as the head movements described in this report, which can be used as early risk indices and targets for intervention. By combining multiple fea-tures that reflect different aspects of sensorimotor function, including patterns of facial expressiomn, orienting, midline head movements, reaching behavior, and others, it might be possible to create a reliable, objective and automated risk profile for ASD and other neurodevelopmental disorders.\\nReferences\\n 1. Teitelbaum, P ., Teitelbaum, O., Nye, J., Fryman, J. & Maurer, R. G. Movement analysis in infancy may be useful for early diagnosis of \\nautism. Proc Natl Acad Sci USA 95, 13982–13987 (1998).\\n 2. Esposito, G., Venuti, P ., Maestro, S. & Muratori, F. An exploration of symmetry in early autism spectrum disorders: analysis of lying. Brain & development  31, 131–138, https://doi.org/10.1016/j.braindev.2008.04.005 (2009).\\n 3. Flanagan, J. E., Landa, R., Bhat, A. & Bauman, M. Head lag in infants at risk for autism: a preliminary study. The American journal of \\noccupational therapy: official publication of the American Occupational Therapy Association 66, 577–585, https://doi.org/10.5014/\\najot.2012.004192 (2012).\\n 4. Zappella, M. et al . What do home videos tell us about early motor and socio-communicative behaviours in children with autistic \\nfeatures during the second year of life–An exploratory study. Early human development 91, 569–575, https://doi.org/10.1016/j.\\nearlhumdev.2015.07.006 (2015).\\n 5. Dawson, G., Osterling, J., Meltzoff, A. N. & Kuhl, P . Case Study of the Development of an Infant with Autism from Birth to Two Y ears of Age. Journal of applied developmental psychology  21, 299–313, https://doi.org/10.1016/s0193-3973(99)00042-8 (2000).\\n 6. Brisson, J., Warreyn, P ., Serres, J., Foussier, S. & Adrien-Louis, J. Motor anticipation failure in infants with autism: a retrospective analysis \\nof feeding situations. Autism: the international journal of research and practice 16, 420–429, https://doi.org/10.1177/1362361311423385 \\n(2012).\\n 7. Gima, H. et al . Early motor signs of autism spectrum disorder in spontaneous position and movement of the head. Experimental \\nbrain research  236, 1139–1148, https://doi.org/10.1007/s00221-018-5202-x (2018).\\n 8. Hytonen, M., Pyykko, I., Aalto, H. & Starck, J. Postural control and age. Acta oto-laryngologica 113, 119–122 (1993).\\n 9. Ghanouni, P ., Memari, A. H., Gharibzadeh, S., Eghlidi, J. & Moshayedi, P . Effect of Social Stimuli on Postural Responses in Individuals with Autism Spectrum Disorder. J Autism Dev Disord  47, 1305–1313, https://doi.org/10.1007/s10803-017-3032-5  \\n(2017).\\n 10. Minshew, N. J., Sung, K., Jones, B. L. & Furman, J. M. Underdevelopment of the postural control system in autism. Neurology 63, \\n2056–2061 (2004).\\n 11. Gouleme, N. et al . Postural Control and Emotion in Children with Autism Spectrum Disorders. Translational neuroscience 8, \\n158–166, https://doi.org/10.1515/tnsci-2017-0022 (2017).\\n 12. Campbell, K. et al. Computer vision analysis captures atypical attention in toddlers with autism. Autism , 1362361318766247, https://\\ndoi.org/10.1177/1362361318766247 (2018).\\n 13. Anzulewicz, A., Sobota, K. & Delafield-Butt, J. T. Toward the Autism MotorSignature: Gesture patterns during smart tablet gameplay identify children with autism. Scientific reports  6, 31107, https://doi.org/10.1038/srep31107 (2016).\\n 14. Martin, K. B. et al. Objective measurement of head movement differences in children with and without autism spectrum disorder. \\nMolecular autism  9, 14, https://doi.org/10.1186/s13229-018-0198-4 (2018).\\n 15. Wu, D., Jose, J. V ., Nurnberger, J. I. & Torres, E. B. A Biomarker Characterizing Neurodevelopment with applications inAutism. \\nScientific reports  8, 614, https://doi.org/10.1038/s41598-017-18902-w (2018).MovieAdjusted Rate Ratio (95% \\nConfidence Interval) for ASD vs TD P-value\\nVideo Bubbles 1 1.58 (1.11, 2.24) 0.0109\\nVideo Bunny 2.34 (1.67, 3.30) <0.0001\\nVideo Puppets 2.38 (1.62, 3.50) <0.0001\\nVideo Rhymes and Toys2.54 (1.81, 3.57) <0.0001\\nVideo Bubbles 2 1.50 (1.00, 2.26) 0.0496\\nTable 2. Adjusted rate ratios for the associations between diagnostic group and rate of head movement after \\nremoving LD/DD participants.', metadata={'source': 'papers/Dawson.pdf', 'page': 5}),\n",
       "  Document(page_content='www.nature.com/scientificreports/7\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8 16. Gotham, K., Risi, S., Pickles, A. & Lord, C. The Autism Diagnostic Observation Schedule: revised algorithms for improved \\ndiagnostic validity. J Autism Dev Disord  37, 613–627, https://doi.org/10.1007/s10803-006-0280-1 (2007).\\n 17. Hashemi, J. et al. In Proceedings of the EAI International Conference on Wireless Mobile Communication and Healthcare. MobiHealth \\n(2015).\\n 18. De La Torre, F. IntraFace. Proceedings  of the IEEE International Conference on Automatic Face and Gesture Recognition Workshops  \\n(2015).\\n 19. Dementhon, D. D. L.D. Model-based object pose in 25 lines of code. International Journal of Computer Vision  15, 123–141 (1995).\\n 20. Hashemi, J. et al. Computer vision tools for low-cost and noninvasive measurement of autism-related behaviors in infants. Autism \\nRes Treat.  2014, 935686, https://doi.org/10.1155/2014/935686 (2014).\\n 21. Hashemi, J. et al Computer vision analysis for quantification of autism risk behaviors. IEEE Transactions on Affective Computing, 1–1 \\n(2018).\\n 22. Heiser, P . et al. Objective measurement of hyperactivity, impulsivity, and inattention in children with hyperkinetic disorders before \\nand after treatment with methylphenidate. European child & adolescent psychiatry  13, 100–104, https://doi.org/10.1007/s00787-004-\\n0365-3 (2004).\\n 23. Reiersen, A. M., Constantino, J. N. & Todd, R. D. Co-occurrence of motor problems and autistic symptoms in attention-deficit/hyperactivity disorder. J Am Acad Child Adolesc Psychiatry 47, 662–672, https://doi.org/10.1097/CHI.0b013e31816bff88 (2008).\\n 24. Cook, J. L., Blakemore, S. J. & Press, C. Atypical basic movement kinematics in autism spectrum conditions. Brain: a journal of \\nneurology 136, 2816–2824, https://doi.org/10.1093/brain/awt208 (2013).\\n 25. Lim, Y . H. et al . Effect of Visual Information on Postural Control in Adults with Autism Spectrum Disorder. J Autism Dev Disord , \\nhttps://doi.org/10.1007/s10803-018-3634-6 (2018).\\n 26. Morris, S. L. et al . Differences in the use of vision and proprioception for postural control in autism spectrum disorder. Neuroscience  \\n307, 273–280, https://doi.org/10.1016/j.neuroscience.2015.08.040 (2015).\\n 27. Marko, M. K. et al . Behavioural and neural basis of anomalous motor learning in children with autism. Brain  138, 784–797, https://\\ndoi.org/10.1093/brain/awu394 (2015).\\n 28. Esposito, G., Venuti, P ., Apicella, F. & Muratori, F. Analysis of unsupported gait in toddlers with autism. Brain & development  33, \\n367–373, https://doi.org/10.1016/j.braindev.2010.07.006 (2011).\\nAcknowledgements\\nFunding for this work was provided by NICHD 1P50HD093074, Duke Department of Psychiatry and Behavioral \\nSciences PRIDe award, Duke Education and Human Development Initiative, Duke-Coulter Translational Partnership Grant Program, National Science Foundation, and the Department of Defense. Some of the stimuli used for the movies were created by Geraldine Dawson, Michael Murias, and Sara Webb at the University of Washington. We gratefully acknowledge the editorial assistance of Elizabeth Sturdivant and the participation of \\nthe children and families in this study.\\nAuthor Contributions\\nG.D. and G.S. were responsible for conceptualizing and drafting the manuscript. All other authors reviewed and \\ncontributed to the manuscript. G.S. and J.H. were responsible for carrying-out the computer vision analyses. S.L. and V .S. were responsible for the statistical analyses. G.D., K.C., K.C. and S.V . were responsible for collection of the data and diagnostic confirmations. All authors were responsible for the design of the study and/or data analyses. K.C. and H.E. performed this work while employed at Duke University,\\nAdditional\\xa0 Information\\nSupplementary information accompanies this paper at https://doi.org/10.1038/s41598-018-35215-8.\\nCompeting Interests : Geraldine Dawson is on the Scientific Advisory Boards of Janssen Research and \\nDevelopment, Akili, Inc., LabCorps, and Roche Pharmaceutical Company, has received grant funding from \\nJanssen Research and Development, L.L.C. and PerkinElmer, speaker fees from ViaCord, and receives royalties from Guilford Press and Oxford University Press. Geraldine Dawson and Guillermo Sapiro are affiliated with \\nDASIO, LLC.\\nPublisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and \\ninstitutional affiliations.\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \\nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-ative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \\nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/. © The Author(s) 2018', metadata={'source': 'papers/Dawson.pdf', 'page': 6})],\n",
       " [Document(page_content='sensors\\nLetter\\nDeep-Learning-Based Detection of Infants with\\nAutism Spectrum Disorder Using Auto-Encoder\\nFeature Representation\\nJung Hyuk Lee1, Geon Woo Lee1, Guiyoung Bong2, Hee Jeong Yoo2,3and Hong Kook Kim1,*\\n1School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology,\\nGwangju 61005, Korea; ljh0412@gist.ac.kr (J.H.L.); geonwoo0801@gist.ac.kr (G.W.L.)\\n2Department of Psychiatry, Seoul National University Bundang Hospital, Seongnam-si,\\nGyeonggi-do 13620, Korea; 20409@snubh.org (G.B.); hjyoo@snu.ac.kr (H.J.Y.)\\n3Department of Psychiatry, College of Medicine, Seoul National University, Seoul 03980, Korea\\n*Correspondence: hongkook@gist.ac.kr\\nReceived: 29 October 2020; Accepted: 24 November 2020; Published: 26 November 2020\\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001\\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\\nAbstract: Autism spectrum disorder (ASD) is a developmental disorder with a life-span disability.\\nWhile diagnostic instruments have been developed and qualiﬁed based on the accuracy of the\\ndiscrimination of children with ASD from typical development (TD) children, the stability of such\\nprocedures can be disrupted by limitations pertaining to time expenses and the subjectivity of\\nclinicians. Consequently, automated diagnostic methods have been developed for acquiring objective\\nmeasures of autism, and in various ﬁelds of research, vocal characteristics have not only been reported\\nas distinctive characteristics by clinicians, but have also shown promising performance in several\\nstudies utilizing deep learning models based on the automated discrimination of children with\\nASD from children with TD. However, di ﬃculties still exist in terms of the characteristics of the\\ndata, the complexity of the analysis, and the lack of arranged data caused by the low accessibility\\nfor diagnosis and the need to secure anonymity. In order to address these issues, we introduce\\na pre-trained feature extraction auto-encoder model and a joint optimization scheme, which can\\nachieve robustness for widely distributed and unreﬁned data using a deep-learning-based method\\nfor the detection of autism that utilizes various models. By adopting this auto-encoder-based\\nfeature extraction and joint optimization in the extended version of the Geneva minimalistic acoustic\\nparameter set (eGeMAPS) speech feature data set, we acquire improved performance in the detection\\nof ASD in infants compared to the raw data set.\\nKeywords: auto-encoder; bidirectional long short-term memory (BLSTM); joint optimization; acoustic\\nfeature extraction; autism spectrum disorder\\n1. Introduction\\nAutism spectrum disorder (ASD) is a developmental disorder with a high probability of causing\\ndiﬃculties in social interactions with other people [ 1]. According to the Diagnostic and Statistical\\nManual of Mental Disorders, Fifth Edition (DSM-5), ASD involves several characteristics such as being\\nconﬁned to speciﬁc interests or behaviors, delayed linguistic development, and poor functionality in\\nterms of communicating or functioning in social situations [ 2]. As there is wide variation in terms of\\nthe types and severities of ASD based on its characteristics, the disorder is referred to as a spectrum [ 1].\\nNot only does ASD have the characteristics of a developmental disorder with a life-span disability, but\\nits prevalence is also increasing—from 1 in 150 children in 2000 to 1 in 54 children in 2016 [ 3]. As diverse\\nevidence has been obtained from previous research showing that the chance of improvement in the\\nSensors 2020 ,20, 6762; doi:10.3390 /s20236762 www.mdpi.com /journal /sensors', metadata={'source': 'papers/LEE.pdf', 'page': 0}),\n",
       "  Document(page_content='Sensors 2020 ,20, 6762 2 of 11\\nsocial abilities of people with ASD increases when an earlier clinical intervention is performed [ 4], the\\nearly detection of ASD characteristics has become a key point of current ASD research.\\nVarious instruments for discriminating ASD have been developed, and the commonly accepted\\ngold standard schemes are behavioral assessments, which are time-consuming procedures and require\\nmultidisciplinary teams (MDTs). However, most behavioral assessments su ﬀer in terms of the stability\\nof their ASD diagnosis as a result of the issues of accessibility or subjectivity and interpretive bias\\nbetween professions [ 5]. Therefore, several attempts to develop objective and precise diagnostic\\nmethods have been made in multiple ﬁelds, such as genetic determination [ 6], principle analysis of\\nbrain images [7], and physiological approaches [8].\\nOne prominent area of behavioral observations is that of infants’ vocal characteristics. Children\\nwith ASD are known to have abnormalities in their prosody resulting from deﬁcits in their ability to\\nrecognize the inherent mental conditions of others [ 9], and their atypical vocalizations are known to be\\nmonotonous or exaggerated, which can be revealed using various acoustic characteristics, followed\\nby engineering approaches for the discrimination of ASD or typical development (TD) in children\\nbased on the vocal and acoustic features. For example, in [ 10], the researchers estimated deﬁcits in the\\nvocalization of children with ASD at an average age of 18 months, such as “ﬂat” intonation, atypical\\npitch, or control of volume based on the variability of pitch and the long-term average spectrum\\n(LTAS) using fast Fourier transform, where signiﬁcant di ﬀerences were observed in the spectral\\ncomponents at low-band frequencies, as well as spectral peaks and larger pitch ranges and standard\\ndeviations. The development of linguistic abilities is also considered to be a distinguishable feature of\\ndelayed development in children with ASD. Earlier vocal patterns at age 6–18 months were proven\\nto be di ﬀerentiable in a study [ 11] that aimed to conﬁrm the hypothetical vocal patterns and social\\nquality of vocal behavior in order to di ﬀerentiate between ASD and TD cohorts in groups of children\\naged 0–6, 6–12, and 12–18 months in terms of categorized speech patterns consisting of vocalization,\\nlong reduplicated babbling, two-syllable babbling, and ﬁrst words. Evidence of abnormalities in\\nchildren with ASD were shown, in these cases, as a signiﬁcant decrease in vocalization and ﬁrst word\\nrate, while the di ﬀerence in babbling ability between children with ASD and TD was negligible.\\nGiven the development and improvement of machine learning algorithms, as the achievement\\nin the performance of state-of-the-art classiﬁcation and discrimination tasks [ 12], recent attempts to\\ndevelop automated classiﬁcation methods based on machine learning techniques have been based\\non the distinctiveness of vocal characteristics, and have been shown to be promising alternatives to\\nthe conventional methods in many publications [ 13]. For examples of machine learning classiﬁcation,\\nthe researchers of [ 14] employed various acoustic–prosodic features, including fundamental frequency,\\nformant frequencies, harmonics, and root mean square signal energy. In their research, support vector\\nmachines (SVMs) and probabilistic neural networks (PNNs) were adopted as classiﬁers, which showed\\neﬀectual accuracy in discriminating children with ASD from children with TD. Meanwhile, the authors\\nof [15] employed more recent deep learning techniques, such as convolutional neural networks (CNNs)\\nand recurrent neural networks (RNNs) with spectral features from short-time Fourier transform (STFT)\\nand constant Q transform (CQT), to classify children diagnosed using the autism diagnostic observation\\nschedule (ADOS), also showing promising results in multiple outcomes from SVMs, RNNs, and a\\ncombination of CNN and RNN classiﬁers.\\nA generalized acoustic feature set, an extended version of the Geneva minimalistic acoustic\\nparameter set (eGeMAPS) [ 16], and the bidirectional long short-term memory (BLSTM) model were\\nadopted to di ﬀerentiate between children with ASD and children with TD in [ 17], showing that 75% of\\nthe subjects’ utterances were correctly classiﬁed with the simple application of a deep learning model\\nand feature sets. While the quality of previous research based on various acoustic features has proven\\nthe eﬀectiveness of acoustic features and classiﬁcation algorithms for the detection of abnormalities in\\nchildren’s voices in ASD group compared to those of TD group, the complexity and relationship being\\ninherent between the features will remain uncertain until a large amount of data can be accumulated.\\nFurthermore, a limitation still remains in terms of the problems regarding data collection, since there are', metadata={'source': 'papers/LEE.pdf', 'page': 1}),\n",
       "  Document(page_content='Sensors 2020 ,20, 6762 3 of 11\\ndiﬃculties pertaining to the need to secure the anonymity of infant subjects, as well as the unintended\\nignorance of parents at earlier stages of their infant’s development. The data of infants are, accordingly,\\ndispersed by gender, age, and number of vocalizations, or consist of comparably small volumes of\\naudio engineering data in general. These problems were typically overlooked by previous research\\nwith controlled and small amounts of data.\\nIn order to provide suggestions for a method to overcome the abovementioned restrictions, we focus\\non examining the feasibility of neural networks as a feature extractor, employing an auto-encoder (AE),\\nwhich can modify acoustic features into lowered and separable feature dimensions [ 18]. We construct a\\nsimple six-layered stacked AE that contains an input layer, three fully connected (FC) layers, an output\\nlayer, and one auxiliary output layer, which has categorical targets for ASD and TD for the optimization\\nof the latent feature space of the AE. We train the AE and deep learning models and compare the\\nresults for each model based on SVMs and vanilla BLSTM, while adopting the same model parameters\\nfrom the method suggested in [17].\\nThe remainder of this paper is organized as follows. Section 2 describes the speciﬁcations of the\\nparticipants’ data, data processing, feature extraction, statistical analysis, and experimental setup.\\nSection 3 presents the performance evaluations for each algorithm of the SVMs and vanilla BLSTM.\\nLastly, Section 4 concludes the paper.\\n2. Proposed Method\\n2.1. Data Collection and Acoustic Feature Extraction\\nThis study was based on the audio data from video recordings of ASD diagnoses, which were\\ncollected from 2016 to 2018 at Seoul National University Bundang Hospital (SNUBH). We received\\napproval from the Institutional Review Board (IRB) at SNUBH to use fully anonymized data for\\nretrospective analysis (IRB no: B-1909 /567-110) from existing research (IRB no: B-1607 /353-005).\\nWe collected the audio data of 39 infants who were assessed using seven multiple instruments,\\nconsisting of (1) ADOS, second edition (ADOS-2), (2) the autism diagnostic interview, revised (ADI-R),\\n(3) the behavior development screening for toddlers interview (BeDevel-I), (4) the behavior development\\nscreening for toddlers play (BeDevel-P), (5) the Korean version of the childhood autism rating scale\\n(K-CARS) reﬁned from CARS-2, (6) the social communication questionnaire (SCQ), and (7) the social\\nresponsiveness scale (SRS) [ 19–22]. The ﬁnal diagnosis was based on the best clinical estimate diagnosis\\naccording to the DSM-5 ASD criteria by a licensed child psychiatrist using all of the available participant\\ninformation. The participants’ ages ranged between 6 and 24 months, where the average age was\\n19.20 months with a standard deviation (SD) of 2.52 months. Note here that the age means the age\\nat the time when each infant visited the hospital to undergo an initial diagnosis examination. There\\nwere four males and six females diagnosed with ASD, whose average age was 14.72 months with a\\nSD of 2.45. The remaining participants consisted of TD children (19 males and 10 females). Table 1\\ndisplays the collected data distribution, while Table 2 shows detailed information of collected data\\nfrom the infants.\\nTable 1. Distribution of age and gender (male /female).\\nAges (Month)No. of Subjects\\nDiagnosed as ASDNo. of Subjects\\nDiagnosed as TDNo. of Infant Subjects\\n6–12 months 0 5 M /1 F 5 M /1 F\\n12–18 months 1 M /3 F 14 M /9 F 15 M /12 F\\n18–24 months 3 M /3 F 0 3 M /3 F\\nAge (average±SD) 19.20 ±2.52 14.72 ±2.45 15.92 ±3.17', metadata={'source': 'papers/LEE.pdf', 'page': 2}),\n",
       "  Document(page_content='Sensors 2020 ,20, 6762 4 of 11\\nTable 2. Detailed information on the age, gender, and initial and deﬁnite diagnosis dates of each infant\\nin Table 1.\\nInfant IDAge (Months) on\\nInitial Diagnosis\\nDateGenderInitial Diagnosis\\nDate\\n(Year /Month /Day)Deﬁnite Final\\nDiagnosis Date\\n(Year /Month /Day)ASD /TD\\n1 18 Male 2018 /07/28 2018 /08/28 TD\\n2 18 Male 2017 /07/27 2017 /08/27 TD\\n3 10 Male 2018 /08/10 2018 /09/10 TD\\n4 13 Male 2017 /06/10 2017 /07/10 TD\\n5 22 Female 2018 /01/31 2018 /02/28 ASD\\n6 16 Male 2018 /03/17 2018 /04/17 TD\\n7 17 Female 2018 /06/30 2018 /07/30 TD\\n8 14 Female 2018 /01/06 2018 /02/06 TD\\n9 18 Male 2018 /07/17 2018 /08/17 TD\\n10 14 Male 2017 /11/04 2017 /12/04 TD\\n11 17 Female 2017 /06/29 2017 /07/29 ASD\\n12 12 Female 2018 /01/20 2018 /02/20 TD\\n13 9 Male 2017 /02/18 2017 /03/18 TD\\n14 18 Female 2017 /03/04 2017 /04/04 ASD\\n15 18 Male 2018 /05/19 2018 /06/19 TD\\n16 24 Female 2018 /08/08 2018 /09/08 ASD\\n17 19 Male 2018 /02/24 2018 /03/24 ASD\\n18 19 Male 2017 /04/18 2017 /05/18 ASD\\n19 18 Female 2017 /03/04 2017 /04/04 TD\\n20 12 Male 2016 /12/31 2017 /01/31 TD\\n21 16 Female 2018 /03/16 2018 /04/16 TD\\n22 20 Male 2017 /10/14 2017 /11/14 ASD\\n23 15 Male 2018 /05/09 2018 /06/09 ASD\\n24 17 Female 2017 /02/04 2017 /03/04 TD\\n25 16 Male 2018 /03/17 2018 /04/17 TD\\n26 12 Male 2018 /03/29 2018 /04/29 TD\\n27 17 Female 2017 /01/25 2017 /02/25 TD\\n28 17 Male 2018 /02/08 2018 /03/08 ASD\\n29 14 Male 2018 /01/13 2018 /02/13 TD\\n30 16 Male 2016 /11/30 2016 /12/30 TD\\n31 12 Male 2017 /03/22 2017 /04/22 TD\\n32 15 Male 2017 /03/11 2017 /04/11 TD\\n33 16 Male 2017 /12/05 2018 /01/05 TD\\n34 13 Female 2017 /12/13 2018 /01/13 TD\\n35 15 Female 2017 /03/25 2018 /04/25 TD\\n36 13 Male 2018 /08/25 2018 /09/25 TD\\n37 21 Male 2017 /06/24 2017 /07/24 ASD\\n38 14 Male 2017 /02/22 2017 /03/22 TD\\n39 14 Male 2018 /01/27 2018 /02/27 TD\\nAs each infant’s audio data were recorded during the clinical procedure to elicit behaviors from\\ninfants, with the attendance of one doctor or clinician and one or both parents with the child in the\\nclinical area, the audio components consisted of various speeches from the child, the clinician, and the\\nparent(s), as well as noises from toys or dragging chairs. Note here that the recordings were done in one\\nof two typical clinical rooms in SNUBH, where the room dimensions were 365 cm×400 cm×270 cm\\nand 350 cm×350 cm×270 cm, and the hospital noise level was around 40 dB. In order to analyze\\nthe vocal characteristics of the infants, each audio clip was processed and split into audio segments\\ncontaining the infant’s voice, not disturbed by music or clattering noises from toys or overlapped\\nby the voices of the clinician or parent(s). Each segment was classiﬁed into one of ﬁve categories,\\nlabeled from 0 to 4, for measuring the data distribution. Each label was intended to show di ﬀerentiable\\ncharacteristics relative to the children’s linguistic development: (1) 0 for one syllable, which is a short,', metadata={'source': 'papers/LEE.pdf', 'page': 3}),\n",
       "  Document(page_content='Sensors 2020 ,20, 6762 5 of 11\\nmomentary single vocalization such as “ah” or “ba”; (2) 1 for two syllables, commonly denoted as\\ncanonical babbling, as a reduplication of clear babbling of two identical or variant syllables such as\\n“baba” or “baga”; (3) 2 for babbling, not containing syllables; (4) 3 for ﬁrst word, such as “mother” or\\n“father”; and (5) 4 for atypical voice, including screaming or crying. The distribution of each type of\\nvocalization in seconds is shown in Table 3. The number of vocalizations per category is presented\\nalong with a rational value considering the di ﬀerence between the ASD and TD groups. While the\\ndata were unbalanced and very small, the distribution of ASD and TD vocalizations show the same\\ntendency as reported in [ 10], where the ASD group showed a signiﬁcantly lower ratio of ﬁrst words\\nand an increased ratio of atypical vocalizations, revealing developmental delay in linguistic ability.\\nTable 3. Amount (ratio) of each type of vocalization in seconds.\\nVocal Label ASD TD\\n0 80.134 (0.104) 267.897 (0.250)\\n1 314.405 (0.409) 443.498 (0.414)\\n2 33.241 (0.043) 34.766 (0.032)\\n3 8.311 (0.011) 57.286 (0.054)\\n4 333.400 (0.433) 266.794 (0.249)\\nTotal 769.491 1070.241\\nFor acquiring qualiﬁed and e ﬀective feature sets for the vocal data, eGeMAPS was employed\\nfor voice feature extraction. GeMAPS is a popular feature set providing minimalistic speech features\\ngenerally utilized for automatic voice analysis rather than as a large brute force parameter set. As an\\nextended version, eGeMAPS contains 88 acoustic features that were fully utilized in this experiment.\\nEach recorded set of audio data stored as a 48 kHz stereo ﬁle was down-sampled and down-mixed into a\\n16 kHz mono-audio ﬁle, taking into consideration its usability and resolution in mel-frequency cepstral\\ncoeﬃcients (MFCCs). To extract the speech features for ASD classiﬁcation, each infant’s utterances\\nwere segmented into 25 ms frames with a 10 ms overlap between frames. Then, 88 di ﬀerent features of\\nthe eGeMAPS were extracted for each frame with open source speech and music interpretation using\\nthe large-space extraction (OpenSMILE) toolkit [ 23], and these features were normalized by mean and\\nstandard deviation. The normalization scaling was acquired and ﬁxed by normalizing the factors of\\nthe training data set. The features were grouped for each ﬁve frames considering the time-relevant\\ncharacteristics of the speech data.\\n2.2. Pre-Trained AE for Acoustic Features\\nTo further process and reﬁne the acoustic data, a feature-extracting AE was introduced. An AE is a\\nhierarchical structure that is trained as a regression model for reproducing the input parameters. The AE\\ntakes inputs and converts them into latent representations, and then reconstructs the input parameters\\nfrom the latent values [ 24]. If we consider an input of AE, x∈Rd, then the latent representation z∈Rd′\\nand the reconstruction of the input y∈Rdare obtained by applying a nonlinear activation function\\nfto the weight sum of zusing a weighting matrix W∈Rd×d′and a bias vector b∈Rd′, such as\\nz=f(\\nWTx+b)\\n(1)\\ny=f(\\nWTz+b′)\\n(2)\\nwhere Tis a matrix transpose operator. When the latent dimension d′<d, the output from the latent\\nlayer is considered to be a compressed, meaningful value extracted from the input, which is also noted\\nas a bottleneck feature [25].\\nThe normalized eGeMAPS features were applied to train the feature-extracting AE, applying\\nthe same data as the input and the target. The AE model contained a latent layer with a lowered,\\ncompacted feature dimension compared to the input layer to achieve the useful bottleneck feature.', metadata={'source': 'papers/LEE.pdf', 'page': 4}),\n",
       "  Document(page_content='Sensors 2020 ,20, 6762 6 of 11\\nThe model was symmetrically structured, centering around the latent layer, and the model could be\\ndivided into two components: the encoder, consisting of layers from the input to the latent layers,\\nand a decoder, consisting of layers from the bottleneck to the output layers.\\nThe AE structure is depicted in Figure 1. Our AE model consisted of FC layers, with the dimensions\\nof 88, 70, 54, 70, and 88 nodes for the input, hidden, latent, hidden, and output layers, respectively.\\nThe hidden dimension was selected experimentally and the bottleneck feature dimension was used for\\ncomparison with previous research [ 17], where 54 features were selected considering the statistical\\ndissimilarity of the distributions between the ASD and TD features based on the Mann–Whitney U\\ntest [ 26]. We additionally introduced an auxiliary output as the binary categorical target for ASD\\nand TD, which is known as the semi-supervised method, to train the AE model e ﬀectively [ 27]. The\\nauxiliary output is depicted as Aux in Figure 1. The reconstructed features and auxiliary classiﬁcation\\ncan be written as\\nzi=f(Wi−1,izi−1+bi−1,i) (3)\\nwhere z1=f(W0,1x+b0,1), and\\nyrec=W3,4z3+b3,4 (4)\\nyaux=∂(W2,az2+b2,a) (5)\\nwhere yrecrefers to the reconstructed eGeMAPS features, yauxis the auxiliary classiﬁcation result, fis\\nthe activation function, and ∂is the softmax activation.\\nSensors 2020, 20, x FOR PEER REVIEW  6 of 12 \\n where T is a matrix transpose operator. When the latent dimension 𝑑′<𝑑, the output from the latent \\nlayer is considered to be a compressed, meaningful value extracted from the input, which is also \\nnoted as a bottleneck feature [25].  \\nThe normalized eGeMAPS feature s were applied to train the feature -extracting AE, applying the \\nsame data as the input and the target. The AE model contained a latent layer with a lowered, \\ncompacted feature dimension compared to the input layer to achieve the useful bottleneck feature. \\nThe model was symmetrically structured, centering around the latent layer, and the model could be \\ndivided into two components: the encoder, consisting of layers from the input to the latent layers, \\nand a decoder, consisting of layers from the bottleneck to the output layers.  \\nThe AE structure is depicted in Figure 1. Our AE model consisted of FC layers, with the \\ndimensions of 88, 70, 54, 70, and 88 nodes for the input, hidden, latent, hidden, and output layers, \\nrespectively. The hidden dimension was selected experimentally and the bottleneck feature \\ndimension was used for comparison with previous research [17], where 54 features were selected \\nconsidering the statistical dissimilarity of the distributions between the ASD and TD features based \\non the Mann –Whitne y U test [26]. We additionally introduced an auxiliary output as the binary \\ncategorical target for ASD and TD, which is known as the semi -supervised method, to train the AE \\nmodel effectively [27]. The auxiliary output is depicted as Aux in Figure 1. The reconstructed features \\nand auxiliary classification can be written as  \\n𝒛𝑖=𝑓(𝑾𝑖−1,𝑖𝒛𝑖−1+𝒃𝑖−1,𝑖) (3) \\nwhere 𝒛1=𝑓(𝑾0,1𝒙+𝒃0,1), and  \\n𝒚𝑟𝑒𝑐=𝑾3,4𝒛3+𝒃3,4 (4) \\n  𝒚𝑎𝑢𝑥=𝜕(𝑾2,𝑎𝒛2+𝒃2,𝑎) (5) \\nwhere 𝒚𝑟𝑒𝑐 refers to the reconstructed eGeMAPS features, 𝒚𝑎𝑢𝑥 is the auxiliary classification result, \\n𝑓 is the activation function, and 𝜕 is the softmax activation.  \\n \\nFigure 1. Structure of a semi -supervised auto -encoder (AE) model. eGeMAPS , extended version of \\nthe Geneva minimalistic acoustic parameter set; ASD, autism spectrum disorder; TD, typical \\ndevelopment.  \\nThe losses of the reconstruction error for main AE target are measured using the mean absolute \\nerror, while the auxiliary ASD/TD t arget loss is the binary cross -entropy, and they are added and \\nsimultaneously optimized with rational hyper -parameters. The overall loss equation is  \\nFigure 1. Structure of a semi-supervised auto-encoder (AE) model. eGeMAPS, extended version of the\\nGeneva minimalistic acoustic parameter set; ASD, autism spectrum disorder; TD, typical development.\\nThe losses of the reconstruction error for main AE target are measured using the mean absolute\\nerror, while the auxiliary ASD /TD target loss is the binary cross-entropy, and they are added and\\nsimultaneously optimized with rational hyper-parameters. The overall loss equation is\\nLrecon =1\\nNN∑\\ni=1⏐⏐⏐yirec−yigt⏐⏐⏐ (6)\\nLaux=−y1gtlog(y1aux)−(1−t)y1gtlog(y1aux) (7)\\nLtotal=Lrecon +αLaux (8)\\nwhere Lrecon,Laux, and Ltotaldenote the reconstruction error, auxiliary loss using a binary cross-entropy\\nloss function, and total loss, respectively.', metadata={'source': 'papers/LEE.pdf', 'page': 5}),\n",
       "  Document(page_content='Sensors 2020 ,20, 6762 7 of 11\\nFor our stacked AE model, a rational value of α=0.3was selected experimentally, considering\\nthe proportion of each loss. In order to train the AE e ﬀectively, both L2 normalization for weight\\nnormalization and batch normalization were adopted [ 28,29]. After the training was completed,\\nwe fetched the encoder of the AE as the feature extraction part for the joint optimization model in the\\ntraining procedures of the deep learning model.\\n2.3. Establishing and Training the Deep Learning Model for ASD Detection\\nAs the eGeMAPS data were set and the AE was trained through semi-supervised learning,\\nthe machine learning models, such as SVMs, BLSTM, and joint optimized BLSTM were constructed.\\nEach model had its own input parameter dimensions and the same output targets as ASD and TD\\nclassiﬁcation labels. The eGeMAPS feature data were paired with the diagnostic results for the\\nsupervised learning of the neural network models. For the binary decision, ASD was labeled as\\na positive data point, with a label of (0, 1), while TD was labeled as a negative data point (1, 0).\\nWe composed four kinds of models with the paired data: SVMs with linear kernel, the vanilla BLSTM\\nwith 88 eGeMAPS features, the vanilla BLSTM with 54 eGeMAPS features, and the jointly optimized\\nBLSTM layer with the AE. The joint optimization model is depicted in Figure 2. As the data set was\\nprepared as the input with ﬁve sequential frames, i.e., the grouped eGeMAPS features in Figure 2,\\nthe SVMs received a single frame parameter of 440 dimension which was ﬂattened from the original\\nﬁve input frames. For the deep learning models, batch normalization, rectangular linear unit (ReLU)\\nactivation, and dropout were applied for each layer, except for the output layer [ 30,31], and the\\nadaptive momentum (ADAM) optimizer [ 32] was used to train the network. The training procedure\\nwas controlled by early stopping for minimizing the validation error with 100 epoch patience, while\\nsaving the best models for improvement of the validation loss by each epoch. Because the amount\\nof speech data was relatively small for a deep learning model compared to the disparate ﬁeld of\\naudio engineering, we grouped the data into ﬁve segments, while the test utterances were separated\\nformerly, which were selected randomly for 10% of the total data, were evenly distributed across each\\nvocalization type, and underwent ﬁve-fold cross-validation for training; then, the best-performing\\nmodel was chosen. Our model was trained with the TensorFlow framework [ 33]. For comparison,\\nan SVM model with linear kernel was trained with the same data split as the proposed deep learning\\nmodel, and as well as the vanilla BLSTM suggested in [17], which has single BLSTM with eight cells.\\nSensors 2020, 20, x FOR PEER REVIEW  8 of 12 \\n  \\nFigure 2. Structure of a joint optimization model of an auto -encoder (AE) and bidirectional long short -\\nterm memory (BLSTM).  \\n3. Performance Evaluation  \\nThe performance of each method was evaluated through five -fold cross validation, where 95 \\naverage ASD utterances and 130 average TD utterances were proportionally distributed over five \\ncases of vocalizations for the gener alized estimation of unconcentrated utterance data. The averaged \\nperformances of the five validation splits of each model are described in Table 4. The labeled names \\nof the BLSTM were used as the features for training the BLSTM model, where eGeMAPS -88 deno tes \\n88 features of eGeMAPS, eGeMAPS -54 denotes 54 features selected by the Mann –Whitney U test, and \\nAE-encoded denotes the joint optimized model. In the classification stage, one utterance was \\nprocessed in the frame -wise method and the softmax output was c onverted to class indices 0 and 1, \\nand if the average of class indices of the frames was over 0.5, then the utterance was considered an \\nASD child’s utterance. The performances were scored with conventional measures, as well as \\nunweighted average recall (UA R) and weighted average recall (WAR), chosen in the INTERSPEECH \\n2009 Emotion challenge, which considered imbalanced classes [34]. In the experiment, the SVM \\nmodel showed very low precision, which was extremely biased toward the TD class. The BLSTM \\nclassifi er with 88 features of eGeMAPS and the AE model showed considerable quality in terms of \\nclassifying ASD and TD children, while the AE model showed only marginal improvement in \\ncorrectly classifying children with ASD compared to eGeMAPS -88. The 54 selected features showed \\ndegraded quality compared to eGeMAPS -88, obtaining more biased results toward children with TD.  \\nTable 4. Classification results from the support vector machine (SVM), BLSTM with 88 or 54 \\neGeMAPS features, 54 selected eGeMAPS features, and B LSTM with AE -encoded features.  \\nModels  SVM  BLSTM \\n(eGeMAPS -54) BLSTM \\n(eGeMAPS -88) BLSTM  (AE-\\nEncoded)  \\nPredicted To ASD  TD ASD  TD ASD  TD ASD  TD \\nASD  62 18 170 103 196 99 215 98 \\nTD 413 632 305 547 279 551 260 552 \\nAccuracy  0.6178  0.6373  0.6640  0.6818  \\nPrecision  0.1305  0.3579  0.4126  0.4526  \\nRecall  0.7750  0.6227  0.6644  0.6869  \\nF1 score  0.2234  0.4545  0.5091  0.5457  \\nUAR  0.5514  0.5997  0.6302  0.6509  \\nUAR, unweighted average recall.  \\n  \\nFigure 2. Structure of a joint optimization model of an auto-encoder (AE) and bidirectional long\\nshort-term memory (BLSTM).\\n3. Performance Evaluation\\nThe performance of each method was evaluated through ﬁve-fold cross validation, where\\n95 average ASD utterances and 130 average TD utterances were proportionally distributed over ﬁve', metadata={'source': 'papers/LEE.pdf', 'page': 6}),\n",
       "  Document(page_content='Sensors 2020 ,20, 6762 8 of 11\\ncases of vocalizations for the generalized estimation of unconcentrated utterance data. The averaged\\nperformances of the ﬁve validation splits of each model are described in Table 4. The labeled names\\nof the BLSTM were used as the features for training the BLSTM model, where eGeMAPS-88 denotes\\n88 features of eGeMAPS, eGeMAPS-54 denotes 54 features selected by the Mann–Whitney Utest,\\nand AE-encoded denotes the joint optimized model. In the classiﬁcation stage, one utterance was\\nprocessed in the frame-wise method and the softmax output was converted to class indices 0 and 1, and\\nif the average of class indices of the frames was over 0.5, then the utterance was considered an ASD\\nchild’s utterance. The performances were scored with conventional measures, as well as unweighted\\naverage recall (UAR) and weighted average recall (WAR), chosen in the INTERSPEECH 2009 Emotion\\nchallenge, which considered imbalanced classes [ 34]. In the experiment, the SVM model showed very\\nlow precision, which was extremely biased toward the TD class. The BLSTM classiﬁer with 88 features\\nof eGeMAPS and the AE model showed considerable quality in terms of classifying ASD and TD\\nchildren, while the AE model showed only marginal improvement in correctly classifying children\\nwith ASD compared to eGeMAPS-88. The 54 selected features showed degraded quality compared to\\neGeMAPS-88, obtaining more biased results toward children with TD.\\nTable 4. Classiﬁcation results from the support vector machine (SVM), BLSTM with 88 or 54 eGeMAPS\\nfeatures, 54 selected eGeMAPS features, and BLSTM with AE-encoded features.\\nModels SVMBLSTM\\n(eGeMAPS-54)BLSTM\\n(eGeMAPS-88)BLSTM\\n(AE-Encoded)\\nPredicted To ASD TD ASD TD ASD TD ASD TD\\nASD 62 18 170 103 196 99 215 98\\nTD 413 632 305 547 279 551 260 552\\nAccuracy 0.6178 0.6373 0.6640 0.6818\\nPrecision 0.1305 0.3579 0.4126 0.4526\\nRecall 0.7750 0.6227 0.6644 0.6869\\nF1 score 0.2234 0.4545 0.5091 0.5457\\nUAR 0.5514 0.5997 0.6302 0.6509\\nUAR, unweighted average recall.\\n4. Discussion\\nThe vanilla BLSTM model presented in [ 17] conducted discrimination on well-classiﬁed subjects\\nwith 10-month-old children and sorted 54 features from eGeMAPS that had a distinctive distribution\\nbetween ASD and TD selected by the Mann–Whitney Utest using the three-fold cross-validation\\nmethod. However, because the di ﬀerence in the data distribution failed to achieve the same eGeMAPS\\nfeature selection between the test and classiﬁcation results with the speciﬁed feature set presented\\nherein, the application of an identical model structure and the adoption of the same feature domain\\nwill allow both approaches to be indirectly comparable.\\nThese results can be interpreted by the data distributions, and we performed t-stochastic neighbor\\nembedding (t-SNE) analysis [ 35] on the training data set, which can nonlinearly squeeze the data\\ndimension based on a machine learning algorithm. Figure 3 shows each data distribution as a\\ntwo-dimensional scatter plot. In the ﬁgure, the eGeMAPS features from eGeMAPS-88 and eGeMAPS-54\\nshowed almost identical distribution, except for the amount of ASD outliers, which implies that the\\nASD and TD features in the eGeMAPS features show similar distributions in this experiment. As shown\\nin [16], eGeMAPS includes temporal features that are relevant to vocalizations and utterances; thus, these\\nfeatures might cause confusion regarding the discrimination between ASD and TD. The AE-encoded\\nfeatures, however, showed a redistributed feature map with a more characteristic distribution compared\\nto the eGeMAPS features. This is because the AE-encoded features were compressed into a bottleneck\\nfeature, which was derived by weighting the matrix, paying attention to the signiﬁcant parameters', metadata={'source': 'papers/LEE.pdf', 'page': 7}),\n",
       "  Document(page_content='Sensors 2020 ,20, 6762 9 of 11\\nwhile reducing the inﬂuence from the ambiguous parameters. While the joint optimization model\\nachieved only marginally improved results compared to eGeMAPS-88, the distribution of the feature\\nmap would be more noticeable in improved feature extraction models, as well as more di ﬀerentiable in\\ncomplex models, although BLSTM with eight cells was employed for a comparison with conventional\\nresearch in this experiment.\\nSensors 2020, 20, x FOR PEER REVIEW  9 of 12 \\n 4. Discussion  \\nThe vanilla BLSTM model presented in [17] conducted discrimination on well -classified subjects \\nwith 10 -month -old children and sorted 54 features from eGeMAPS that had a distinctive distribution \\nbetween ASD and TD selected by the Mann –Whitney U test using t he three -fold cross -validation \\nmethod. However, because the difference in the data distribution failed to achieve the same \\neGeMAPS feature selection between the test and classification results with the specified feature set \\npresented herein, the applicatio n of an identical model structure and the adoption of the same feature \\ndomain will allow both approaches to be indirectly comparable.  \\nThese results can be interpreted by the data distributions, and we performed t -stochastic \\nneighbor embedding (t -SNE) analy sis [35] on the training data set, which can nonlinearly squeeze the \\ndata dimension based on a machine learning algorithm. Figure 3 shows each data distribution as a \\ntwo-dimensional scatter plot. In the figure, the eGeMAPS features from eGeMAPS -88 and eGeM APS -\\n54 showed almost identical distribution, except for the amount of ASD outliers, which implies that \\nthe ASD and TD features in the eGeMAPS features show similar distributions in this experiment. As \\nshown in [16], eGeMAPS includes temporal features that are relevant to vocalizations and utterances; \\nthus, these features might cause confusion regarding the discrimination between ASD and TD. The \\nAE-encoded features, however, showed a redistributed feature map with a more characteristic \\ndistribution compared to the eGeMAPS features. This is because the AE -encoded features were \\ncompressed into a bottleneck feature, which was derived by weighting the matrix, paying attention \\nto the significant parameters while reducing the influence from the ambiguous parameters . While the \\njoint optimization model achieved only marginally improved results compared to eGeMAPS -88, the \\ndistribution of the feature map would be more noticeable in improved feature extraction models, as \\nwell as more differentiable in complex models, alt hough BLSTM with eight cells was employed for a \\ncomparison with conventional research in this experiment.  \\nWhile the overall performance scores were comparably low for general classification problems \\non account of the subjectivity and complexity of problems, and the limitation in terms of the shortage \\nof data, the results of the jointly optimized model imply the possibility of deep -learning -based feature \\nextraction for the improvement of automated ASD/TD diagnosis under restricted circumstances.  \\n   \\n(a) (b) (c) \\nFigure 3. Two -dimensional scatter plot for ( a) eGeMAPS -88, ( b) eGeMAPS -54, and ( c) the AE \\nprocessed by t -stochastic neighbor embedding (t -SNE).  \\n5. Conclusion s \\nIn this paper, we conducted experiments for discovering the possibility of auto -encoder -based \\nfeature extraction and a joint optimization  method for the automated detection of atypicality in voices \\nof children with ASD during early developmental stages. Un der the condition of an insufficient and \\ndispersed data set, the clas sification results were relatively poor in comparison to the general \\nclassification tasks based on deep learning. Although our investigation used a limited number of \\nsubjects and an unbal anced data set, the suggested auto -encoder -based feature extraction and joint \\noptimization method revealed the possibility of feature dimension and a slight improvement in \\nmodel -based diagnosis under such uncertain circumstances.  \\nFigure 3. Two-dimensional scatter plot for ( a) eGeMAPS-88, ( b) eGeMAPS-54, and ( c) the AE processed\\nby t-stochastic neighbor embedding (t-SNE).\\nWhile the overall performance scores were comparably low for general classiﬁcation problems on\\naccount of the subjectivity and complexity of problems, and the limitation in terms of the shortage of\\ndata, the results of the jointly optimized model imply the possibility of deep-learning-based feature\\nextraction for the improvement of automated ASD /TD diagnosis under restricted circumstances.\\n5. Conclusions\\nIn this paper, we conducted experiments for discovering the possibility of auto-encoder-based\\nfeature extraction and a joint optimization method for the automated detection of atypicality in voices\\nof children with ASD during early developmental stages. Under the condition of an insu ﬃcient\\nand dispersed data set, the classiﬁcation results were relatively poor in comparison to the general\\nclassiﬁcation tasks based on deep learning. Although our investigation used a limited number of\\nsubjects and an unbalanced data set, the suggested auto-encoder-based feature extraction and joint\\noptimization method revealed the possibility of feature dimension and a slight improvement in\\nmodel-based diagnosis under such uncertain circumstances.\\nIn future work, we will focus on increasing the reliability of the proposed method by addition\\nof a number of infants’ speech data, reﬁnement of the acoustic features, an auto-encoder for feature\\nextraction, and better, deeper, and up-to-date model structures. This research can also be extended to\\nchildren with the age of 3 or 4 who can speak several sentences. In this case, we will investigate the\\nlinguistic features, as well as acoustic features, such as we have done in this paper. In addition to ASD\\ndetection, this research can be applied to the detection of infants with development delays.\\nAuthor Contributions: All authors discussed the contents of the manuscript. H.K.K. contributed to the research\\nidea and the framework of this study; G.B. and H.J.Y. provided the database and helped with the discussion; J.H.L.\\nperformed the experiments; G.W.L. contributed to the data collection and pre-processing. All authors have read\\nand agreed to the published version of the manuscript.\\nFunding: This work was supported by the Institute of Information & communications Technology Planning &\\nevaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00330, Development of AI Technology\\nfor Early Screening of Infant /Child Autism Spectrum Disorders based on Cognition of the Psychological Behavior\\nand Response).\\nConﬂicts of Interest: The authors declare no conﬂict of interest.', metadata={'source': 'papers/LEE.pdf', 'page': 8}),\n",
       "  Document(page_content='Sensors 2020 ,20, 6762 10 of 11\\nReferences\\n1. National Institute of Mental Health. Autism Spectrum Disorder. Available online: https: //www.nimh.nih.\\ngov/health /topics /autism-spectrum-disorders-asd /index.shtml (accessed on 26 October 2020).\\n2. American Psychiatric Association. Diagnostic and Statistical Manual of Mental Disorders: DSM-5 ; American\\nPsychiatric Publishing: Washington, DC, USA, 2013.\\n3. Centers for Disease Control and Prevention (CDC). Data & Statistics on Autism Spectrum Disorder. Available\\nonline: https: //www.cdc.gov /ncbddd /autism /data.html (accessed on 26 October 2020).\\n4. Fenske, E.C.; Zalenski, S.; Krantz, P .J.; McClannahan, L.E. Age at intervention and treatment outcome for\\nautistic children in a comprehensive intervention program. Anal. Interv. Devel. Disabil. 1985 ,5, 49–58.\\n[CrossRef]\\n5. Falkmer, T.; Anderson, K.; Falkmer, M.; Horlin, C. Diagnostic procedures in autism spectrum disorders:\\nA systematic literature review. Eur. Child Adolesc. Psychiatry 2013 ,22, 329–340. [CrossRef] [PubMed]\\n6. Bailey, A.; Le Couteur, A.; Gottesman, I.; Bolton, P .; Simono ﬀ, E.; Yuzda, E.; Rutter, M. Autism as a strongly\\ngenetic disorder: Evidence from a British twin study. Physiol. Med. 1995 ,25, 63–77. [CrossRef] [PubMed]\\n7. Duﬀy, F.H.; Als, H. A stable pattern of EEG spectral coherence distinguishes children with autism from\\nneuro-typical controls—A large case control study. BMC Med. 2012 ,10, 64. [CrossRef] [PubMed]\\n8. Chaspari, T.; Lee, C.-C.; Narayanan, S.S. Interplay between verbal response latency and physiology of\\nchildren with autism during ECA interactions. In Proceedings of the Annual Conference of the International\\nSpeech Communication Association (Interspeech), Portland, OR, USA, 9–13 September 2012; pp. 1319–1322.\\n9. Baron-Cohen, S. Social and pragmatic deﬁcits in autism: Cognitive or a ﬀective? J. Autism Dev. Disord. 1988 ,\\n18, 379–402. [CrossRef] [PubMed]\\n10. Bonneh, Y.S.; Levanon, Y.; Dean-Pardo, O.; Lossos, L.; Adini, Y. Abnormal speech spectrum and increased\\npitch variability in young autistic children. Front. Hum. Neurosci. 2011 ,4, 237. [CrossRef] [PubMed]\\n11. Chericoni, N.; de Brito Wanderley, D.; Costanzo, V .; Diniz-Gonçalves, A.; Gille, M.L.; Parlato, E.; Cohen, D.;\\nApicella, F.; Calderoni, S.; Muratori, F. Pre-linguistic vocal trajectories at 6–18 months of age as early markers\\nof autism. Front. Psychol. 2016 ,7, 1595. [CrossRef] [PubMed]\\n12. Alom, M.Z.; Taha, T.M.; Yakopcic, C.; Westberg, S.; Sidike, P .; Nasrin, M.S.; Hasan, M.; van Essen, B.C.;\\nAwwal, A.A.S.; Asari, V .K. A state-of-the-art survey on deep learning theory and architectures. Electronics\\n2019 ,8, 292. [CrossRef]\\n13. Song, D.-Y.; Kim, S.Y.; Bong, G.; Kim, J.M.; Yoo, H.J. The use of artiﬁcial intelligence in screening and\\ndiagnosis of autism spectrum disorder: A literature review. J. Korean Acad. Child. Adolesc. Psychiatry 2019 ,30,\\n145–152. [CrossRef] [PubMed]\\n14. Santos, J.F.; Brosh, N.; Falk, T.H.; Zwaigenbaum, L.; Bryson, S.E.; Roberts, W.; Smith, I.M.; Szatmari, P .;\\nBrian, J.A. Very early detection of autism spectrum disorders based on acoustic analysis of pre-verbal\\nvocalizations of 18-month old toddlers. In Proceedings of the IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), Vancouver, BC, Canada, 26–31 May 2013; pp. 7567–7571.\\n15. Li, M.; Tang, D.; Zeng, J.; Zhou, T.; Zhu, H.; Chen, B.; Zou, X. An automated assessment framework for\\natypical prosody and stereotyped idiosyncratic phrases related to autism spectrum disorder. Comput. Speech\\nLang. 2019 ,56, 80–94. [CrossRef]\\n16. Eyben, F.; Scherer, K.R.; Schuller, B.W.; Sundberg, J.; Andr é, E.; Busso, C.; Devillers, L.Y.; Epps, J.; Laukka, P .;\\nNarayanan, S.S.; et al. The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and\\naﬀective computing. IEEE Trans. A ﬀect. Comput. 2016 ,7, 190–202. [CrossRef]\\n17. Pokorny, F.B.; Schuller, B.W.; Marschik, P .B.; Brueckner, R.; Nyström, P .; Cummins, N.; Bölte, S.;\\nEinspieler, C.; Falck-Ytter, T. Earlier identiﬁcation of children with autism spectrum disorder: An automatic\\nvocalisation-based approach. In Proceedings of the Annual Conference of the International Speech\\nCommunication Association (Interspeech), Stockholm, Sweden, 20–24 August 2017; pp. 309–313.\\n18. Xing, C.; Ma, L.; Yang, X. Stacked denoise autoencoder based feature extraction and classiﬁcation for\\nhyperspectral images. J. Sens. 2016 ,2016 , 3632943. [CrossRef]\\n19. Bong, G.; Kim, J.; Hong, Y.; Yoon, N.; Sunwoo, H.; Jang, J.; Oh, M.; Lee, K.; Jung, S.; Yoo, H. The feasibility and\\nvalidity of autism spectrum disorder screening instrument: Behavior development screening for toddlers\\n(BeDevel)—A pilot study. Autism Res. 2019 ,12, 1112–1128. [CrossRef] [PubMed]', metadata={'source': 'papers/LEE.pdf', 'page': 9}),\n",
       "  Document(page_content='Sensors 2020 ,20, 6762 11 of 11\\n20. Center for Autism Research. Social Communication Questionnaire (SCQ). Available online: https: //www.\\ncarautismroadmap.org /social-communication-questionnaire-scq /?print =pdf (accessed on 26 October 2020).\\n21. Center for Autism Research. Childhood Autism Rating Scale, 2nd Edition (CARS2). Available online: https:\\n//www.carautismroadmap.org /childhood-autism-rating-scale /?print =pdf (accessed on 26 October 2020).\\n22. Center for Autism Research. Social Responsiveness Scale, 2nd Edition (SRS-2). Available online: https:\\n//www.carautismroadmap.org /social-responsiveness-scale /?print =pdf (accessed on 26 October 2020).\\n23. Eyben, F.; Wöllmer, M.; Schuller, B. OpenSMILE—The Munich versatile and fast open-source audio feature\\nextractor. In Proceedings of the 18th ACM International Conference on Multimedia, Firenze, Italy, 25–29\\nOctober 2010; pp. 1459–1462.\\n24. Masci, J.; Meier, U.; Cire¸ san, D.; Schmidhuber, J. Stacked Convolutional Auto-Encoders for Hierarchical\\nFeature Extraction. In Artiﬁcial Neural Networks and Machine-ICANN 2011 ; Honkela, T., Duch, W., Girolami, M.,\\nKaski, S., Eds.; Springer: Berlin /Heidelberg, Germany, 2011; pp. 52–59.\\n25. Sainath, T.; Kingsbury, B.; Ramabhadran, B. Auto-encoder bottleneck features using deep belief networks. In\\nProceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\\nKyoto, Japan, 25–30 March 2012.\\n26. Nachar, N. The Mann-Whitney U: A test for assessing whether two independent samples come from the\\nsame distribution. Tutor. Quant. Methods Psychol. 2008 ,4, 13–20. [CrossRef]\\n27. Le, L.; Patterson, A.; White, M. Supervised autoencoders: Improving generalization performance with\\nunsupervised regularizers. In Advances in Neural Information Processing Systems ; Bengio, S., Wallach, H.,\\nLarochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R., Eds.; Curran Associates, Inc.: New York, NY,\\nUSA, 2018; pp. 107–117.\\n28. van Laarhoven, T. L2 regularization versus batch and weight normalization. arXiv 2017 , arXiv:1706.05350.\\n29. Ioﬀe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate\\nshift. In Proceedings of the International Conference on Machine Learning, Lille, France, 6–11 July 2015;\\npp. 448–456.\\n30. Nair, V .; Hinton, G.E. Rectiﬁed linear units improve restricted Boltzmann machines. In Proceedings of the\\n27th International Conference on Machine Learning, Haifa, Israel, 21–24 June 2010; pp. 807–814.\\n31. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent\\nneural networks from overﬁtting. J. Mach. Learn. Res. 2014 ,15, 929–1958.\\n32. Kingma, D.P .; Ba, J.L. ADAM: A method for stochastic optimization. In Proceedings of the 3rd International\\nConference on Learning Representations, San Diego, CA, USA, 7–9 May 2015; pp. 1–15.\\n33. Abadi, M.; Barham, P .; Chen, J.; Chen, Z.; Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; Irving, G.; Isard, M.;\\net al. TensorFlow: A system for large-scale machine learning. In Proceedings of the 12th USENIX Symposium\\non Operating Systems Design and Implementation, Savannah, GA, USA, 2–4 November 2016; pp. 265–283.\\n34. Schuller, B.; Steidl, S.; Batliner, A. The Interspeech 2009 emotion challenge. In Proceedings of the Annual\\nConference of the International Speech Communication Association (Interspeech), Brighton, UK, 6–10\\nSeptember 2009; pp. 312–315.\\n35. van der Maaten, L.; Hinton, G. Visualizing data using t-SNE. J. Mach. Learn. Res. 2008 ,9, 2579–2605.\\nPublisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional\\naﬃliations.\\n©2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access\\narticle distributed under the terms and conditions of the Creative Commons Attribution\\n(CC BY) license (http: //creativecommons.org /licenses /by/4.0/).', metadata={'source': 'papers/LEE.pdf', 'page': 10})],\n",
       " [Document(page_content='Vocal patterns in infants with Autism Spectrum Disorder:\\nCanonical babbling status and vocalization frequency\\nElena Patten, Ph.D.1, Katie Belardi, M.S.2, Grace T. Baranek, Ph.D.2, Linda R. Watson, Ed.D.\\n2, Jeffrey D. Labban, Ph.D.1, and D. Kimbrough Oller, Ph.D.3\\n1Univ. of North Carolina, Greensboro\\n2Univ. of North Carolina, Chapel Hill\\n3Univ. of Memphis, and Konrad Lorenz Institute for Evolution and Cognition Research,\\nKlosterneuburg, Austria\\nAbstract\\nCanonical babbling is a critical milestone for speech development and is usually well in place by\\n10 months. The possibility that infants with ASD show late onset of canonical babbling has so far\\neluded evaluation. Rate of vocalization or “volubility” has also been suggested as possibly\\naberrant in infants with ASD. We conducted a retrospective video study examining vocalizations\\nof 37 infants at 9–12 and 15–18 months. Twenty-three of the 37 infants were later diagnosed with\\nASD and indeed produced low rates of canonical babbling and low volubility by comparison with\\nthe 14 typically developing infants. The study thus supports suggestions that very early vocal\\npatterns may prove to be a useful component of early screening and diagnosis of ASD.\\nKeywords\\ncanonical babbling; volubility; vocal patterns; early detection\\nASD and early vocal development\\nEarly intervention is critical for positive outcomes for children with Autism Spectrum\\nDisorder (ASD). Early identification of atypical behaviors that manifest during infancy\\ncould significantly impact age of diagnosis and subsequent initiation of intervention.\\nCurrently, the minimum age at which the majority of children with ASD can be reliably\\ndiagnosed with relative stability is two years (e.g., Chawarska et al., 2009; Lord, 1995), but\\naccording to recent data from the Centers for Disease Control, many children are not\\nCorrespondence concerning this article should be addressed to: Elena Patten. University of North Carolina at Greensboro, 300\\nFerguson Building, P. O. Box 26170, Greensboro, NC 27402-6170. e_patten@uncg.edu.\\nElena Patten, UNC Greensboro, 300 Ferguson Building, Greensboro, NC 27412-6170\\nKatie Belardi, UNC Chapel Hill, Bondurant Hall, CB#7190, Chapel Hill, NC 27599-7190\\nGrace Baranek, UNC Chapel Hill, Bondurant Hall, CB #7122, Chapel Hill, NC 27599-7190\\nLinda Watson, UNC Chapel Hill, Bondurant Hall, CB#7190, Chapel Hill, NC 27599-7190\\nJeffrey Labban, UNC Greensboro, 231 HHP Building, Greensboro, NC 27412\\nD. Kimbrough Oller, The University of Memphis, 807 Jefferson Avenue, Memphis, TN 38105\\nElena Patten and Jeffrey Labban are at UNC Greensboro in North Carolina, USA; Katie Belardi, Grace Baranek and Linda Watson are\\nat UNC Chapel Hill in North Carolina, USA; D. Kimbrough Oller is at the University of Memphis in Tennessee, USA.\\nNIH Public Access\\nAuthor Manuscript\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nPublished in final edited form as:\\nJ Autism Dev Disord . 2014 October ; 44(10): 2413–2428. doi:10.1007/s10803-014-2047-4.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 0}),\n",
       "  Document(page_content='diagnosed until preschool or kindergarten age (2012). Research targeting early detection has\\nprimarily focused on behaviors exhibited during toddlerhood (12–36 months) and preschool\\nyears (36–60 months) (e.g., Matson, Fodstad, & Dempsey, 2009; Volkmar & Chawarska,\\n2008) after diagnosis has been made. Use of retrospective video analyses and studies of\\ninfant siblings of children diagnosed with ASD has allowed examination of possible\\nindicators of ASD in the first year of life (e.g., Baranek, 1999; Osterling, Dawson, &\\nMunson, 2002; Sheinkopf, Iverson, Rinaldi, & Lester, 2012; Zwaigenbaum et al., 2005).\\nStill, the most widely used autism screening tool for young children, the Modified Checklist\\nfor Autism Toddlers (MCHAT: Robins, Fein, Barton, & Green, 2001) is recommended for\\nages 16–30 months.\\nWe sought to identify potential communication markers of ASD that might be observed\\nwithin the first year of life in a retrospective evaluation of data from infants recorded at\\nhome and later diagnosed with ASD. We focused on presumed precursors to language for\\ntwo reasons: First, communication impairment is a core deficit in ASD, and second,\\nevaluation of very early vocal behaviors in typically developing infants has already\\nestablished markers that are critical to normal vocal communicative development. One\\nrobust pre-speech vocal milestone is the onset of canonical babbling. A canonical syllable\\n(e.g., [ba]) is comprised of a consonant-like sound and a vowel-like sound, with a rapid\\ntransition between them (Oller, 1980, 2000). A second potentially important vocal measure\\nthat we considered is volubility, the rate of infant vocalization independent of vocal type\\n(Nathani, Lynch, & Oller, 1999; Obenchain, Menn, & Yoshinaga-Itano, 1998).\\nCanonical babbling as a key milestone\\nIn typical development, infants from birth produce vegetative vocalizations (e.g., coughs,\\nburps, etc.) and cry, as well as vowel-like sounds that become more elaborate with time,\\nincorporating supraglottal articulations until canonical syllables emerge, usually by early in\\nthe second half-year of life. Robust onset of canonical babbling has been well documented\\nin typically developing infants by not later than 10 months (Koopmans-van Beinum & van\\nder Stelt, 1986; Oller, 1980; Stark, 1980). The impression of robustness has been reinforced\\nby the fact that no delay in onset of canonical babbling has been discerned in infants\\nanticipated to be at-risk for communication deficits due to premature birth or low\\nsocioeconomic status (Eilers et al., 1993; Oller, Eilers, Basinger, Steffens, & Urbano, 1995).\\nEven infants with Down syndrome usually show normal ages of onset, although a group\\nlevel delay of a month or more is detectable (Lynch et al., 1995). Furthermore, infants\\ntracheostomized at birth to provide an artificial airway that prevents or substantially inhibits\\nvocalization for many months tend to produce age-appropriate canonical syllables within a\\nshort period after decannulation (Bleile, Stark, & McGowan, 1993; Locke & Pearson, 1990;\\nRoss, 1983; Simon, Fowler, & Handler, 1983).\\nOnly profound hearing impairment and Williams syndrome have been shown to produce\\nconsistent substantial delays in the onset of canonical babbling (Kent, Osberger, Netsell, &\\nHustedde, 1987; Koopmans-van Beinum, Clement, & van den Dikkenberg-Pot, 1998;\\nMasataka, 2001; Oller & Eilers, 1988; Stoel-Gammon & Otomo, 1986). Further supporting\\nthe idea that restricted hearing prevents experiences critical to onset of canonical babbling,Patten et al. Page 2\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 1}),\n",
       "  Document(page_content='age of onset in severely or profoundly hearing impaired infants has been reported to be\\npositively correlated with age of amplification (Eilers & Oller, 1994).\\nIn infants without known disorders, onset of canonical babbling after ten months has been\\nshown to be a significant predictor of language delay or other developmental disabilities\\n(Oller, Eilers, Neal, & Schwartz, 1999; Stark, Ansel, & Bond, 1988; Stoel-Gammon, 1989).\\nBut late onset of canonical babbling is a rare occurrence in infants without easily diagnosed\\nphysical or mental limitations. The seeming resistance to derailment of this developmental\\nmilestone suggests that canonical babbling is of such importance in human development that\\nit has been evolved to emerge within a relatively tightly constrained time period in spite of\\nsubstantial variations in home environments and perinatal events. The importance of\\ncanonical babbling in predicting later language functioning is assumed to be due to the fact\\nthat words are overwhelmingly composed of canonical syllables, and thus lexical learning\\ndepends on control of canonical syllables.\\nTo date, only two studies of which we are aware have targeted canonical babbling in ASD\\nand neither specifically examined the onset  of canonical babbling, But reasons for optimism\\nthat delays in onset of canonical babbling could constitute an early ASD marker can be\\nfound in research showing that various aspects of vocalization appear to be disrupted in\\nyoung children with ASD (Paul, Augustyn, Klin, & Volkmar, 2005; Peppe, McCann,\\nGibbon, O’Hara, & Rutherford, 2007; Sheinkopf, Mundy, Oller, & Steffens, 2000; Warren,\\nGilkerson, Richards, & Oller, 2010; Wetherby et al., 2004). Research using automated\\nanalysis of all-day recordings based on the automated LENA (Language ENvironment\\nAnalysis) system of classification has shown clear indications that young children with ASD\\n(16–48 months) display low rates of canonical syllable production compared with typically\\ndeveloping infants, even after matching of subgroups for expressive language (Oller et al.,\\n2010). Even more to the point, one recent study has assessed the usage of canonical syllables\\n(though not the onset  of canonical babbling) in infants at high-risk  for ASD because they\\nwere siblings of children with ASD; seven of 24 participants in the study received a\\nprovisional diagnosis of ASD at 24 months (Paul, Fuerst, Ramsay, Chawarska, & Klin,\\n2011). As a group, the at-risk infants (all 24) produced significantly lower mean canonical\\nbabbling ratios (canonical syllables divided by all “speech-like” vocalizations, i.e., those\\ndeemed “transcribable” by the researchers) compared to low-risk infants at nine-months of\\nage, but there were no significant differences at 12 months. “Non-speech” vocalizations\\n(those deemed “not transcribable” e.g., yells, squeals, growls) were not included in the\\nevaluation of canonical babbling. Other vocal measures—especially number of consonant-\\nlike elements and number of speech-like and proportion of non-speech-like vocalizations—\\nalso appeared to be potentially useful indicators of emergent ASD.\\nVolubility in ASD\\nVolubility, or rate of vocalization, measured in terms of frequency of syllable or utterance\\nproduction, may be limited in ASD, a possibility that is supported by automated analysis of\\ndata showing low volubility in ASD from all-day recordings on children from 16 to 48\\nmonths of age based on the LENA system (Warren et al., 2010). Volubility in infants with\\nsevere or profound hearing loss and in infants with Down syndrome has not been found toPatten et al. Page 3\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 2}),\n",
       "  Document(page_content='be depressed compared with typically developing infants; however, infants from lower\\nsocio-economic status (SES) have been shown consistently to produce fewer utterances per\\nminute than their middle or high SES peers (Eilers et al., 1993; Oller et al., 1995). Research\\nsuggests that children in low SES experience less communication from caregivers (Hart &\\nRisley, 1995; Snow, 1995). The lower volubility of these infants may be a product of\\ndecreased social-communication from adults, potentially resulting in lower levels of social\\nmotivation in the infants.\\nVariability in moment-to-moment parental interactivity clearly does affect infant volubility\\nby the middle of the first year of life, as indicated by research on parent-infant interaction in\\nthe “still-face” paradigm. The work suggests a strong tendency in the particular case of\\nparent still-face for infants to increase vocalization rate. Specifically, volubility during a\\nbaseline period of one to three minutes of face-to-face vocal interaction is substantially\\nlower than during a following still-face period of one to three minutes where the parent\\nwithholds any facial or vocal reaction while continuing to look directly at the infant. This\\npattern is seen in infants after 5 months, but not at 3 months, where volubility does not\\nchange at the shift from face-to-face interaction to still-face (Delgado, Messinger, & Yale,\\n2002; Goldstein, Schwade, & Bornstein, 2009; Yale, Messinger, Cobo-Lewis, Oller, &\\nEilers, 1999). The results from the still-face paradigm are interpreted to mean that infants\\nseek to re-engage the withdrawn parent during the still-face period, having learned by the\\nmiddle of the first year that their vocalizations can have impact (Tronick, 1982). This effect\\nraises the question of whether infants with emergent ASD similarly increase their volubility\\nto re-engage their caregivers after a period of withdrawn caregiver attention, or whether they\\ndecrease volubility, possibly due to diminished motivation to engage socially with others.\\nFrequency of vocalizations directed at others has been reported to be significantly lower in\\ninfants later diagnosed with ASD compared to typically developing infants at 12 months but\\nnot at 6 months (Ozonoff, Iosif, Baguio, Cook, Hill, et al., 2010). It is also notable that\\nfrequency of vocalization based on parent report is predictive of language abilities in\\ntoddlers with ASD (Weismer, Lord, & Esler, 2010). Paul et al. (2011) assessed frequency of\\nvocalization in infants at high-risk and low-risk for developing ASD and found no difference\\nbetween groups. However, the study did not actually test for volubility the way volubility is\\ndefined here and in much prior research. Frequency of vocalization was tallied in a special\\nway in the Paul et al. study, by counting all speech-like (phonetically transcribable) and\\nnonspeech-like (not phonetically transcribable) vocalizations that occurred within the first\\n50 speech-like vocalizations of each recorded sample. But not all participants produced 50\\nspeech-like utterances, and in the ones who did, the length of recording required to reach the\\n50 speech-like utterance criterion was variable. Thus, rate of vocalizations per unit of time\\nwas not examined in this study; consequently, given the common usage of the term\\nvolubility, it is not possible to determine whether there was a difference in volubility\\nbetween the groups. In addition, participants in this study were at high-risk for ASD—some\\nwere later diagnosed with ASD while some were not. This mixture may have attenuated\\ngroup differences. It should also be noted that Weismer et al. included only child\\nvocalizations directed at others while Paul et al. included vocalizations. Although ASD has\\nroots in social impairments, vocalizations directed at others as well as independent vocal\\nplay might well be abnormal in ASD.Patten et al. Page 4\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 3}),\n",
       "  Document(page_content='A new study of early vocal development in ASD\\nOne reason the development of pre-speech vocal behaviors in ASD has not been well\\ndocumented may be that ASD is not reliably diagnosed until long after canonical syllables\\nare expected to emerge, thus making prospective analyses challenging. Retrospective\\ninterviews with parents whose children have been diagnosed with ASD regarding age at\\nwhich canonical syllables emerged may be hindered by poor parent recall, given that parents\\nare generally asked to remember the nature of child babbling that occurred one or more\\nyears prior to the time of the interview; also, parents’ awareness of the diagnosis may bias\\ntheir recall of the onset of canonical babbling. The effort by Paul et al. (2011) cited above\\nrepresents a key advancement in methodology because they assessed infants known to be at-\\nrisk in a prospective fashion. Our approach seizes an additional opportunity afforded by the\\nfortuitous existence of home video data from the first year of life that can be analyzed after\\ndiagnosis of ASD for comparison with similar video data from infants who did not receive\\nthe diagnosis.\\nAs indicated in studies cited above, emergence of canonical syllables is a critical milestone\\nin the development of spoken language, and delayed onset has been shown to be predictive\\nof significant communication impairment. Canonical babbling and volubility have not been\\nwell characterized in infants with ASD. To arrive at a better understanding of these two\\nvariables as potential indicators of ASD risk in infants, we investigated vocalizations of\\ninfants later diagnosed with ASD and typically developing (TD) infants at two age ranges,\\n9–12 months and 15–18 months, using retrospective video analysis methods. Previous\\nresearch has suggested that nearly all TD infants reach the canonical babbling stage by 9–12\\nmonths (Eilers & Oller, 1994), and on the assumption that a delay might be present in the\\nchildren later diagnosed with ASD, we predicted such delay would be observed in this age\\nrange. We took the opportunity also to evaluate the available data at 15–18 months because\\nany infant with a failure to show canonical babbling at that age would be greatly delayed in\\ncanonical babbling onset and would be considered at very high risk for a variety of\\ndisorders.\\nThe coding scheme for this study is based on a widely applied method for laboratory-based\\nevaluation of canonical babbling (Oller, 2000). In accord with this method, infants are\\nassumed to be in the canonical stage if they show a canonical babbling ratio (canonical\\nsyllables divided by all syllables) of at least .15, a value based on coding by trained listeners\\nof a recording. A value of .15 or greater from such laboratory coding has been empirically\\ndetermined in prior research as corresponding to parent judgments that infants are in the\\ncanonical stage (Lewedag, 1995). It has been reasoned that parent judgments constitute the\\nmost appropriate standard for establishing this criterion value (Oller, 2000). This reasoning\\nis based on three points: 1) Parents respond to interview questions by providing very\\nconsistent and accurate information about canonical babbling in their infants (Papoušek,\\n1994; Oller, Eilers, & Basinger, 2001); 2) this parental capability is predictable, given that\\nrecognizing canonical babbling represents nothing more than being able to recognize\\nsyllables as being well-formed enough that they could form parts of words in real speech\\n(and of course normal adults can easily recognize vocalizations of humans as speech or non-\\nspeech); and 3) parents appear to intuitively understand that the onset of canonical babblingPatten et al. Page 5\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 4}),\n",
       "  Document(page_content='is an emergent foundation for speech, as evidenced by the fact that they initiate intuitive\\nlexical teaching as soon as they begin to recognize canonical babbling in their infants\\n(Papoušek, 1994). Consistent parent recognition of the onset of canonical babbling runs in\\nparallel with recognition of other developmental milestones (e.g., sitting unsupported,\\ncrawling, walking). In our study we could not use parents as informants about the age of\\nonset of canonical babbling since that onset had occurred a very long time before our first\\ncontact with them. Consequently, the canonical babbling ratio, determined from recordings\\ncoded in our laboratory, provided the best available measure upon which to base inference\\nabout whether infants had reached the canonical stage.\\nIn the present study the following hypotheses were tested:\\n1.Infants later diagnosed with ASD will be less likely than TD infants to be in the\\ncanonical stage at each age (9–12 and 15–18 months), as determined by whether\\ntheir canonical babbling ratios exceed the .15 criterion.\\n2.Infants later diagnosed with ASD will demonstrate significantly lower canonical\\nbabbling ratios (independent of the canonical stage criterion) compared to TD\\ninfants.\\n3.Infants later diagnosed with ASD will demonstrate significantly fewer total\\nvocalizations (lower volubility) at both age ranges compared to TD infants.\\n4.A combined analysis using both volubility and canonical babbling status will\\nsignificantly predict group membership.\\nMethod\\nParticipants\\nA total of 37 participants were included in the present study, 23 individuals later diagnosed\\nwith ASD and 14 individuals in the TD group (Table 1). There was one set of fraternal twins\\nin the ASD group. Participants were drawn from a larger study conducted at the University\\nof North Carolina-Chapel Hill based on availability of video recordings; participants must\\nhave had two five-minute edited video segments at 9–12 months and at least one edited\\nvideo segment at 15–18 months. As part of the larger study, participants were recruited from\\nthe Midwest and Southeast over a 15-year time period. Recruitment criteria included: (1)\\nchild age between two and seven years at the time of recruitment; (2) available home\\nvideotapes of the child between birth and two years of age that parents were willing to share;\\nand (3) enough video footage for at least one 5-minute codable segment (see video editing\\nsection below) of the child at either 9–12 or 15–18 months of age.\\nAll participants included in the ASD group received a clinical diagnosis of ASD from a\\nlicensed psychologist and/or physician at a point after the recordings were made. Thus, our\\ndesign is a retrospective analysis similar to others that have used home movies of children\\nlater diagnosed with ASD (Baranek, 1999; Werner, Dawson, Osterling, & Dinno, 2000). A\\ntrained research staff member validated diagnoses for each participant using criteria from\\nthe Diagnostic and Statistical Manual IV (American Psychiatric Association, 2000) and\\nfrom one or more ASD screening and diagnostic tools, including: the Childhood AutismPatten et al. Page 6\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 5}),\n",
       "  Document(page_content='Rating Scale (CARS; Schopler, Reichler, & Renner, 1992), the Autism Diagnostic\\nObservation Schedule (ADOS; Lord, et al., 1999), and/or the Autism Diagnostic Interview-\\nRevised (ADI-R; Rutter, LeCouteur, & Lord, 2003). All participants had CARS scores and\\neach participant in the ASD group had ADI/ADI-R scores and 13 of the 23 ASD participants\\nhad ADOS scores.\\nTypically developing group membership was based in part on scores within normal limits\\n(i.e., not more than one standard deviation below the mean) on the Mullen Scales of Early\\nLearning (Mullen, 1995) and/or the Vineland Adaptive Behavior Scales (VABS; Sparrow,\\nBalla, & Cicchetti, 1984). An additional exclusionary criterion for any participants in the TD\\ngroup was any history of learning or developmental difficulties per parent report. Individuals\\nwith significant physical, visual or hearing impairments or known genetic conditions (e.g.,\\nFragile X or Rett’s Syndrome) associated with ASD were excluded. As indicated in Table 1,\\nmean age (in months) was very similar across groups, gender was balanced, and the two\\ngroups were also similar with regard to SES based on maternal education. Our families were\\nmostly middle SES with access to videotaping equipment.\\nThe University of North Carolina-Chapel Hill Institutional Review Board approved the\\nstudy, and all families signed informed consents. For more information regarding\\nrecruitment and inclusion criteria see Baranek (1999).\\nVideo Editing Procedures\\nFamilies provided home videos of their child from birth to two years as available. The\\nvideotapes included footage from a variety of contexts including family play situations,\\nvacations, outings, special events, and familiar routines (e.g., mealtimes), with individual\\nvariation in situational content of each family’s videotapes as would be expected in home\\nvideotapes. All videotapes were copied, transformed to digital formats, and originals were\\nreturned to participating families.\\nVideo editing guidelines first focused on the identification of video footage during which the\\nchild was consistently visible and for which the parents felt they could accurately identify\\nthe child’s age. The two age ranges were originally selected for another study on early\\nbehavior in ASD (Baranek, 1999). At the same time, the two age ranges are well-suited to\\nour current purposes. The 9–12 month age range is the earliest age range in which parents\\nhad sufficient videotape footage for it to be useful in our research and represents the time\\nperiod when a number of communicative behaviors emerge. Further, this is a time frame\\nduring which the vast majority of TD children would be expected to already be in the\\ncanonical babbling stage. The 15–18 month range provided follow-up on the same children\\nwith the expectation that monitored behaviors would be more consistent and would allow for\\nconfirmation or clarification of data from the earlier age. In TD children, canonical babbling\\nis usually well consolidated by the 15–18 month age range (Vihman, 1996; Oller, 2000).\\nIn editing tapes for the larger study, the aim was to compile two 5-minute video segments\\nfor each child in the 9–12 age range, and two 5-minute segments in the 15–18 month age\\nrange. On average, each 5-minute segment consisted of 5 scenes. Research assistants who\\nwere blind to the research questions and not informed of the diagnostic status of thePatten et al. Page 7\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 6}),\n",
       "  Document(page_content='participants edited the videotapes and coded each scene for the following content variables:\\n(a) number of people present; (b) amount of physical restriction on child’s freedom to move,\\nrated as low, medium, or high; (c) the amount of social intrusion another person was using\\nto engage the child in interaction, rated as low, medium, or high; (d) and the types of events\\n(e.g., meal time, bath time, active play, special events) (Baranek, 1999). The assistants were\\ninstructed to quasi-randomly select a cross-section of scenes from the available footage in\\nthe designated age ranges, purposely including scenes from each one-month age interval for\\nwhich video footage was available within each age range, provided that the child was visible\\nin each selected scene. All participants included in the current study had two 5-minute\\ncompilations (i.e., 10 minutes total) for the 9–12 month age range, but at the 15–18 month\\nage range, there were three TD infants and one infant with ASD for whom only a single 5-\\nminute segment was assembled due to insufficient video footage. As a result, the mean\\nduration of samples at the 15–18 month age range was 9.5 minutes rather than 10.\\nAlthough vocalization from the infants was common in these scenes, the segments were not\\nspecifically selected to capture vocal behavior. Therefore, volubility estimated from the\\npresent study may be lower than in prior works where infants have been observed in settings\\ndesigned to maximize vocal interaction. Similarly, the video segment selection procedure\\nmay yield differences in canonical babbling from prior studies. In most studies, 20–30\\nminutes of vocal interaction have been recorded, whereas here we had less than half that\\namount of data per sample. Our procedure can be predicted to produce greater variability in\\ncanonical babbling ratios than in studies with longer sampling periods (Molemans, 2011;\\nMolemans et al., 2011). Additionally, the audio-video quality of these home movies was not\\nas good as would be expected in laboratory studies, another factor that could reduce\\nperceived canonical babbling and volubility.\\nTo ensure that the contexts in which children were recorded were comparable, specific\\ncontent parameters were identified and compared (Tables 2 and 3). No differences were\\nfound between the groups on any content parameter including: number of people present,\\nlevel of physical restriction (i.e., amount of physical confinement such as a highchair versus\\nfree play; rated as low, medium or high), amount of social intrusion (rated as low, medium\\nor high), and the total number of event types (e.g., meal time, active play). The number of\\ntimes each event type (e.g., bath time, playtime) was represented in the ASD group versus\\nthe TD group for each age was compared using chi-square analyses. Results for the omnibus\\nchi-square test failed to reach significance in the 9–12 month age group ( p > 0.05), but did\\nreach significance in the 15–18 months age group ( p = 0.046). Typically developing children\\nwere more likely to be engaged in passive activities at the 15–18 month age range ( p =\\n0.046; TD = 16.6%, ASD = 4.6%) according to follow-up analysis of the six event\\ncategories. See Tables 4 and 5 for the percentage in each category. For a comprehensive\\ndescription of the coding procedures that yielded the data on situational context see Watson,\\nCrais, Baranek, Dykstra, and Wilson (2012).\\nCoding Procedure and Observer Agreement\\nThe videotapes analyzed in this study were coded for infant production of all syllables in\\nspeech-like vocalizations by two certified speech-language pathologists who were notPatten et al. Page 8\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 7}),\n",
       "  Document(page_content='informed of the diagnostic group of the infants. The intent was, of course, for the coders to\\nbe blind to diagnostic category, and with the exception of one infant to be discussed below,\\nthe coders reported they saw no reason to suspect any infant of having ASD.\\nWe defined speech-like vocalizations (as in the primary literature on canonical babbling) to\\ninclude both canonical and precanonical infant vocalizations (regardless of whether they\\nwould be deemed “transcribable”). Training of the two coders was provided by the last\\nauthor, who originated the definition of “canonical syllable” used in this study, and who has\\nconducted and collaborated on numerous studies on onset of canonical babbling, rate of\\ncanonical babbling, and volubility in infants (Cobo-Lewis, Oller, Lynch, & Levine, 1996;\\nLynch et al., 1995; Oller & Eilers, 1982, 1988; Oller, Eilers, & Basinger, 2001; Oller et al.,\\n1995; Oller, Eilers, Neal, & Cobo-Lewis, 1998). The two observers were trained in\\nidentifying canonical syllables and in counting all syllables independent of their canonical\\nstatus. The video samples used during training were separate (although drawn from similar\\nmaterials based on the home recordings) and not included in the analyses for this\\ninvestigation.\\nSyllables were defined as rhythmic units of speech-like vocalization, excluding raspberries,\\neffort “grunt” sounds (i.e., a schwa-like sounds produced as an artifact of physical exertion),\\ningressive sounds, sneezes, hiccups, crying and laughing. Within an “utterance”, which was\\ndefined as a vocal breath group (Lynch, Oller, Steffens, & Buder, 1995), it was possible to\\nidentify syllables as corresponding to sonority peaks (high points of pitch and/or amplitude)\\nthat are intuitively recognized by mature listeners. These rhythmic events occur in time\\nframes typical of syllables in real speech (usually with durations of 200–400 ms). A\\ncanonical syllable is defined as including a vowel-like nucleus, at least one margin (or\\nconsonant-like sound) and a transition between margin and nucleus that is rapid and\\nuninterrupted. In general, transitions that are too fast to be tracked auditorily (too fast to be\\nheard “as transitions”) are instead heard as gestalt syllables. Auditory tracking of these\\ntransitions focuses on formant (acoustic energy) transitions that can be measured on\\nspectrograms as typically < 120 ms (Oller, 2000). Formants are audible bands of energy\\ncorresponding to resonant frequencies of the vocal tract that change as the tract changes\\nshape or size. Audible formant transitions occur, then, when the vocal tract moves during\\nopening from a consonantal closure into a vowel or vice versa.\\nExamples of canonical utterances (which must include at least one canonical syllable) are\\nsyllables that a listener might perceive as ba, taka, or gaga. Vocalizations produced while\\nmouthing objects (e.g., toys or fingers) or eating were excluded from our analyses on the\\ngrounds that we could not be sure what role movement of the hands may have played in the\\napparent syllabification.\\nVideos were randomized and randomly distributed across the two coders with regard to\\ndiagnostic group. The 37 participants’ videos were randomly split between the coders by\\nparticipant and included both age ranges. The coders independently watched the videos,\\ncounting both syllables and canonical syllables in real time. This procedure is utilized\\nregularly in the laboratories of the last author in accord with reasoning presented in recentPatten et al. Page 9\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 8}),\n",
       "  Document(page_content='papers, especially Ramsdell et al. (2012). This naturalistic listening approach mimics how a\\nmother would hear her child, listening to each utterance only once.\\nThe measure of canonical babbling ratio used here (number of canonical syllables divided\\nby number of all syllables) is the measure utilized in the bulk of research on onset of\\ncanonical babbling to date. However, some studies have used a different ratio (number of\\ncanonical syllables divided by number of utterances). The former procedure is generally\\npreferred nowadays because the resulting value can be interpreted as a proportion with\\nvalues varying from 0 to 1, whereas the latter procedure yields a ratio with no effective\\nupper limit (Oller, 2000).\\nIn a coder agreement test, both observers independently coded twenty samples consisting of\\ntwo five-minute segments of ten participants’ video footage. A research assistant unaware of\\nthe study goals selected these test samples, and they represented both diagnostic groups and\\nboth ages. Reliability was gauged in accord with the degree to which coders agreed upon\\ncanonical syllables, total syllables, and whether the child was in the canonical babbling stage\\n(i.e., had a canonical babbling ratio > 0.15, the standard criterion). Inter-rater agreement\\nranged from good to excellent for canonical syllables (ICC = .98, CI95 = .96 – .99) and for\\ntotal syllables (ICC = .87, CI95 = .61 – .95). Reliability for canonical babbling ratios was\\nalso good (ICC = .89, CI95 = .69 – .96), with agreement on the canonical stage criterion at\\n95% for the twenty samples. Additionally, the coders differed by an average of only 10% of\\nthe total range of canonical babbling ratios obtained, and the correlation across the ratios for\\nthe twenty samples for the two coders was .89. For volubility, the coders differed by an\\naverage of 13% of the total range for volubility values, and the correlation across the twenty\\nsample videos for the two coders was .91.\\nResults\\nAnalyses were performed to confirm that the groups were matched on demographic\\nvariables. These analyses did not reveal significant differences between groups on any\\nvariable (see Table 1).\\nInitial descriptive statistics for within- and between-group variables revealed two outliers in\\nthe ASD group. Both cases produced very high canonical babbling ratios in the 9–12 month\\nrange (.93 and .64) relative to the mean for both groups (ASD = .12 for the 23 cases, TD = .\\n17) (see Figure 1). Based on prior research, the canonical babbling ratios observed for these\\ntwo ASD cases were substantially higher than would be expected in TD infants in the 9–12\\nmonth age range—infants grouped as having English or Spanish at home, as high or low\\nSES, and as born at term or prematurely all showed mean canonical babbling ratios under .4\\nfrom 8 to 12 months of age (Oller, Eilers, Urbano, & Cobo-Lewis, 1997; Oller, Eilers,\\nSteffens, Lynch, & Urbano, 1994). Analysis of z-scores revealed that infant 22 was 3.96\\nstandard deviations above the mean for the present sample, and infant 23 was 2.73 standard\\ndeviations above the mean, further suggesting outlier status. On this basis we decided to\\neliminate these two cases in the primary analyses on canonical babbling; the remaining 35\\ncases (21 ASD, 14 TD) were analyzed to address our research questions regarding canonical\\nbabbling (see Figures 1 and 2 for canonical babbling ratios by participant at both ages, withPatten et al. Page 10\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 9}),\n",
       "  Document(page_content='the two outliers indicated). However, there were no significant outliers with regard to\\nvolubility, and thus we included data from all 37 cases for that analysis (see Figures 3 and 4\\nfor syllable volubility by participant at both ages).\\nHypothesis 1: Infants later diagnosed with ASD will be less likely than typically developing\\ninfants to be in the canonical stage at each age (9–12 and 15–18 months)\\nLog odds ratios (log OR) were calculated to compare the classifications of both ASD and\\ntypically developing children with regard to their canonical babbling. The criterion for\\ncanonical babbling stage was set at 15% or greater canonical syllables compared to all\\nsyllables; this is a common criterion in studies of canonical babbling, and is based on data\\nreviewed in Oller (2000). TD infants were significantly more likely to have reached the\\ncanonical babbling stage based on the criterion than were infants later diagnosed with ASD\\nat the 9–12 months age range (N = 35, log OR = 2.84, CI 95 = 1.02 to 4.66, p = 0.002), and\\nremained more likely at the 15–18 month age range (N = 35, log OR = 1.78, CI 95 = −0.04 to\\n3.61, p = 0.054). As an easily interpretable effect size measure, the simple odds ratios (as\\nopposed to the log odds ratio, which is statistically preferable for significance testing with\\nsmall N’s) can be considered; the simple ORs indicated TD infants were 17 times more\\nlikely (OR = 17.1) to be categorized as in the canonical stage than ASD infants at 9–12\\nmonths and 6 times more likely (OR = 5.96) at 15–18 months.\\nHypothesis 2: Infants later diagnosed with ASD will demonstrate significantly lower\\ncanonical babbling ratios (independent of the canonical stage criterion) compared to\\ntypically developing infants\\nCanonical babbling ratios of infants later diagnosed with ASD and TD infants were\\ncontrasted using a Mixed ANOVA. The between-subjects variable was diagnostic category\\n(ASD vs. TD) and the within-subjects variable was age range (9–12 months and 15–18\\nmonths). The mean canonical babbling ratios at 9–12 months were .06 (SD = .06) for the 21\\ninfants later diagnosed with ASD and .17 (SD = .13) for the 14 TD infants; at 15–18 months\\nthe values were .16 (SD = .22) and .28 (SD = .16) respectively (Figure 5). Analyses revealed\\na significant main effect for diagnostic category ( F (1,1) = 6.79, p = .01, ŋp2 = 0.17), with\\ninfants later diagnosed with ASD producing significantly lower canonical babbling ratios,\\nand a significant main effect for age ( F (1,1) = 7.86, p < .01, ŋp2 = 0.19), with higher\\ncanonical babbling ratios at the older age. The effect size between groups for 9–12 months\\nwas d = 1.09 (a large effect) and for 15–18 months was .62 (a moderate effect; Cohen,\\n1992). The age by diagnosis interaction was not significant ( p > 0.66).\\nHypothesis 3: Infants later diagnosed with ASD will demonstrate significantly fewer total\\nvocalizations (lower volubility) at both age ranges compared to typically developing\\ninfants\\nFor this analysis, all 37 infants were included because there were no significant outliers.\\nVolubility of infants later diagnosed with ASD and TD infants were contrasted using a\\nMixed ANOVA. The between-subjects variable was diagnostic category (ASD vs. TD), and\\nthe within-subjects variable was age range (9–12 months and 15–18 months). Infants later\\ndiagnosed with ASD produced a mean of 4.55 (SD = .59) syllables per minute while TDPatten et al. Page 11\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 10}),\n",
       "  Document(page_content='infants produced a mean of 5.86 (SD = .67) syllables per minute at 9–12 months. At 15–18\\nmonths, infants later diagnosed with ASD produced a mean of 3.24 (SD = .49) syllables per\\nminute while TD infants produced a mean of 4.63 (SD = .51) syllables per minute (see\\nFigure 6). Analyses revealed a significant main effect for diagnostic category ( F (1,1) =\\n4.85, p = .034, ŋp2 = 0.12), and for age ( F (1,1) = 4.96, p = .032, ŋp2 = 0.12). Thus, infants\\nlater diagnosed with ASD displayed significantly lower volubility than TD infants. The\\neffect size for group at 9–12 months was d = 2.07 (large) and at 15–18 months was 2.77\\n(large).\\nHypothesis 4: A combined analysis using both volubility and canonical babbling status\\nwill significantly predict group membership\\nLogistic regression analysis was conducted to test whether canonical babbling status\\n(whether each participant met the .15 criterion) and volubility at age ranges of 9–12 months\\nand 15–18 months could reliably predict later diagnosis status (group membership). This test\\nwas conducted with all 37 cases included, partly in order to match the number of cases for\\nthe two predictor variables and partly because the goal of the analysis was to determine the\\npotential practical utility of identification of these children without any information other\\nthan volubility and canonical babbling ratio. This test may thus be the one of primary\\nclinical interest, since it evaluates the circumstance that screening implies, where there\\nwould be no basis for knowing whether an infant might be an outlier on any variable.\\nWithout this evaluation there would be no direct indication in our results of the degree of\\ngroup discriminability.\\nStatistical significance was reached in a test of the full model against a constant-only model,\\nwhich indicated that, as a set, canonical babbling status and volubility reliably predicted\\nlater diagnosis ( χ2 = 9.82, p = 0.044, df = 4). A small-to-moderate relationship between\\nprediction and grouping was observed ( Nagelkerke’s R2 = 0.317), with an overall prediction\\nsuccess of 75% (64% for TD and 82% for ASD). However, further examination of the\\npredictors using the Wald criterion revealed that when all four predictor variables were\\nincluded in the model, none significantly contributed to prediction of group membership at\\nan individual level ( p > 0.05). The status of infants with regard to canonical babbling stage\\nat the 9–12 months age range provided the largest observed predictive contribution, Wald =\\n3.06, p = 0.08, EXP(B) = 0.198. The contribution to group discriminability by volubility at\\n9–12 and 15–18 months age ranges approached nil, EXP(B) = 0.992 and 0.985 respectively.\\nExamination of the correlations among the predictor variables showed that all but volubility\\nat 9–12 months were significantly correlated with all other predictors (Table 6), with\\nvolubility at 9–12 months significantly correlated with only canonical babbling at 9–12\\nmonths. This inter-relation among the predictor variables suggests that, to some degree, they\\naccount for some of the same variance in diagnosis. However, the observed EXP(B)  values\\n(odds ratios of the outcomes given the value of an individual predictor) more strongly\\nsuggest that canonical babbling at 9–12 months accounted for the bulk of the variability in\\ndiagnosis.\\nIt seems clear that significance of the individual predictors in the logistic regression may\\nhave been hampered by the high level of relation among them. Individually, the volubilityPatten et al. Page 12\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 11}),\n",
       "  Document(page_content='variables did not appear to have much influence given small Betas and high p-values. When\\npredictors were entered into the model in a hierarchical fashion, no matter how predictor\\nentry was ordered (9–12 month variables at step 1 and 15–18 month variables at step 2, or\\nCB variables at step 1 and volubility at step 2), only the 9–12 month CB variable was a\\nsignificant independent predictor. R2 changes and diagnostic ability for all of the regression\\nand regression step iterations suggested little was added to the R2 by adding variables in the\\nsecond step (even with two more predictor variables, only ~ 2–3% was added to R2), nor did\\nthese additions substantially alter the ability of the model to predict later diagnosis. The\\nmost efficient model appeared to be a logistic regression with 9–12 month CB as the only\\npredictor.\\nDiscussion\\nThe importance of early intervention for children with ASD has resulted in attempts to\\nquantify behaviors in infancy that may lead to early detection. Substantial effort has\\naddressed gestural and social development and their potential roles in detection within the\\nfirst year of life (e.g., Watson, Crais, Baranek, Dykstra, & Wilson, 2013). The present\\nresults offer parallel findings in the domain of vocal development by demonstrating\\nsignificant group differences in canonical babbling status, canonical babbling ratio, and total\\nsyllables produced (volubility) during the first year of life.\\nIn our study, infants later diagnosed with ASD were significantly less likely to be classified\\nas being in the canonical babbling stage, and demonstrated significantly reduced canonical\\nbabbling ratios compared to TD peers. Although significant group differences were apparent\\nin both age ranges (9–12 and 15–18 months), the effect sizes for canonical babbling were\\nlarger at 9–12 months. Paul et al. (2011) demonstrated similar results in infants at high-risk\\nfor developing ASD who produced significantly lower canonical babbling ratios compared\\nto low risk infants at 9 months, though at 12 months the differences were not statistically\\nsignificant. Combined with the finding from Oller et al. (2010) that children with ASD up to\\n48 months of age show low canonical syllable production, the data here suggest that low\\nproduction of canonical syllables may be a helpful marker for ASD from infancy into early\\nchildhood.\\nSince canonical babbling is well established in the vast majority of TD infants by 10 months\\n(Eilers and Oller, 1994), it might seem odd that several of the TD infants (5 at 9–12 months\\nand 3 at 15–18 months) in the present study provided samples that did not meet the .15\\ncanonical babbling ratio criterion for assignment to the canonical stage of vocal\\ndevelopment. However, it is important to consider the fact that even infants who are clearly\\nin the canonical stage based on parent report often fail to reach the criterion in a single\\nlaboratory sample of 20–30 minutes (Lewedag, 1995). In addition, unlike the samples in\\nprior research on canonical babbling, the samples here were not designed to elicit\\nvocalizations, and consequently they may have been less rich in quantity and variety of\\nvocalization than the samples that were used to develop the criterion. Further, our samples at\\n9–12 months were only 10 minutes in duration, and at 15–18 months an average of slightly\\nless than 10 minutes; it has been shown that variability in obtained canonical babbling ratios\\nincreases as the length of samples decreases (Molemans, 2011; Molemans, Van den Berg,Patten et al. Page 13\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 12}),\n",
       "  Document(page_content='Van Severen, & Gillis, 2011). Finally, our samples were based on home recordings with\\nconsiderable noise and variable camera management that may have impeded our ability to\\nrecognize vocalizations in the samples. Consequently, we are not surprised that some of the\\nTD infants failed to reach the criterion used to determine canonical status based on\\nlaboratory samples.\\nGiven the strong links between the onset of canonical babbling and language development\\n(Oller, Eilers, Urbano, & Cobo-Lewis, 1997; Stoel-Gammon, 1989), delayed onset of\\ncanonical babbling in infants with ASD may reflect latent communication impairment. It\\nalso may be that delayed canonical babbling directly contributes to communication\\nsymptoms in ASD. Canonical babbling requires motor ability as well as motivation to\\nproduce syllables, and practice in babbling may lay critical foundations for speech.\\nProspective research on motor development in infants later diagnosed with ASD is sparse\\nand often limited to high-risk groups, but available research does indicate that early motor\\nimpairment may be present (e.g., Matson, Mahan, Fodstad, Hess, & Neal, 2010; Manjiviona\\n& Prior 1995; Page & Boucher, 1998; Teitelbaum, Teitelbaurm, Nye, Fryman, & Maurer,\\n1998). Thus delayed canonical babbling may reflect an immature or disordered motor\\nsystem with specific implications for speech.\\nIf language develops as a consequence of social reinforcement of speech-like sounds that\\neventually evolve into true words (consider behavioral models of language development as\\nin Hulit & Howard, 2002; Goldstein, King, & West, 2003; Goldstein & Schwade, 2008;\\nGoldstein & West, 1999), then social reinforcement may encourage the production of\\ncanonical babbling. Children with ASD may be less motivated by social reinforcement,\\nyielding less frequent vocal exploration and production of canonical syllables than in TD\\ninfants. To add to the problem, a delay in canonical babbling may result in reduction in\\ncaregiver social-communication directed toward the infant. On average, by six to seven\\nmonths and very rarely later than ten months, canonical babbling emerges in TD infants\\n(Eilers & Oller, 1994). In response to recognition of canonical babbling, caregivers alter\\ntheir communication pattern, sometimes attempting to direct the infant toward using\\ncanonical syllables meaningfully—for example, the parent who hears [baba] may reply,\\n“Yes, that’s a bubble” (Papoušek, 1994; Stoel-Gammon, 2011). Therefore, infants who are\\ndelayed in canonical babbling may also be delayed in their exposure to important linguistic\\ninput, and thus may be given less opportunity to learn words. A final point is that infants\\nwith ASD may simply have lower motivation to vocalize socially in the first place. This\\nlower motivation could provide a further basis for slow vocabulary learning.\\nOur results on volubility included two statistically reliable findings. First, children in both\\ngroups had lower volubility at the second age than at the first. We attribute no particular\\ntheoretical importance to this finding but we take note of the fact that the lower level of\\nvolubility at 15–18 months compared to 9–12 months did correspond to greater physical\\nmovement of the children at the older age. In both groups combined, level of physical\\nrestriction during the selected recording samples was significantly less at the older age (p < .\\n001). As reported earlier, level of physical restriction was not significantly different between\\ndiagnostic groups.Patten et al. Page 14\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 13}),\n",
       "  Document(page_content='The second volubility finding is that infants later diagnosed with ASD produced\\nsignificantly fewer vocalizations deemed to be relevant for the emergence of speech (both\\ncanonical and non-canonical sounds) at both age ranges (9–12 and 15–18 months) compared\\nto TD peers. Other research has demonstrated that infants with ASD direct fewer\\nvocalizations to others (Ozonoff et al., 2010); our study extends this finding to a more\\ngeneral measure of volubility in terms of total vocalizations (syllables) rather than only ones\\ndirected to others. Our finding is also congruent with results from automated analysis of all-\\nday recordings indicating low volubility in children with ASD at 16–48 months of age\\n(Warren et al. 2010). The results may seem to run counter to Paul et al. (2011) whose\\nsample of high-risk infants were reported to not produce significantly fewer vocalizations\\nthan low-risk infants. However, as described in the introduction above, the Paul et al. study\\ndid not report data in a way that can be directly compared with the volubility data reported\\nhere.\\nSome disability groups (e.g., hearing impaired infants and infants with cleft palate) have\\nbeen reported to exhibit volubility similar to that of TD infants (Clement, 2004; Chapman,\\nHardin-Jones, Schulte, & Halter, 2001; Van den Dikkenberg-Pot, Koopmans-van Beinum, &\\nClement, 1998; Nathani, Oller, & Neal, 2007; Davis, Morrison, von Hapsburg, & Warner-\\nCzyz, 2005); however, infants from low SES households have been reported to have\\nsignificantly decreased volubility in comparison to those from higher SES households\\n(Oller, Eilers, Basinger, Steffens, & Urbano, 1995). Children from low SES backgrounds are\\noften presumed to be at-risk for language deficits. Although it would be impossible to\\nidentify and quantify all of the mechanisms through which poverty may affect language\\ndevelopment, research has demonstrated that the amount of communication caregivers direct\\ntoward their children is decreased in low SES situations (Hart & Risley, 1995). This\\nimpoverished linguistic environment may result in decreased dyadic social and\\ncommunicative interactions and thus in a decrease in overall volubility of infants.\\nIt is important to note that the relatively well-matched SES between our two groups suggests\\nthat the differences in volubility were not attributable to differences in SES. In the case of\\nlow SES households, an impoverished linguistic environment due to lack of parent\\nresponsiveness might be expected to lead to decreased volubility of the infant and later\\nlanguage difficulty. For infants later diagnosed with ASD, reduced volubility may be\\naffected by multiple factors, not related to inherent parental responsiveness, but related\\ninstead to the social impairments of ASD. One issue is that these children may experience\\nless linguistic stimulation due to having disrupted sensory processing systems corresponding\\nto sensory hyporesponsiveness; children with ASD are less likely to respond, or require\\nsubstantially more stimulation to respond to environmental events (Baranek, 1999; Baranek\\net al., 2013); Miller, Reisman, McIntosh, & Simon, 2001; Rogers & Ozonoff, 2005). This\\ncharacteristic of ASD is also reflected in the tendency for infants as young as eight months\\nwho will later be diagnosed with ASD to be less likely than TD infants to respond to their\\nname being called (Werner, Dawson, Osterling, & Dinno, 2000). This lack of\\nresponsiveness may indicate that infants with ASD are less affected by vocal\\ncommunication from caregivers than TD infants. If so, the lack of responsiveness may\\nreflect an effectively impoverished linguistic environment because of attenuated reception of\\ncaregiver input by infants with ASD and subsequent communication impairments. Indeed,Patten et al. Page 15\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 14}),\n",
       "  Document(page_content='sensory hyporesponsiveness has been shown to be associated with poorer language\\nfunctioning in children with ASD (Watson et al., 2011).\\nAn additional way that the environment for children with ASD may be impoverished could\\ninvolve a social feedback loop (Warlaumont et al., 2010) that is under investigation using\\nautomated analysis of vocalizations of parents and infants from all-day home recordings.\\nSince infants with ASD produce fewer canonical syllables than TD infants, and since\\nparents respond strongly with language stimulation to canonical syllables, an infant with\\nASD may actually hear less language from parents, because parents provide input that is tied\\nto the infant’s output. The infant’s low volubility may then be aggravated by lower input\\nlevels resulting from the infant’s own anomalous pattern of vocalization.\\nFinally, the logistic regression analysis with four independent variables (age 1 and age 2\\ncanonical babbling classification and age 1 and age 2 volubility) demonstrated that\\nclassification of diagnostic category (ASD vs. TD) could be predicted with 75% accuracy,\\neven when the two outliers were included. The model more accurately classified infants later\\ndiagnosed with ASD (Sensitivity = 82.6%) than TD infants (Specificity = 64.3%). The\\nstrongest predictor of group membership was canonical babbling classification at 9–12\\nmonths as it alone correctly classified 90% of infants later diagnosed with ASD and 63% of\\nTD infants. Thus, in the search for markers of ASD risk in infancy, canonical babbling\\nstatus at 9–12 months appears to be the single best candidate among the variables considered\\nin the current study. The utility of the measure as a group marker is age dependent, since a\\nlarger proportion of infants in the ASD group at 15–18 months had reached the canonical\\nstage than at 9–12 months.\\nTo help better understand the high canonical babbling ratios of the two outliers, the coders,\\nboth certified speech-language pathologists, viewed the videos from those infants again after\\ntheir outlier status was identified. We speculated that the outlier status of these two infants\\nmay be related to the phenomenon of motor stereotypy that is common in ASD, that is, that\\nthe two infants were engaged, at least in the 9–12 month samples, in a motor stereotypy\\nfocused precisely on canonical babbling. In re-examining the videos of the two outliers, the\\ncoders looked for qualitative evidence that might speak to the credibility of this speculation.\\nIn the second viewing of the recordings, the coders noticed that the first outlier infant\\nproduced the majority of the canonical syllables during a single scene while walking\\noutside. He repeatedly produced a [da] syllable during this brief episode, but did not direct\\nhis vocalizations to the caregiver. The sense that a prelinguistic vocal stereotypy may have\\nbeen operating was enhanced by the fact that the same syllable was repeated throughout.\\nThe stereotypy of canonical babbling in this infant was reported by the coders as\\nconstituting the only evidence either had noticed as specifically suggesting the possibility of\\nASD while they were coding, and thus, this was the single case where the intended blinding\\nof the coders to diagnostic group seems to have been foiled. The coders did not observe any\\nother stereotypic behaviors vocal or otherwise in these samples. The second infant engaged\\nin high canonical babble production while roughhousing with his father, but to our clinical\\neyes, that behavior did not seem particularly unusual. Further research on the possibility that\\nbabbling can be a focus of motor stereotypy in ASD seems in order. It may be worthy ofPatten et al. Page 16\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 15}),\n",
       "  Document(page_content='note that the two outliers’ CARS scores (25 and 31) fell within the range of the scores for\\nthe ASD group (23–50).\\nIn addition to the findings suggesting possible clinically useful markers for ASD, the present\\nresults provide a new scientific view on the robustness of canonical babbling. There has\\nbeen no prior empirical indication that canonical babbling onset is delayed in ASD, nor that\\nvolubility is low in infants later diagnosed with ASD. Our results thus suggest that the\\ndevelopment of vocalization in infancy is affected by whatever the fundamental disorders of\\nASD may be. Assuming ASD to be a social  disorder, it is not obvious that babbling would\\nnecessarily be disturbed in the disorder because the extent to which babbling is a social (as\\nopposed to an endogenously generated) phenomenon is itself an empirical question. Our\\nresults can then be thought to provide a new empirical perspective on the possible social\\nnature of babbling. The results also suggest that the vocal differentiation of the two groups is\\nrobust, given the relative clarity of the results indicating low canonical babbling and\\nvolubility in the infants in the ASD group, even though we had samples of low recording\\nquality and very limited duration. The results seem especially significant in the context of a\\nbroad body of research cited above on the robustness of canonical babbling as a foundation\\nfor language and on the robust resistance  of canonical babbling to delay as seen in prior\\nstudies cited in our paper—no delay has been found in cases of prematurity, low SES, or\\nmultilingual exposure.\\nFuture Directions and Limitations\\nThis study provides a proof of concept regarding the notion of atypical emergence of the\\ncanonical babbling stage in the developing infant who will later be diagnosed with ASD and\\nthe possibility that tracking canonical babbling in infancy may add to our repertoire of\\nmarkers for ASD prior to one year of age. Future research to address some of the limitations\\nof the current study and advance our understanding of the development of canonical\\nbabbling among infants with ASD is warranted by the findings of the current study. One\\nlimitation in the current study was the lack of a comparison group of infants with later\\ndiagnoses of non-ASD disabilities, which prevents us from definitively attributing the\\ndifferences found in this study to ASD rather than general impairments in cognition or\\ncommunication. Our working hypothesis to test in future studies will be that these\\ndifferences in canonical babbling onset and in volubility are specific to ASD.\\nAnother limitation was that our study used only short video segments from each time point,\\nwhich surely impacted our ability to precisely assess important aspects of vocalization,\\nbecause it has been shown that variability in obtained canonical babbling ratios increases as\\nthe length of samples decreases (Molemans, 2011; Molemans, Van den Berg, Van Severen,\\n& Gillis, 2011). The low canonical babbling ratios obtained for a few of the TD infants\\npresumably would not have occurred with larger sample sizes. In future studies we hope to\\nobtain longer samples, and if possible to more precisely identify canonical babbling onset\\nthrough longitudinal laboratory assessments paired with caregiver report of onset. But of\\ncourse to make this possible, prospective studies may be necessary, with several years of\\nfollow-up, presumably taking advantage of the opportunity presented by sibling studies.\\nSuch studies would also afford the opportunity to obtain much better recordings than arePatten et al. Page 17\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 16}),\n",
       "  Document(page_content='available in retrospective studies such as the present one. Indeed, sibling studies can now\\ncapitalize on all-day recording, yielding the opportunity to assess vocal development in ASD\\nwith much greater ecological validity and representativeness.\\nOnset of canonical babbling usually occurs between 5 and 9 months in TD infants. It\\nappears from the present data that onset may occur within a much wider range in ASD.\\nQuantification of onset in ASD may yield prognostic value regarding core communication\\nsymptoms. For example, if canonical stage onset is delayed beyond a certain threshold, the\\ninfant may be at especially high-risk for remaining nonverbal. Discovery of such a delay\\ncould allow specific interventions to be tailored based on prognosis earlier in development.\\nFuture research should also focus on caregivers and their roles in canonical stage\\ndevelopment and its identification. Prior work suggests that with TD infants, parents are\\nextremely accurate in their reports of the onset of canonical babbling (Oller et al., 2001). If\\ncaregivers of infants with ASD are similarly capable of identifying onset of canonical\\nbabbling, it may be possible to use canonical babbling onset as part of a parent-report\\nscreening tool for early identification. In addition, alterations in communication directed to\\ninfants by caregivers as canonical babbling emerges may help to elicit and maintain social-\\ncommunicative interaction, and subsequently impact language development.\\nOur findings on volubility represent another potential avenue for understanding early social-\\ncommunication development processes in ASD. Perhaps the most intriguing aspect of this\\npossibility is suggested by the proposal that there may be a feedback loop involving low\\ncanonical syllable production in ASD followed by low parental rate of vocalization to\\ninfants, aggravating the low volubility and low rate of canonical syllables in ASD\\n(Warlaumont et al., 2010). We anticipate rapid growth of studies tracking this possibility,\\nespecially since there is a rapidly growing possibility of conducting some aspects of such\\nanalysis based on automated classification of vocalizations in all-day recordings as indicated\\nby the growth of LENA system studies.\\nClinical Implications\\nOur findings suggest that canonical babbling should be considered an important milestone in\\ninfancy that may be delayed in infants who are later diagnosed with ASD. If infants\\ndemonstrate delays in canonical babbling, a developmental assessment that includes\\nevaluation of early warning signs for ASD should be administered. Although volubility\\nappears less promising as a marker for ASD, it may be useful in combination with other\\nitems in the context of early identification screening tools. For infants demonstrating either\\nlow canonical babbling ratios or low volubility, interventions to draw infants’ attention to\\nsocial-communicative stimuli in that context of dyadic interactions may help stimulate\\ngrowth of vocal communication.\\nAcknowledgments\\nThis research was made possible through a grant from the National Institute for Child Health and Human\\nDevelopment (R01-HD42168) and a grant from Cure Autism Now Foundation (Sensory-Motor and Social-\\nCommunicative Symptoms of Autism in Infancy). We thank the families whose participation made this study\\npossible and the staff who collected and processed data for this project.Patten et al. Page 18\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 17}),\n",
       "  Document(page_content='References\\nAcevedo MC. The role of acculturation in explaining ethnic differences in the prenatal health-risk\\nbehaviors, mental health, and parenting beliefs of Mexican American and European American at-\\nrisk women. Child Abuse & Neglect. 2000; 24:111–127. [PubMed: 10660014]\\nAmerican Psychiatric Association. Diagnostic and statistical manual of mental disorders. 4.\\nWashington, DC: Author; 2000. text rev\\nBaranek GT. Autism during infancy: A retrospective video analysis of sensory-motor and social\\nbehaviors at 9–12 months of age. Journal of Autism and Developmental Disorders. 1999; 29:213–\\n224. [PubMed: 10425584]\\nBaranek GT, David FJ, Poe MD, Stone WL, Watson LR. Sensory Experiences Questionnaire:\\nDiscriminating sensory features in young children with autism, developmental delays, and typical\\ndevelopment. Journal of Child Psychology and Psychiatry. 2006; 47(6):591–601. [PubMed:\\n16712636]\\nBaranek GT, Watson LR, Boyd BA, Poe MD, David FJ, McGuire L. Hyporesponsiveness to social and\\nnonsocial sensory stimuli in children with autism, children with developmental delays, and typically\\ndeveloping children. Development and Psychopathology. 2013; 25(2013):307–320. [PubMed:\\n23627946]\\nBleile KM, Stark RE, McGowan JS. Speech development in a child after decannulation: Further\\nevidence that babbling facilitates later speech development. Clinical Linguistics and Phonetics.\\n1993; 7:319–337.\\nCenter for Disease Control and Prevention. Prevalence of autism spectrum disorders –Autism and\\ndevelopmental disabilities monitoring network, 14 Sites, United States, 2008. Morbidity and\\nMortality Weekly Report Surveillance Summaries. 2012; 61:1–19. Retrieved from http://\\nwww.cdc.gov/mmwr/preview/mmwrhtml/ss6103a1.htm .\\nClarke E, Reichard U, Zuberbühler K. The anti-predator behaviour of wild white-handed gibbons\\n(Hylobates lar). Behavioral Ecology and Sociobiology. 2012; 66:85–96.10.1007/\\ns00265-011-1256-5\\nChapman K, Hardin-Jones M, Schulte J, Halter K. Vocal development of 9 month-old babies with cleft\\npalate. Journal of Speech, Language and Hearing Research. 2001; (44):1268–1283.\\nChawarska K, Klin A, Paul R, Macari Volkmar F. A prospective study of toddlers with ASD: Short-\\nterm diagnostic and cognitive outcomes. Journal of Autism and Developmental Disorders. 2009;\\n50(10):1235–1245.\\nClement, CJ. PhD Dissertation. Netherlands Graduate School of Linguistics; Amsterdam: 2004.\\nDevelopment of vocalizations in deaf and normally hearing infants.\\nCobo-Lewis AB, Oller DK, Lynch MP, Levine SL. Relations of motor and vocal milestones in\\ntypically developing infants and infants with Down syndrome. American Journal on Mental\\nRetardation. 1996; 100:456–467. [PubMed: 8852298]\\nCohen, J. Statistical Power Analysis for the Behavioral Sciences. Hillsdale,, NJ: Erlbaum Associates;\\n1988.\\nDavis BL, Morrison HM, von Hapsburg D, Warner AD. Early vocal patterns in infants with varied\\nhearing levels. Volta Review. 2005; 105(1):7–27.\\nDelgado CEF, Messinger DS, Yale ME. Infant responses to direction of parental gaze: A comparison\\nof two still-face conditions. Infant Behavior and Development. 2002; 25(3):311–318.\\nEilers RE, Oller DK, Levine S, Basinger D, Lynch MP, Urbano R. The role of prematurity and\\nsocioeconomic status in the onset of canonical babbling in infants. Infant Behavior and\\nDevelopment. 1993; 16:297–315.\\nEilers RE, Oller DK. Infant vocalizations and the early diagnosis of severe hearing impairment. The\\nJournal of Pediatrics. 1994; 124(2):199–203. [PubMed: 8301422]\\nGoldstein MH, Schwade JA, Bornstein MH. The value of vocalizing: Five-month-old infants associate\\ntheir own noncry vocalizations with responses from adults. Child Development. 2009; 80:636–\\n644. [PubMed: 19489893]Patten et al. Page 19\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 18}),\n",
       "  Document(page_content='Goldstein MH, King AP, West MJ. Social interaction shapes babbling: Testing parallels between\\nbirdsong and speech. Proceedings of the National Academy of Sciences. 2003; 100(13):8030–\\n8035.\\nGoldstein MH, Schwade JA. Social feedback to infants’ babbling facilitates rapid phonological\\nlearning. Psychological Science. 2008; 19:515–522. [PubMed: 18466414]\\nGoldstein MH, West MJ. Consistent responses of human mothers to prelinguistic infants: The effect of\\nprelinguistic repertoire size. Journal of Comparative Psychology. 1999; 113(1):52–58. [PubMed:\\n10098268]\\nHart, B.; Risley, TR. Meaningful differences in the everyday experience of young American children.\\nBaltimore: Paul H. Brookes; 1995.\\nHulit, LM.; Howard, MR. Born to talk: An introduction to speech and language development. Boston:\\nAllyn and Bacon; 2002.\\nKent R, Osberger MJ, Netsell R, Hustedde CG. Phonetic development identical twins differing in\\nauditory function. Journal of Speech and Hearing Disorders. 1987; 52:64–75. [PubMed: 3807347]\\nKoopmans-van Beinum, FJ.; Clement, CJ.; van den Dikkenberg-Pot, I. Influence of lack of auditory\\nspeech perception on sound productions of deaf infants. Berne, Switzerland: International Society\\nfor the Study of Behavioral Development; 1998.\\nKoopmans-van Beinum, FJ.; van der Stelt, JM. Early stages in the development of speech movements.\\nIn: Lindblom, B.; Zetterstrom, R., editors. Precursors of early speech. New York: Stockton Press;\\n1986. p. 37-50.\\nLewedag, VL. Doctoral Dissertation. University of Miami; Coral Gables, FL: 1995. Patterns of onset\\nof canonical babbling among typically developing infants.\\nLocke JL, Pearson D. Linguistic significance of babbling: Evidence from a tracheostomized infant.\\nJournal of Child Language. 1990; 17:1–16. [PubMed: 2312634]\\nLord C. Follow-up of two-year-olds referred for possible autism. Journal of Child Psychology and\\nPsychiatry, and Allied Disciplines. 1995; 36(8):1365–1382.\\nLord, C.; Rutter, M.; DiLavore, P.; Risi, S. Autism Diagnostic Observation Schedule (ADOS). Los\\nAngeles, CA: Western Psychological Services; 1999.\\nLynch MP, Oller DK, Steffens ML, Levine SL, Basinger D, Umbel V. The onset of speech-like\\nvocalizations in infants with Down syndrome. American Journal of Mental Retardation. 1995;\\n100(1):68–86. [PubMed: 7546639]\\nLynch MP, Oller DK, Steffens ML, Buder EH. Phrasing in prelinguistic vocalizations. Developmental\\nPsychobiology. 1995; 28:3–23. [PubMed: 7895922]\\nManjiviona J, Prior M. Comparison of Asperger syndrome and high-functioning autistic children on a\\ntest of motor impairment. Journal of Autism and Developmental Disorders. 1995; 25:23–29.\\n[PubMed: 7608032]\\nMasataka N. Why early linguistic milestones are delayed in children with Williams syndrome: Late\\nonset of hand banging as a possible rate-limiting constraint on the emergence of canonical\\nbabbling. Developmental Science. 2001; 4:158–164.\\nMatson JL, Fodstad JC, Dempsey T. What symptoms predict the diagnosis of autism or PDD-NOS in\\ninfants and toddlers with developmental delays using the Baby and Infant Screen for Autism\\nTraits. Developmental Neurorehabilitation. 2009; 12(6):381–388. [PubMed: 20205546]\\nMatson JL, Mahan S, Hess JA, Fodstad JC, Neal D. Convergent validity of the Autism Spectrum\\nDisorder-Diagnostic for Children (ASD-DC) and Childhood Autism Rating Scales (CARS).\\nResearch in Autism Spectrum Disorders. 2010; 4(4):633–638.\\nMiller, LJ.; Reisman, JE.; McIntosh, DN.; Simon, J. An ecological model of sensory modulation:\\nPerformance in children with fragile X syndrome, autistic disorder, attention–deficit/hyperactivity\\ndisorder, and sensory modulation dysfunction. In: Smith-Roley, S.; Blanche, EI.; Schaaf, RC.,\\neditors. Understanding the Nature of Sensory Integration with Diverse Populations. San Antonio,\\nTX: Therapy Skill Builders; 2001. p. 57-88.\\nMolemans, I. PhD. University of Antwerp; Antwerp, Belgium: 2011. Sounds like babbling: A\\nLongitudinal investigation of aspects of the prelexical speech repertoire in young children\\nacquiriing Dutch: Normally hearing children and hearing impaired children with a cochlear\\nimplant.Patten et al. Page 20\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 19}),\n",
       "  Document(page_content='Molemans I, Van den Berg R, Van Severen L, Gillis S. How to measure the onset of babbling reliably.\\nJournal of Child Language. 2011; 39:1–30. [PubMed: 21418730]\\nMullen, EM. Mullen Scales of Early Learning: AGS Edition. Circle Pines, MN: American Guidance\\nService; 1995.\\nNathani S, Oller DK, Neal AR. On the robustness of vocal development: An examination of infants\\nwith moderate-to-severe hearing loss and additional risk factors. Journal of Speech, Language, and\\nHearing Research. 2007; 50(6):1425–1444.\\nObenchain P, Menn L, Yoshinaga-Itano C. Can speech development at 36 months in children with\\nhearing loss be predicted from information available in the second year of life? Volta Review.\\n1998; 100:149–180.\\nOller, DK. The emergence of the sounds of speech in infancy. In: Yeni-Komshian, G.; Kavanagh, J.;\\nFerguson, C., editors. Child phonology, Vol 1: Production. New York: Academic Press; 1980. p.\\n93-112.\\nOller, DK. The Emergence of the Speech Capacity. Mahwah, NJ: Lawrence Erlbaum Associates;\\n2000.\\nOller DK, Eilers RE. Similarities of babbling in Spanish- and English-learning babies. Journal of Child\\nLanguage. 1982; 9:565–578. [PubMed: 7174757]\\nOller DK, Eilers RE. The role of audition in infant babbling. Child Development. 1988; 59:441–449.\\n[PubMed: 3359864]\\nOller DK, Eilers RE, Steffens ML, Lynch MP, Urbano R. Speech-like vocalizations in infancy: an\\nevaluation of potential risk factors. Journal of Child Language. 1994; 21:33–58. [PubMed:\\n8006094]\\nOller DK, Eilers RE, Urbano R, Cobo-Lewis AB. Development of precursors to speech in infants\\nexposed to two languages. Journal of Child Language. 1997; 27:407–425. [PubMed: 9308425]\\nOller DK, Eilers RE, Basinger D. Intuitive identification of infant vocal sounds by parents.\\nDevelopmental Science. 2001; 4:49–60.\\nOller DK, Eilers RE, Basinger D, Steffens ML, Urbano R. Extreme poverty and the development of\\nprecursors to the speech capacity. First Lang. 1995; 15:167–188.\\nOller DK, Eilers RE, Neal AR, Cobo-Lewis AB. Late onset canonical babbling: a possible early\\nmarker of abnormal development. American Journal on Mental Retardation. 1998; 103:249–265.\\n[PubMed: 9833656]\\nOller DK, Eilers RE, Neal AR, Schwartz HK. Precursors to speech in infancy: the prediction of speech\\nand language disorders. Journal of Communication Disorders. 1999; 32:223–246. [PubMed:\\n10466095]\\nOller DK, Niyogi P, Gray S, Richards JA, Gilkerson J, Xu D, Warren SF. Automated Vocal Analysis\\nof Naturalistic Recordings from Children with Autism, Language Delay and Typical\\nDevelopment. Proceedings of the National Academy of Sciences. 2010; 107:13354–13359.\\nOsterling JA, Dawson G, Munson JA. Early recognition of 1-year-old infants with autism spectrum\\ndisorder versus mental retardation. Development and Psychopathology. 2002; 14(2):239–251.\\n[PubMed: 12030690]\\nOzonoff S, Iosif A, Baguio F, Cook IC, Hill MM, Hutman T, Rogers SJ, Rozga A, Sangha S, Sigman\\nM, Steinfeld MB, Young GS. A prospective study of the emergence of early behavioral signs of\\nautism. Journal of the American Academy of Child & Adolescent Psychiatry. 2010; 49(3):256–\\n266. [PubMed: 20410715]\\nPage J, Boucher J. Motor impairments in children with autistic disorder. Child Language Teaching and\\nTherapy. 1998; 14(3):233.\\nPapoušek, M. Vom ersten Schrei zum ersten Wort: Anfänge der Sprachentwickelung in der\\nvorsprachlichen Kommunikation. Bern: Verlag Hans Huber; 1994.\\nPaul R, Augustyn A, Klin A, Volkmar FR. Perception and production of prosody by speakers with\\nASD spectrum disorders. Journal of Autsim and Developmental Disorders. 2005; 35:205–220.\\nPaul R, Fuerst Y, Ramsay G, Chawarska K, Klin A. Out of the mouths of babes: Vocal production in\\ninfant siblings of children with ASD. Journal of Child Psychology and Psychiatry. 2011; 52(5):\\n588–598. [PubMed: 21039489]Patten et al. Page 21\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 20}),\n",
       "  Document(page_content='Peppe S, McCann J, Gibbon F, O’Hara A, Rutherford M. Receptive and expressive prosodic ability in\\nchildren with high-functioning ASD. Journal of Speech, Language, and Hearing Research. 2007;\\n50:1015–1028.\\nRamsdell HL, Oller DK, Buder EH, Ethington CA, Chorna L. Identification of prelinguistic\\nphonological categories. Journal of Speech Language and Hearing Research. 2012; 55:1626–1629.\\nRobins DI, Fein D, Barton MI, Green JA. The modified checklist for autism in toddlers: An initial\\nstudy investigating the early detection of autism and pervasive developmental disorders. Journal of\\nAutism and Developmental Disorders. 2001; 31:131–144. [PubMed: 11450812]\\nRogers SJ, Ozonoff S. Annotation: What do we know about sensory dyfunction in autism? A critical\\nreview of the empirical evidence. Journal of Child Psychology and Psychiatry. 2005; 46(12):1255–\\n1268. [PubMed: 16313426]\\nRoss GS. Language functioning and speech development of six children receiving tracheostomy in\\ninfancy. Journal of Communication Disorders. 1983; 15:95–111. [PubMed: 7096617]\\nRutter, M.; Le Couteur, A.; Lord, C. Autism Diagnostic Interview-Revised. Los Angeles, CA: Western\\nPsychological Services; 2003.\\nSchopler, E.; Reichler, RJ.; Rochen Renner, B. The Childhood Autism Rating Scale. Lost Angeles,\\nCA: Western Psychological Services; 1992.\\nSheinkopf SJ, Mundy P, Oller DK, Steffens M. Vocal atypicalities of preverbal autistic children.\\nJournal of Autism and Developmental Disorders. 2000; 30:345–353. [PubMed: 11039860]\\nSimon BM, Fowler SM, Handler SD. Communication development in young children with long-term\\ntracheostomies: Preliminary report. International Journal of Otorhinolaryngology. 1983; 6:37–50.\\nSnow, CE. Issues in the study of input: fine-tuning universality, individual and devlopmental\\ndifferences and necessary causes. In: MacWhinney, B.; Fletcher, P., editors. NETwerken:\\nBijdragen van het vijfde NET symposium: Antwerp Papers in Linguistics. Vol. 74. Antwerp:\\nUniversity of Antwerp; 1995. p. 5-17.\\nSparrow, SS.; Balla, DA.; Cicchetti, DV. Vineland Adaptive Behavior Scales. Circle Pines, MN:\\nAmerican Guidance Service; 1984.\\nStark, RE. Stages of speech development in the first year of life. In: Yeni-Komshian, G.; Kavanagh, J.;\\nFerguson, C., editors. Child Phonology. Vol. 1. New York: Academic Press; 1980. p. 73-90.\\nStark, RE.; Ansel, BM.; Bond, J. Are prelinguistic abilities predictive of learning disability? A follow-\\nup study. In: Masland, RL.; Masland, M., editors. Preschool Prevention of Reading Failure.\\nParkton, MD: York Press; 1988.\\nStoel-Gammon C. Prespeech and early speech development of two late talkers. First Language. 1989;\\n9:207–224.\\nStoel-Gammon C, Otomo K. Babbling development of hearing impaired and normally hearing\\nsubjects. Journal of Speech and Hearing Disorders. 1986; 51:33–41. [PubMed: 3945058]\\nStoel-Gammon C. Relationships between lexical and phonological development in young children.\\nJournal of Child Language. 2011; 38(1):1–34. [PubMed: 20950495]\\nTeitelbaum P, Teitelbaum O, Nye J, Fryman J, Maurer RG. Movement analysis in infancy may be\\nuseful for early diagnosis of autism. Proceedings of the National Academy of Sciences of the\\nUnited States of America. 1998; 95(23):13982–13987. [PubMed: 9811912]\\nTronick, EZ. Social interchange in infancy. Baltimore: University Park Press; 1982.\\nVan den Dikkenberg-Pot I, Koopmans-van Beinum F, Clement C. Influence of lack of auditory speech\\nperception of sound productions of deaf infants. Proceedings of the Institute of Phonetic Sciences,\\nUniversity of Amsterdam. 1998; 22:47–60.\\nVihman, MM. Phonological Development: The Origins of Language in the Child. Cambridge, MA:\\nBlackwell Publishers; 1996.\\nVolkmar FR, Chawarska K. Autism in infants: An update. World Psychiatry: Official Journal of the\\nWorld Psychiatric Association. 2008; 7(1):19–21.\\nWatson LR, Patten E, Baranek GT, Boyd BA, Freuler A, Lorenzi J. Differential associations between\\nsensory response patterns and language, social, and communication measures in children with\\nautism or other developmental disabilities. Journal of Speech, Language, and Hearing Research.\\n2011; 54(6):1562–1576.Patten et al. Page 22\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 21}),\n",
       "  Document(page_content='Watson LR, Crais ER, Baranek GT, Dykstra JR, Wilson KP. Communicative Gesture Use in Infants\\nwith and without Autism: A Retrospective Home Video Study. American Journal of Speech-\\nLanguage Pathology. 2013; 22:25–39. [PubMed: 22846878]\\nWarlaumont, AS.; Oller, DK.; Dale, R.; Richards, JA.; Gilkerson, J.; Xu, D. Vocal Interaction\\nDynamics of Children With and Without Autism. Paper presented at the Proceedings of the 32nd\\nAnnual Conference of the Cognitive Science Society; Austin, TX. 2010.\\nWarren SF, Gilkerson J, Richards JA, Oller DK. What Automated Vocal Analysis Reveals About the\\nLanguage Learning Environment of Young Children with Autism. Journal of Autism and\\nDevelopmental Disorders. 2010; 40:555–569. [PubMed: 19936907]\\nWeismer SE, Lord C, Esler A. Early language patterns of toddlers on the autism spectrum compared to\\ntoddlers with developmental delay. Journal of Autism and Developmental Disorders. 2010;\\n40(10):1259–1273. [PubMed: 20195735]\\nWerner E, Dawson G, Osterling J, Dinno N. Brief report: Recognition of autism spectrum disorder\\nbefore one year of age: A retrospective study based on home videotapes. Journal of Autism and\\nDevelopmental Disorders. 2000; 30:157–162. [PubMed: 10832780]\\nWetherby AM, Woods J, Allen L, Cleary J, Dickinson H, Lord C. Early indicators of ASD spectrum\\ndisorders in the second year of life. Journal of ASD and Developmental Disorders. 2004; 34:473–\\n493.\\nYale ME, Messinger DS, Cobo-Lewis AB, Oller DK, Eilers RE. An event-based analysis of the\\ncoordination of early infant vocalizations and facial actions. Developmental Psychology. 1999;\\n35(2):505–513. [PubMed: 10082021]\\nZwaigenbaum L, Bryson S, Rogers T, Roberts W, Brian J, Szatmari P. Behavioral manifestations of\\nautism in the first year of life. International Journal of Developmental Neuroscience. 2005; 23(2–\\n3):143–152. [PubMed: 15749241]Patten et al. Page 23\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 22}),\n",
       "  Document(page_content='Figure 1.\\nCanonical babbling ratios by participant at 9–12 monthsPatten et al. Page 24\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 23}),\n",
       "  Document(page_content='Figure 2.\\nCanonical babbling ratios by participant at 15–18 monthsPatten et al. Page 25\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 24}),\n",
       "  Document(page_content='Figure 3.\\nSyllable volubility by participant at 9–12 months.Patten et al. Page 26\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 25}),\n",
       "  Document(page_content='Figure 4.\\nSyllable volubility by participant at 15–18 months.Patten et al. Page 27\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 26}),\n",
       "  Document(page_content='Figure 5.\\nCanonical babbling ratios by age and diagnosisPatten et al. Page 28\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 27}),\n",
       "  Document(page_content='Figure 6.\\nVolubility by age and diagnosisPatten et al. Page 29\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 28}),\n",
       "  Document(page_content='NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 30\\nTable 1\\nParticipant Demographics\\nASD; n=23 TD; n=14\\nAge at 9–12months; mean (SD) 10.89 (1.39) 10.63 (.53)\\nAge at 15–18 months; mean (SD) 16.33 (.83) 16.28 (.70)\\nSex 19 males, 4 females 11 males, 3 females\\nRace 23 White, 1 Black 13 White, 1 Asian\\nMaternal education 1 5.485.82\\nChildhood Autism Rating Scale; mean (SD) 34.17 (1.52)16.15 (.39)3\\n1Maternal education: 1=6th grade or lower; 2=7th to 9th grade; 3=partial high school; 4=high school graduate/GED; 5=associate of arts/associate\\nof science or technical training or partial college training; 6=bachelor of arts/science; 7=master of arts/science or doctorate or other professional\\ndegree completed\\n2missing information for two participants\\n3missing information for four participants\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 29}),\n",
       "  Document(page_content='NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 31\\nTable 2\\nContent Variables for Videos, 9–12 months\\nASD; mean (SD) TD; mean (SD)\\nNumber of people present 3.22 (1.53) 3.28 (1.24)\\nAmount of physical restrictiona 1.58 (.35) 1.51 (.32)\\nAmount of social intrusiona 2.02 (.38) 2.04 (.32)\\nTotal number of different event types 5.32 (1.05) 5.07 (1.02)\\naRated by coders on a 1 to 3 scale\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 30}),\n",
       "  Document(page_content='NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 32\\nTable 3\\nContent Variables for Videos, 15–18 months\\nASD; mean (SD) TD; mean (SD)\\nNumber of people present 2.84 (1.20) 2.82 (1.24)\\nAmount of physical restrictiona 1.37 (.29) 1.28 (.33)\\nAmount of social intrusion a 2.06 (.40) 1.95 (.34)\\nTotal number of different event types 5.34 (1.17) 5.23 (1.11)\\naRated by coders on a 1 to 3 scale\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 31}),\n",
       "  Document(page_content='NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 33\\nTable 4\\nPercentage of each activity type, 9–12 month videos\\nASD; n=23 TD; n=14\\nMealtime 10% 11%\\nActive 53.9% 60.6%\\nBathtime 4.5% 5.5%\\nOther 2.5% 4.1%\\nspecial activity 20.3% 16.5%\\npassive activity 8.7% 3.4%\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 32}),\n",
       "  Document(page_content='NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 34\\nTable 5\\nPercentage of each activity type, 15–18 month videos\\nASD; n=23 TD; n=14\\nMealtime 7.5% 2.6%\\nActive 64% 72.8%\\nBathtime 2.5% 1.8%\\nOther 8.3% 11.4%\\nSpecial activity 12.9% 4.4%\\nPassive activity 4.6% 16.6%\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 33}),\n",
       "  Document(page_content='NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 35\\nTable 6\\nIntercorrelations Between Canonical Babbling Ratios and Volubility\\n12 3 4\\n1. Canonical Babbling 9–12 mos __.352*.528**.354*\\n2. Volubility 9–12 mos __ 0.21 0.14\\n3. Canonical Babbling 15–18 mos __.510**\\n4. Volubility 15–18 mos __\\n*p < .05,\\n**p < .01\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.', metadata={'source': 'papers/Patten_Audio.pdf', 'page': 34})],\n",
       " [Document(page_content='ORIGINAL RESEARCH\\npublished: 05 June 2020\\ndoi: 10.3389/fped.2020.00290\\nFrontiers in Pediatrics | www.frontiersin.org 1 June 2020 | Volume 8 | Article 290Editedby:\\nSaraCalderoni,\\nFondazioneStellaMaris(IRCCS),Italy\\nReviewedby:\\nWhitneyI.Mattson,\\nNationwideChildren’sHospital,\\nUnitedStates\\nLori-AnnRosalindSacrey,\\nUniversityofAlberta,Canada\\n*Correspondence:\\nXiaoyanKe\\nkexiaoyan@njmu.edu.cn\\nSpecialtysection:\\nThisarticlewassubmittedto\\nChildandAdolescentPsychiatry,\\nasectionofthejournal\\nFrontiersinPediatrics\\nReceived: 05January2020\\nAccepted: 07May2020\\nPublished: 05June2020\\nCitation:\\nQiuN,TangC,ZhaiM,HuangW,\\nWengJ,LiC,XiaoX,FuJ,ZhangL,\\nXiaoT,FangHandKeX(2020)\\nApplicationoftheStill-FaceParadigm\\ninEarlyScreeningforHigh-Risk\\nAutismSpectrumDisorderinInfants\\nandToddlers.Front.Pediatr.8:290.\\ndoi:10.3389/fped.2020.00290Application of the Still-Face\\nParadigm in Early Screening for\\nHigh-Risk Autism Spectrum Disorder\\nin Infants and Toddlers\\nNanaQiu1,ChuangaoTang2,MengyaoZhai1,WanqingHuang3,JiaoWeng1,ChunyanLi1,\\nXiangXiao1,JunliFu1,LiliZhang4,TingXiao1,HuiFang1andXiaoyanKe1*\\n1NanjingBrainHospitalAfﬁliatedtoNanjingMedicalUnivers ity,Nanjing,China,2SchoolofBiologicalScience&Medical\\nEngineering,SoutheastUniversity,Nanjing,China,3CollegeofTelecommunications&InformationEngineering,N anjing\\nUniversityofPostsandTelecommunications,Nanjing,Chin a,4WuxiChildren’sHospital,Wuxi,China\\nBackground: Although autism spectrum disorder (ASD) can currently be di agnosed at\\nthe age of 2 years, age at ASD diagnosis is still 40 months or ev en later. In order to early\\nscreeningforASDwithmoreobjectivemethod,behavioralvi deoswereusedinanumber\\nof studies in recent years.\\nMethod: The still-face paradigm (SFP) was adopted to measure the fre quency and\\nduration of non-social smiling, protest behavior, eye cont act, social smiling, and active\\nsocial engagement in high-risk ASD group (HR) and typical de velopment group (TD)\\n(HR:n=45; TD:n=43). The HR group was follow-up until they were 2 years old\\nto conﬁrm ﬁnal diagnosis. Machine learning methods were use d to establish models for\\nearly screening of ASD.\\nResults: During the face-to-face interaction (FF) episode of the SFP , there were\\nstatistically signiﬁcant differences in the duration and f requency of eye contact, social\\nsmiling,andactivesocialengagementbetweenthetwogroup s.Duringthestill-face(SF)\\nepisode, there were statistically signiﬁcant differences in the duration and frequency of\\neye contact and active social engagement between the two gro ups. The 45 children in\\ntheHRgroupwerereclassiﬁedintotwogroupsafterfollow-u p:ﬁvechildrenintheN-ASD\\ngroup who were not meet the criterion of ASD and 40 children in the ASD group. The\\nresults showed that the accuracy of Support Vector Machine ( SVM) classiﬁcation was\\n83.35% for the SF episode.\\nConclusion: The use of the social behavior indicator of the SFP for a child with HR\\nbefore2yearsoldcaneffectivelypredicttheclinicaldiag nosis of thechildattheageof2\\nyears.ThescreeningmodelconstructedusingSVMbasedonth eSFepisodeoftheSFP\\nwasthebest.ThisalsoprovesthattheSFPhascertainvaluei nhigh-riskautismspectrum\\ndisorderscreening.Inaddition,becauseofitsconvenient ,itcanprovideaself-screening\\nmode for use at home.\\nTrial registration: Chinese Clinical Trial Registry, ChiCTR-OPC-17011995.\\nKeywords: high-risk autism spectrum disorder, the Still-F ace Paradigm, social behavior, machine learning, model\\nfor early screening', metadata={'source': 'papers/Qiu.pdf', 'page': 0}),\n",
       "  Document(page_content='Qiu et al. Early Screening for High-Risk ASD\\nINTRODUCTION\\nAutism spectrum disorder (ASD) is a serious\\nneurodevelopmental disorder that starts in early childhood\\nandischaracterizedbysocialcommunicationbarriers,res tricted\\ninterests, repetitive stereotyped behaviors, and abnormali ties in\\nperception ( 1). In recent years, epidemiological survey data on\\nthe incidence of ASD showed that the prevalence rate increase d\\nfrom 0.07 to 1.8% in China ( 2). A large number of studies on\\nASD have shown that early intervention helps improve patient\\nprognosis ( 3). However, age at ASD diagnosis is still 40 months\\nor even later ( 4). Therefore, early detection, early diagnosis and\\neﬀective intervention are essential to achieve a better progn osis.\\nUnderstanding the early childhood behaviors of ASD will help\\nfacilitate early intervention in infants and toddlers who a re\\nsuspectedofhavingASDandimprovetheirprognosis,whichhas\\nveryimportantsocialandeconomicimplications.\\nThe early social interaction between adults and children is\\nthe basis of more complex social cognition. For children with\\nASD, the lack of social abilities is especially prominent in th eir\\nearly life. According to the 2013 version of the Diagnostic and\\nStatistical Manual of Mental Disorders , ﬁfth edition ( DSM-5),\\nASD symptoms are usually present in young children at the\\nage of 1–2 years. Occasionally, initial symptoms often invol ve\\ndelayed language development, accompanied by a lack of social\\ninterest or unusual social interactions, quirky play modes a nd\\nunusual communication patterns ( 1). A study by Barbaro and\\nDissanayakeonfamilyvideosandparentalreportsrevealedea rly\\nwarning signs in social interactions of children with ASD ag ed\\n12 to 24 months old ( 5), which included lack of joint attention,\\nlack of eye contact, lack of social smiling, lack of social in terest\\nand sharing, no response to calling name, lack of gestures, an d\\ncommunication impairments ( 5–9). This study would focus on\\nthe diﬀerences of early social behavior between HR group and\\nTD group. Previous studies on early behavioral abnormalitie s in\\nASD were mostly in the form of retrospective interviews with\\nparents or scale-based evaluations, which are highly subjec tive\\nand unfavorable for widespread promotion. In recent years, an\\nincreasing number of studies have adopted objective methods\\ninvolvingvideoforusingbehavioralcoding( 10–12).\\nTronick et al. proposed the still-face paradigm (SFP) to test\\ninfants’ emotion regulation ability and social expectation s in\\nsocial interaction ( 13). Early maternal-infant interaction is the\\ncore basis of infants’ social emotion, emotion regulation, and\\nsocial and communication development ( 14). The maternal-\\ninfant relationship is the ﬁrst relationship developed in ea rly\\nchildhood. Flexible and frequent interaction is the basis f or\\nearlychildhoodemotionalorganization,attentionswitch ingand\\nthe emergence of social skills ( 15). In the early maternal-\\ninfant interaction, non-verbal communication is dominant . In\\naddition, in this process, young children learn the rules of\\nsocial participation and expressing forms of social expectatio ns,\\nwhich provide a social framework for future social interacti ons\\nand relationships ( 16). There were studies applied SFP in\\nemotional regulatory of ASD and they found most of children\\nwith ASD employed more simple regulatory behavior and\\nless complex strategies ( 17,18). Additional, Cassel et al. alsofound that there were diﬃculties for children with ASD to\\ndevelop socioemotional ability ( 10,19). In this study, the\\nSFP was used to measure social behavior of HR group and\\nTDgroup.\\nTherewerestudiesappliedmachinelearningmethodstobuild\\nmodel for early screening of ASD based on the characteristic\\nvalues of biological indicators, such as electro-encephalog ram\\n(20) and brain images ( 21), and they found the accuracy was\\nmore than 80%. And in order to improve the stability and\\nreliability of model for early screening of ASD, it is essenti al to\\ncombiningsocialbehavioralindicatorswithbiologicalin dicators\\nin the subsequent research. In this study, we would try to\\nbuild a model for early screening of ASD based on social\\nbehavioralindicators.\\nMETHODS\\nParticipants\\nForty-ﬁve infants and toddlers with high-risk autism spectrum\\ndisorder (HR) who sought treatment at the outpatient clinic of\\nChild Mental Health Research Center, Nanjing Brain Hospital\\nAﬃliated to Nanjing Medical University, from December 2017\\ntoDecember2018wereenrolledintheHRgroup,and43infants\\nand toddlers with typical development (TD) in the Nanjing area\\nwere recruited during the same period and enrolled into the\\ncontrolgroup(theTDgroup).\\nThe inclusion criteria for the HR group were as follows:\\n(1) children with positive results based on the Modiﬁed\\nChecklist for Autism in Toddlers (M-CHAT) ( 22); (2) pediatric\\npsychiatrist recognized that the children met the core crite ria\\nof ASD in DSM-5 but the months age was under 24 months;\\n(3) children aged 8 to 23 months old; (4) children whose\\nprimary caregiver was the mother; and (5) children whose\\nguardian(s) agreed to participate in this study. The exclusion\\ncriteria for the HR group were as follows: (1) children with\\ngenetic or metabolic diseases, such as Rett syndrome and\\nfragile X syndrome; (2) children with neurodevelopmental\\ndisorders other than ASD, such as language development\\ndisorders alone and intellectual disability; (3) children with\\na clear history of craniocerebral trauma; and (4) children\\nwith a history of nervous system diseases and serious\\nphysicalillnesses.\\nThe inclusion criteria for the TD group were as follows:\\n(1) children with TD whose sex matched that of the children\\nin the HR group; (2) children aged 8 to 23 months old; (3)\\nchildren whose primary caregiver was the mother; and (4)\\nchildren whose guardian(s) agreed to participate in this stud y.\\nThe exclusion criteria for the TD group were as follows: (1)\\nchildrenwhosuﬀeredfromvarioustypesofneurodevelopmental\\ndisorders and mental disorders; (2) children with a clear his tory\\nof craniocerebral trauma; and (3) children with a history of\\nnervoussystemdiseasesandseriousphysicalillness.\\nThisstudywasapprovedbytheMedicalEthicsCommitteeof\\nNanjing Brain Hospital Aﬃliated to Nanjing Medical University\\n(2017-KY089-01).Allsubjects’guardiansagreedtopartici patein\\nthisstudyandsignedinformedconsentforms.\\nFrontiers in Pediatrics | www.frontiersin.org 2 June 2020 | Volume 8 | Article 290', metadata={'source': 'papers/Qiu.pdf', 'page': 1}),\n",
       "  Document(page_content='Qiu et al. Early Screening for High-Risk ASD\\nMeasure and Procedure\\nGeneral Psychological Evaluation of the HR and TD\\nGroups\\nA self-guided general information questionnaire was used t o\\ncollect the general demographic data, past history, medicati on\\nhistoryandfamilyhistoryofthestudyparticipants.\\nThe Gesell Developmental Scale was used to assess the\\ndevelopmental levels of all subjects after they were enrolle d.\\nThe Gesell Developmental Scale was used to evaluate the\\ndevelopmental quotient (DQ) of children from 5 skill domains :\\nadaptive,grossmotor,ﬁnemotor,languageandpersonal-soci al.\\nThe severity of ASD symptoms in the HR group was\\nassessedusingtheCommunicationandSymbolicBehaviorSca les\\nDevelopmentalProﬁle(CSBS-DP),theChildhoodAutismRatin g\\nScale (CARS) and the Autism Behavior Checklist (ABC). The\\nCSBS-DP has 3 factor scores (social communication, languag e,\\nand symbolic behavior) and a total score. The lower the CSBS-\\nDP factor scores are, the more serious the ASD symptoms. The\\nCARS and the ABC only have a total score. The higher the total\\nscoresare,themoreseveretheASDsymptoms.\\nVideo of the Behaviors of the HR and TD Groups in\\nthe SFP\\nThe classic paradigm consists of three episodes: (1) the basel ine\\nepisode (face-to-face interaction, FF episode), during whic h the\\nmother and the child are required to have normal interaction s;\\n(2) the still-face (SF) episode, during which the mother’s fa ce is\\nrequiredtopresentaneutralexpressionwithoutanyresponset o\\nthe child’s action; and (3) the reunion episode, during which the\\nmother resumes normal interactions with the child. Current ly,\\ntheSFPcanarousechildren’sbehavioralchanges,e.g.,are duction\\nin eye gaze and positive facial emotion and an increase in\\nnegative emotion when transitioning from the baseline episo de\\nto the SF episode ( 13); this eﬀect has been recognized and\\ntermed the SF eﬀect. In relevant studies, researchers also fo und\\nthat the reason for the generation of the SF eﬀect in infants\\nand toddlers is the disappearance of social responses, such\\nas eye contact, when their mothers show a still face; the\\ndisappearanceofsocialsignalscausestheappearanceofnegati ve\\nemotions in infants and toddlers ( 23). The ﬁrst two episodes\\nare often used in research in a randomly presented order. Inthe past few decades, the basic settings of the SFP have been\\nused as a method to explore early childhood social behaviors\\n(24–26), which the reason why this study chosen the ﬁrst\\ntwoepisodes.\\nIn the present study, all participants and their mothers were\\nvideorecordedduringtheSFPinadesignatedobservationro om\\natthetimeofenrollment.Themotherwassittingoppositetothe\\nchild,interactedwiththechildfor2minunderﬁxedinstruc tions,\\nand then stopped the interaction and maintained a neutral\\nface for 1min. Figure1 shows the setup of the experimental\\nenvironmentandprocedure.\\nInfant and Caregiver Engagement Phases (ICEP) ( 27) and\\nNichols et al. ( 28) deﬁne the coding indicators as follows: (1)\\nprotest behavior - the infant shows facial expressions of ange r\\nand frowns; the infant is upset, crying, arching the body, tryi ng\\nto escape and expresses anger using gestures; (2) non-social\\nsmiling—the infant smiles not at the mother but toward other\\ndirections or at other objects; (3) eye contact—the infant l ooks\\ndirectly at the mother’s eyes or face instead of looking at th e\\ncameraortowardotherdirections;(4)socialsmiling—thein fant\\nlooks at the mother and takes the initiative to smile (initia ting\\na smile); after the mother smiles, the infant immediately\\nresponds with a smile (smiling at each other); and (5) active\\nsocial engagement—the infant displays happy facial expressions ,\\nincluding a clear smile, occasional cooing or active vocali zation,\\nlaughing, or babbling, and looks at the mother to initiate\\ninteractions(proactivelyinitiateactivesocialengagemen t),orthe\\ninfant has a positive response to the interaction initiated by the\\nmother(respondingtoactivesocialengagement).\\nThe Observer XT 12 behavioral observation and recording\\nanalysissystemwasusedforcoding.Videocodingwascomplete d\\nby 2 trained graduate students, and the duration and frequen cy\\nofthe5indicatorsduringtheFFandSFepisodeswerecalculate d.\\nThe duration was measured using seconds (s) as the unit, and\\nfrequencywasmeasuredusingthenumberoftimesastheunit. A\\ntotal of 18 videos ( ∼20%) were randomly selected to determine\\nintercoderconsistencyusingtheintraclasscorrelationco eﬃcient\\n(ICC). It was found that the 2 coders had high consistency: the\\nICCs for protest behavior, non-social smiling, eye contact, social\\nsmiling, and activesocial engagement were 0.76, 0.82,0.81 , 0.83,\\nand0.79,respectively.\\nFIGURE 1 | SFP for infants and toddlers with HR.\\nFrontiers in Pediatrics | www.frontiersin.org 3 June 2020 | Volume 8 | Article 290', metadata={'source': 'papers/Qiu.pdf', 'page': 2}),\n",
       "  Document(page_content='Qiu et al. Early Screening for High-Risk ASD\\nTABLE 1 | Comparison of the general conditions between the HR group an d the\\nTD group ( mean±SD).\\nHR group TD group t/χ2P-value\\n(n=45) ( n=43)\\nSex −0.06 0.08\\nMale 40 32\\nFemale 5 11\\nAge\\n(months)\\nDQ19.71±3.43 16.40 ±4.70 3.80 <0.01\\nAdaptive 80.29 ±17.62 92.98 ±7.89−4.34 <0.01\\nGross motor 92.02 ±17.60 92.77 ±8.46−0.25 0.80\\nFine motor 86.64 ±19.03 93.70 ±8.29−2.24 0.03\\nLanguage 60.84 ±21.27 86.51 ±8.353−7.39 <0.01\\nPersonal-social 78.80 ±17.19 92.28 ±7.18−4.76 <0.01\\nHR, high-risk autism spectrum disorder; TD, typical development; DQ, dev elopmental\\nquotientoftheGesellDevelopmentalScale.\\nDiagnostic Evaluation of Children in the HR Group at\\n2 Years of Age\\nAt 2 years of age, the children in the HR group were evaluated\\nusing the Autism Diagnosis Interview-R (ADI-R) and the\\nAutismDiagnosticObservationSchedule(ADOS).Twopediatr ic\\npsychiatrists then clinically diagnosed the children based on the\\nASD diagnostic criteria in the DSM-5 and the aforementioned\\nevaluationresults.AllparticipantswithconﬁrmedASD(theA SD\\ngroup) reached the cut-oﬀ scores for ASD diagnosis for both\\nevaluationscales.\\nAnalytic Approach\\nThesexdiﬀerencebetweentheHRandTDgroupswascompared\\nusing the χ2test. The diﬀerences between the HR group and\\nthe TD group in social behaviors were determined using the\\nindependent samples t-test. The correlations of social behaviors\\nin the HR group with age, DQ, and symptom severity were\\nanalyzed using Pearson’s rho. Finally, models for early ASD\\nscreening were constructed using machine learning methods\\nbased on ASD group and TD group, and the HR group\\n(contained 40 children with conﬁrmed ASD and 5 children\\nwith not met the criterions of ASD) would be used to veriﬁed\\nthe eﬀectiveness of models for early ASD screening. P<0.05\\nindicatedthatthediﬀerencewasstatisticallysigniﬁcant.\\nRESULTS\\nComparison of the General Conditions\\nBetween the HR Group and the TD Group\\nAge (months), adaptive DQ, language DQ, ﬁne motor DQ,\\nand personal-social DQ were signiﬁcantly diﬀerent ( P<0.05)\\nbetweentheHRgroupandtheTDgroup,whilesexDQandgross\\nmotorDQwerenotsigniﬁcantlydiﬀerent( P>0.05)betweenthe\\ntwogroups.See Table1andFigureS1 .Comparison of the Social Behaviors\\nBetween the HR and TD Groups During\\nDifferent SFP Episodes\\nDuring the FF episode of the SFP, there were statistically\\nsigniﬁcant diﬀerences in the duration and frequency of eye\\ncontact,socialsmiling,andactivesocialengagementbetw eenthe\\nHR group and the TD group ( t=-4.93,−6.17,−3.54,−2.90,\\n−9.56,−8.34; allP<0.05), while the diﬀerences in length and\\nfrequency of non-social smiling and protest behaviors betwe en\\ntheHRgroupandtheTDgroupwerenotstatisticallysigniﬁcan t\\n(t=1.89,1.69,1.62,1.55;all P>0.05).See Figures2A,B .\\nDuring the SF episode of the SFP, there were statistically\\nsigniﬁcant diﬀerences in the duration and frequency of eye\\ncontact and active social engagement between the HR group\\nand the TD group ( t= −4.94,−5.34,−4.49,−6.16; allP<\\n0.05), while the diﬀerences in length and frequency of non-\\nsocial smiling, protest behaviors and social smiling betwee n the\\nHR group and the TD group were not statistically signiﬁcant\\n(t=1.91, 1.33, 0.80, −0.01,−1.98,−1.71; allP>0.05). See\\nFigures2C,D .\\nAnalysis of the Correlation of Social\\nBehaviors With Each Factor for the HR and\\nTD Groups During the Different SFP\\nEpisodes\\nIn the TD group, the frequency of eye contact was signiﬁcantly\\npositively correlated with gross motor DQ for the SF episode\\n(P<0.05); the other indicators had no statistically signiﬁcant\\ncorrelationswithageandDQs( P>0.05;Table2).\\nDuring the FF episode, the duration of eye contact,\\nsocial smiling, and active social engagement for the HR\\ngroup were signiﬁcantly positively correlated with the\\nadaptive DQ, and the duration of eye contact and the gross\\nmotor DQ were also signiﬁcantly positively correlated.\\nDuring the SF episode, the duration of eye contact was\\nsigniﬁcantly positively correlated with the language DQ\\nand the ﬁne motor DQ, and the duration of active social\\nengagement and the language DQ were also signiﬁcantly\\npositively correlated (all P<0.05). The other indicators\\nhad no signiﬁcant correlation with age and DQs ( P>0.05;\\nTable3).\\nTheanalysisofthecorrelationbetweenclinicalASDsymptom\\nseverity and social behavior indicators suggested that the re was\\nno statistically signiﬁcant diﬀerence between social behav ior\\nindicators and clinical symptom severity during the FF episode\\n(P>0.05); during the SF episode, the duration of eye contact\\nwaspositivelycorrelatedwithsymbolicbehaviorfactorsco reand\\nthetotalCSBS-DPscore,thedurationofactivesocialengag ement\\nwas positively correlated with the total CSBS-DP score, and t he\\nfrequencyofeyecontactwaspositivelycorrelatedwiththesoc ial\\ncommunication factor score, the language factor score, and the\\ntotal CSBS-DP score (all P<0.05). The other indicators had\\nno signiﬁcant correlation with the symptom severity ( P>0.05;\\nTable3).\\nFrontiers in Pediatrics | www.frontiersin.org 4 June 2020 | Volume 8 | Article 290', metadata={'source': 'papers/Qiu.pdf', 'page': 3}),\n",
       "  Document(page_content='Qiu et al. Early Screening for High-Risk ASD\\nFIGURE 2 | Comparison of the differences in social behaviors between t he HR group and the TD group during the different SFP episodes .(A)Comparison of the\\ndifferences in the duration of social behaviors during the F F episode. (B)Comparison of the differences in the frequency of social beh aviors during the FF episode. (C)\\nComparison of the differences in the duration of social beha vior during the SF episode. (D)Comparison of the differences in the frequency of social beh aviors during\\nthe SF episode. HR, high-risk autism spectrum disorder; TD, typical development; SFP, still-face paradigm; ** P<0.01.\\nUsing Machine Learning to Construct\\nModels for Early ASD Screening\\nThrough the follow-up of the HR group and the re-diagnosis of\\nthe HR group at 2 years of age, we found that 5 (1 female and\\n4 males) out of the 45 infants and toddlers with HR no longer\\nmet the diagnostic criteria for ASD [the non-ASD (N-ASD)\\ngroup]; the other 40 children still met the diagnostic standa rd\\n(the ASD group). And then the models for early ASD screening\\nwereconstructedusingmachinelearningmethodsbasedonAS D\\ngroupandTDgroup.\\nWe used the duration and frequency of eye contact, active\\nsocial engagement, and social smiling during the FF episode a s\\nwellasthedurationandfrequencyofeyecontactandactives ocial\\nengagement during SF episode as the behavioral characterist ics\\nof the samples. The ASD group and the TD group were usedas the samples. In the classiﬁcation of ASD (40 samples) and\\nits comparison group of TD (43 samples), each subject within\\nthese83sampleswasusedfortestingthemodel thatwas trained\\non the rest 82 samples. Then, the prediction labels of each test\\nsample corresponding to 83 classiﬁcation models were collected\\nto calculate the overall accuracy on this dataset. There wer e\\n82 training samples, and 1 sample was selected for testing.\\nThe test set data were classiﬁed using the following machine\\nlearning methods: support vector machine (SVM), naïve Bayes\\nand random forest. And the Python platform was used for\\nanalyses. The Random Forest Classiﬁer, Gaussian NB and SVM\\nfunctions in Scikit-learn toolbox developed by Python were\\nemployedforbuildingclassiﬁcationmodels,respectively.\\nTheresultsshowedthattheaccuracyofBayesianclassiﬁcati on\\nwas80.54%fortheFFepisodeand82.35%fortheSFepisode,and\\nFrontiers in Pediatrics | www.frontiersin.org 5 June 2020 | Volume 8 | Article 290', metadata={'source': 'papers/Qiu.pdf', 'page': 4}),\n",
       "  Document(page_content='Qiu et al. Early Screening for High-Risk ASD\\nTABLE 2 | Analysis of the correlation of social behavior with age and D Q in the TD group during different SFP episodes ( r-value).\\nEpisodes and indicators Age\\n(months)Adaptive DQ Gross motor DQ Fine motor DQ Language DQ Personal-s ocial DQ\\nDuration during the FF episode (s)\\nEye contact −0.17 0.20 0.09 0.09 0.15 0.09\\nSocial smiling −0.04 −0.06 −0.05 −0.22 −0.04 −0.11\\nActive social engagement −0.03 −0.18 −0.12 −0.25 0.11 −0.12\\nFrequency during the FF episode (number of times)\\nEye contact −0.04 0.09 −0.10 0.11 0.03 0.01\\nSocial smiling 0.15 −0.07 −0.08 −0.21 0.03 −0.09\\nActive social engagement 0.05 −0.15 −0.04 0.02 0.02 −0.14\\nDuration during the SF episode (s)\\nEye contact 0.05 −0.09 −0.02 −0.02 0.04 −0.12\\nSocial smiling 0.04 −0.02 0.19 0.06 0.07 0.12\\nActive social engagement −0.13 −0.17 0.07 −0.15 0.13 −0.01\\nFrequency during the SF episode (number of times)\\nEye contact 0.01 −0.08 0.31* 0.01 0.04 −0.22\\nActive social engagement −0.12 −0.03 0.02 −0.04 0.14 −0.01\\nTD,typicaldevelopment;SFP,still-faceparadigm;DQ,developmentalquotie ntoftheGesellDevelopmentalScale;*P<0.05.\\nTABLE 3 | Analysis of the correlation between social behaviors and ag e (months), DQ, and clinical symptoms of the HR group during d ifferent SFP episodes ( r-value).\\nEpisodes and indicators Age\\n(months)DQ CSBS-DP CARS ABC\\nAdaptive Gross\\nmotorFine\\nmotorLanguage Personal-\\nsocialSocial\\ncommunication\\nfactorLanguage\\nfactorSymbolic\\nbehavior factorTotal\\nscore\\nDuration during the FF episode (s)\\nEye contact −0.08 0.43** 0.31* 0.18 0.25 0.29 0.09 −0.01 −0.01 0.03 0.07 −0.15\\nSocial smiling −0.11 0.36* 0.25 0.08 0.21 0.14 0.10 −0.01 −0.04 0.02 0.01 −0.12\\nActive social engagement −0.04 0.39* 0.26 0.18 0.16 0.20 0.27 −0.01 0.19 0.19 −0.04−0.08\\nFrequency during the FF episode (number of times)\\nEye contact −0.18 0.23 0.21 0.28 0.10 0.09 0.14 −0.04 0.05 0.05 −0.13−0.24\\nSocial smiling −0.09 0.09 0.11 0.14 0.01 0.17 0.01 −0.11 −0.02 −0.06−0.04−0.13\\nActive social engagement −0.05 0.17 0.03 0.22 −0.07 0.03 0.21 −0.07 0.21 −0.10 0.15 0.11\\nDuration during the SF episode (s)\\nEye contact −0.04 0.29 0.22 0.37* 0.30* 0.18 0.27 0.17 0.32* 0.30* −0.34−0.25\\nActive social engagement 0.02 0.27 0.23 0.26 0.38* 0.12 0.29 0.24 0.29 0.30* −0.11−0.16\\nFrequency during the SF episode (number of times)\\nEye contact −0.29 0.17 0.12 0.29 0.26 0.12 0.35* 0.33* 0.25 0.36* −0.22−0.18\\nActive social engagement −0.18 0.15 0.27 0.29 0.28 0.29 0.15 −0.25 0.15 −0.18 0.13 −0.17\\nHR, high-risk autism spectrum disorder; SFP, still-face paradigm; DQ , developmental quotient of the Gesell Developmental Scale; CSBS-DP, Comm unication and Symbolic Behavior\\nScalesDevelopmentalProﬁle;CARS,ChildhoodAutismRatingScal e;ABC,AutismBehaviorChecklist;*P<0.05,**P<0.01.\\nthat the accuracy of random forest classiﬁcation was 80.72% for\\nthe FF episode and 83.13% for the SF episode. And the accuracy\\nof SVM classiﬁcation was 81.18% for FF episode and 83.35%\\nfor the SF episode, which has higher accuracy. The confusion\\nmatrix was showed in Table4. And in order to ﬁnd the age\\ndiﬀerences between the kids who were not picked up by the\\nmachine learning, ASD group and TD group (total 83 kids) was\\ndividedinto4groupswithmonthage,respectively,8–11,12–15 ,\\n16–19,and20–23( FigureS2 ).\\nSubsequently, the eﬀectiveness of the SVM classiﬁcation\\nmodel was veriﬁed in 40 children with conﬁrmed ASD and5 N-ASD children in the HR group. Unfortunately, even\\nthough the average classiﬁcation accuracy of SVM was more\\nthan 80%, the 5 N-ASD was not classiﬁed correctly. The lack\\nof enough samples of N-ASD resulted in a large imbalance\\nbetween two groups. Such limited samples with only 10\\nindicators within each sample could be a possible reason for\\nlow recall rate of N-ASD samples. Therefore, it is essential t o\\nexpand the sample of N-ASD in future study to verify the\\neﬀectiveness of the SVM classiﬁcation model. What’s more, it\\nis necessary to build more eﬀective machine learning model in\\nfollowingstudy.\\nFrontiers in Pediatrics | www.frontiersin.org 6 June 2020 | Volume 8 | Article 290', metadata={'source': 'papers/Qiu.pdf', 'page': 5}),\n",
       "  Document(page_content='Qiu et al. Early Screening for High-Risk ASD\\nTABLE 4 | The confusion matrix of SVM.\\nASD TD\\nFF episode\\nASD 35 5\\nTD 10 33\\nSF episode\\nASD 34 6\\nTD 7 36\\nASD, autism spectrum disorder; TD, typical development; FF episode, face to face\\ninteraction;SFepisode,still-faceepisode.\\nDISCUSSION\\nFace-to-face interaction constitutes the beginning of ear ly\\nchildhood learning and deﬁning social interaction, and fac e-\\nto-face interactions between infants and toddlers and prima ry\\ncaregivers allows the former to learn (1) the meaning of self -\\nexpressionbehaviors;(2)thecharacteristicsofpeoplewithwh om\\nthey have a close relationship; and (3) emotional informatio n\\nand the perception of local culture, primary caretaker identit y\\nand self-identity ( 13). Emotion regulation is an important link\\nofearlychildhooddevelopmentmilestonesandiscloselyrel ated\\nto primary caregivers ( 29). Studies have shown that strong\\nemotion regulation abilities in children is associated wit h good\\ndevelopment and can predict social emotional outcomes at late r\\nstages(30–32)andthatweakemotionregulationabilitiesduring\\nearly childhood are associated with behavioral problems and\\ndevelopment problems at later stages ( 29,33,34). Especially\\nfrom 4 to 9 months old, infants quickly learn how to regulate\\nemotionsthroughface-to-faceinteractions;therefore,t hequality\\nofinfant-motherinteractionsiscrucialatthisstage( 24,35).\\nThrough comparative analysis of the diﬀerences in social\\nbehaviorsbetweeninfantsandtoddlerswithHRandinfantsa nd\\ntoddlerswithTDbeforetheageof2,comparedwithinfantsand\\ntoddlerswithTD,infantsandtoddlerswithHRexhibitedsho rter\\ndurations and lower frequencies of eye contact, social smil ing,\\nand active social engagement during the FF episode of the SFP.\\nThis ﬁnding is consistent with those of most studies ( 28,36–\\n38). In the SF episode, compared with infants and toddlers with\\nTD, infants and toddlers with HR showed shorter durations an d\\nlower frequencies of eye contact and active social engageme nt,\\nwhichmeansthatalthoughchildrenwithHRexhibitedbehavi ors\\nto attract the attention of the non-responsive mothers, thei r\\nability to initiate active social engagement was lower than that\\nof children with TD. For infants with HR, avoiding eye contac t\\nresults in a low-quality infant-mother interaction; there fore, the\\ndevelopment of emotion regulation abilities in these infant s and\\ntoddlersmaybedelayed,whichexplainstosomeextentthecau ses\\nof the delayed development of social smiling and active socia l\\nengagementinchildrenwithASD.\\nFrom the results of the correlation analysis, there was a\\ndiﬀerence between age and the developmental level and social\\nbehaviors of some infants and toddlers in the HR group and\\nthe TD group, but the diﬀerence was not representative, i.e., th e\\nage and developmental level of the infants and toddlers did no t\\ninﬂuence their social behaviors under general conditions. Forinfants and toddlers with HR, the analysis of the correlatio n\\nbetween clinical symptoms and social behaviors showed that\\nthere was no correlation between social behaviors and sympto m\\nseverity during the FF episode of the SFP. During the SF\\nepisode, the duration of eye contact by infants and toddlers\\nwith HR was positively correlated with the symbolic behavior\\nfactor score and the total CSBS-DP score; the duration of soc ial\\nsmiling was positively correlated with the social communica tion\\nfactor score and the total CSBS-DP score; and the duration of\\nactivesocialengagementwaspositivelycorrelatedwiththeso cial\\ncommunication factor score, the symbolic behavior factor s core\\nand the total CSBS-DP score; and the frequency of eye contact\\nwas positively correlated with the social communication fac tor\\nscore,thelanguagefactorscoreandthetotalCSBS-DPscore .The\\nresults indicated that the more ﬂexible and appropriate the eye\\ncontact and active social engagement of the infants and todd lers\\nwith HR, the less severe were the ASD symptoms, which is also\\nconsistentwiththeresultsofmoststudies( 39–41).Althoughthe\\nsocial behaviors of infants and toddlers with ASD develop ove r\\ntime, their development level is limited, the gap between inf ants\\nand toddlers with ASD and infants and toddlers with TD also\\nincreases over time, and infants and toddlers with ASD develo p\\nmoreclinicalsymptomsofASD.\\nThe SFP presents changes in children’s expressions, emotions\\nand behaviors in a more microscopic coding mode. On the\\nbasis of setting a normal interaction, the SFP provides a soci al\\nchallenge scenario by setting the SF episode, during which the\\nsocial signals of the mother are completely missing during th e\\nperiod. For typically developing children, their inability to adapt\\nto the loss of social signals stimulated their ability to ini tiate\\nsocialinteractions,expressemotions,regulateemotions,an dbear\\nstress. The analysis of the above results showed that the soci al\\nbehaviors in infants and toddlers with HR, especially their soc ial\\nbehaviors during the SF episode of the SFP, were associated wit h\\nthe core ASD symptoms. According to the extreme male brain\\ntheory of autism ( 42,43) the toddlers with ASD are more prone\\ntooversystematizationandthushavelowerempathicability than\\ndoTDtoddlers,makingthemmorepronetodeﬁcienciesinsocia l\\nand verbal communication. By examining the diﬀerences in\\nsocialbehaviorsbetweentheinfantsandtoddlersintheHRgr oup\\nand the TD group, we found that although there were many\\ndiﬀerencesintheabnormalsocialbehaviorsbetweeninfants and\\ntoddlers with ASD and infants and toddlers with TD during the\\nFF and SF episodes of the SFP, the social behaviors of infants\\nand toddlers with HR, such as eye contact and active social\\nengagement,duringtheSFepisode(afrustrationscenario),we re\\nsigniﬁcantly correlated with core communication impairmen ts,\\nsuch as the social communication factor score, the symbolic\\nbehaviorfactorscoreandthetotalCSBS-DPscore.Thatis,t heSF\\nepisode of the SFP can better induce the social communication\\nimpairments in infants and toddlers with ASD. Markram et al.\\n(44) proposed the intense world theory, suggesting that an\\nexcessively active brain would excessively amplify ordinary\\nsensory experiences, causing the toddlers with ASD to be in a\\nstate of fragmented sensory information and to be overloade d,\\nand because of such a strong reaction, the intense emotions\\nperceived by them from the surrounding environment causes\\nsocial withdrawal, resulting in a series of autism symptoms s uch\\nFrontiers in Pediatrics | www.frontiersin.org 7 June 2020 | Volume 8 | Article 290', metadata={'source': 'papers/Qiu.pdf', 'page': 6}),\n",
       "  Document(page_content='Qiu et al. Early Screening for High-Risk ASD\\nassocialcommunicationimpairmentsandstereotypedbehavio rs.\\nTherefore,facingsocialcommunicationchallengessuchas theSF\\nepisode, infants and toddlers with TD attempted to arouse thei r\\nmothers’ responses by pointing and social smiling. In contras t,\\nforinfantsandtoddlerswithHR,evenforthosewhohadhighe r\\nfunction,theymayhavehadgoodinteractionswiththeirmot hers\\nduring the FF episode, but when their mothers did not respond,\\nsocialpressurewasreduced.Theymadefewerattemptsorshort er\\nattemptstoinitiatesocialinteractions.\\nIn addition, some studies have shown that when responding\\nto emotional reactions, children with ASD have worse emotio n\\nregulation abilities and more unreasonable expression and a re\\nmore likely to show negative emotions ( 45). However, in this\\nstudy, the negative emotions (protest behavior and non-soci al\\nsmiling) in infants and toddlers with HR and TD were not\\ndiﬀerent, and there were no diﬀerences during the frustration\\nscenario,i.e.,whenthemothersusedstillfaces.Furtherv alidation\\nanddiscussionareneededinfuturestudies.\\nBased on the re-diagnosis and regrouping of the children\\nat 2 years of age, machine learning methods, including SVM,\\nnaïve Bayes and random forest, were used to construct models\\nfor early ASD screening. And we found the classiﬁcation mode l\\nestablishedusingtheSVMhadthebestperformance,especiall yit\\nwasfoundtohavebetterscreeningabilityandreliabilityf ortheSF\\nepisode.Unfortunately,whenthemodelwasselectedandapplied\\nto the diﬀerential diagnosis of the children in the HR group, th e\\n5 N-ASD was not classiﬁed correctly. And also, it is the goal t o\\nidentify N-ASD from ASD group in our future eﬀorts. Since the\\nagediﬀerencesbetweentheASDgroupandTDgroup,weadded\\ntheFigureS2 , in which we divided ASD group and TD group\\n(total 83 kids) into 4 groups with month age, respectively, 8–11 ,\\n12–15,16–19,and20–23,andwefoundtherewerenotregulari ty\\nbetweentheclassiﬁcationaccuracyvs.monthage.\\nSimilarly, there are limitations in this study. First, beca use\\ninfants and toddlers with ASD generally have delayed\\ndevelopment, the 2 groups were not matched by age to\\nmake the development level of the HR group the same as that\\nof the TD group. Second, the sample size was small. In view of\\nthese limitations, we will continue to expand the sample size\\nin future studies to further verify the ﬁndings under contro lledphysiological and psychological ages. We hope that the SFP will\\nbe widely applied for the early ASD screening and that a more\\nobjective, standardized and convenient way for self-scree ning at\\nhomewillbeachieved.\\nDATA AVAILABILITY STATEMENT\\nAll datasets generated for this study are included in the\\narticle/SupplementaryMaterial .\\nETHICS STATEMENT\\nThe studies involving human participants were reviewed and\\napproved by the Medical Ethics Committee of Nanjing Brain\\nHospital Aﬃliated to Nanjing Medical University. Written\\ninformed consent to participate in this study was provided\\nby the participants’ legal guardian/next of kin. Written\\ninformed consent was obtained from the individual(s), and\\nminor(s)’ legal guardian/next of kin, for the publication\\nof any potentially identiﬁable images or data included in\\nthisarticle.\\nAUTHOR CONTRIBUTIONS\\nNQ and XK designed experiments. NQ, CT, MZ, JW, CL, XX,\\nJF, LZ, TX, and HF carried out experiments. NQ, CT, and WH\\nanalyzedexperimentalresults.NQwrotethemanuscript.\\nFUNDING\\nKey Research (Social Development) Foundation of Jiangsu\\nProvince (BE2016616); the Fifth ‘‘333 High Level of Cultiva ting\\nTalentsProjects’’inJiangsuProvince.\\nSUPPLEMENTARY MATERIAL\\nThe Supplementary Material for this article can be found\\nonline at: https://www.frontiersin.org/articles/10.338 9/fped.\\n2020.00290/full#supplementary-material\\nREFERENCES\\n1. Marty MA, Segal DL. Diagnosis and Statistical Manual of Mental Disorders\\nDSM-5.AmericanPsychiatricAssociation,W.D.A.P.Association(20 13).\\n2. Kim YS, Leventhal BL, Koh Y-J, Fombonne E, Laska E, Lim E-C, e t al.\\nPrevalence of autism spectrum disorders in a total population sample. Am J\\nPsychiatry. (2011)168:904–12.doi:10.1176/appi.ajp.2011.10101532\\n3. Clark MLE, Vinen Z, Barbaro J, Dissanayake C. School age outco mes of\\nchildren diagnosed early and later with autism spectrum disorder. J Autism\\nDevDisord .(2017)48:92–102.doi:10.1007/s10803-017-3279-x\\n4. Daniels AM, Mandell DS. Explaining diﬀerences in age at autism s pectrum\\ndisorder diagnosis: a critical review. Autism Int J Res Pract. (2014) 18:583.\\ndoi:10.1177/1362361313480277\\n5. Barbaro J, Dissanayake C. ASD in infancy and toddlerhood: a re view of\\nthe evidence on early signs early identiﬁcation tools and early diag nosis.\\nJ Dev Behav Pediatr. (2009) 30:447–59. doi: 10.1097/DBP.0b013e3181\\nba0f9f6. Adrien JL, Lenoir P, Martineau J, Perrot A, Hameury L, Larmande C,\\net al. Blind ratings of early symptoms of autism based upon family\\nhome movies. J Am Acad ChildAdolesc Psychiatry. (1993) 32:617–27.\\ndoi:10.1097/00004583-199305000-00019\\n7. Wetherby AM, Woods J, Allen L, Cleary J, Dickinson H, Lord C. Early\\nindicators of autism spectrum disorders in the second year of lif e.J Autism\\nDevDisord. (2004)34:473–93.doi:10.1007/s10803-004-2544-y\\n8. Barbaro J, Dissanayake C. Early markers of autism spectrum\\ndisorders in infants and toddlers prospectively identiﬁed in the\\nSocial Attention and Communication Study. Autism. (2013) 17:64–86.\\ndoi:10.1177/1362361312442597\\n9. Zwaigenbaum L, Bauman ML, Stone WL, Yirmiya N, Estes A, Hansen RL,\\net al. Early identiﬁcation of autism spectrum disorder: recommendati ons\\nfor practice and research. Pediatrics. (2015) 48:92–102. 136(Suppl 1):S10–40.\\ndoi:10.1542/peds.2014-3667C\\n10. Cassel TD, Messinger DS, Ibanez LV, Haltigan JD, Acosta SI , Buchman AC.\\nEarly social and emotional communication in the infant siblings of children\\nFrontiers in Pediatrics | www.frontiersin.org 8 June 2020 | Volume 8 | Article 290', metadata={'source': 'papers/Qiu.pdf', 'page': 7}),\n",
       "  Document(page_content='Qiu et al. Early Screening for High-Risk ASD\\nwith autism spectrum disorders: an examination of the broad phenot ype.J\\nAutismDevDisord. (2007)37:122–32.doi:10.1007/s10803-006-0337-1\\n11. Heimann M, Laberg KE, Nordøen B. Imitative interaction increas es social\\ninterestandelicitedimitationinnon-verbalchildrenwithauti sm.InfantChild\\nDev.(2010)15:297–309.doi:10.1002/icd.463\\n12. Harker CM, Ibanez LV, Nguyen TP, Messinger DS, Stone WL. Th e eﬀect of\\nparenting style on social smiling in infants at high and low risk for A SD.J\\nAutismDevDisord. (2016)46:2399–407.doi:10.1007/s10803-016-2772-y\\n13. Tronick E, Als H, Adamson L. The infant’s response to entrapment\\nbetween contradictory messages in face-to-face interaction. J Am Acad Child\\nPsychiatry. (1978)17:1–13.doi:10.1016/S0002-7138(09)62273-1\\n14. MantisI,StackDM,NgL,SerbinLA,SchwartzmanAE.Mutualt ouchduring\\nmother–infant face-to-face still-face interactions: inﬂuenc es of interaction\\nperiod and infant birth status. Infant Behav Dev. (2014) 37:258–67.\\ndoi:10.1016/j.infbeh.2014.04.005\\n15. Carey WB. From neurons to neighborhoods: the science of early\\nchildhood development. Aust N Zeal J Psychiatry. (2002) 41:625–6.\\ndoi:10.1097/00004583-200205000-00022\\n16. Mercer J. Understanding Attachment: Parenting, Child Care, and Emoti onal\\nDevelopment Praeger .(2006)53:12–12.doi:10.1179/004049606X132096\\n17. YirmiyaN,GamlielI,PilowskyT,FeldmanR,Baron-CohenS,SigmanM .The\\ndevelopment of siblings of children with autism at 4 and 14 months: so cial\\nengagement,communication,andcognition. JChildPsycholPsychiatry. (2006)\\n47:511.doi:10.1111/j.1469-7610.2005.01528.x\\n18. Ostfeld-Etzion S, Golan O, Hirschler-Guttenberg Y, Zagoory- Sharon\\nO, Feldman R. Neuroendocrine and behavioral response to social\\nrupture and repair in preschoolers with autism spectrum disorders\\ninteracting with mother and father. Mol Autism. (2015) 6:1–13.\\ndoi:10.1186/s13229-015-0007-2\\n19. Merin N, Young GS, Ozonoﬀ S, Rogers SJ. Visual ﬁxation patte rns during\\nreciprocal social interaction distinguish a subgroup of 6-month- old infants\\nat-risk for autism from comparison infants. J Autism Dev Disord. (2007)\\n37:108–21.doi:10.1007/s10803-006-0342-4\\n20. Jamal W, Das S, Oprescu IA, Maharatna K, Apicella F, and Sicca F.\\nClassiﬁcationofautismspectrumdisorderusingsupervisedlearni ngofbrain\\nconnectivity measures extracted from synchrostates. J. Neural Eng. (2014)\\n11:046019.doi:10.1088/1741-2560/11/4/046019\\n21. Abraham A, Milham M, Martino AD, Craddock RC, Samaras D, Thirion\\nB, and Varoquaux G. (2016). Deriving reproducible biomarkers from multi -\\nsite resting-state data: An Autism-based example. Neuroimage . 147:736.\\ndoi:10.1016/j.neuroimage.2016.10.045\\n22. Robins DL, Fein D, Barton ML, Green JA. The modiﬁed checklist f or autism\\nin toddlers: an initial study investigating the early detectio n of autism and\\npervasive developmental disorders. J Autism Dev Disord. (2001) 31:131–44.\\ndoi:10.1023/A:1010738829569\\n23. Legerstee M, Markova G. Intentions make a diﬀerence: infant responses to\\nstill-faceandmodiﬁedstill-faceconditions. InfantBehavDev. (2007)30:232–\\n50.doi:10.1016/j.infbeh.2007.02.008\\n24. Yato Y, Kawai M, Negayama K, Sogon S, Tomiwa K, Yamamoto H. Infan t\\nresponses to maternal still-face at 4 and 9 months. Infant Behav Dev. (2008)\\n31:570–7.doi:10.1016/j.infbeh.2008.07.008\\n25. Montirosso R, Provenzi L, Tavian D, Morandi F, Bonanomi A, Mis saglia S,\\netal.Socialstressregulationin4-month-oldinfants:contributi onofmaternal\\nsocial engagement and infants’ 5-HTTLPR genotype. Early Hum Dev. (2015)\\n91:173–9.doi:10.1016/j.earlhumdev.2015.01.010\\n26. Provenzi L, Fumagalli M, Bernasconi F, Sirgiovanni I, Morandi F , Borgatti\\nR, et al. Very preterm and full-term infants’ response to socio-emotiona l\\nstress: the role of postnatal maternal bonding. Infancy.(2017) 22:695–712.\\ndoi:10.1111/infa.12175\\n27. Tronick EZ, Messinger DS, Weinberg MK, Lester BM, Lagasse L, Seifer R,\\net al. Cocaine exposure is associated with subtle compromises of infa nts’ and\\nmothers’ social-emotional behavior and dyadic features of thei r interaction\\nin the face-to-face still-face paradigm. Dev Psychol. (2005) 41:711–22.\\ndoi:10.1037/0012-1649.41.5.711\\n28. Nichols CM, Ibanez LV, Foss-Feig JH, Stone WL. Social smiling and its\\ncomponentsinhigh-riskinfantsiblingswithoutlaterASDsymptomat ology.J\\nAutismDevDisord. (2014)44:894–902.doi:10.1007/s10803-013-1944-2\\n29. CalkinsSD,DedmonSE.Physiologicalandbehavioralregulatio nintwo-year-\\nold children with aggressive/destructive behavior problems. J Abnorm Child\\nPsychol.(2000)28:103–18.doi:10.1023/A:100511291290630. Schultz D, Izard CE, Ackerman BP, Youngstrom EA. Emotion\\nknowledge in economically disadvantaged children: self-regulatory\\nantecedents and relations to social diﬃculties and withdrawal.\\nDev Psychopathol. (2001) 13:53–67. doi: 10.1017/S095457940100\\n1043\\n31. Denham SA, Blair KA, Demulder E, Levitas J, Sawyer K, Auerbac h-Major S,\\net al. Preschool emotional competence: pathway to social competence ?Child\\nDev.(2003)74:238.doi:10.1111/1467-8624.00533\\n32. Eisenberg N, Zhou Q, Losoya SH, Fabes RA, Shepard SA, Murphy BC,\\net al. The relations of parenting, eﬀortful control, and ego control\\nto children’s emotional expressivity. Child Dev. (2003) 74:875–95.\\ndoi:10.1111/1467-8624.00573\\n33. Calkins SD, Fox NA. Self-regulatory processes in early personality\\ndevelopment: a multilevel approach to the study of childhood social\\nwithdrawal and aggression. Dev Psychopathol. (2002) 14:477–98.\\ndoi:10.1017/S095457940200305X\\n34. Supplee LH, Skuban EM, Shaw DS, Prout J. Emotion regulation\\nstrategies and later externalizing behavior among European American\\nand African American children. Dev Psychopathol. (2009) 21:393.\\ndoi:10.1017/S0954579409000224\\n35. Haley DW, Stansbury K. Infant stress and parent responsiveness: regulation\\nof physiology and behavior during still-face and reunion. Child Dev. (2003)\\n74:1534–46.doi:10.1111/1467-8624.00621\\n36. Ozonoﬀ S, Iosif AM, Baguio F, Cook IC, Hill MM, Hutman T, et al.\\nA prospective study of the emergence of early behavioral signs of\\nautism.J Am Acad Child Adolesc Psychiatry. (2010) 49:256–66e252.\\ndoi:10.1016/j.jaac.2009.11.009\\n37. Lambert-BrownBL,McdonaldNM,MattsonWI,MartinKB,IbanezL V,Stone\\nWL,etal.Positiveemotionalengagementandautismrisk. DevPsychol. (2015)\\n51:848–55.doi:10.1037/a0039182\\n38. Chen X, Zou X, Chen K, Cen C, Cheng S. Analysis of early symptoms of\\nautismspectrumdisorderchildrenbasedonthree-minutevideos. ChinJAppl\\nClinPediatr. (2017)32:777–9.doi:10.3760/cma.j.issn.2095-428X.2017. 10.015\\n39. Cliﬀord SM, Dissanayake C. The early development of joint attent ion\\nin infants with autistic disorder using home video observatio ns\\nand parental interview. J Autism Dev Disord. (2008) 38:791–805.\\ndoi:10.1007/s10803-007-0444-7\\n40. Parlade MV, Iverson JM. The development of coordinated communica tion\\nin infants at heightened risk for autism spectrum disorder. J\\nAutism Dev Disord. (2015) 45:2218–34. doi: 10.1007/s10803-015-\\n2391-z\\n41. Campbell SB, Leezenbaum NB, Mahoney AS, Moore EL, Brownell CA.\\nPretend play and social engagement in toddlers at high and low genet ic\\nrisk for autism spectrum disorder. J Autism Dev Disord. (2016) 46:2305–16.\\ndoi:10.1007/s10803-016-2764-y\\n42. Baron-Cohen S. Autism: the empathizing-systemizing (E-S) th eory.\\nAnn N Y Acad Sci. (2009) 1156:68–80. doi: 10.1111/j.1749-6632.2009.\\n04467.x\\n43. Baron-Cohen S, Lombardo MV, Auyeung B, Ashwin E, Knickmeyer\\nR. Why are autism spectrum conditions more prevalent in\\nmales?PLoS Biol. (2011) 9:e1001081. doi: 10.1371/journal.pbio.10\\n01081\\n44. Markram H, Tania R, Kamila M. The intense world syndrome–an\\nalternative hypothesis for autism. Front Neurosci. (2007) 1:77–96.\\ndoi:10.3389/neuro.01.1.1.006.2007\\n45. Samson AC, Huber O, Gross JJ. Emotion regulation in Asperger’s\\nsyndrome and high-functioning autism. Emotion. (2012) 12:659–65.\\ndoi:10.1037/a0027975\\nConﬂict of Interest: The authors declare that the research was conducted in the\\nabsence of any commercial or ﬁnancial relationships that could be c onstrued as a\\npotentialconﬂictofinterest.\\nCopyright © 2020 Qiu, Tang, Zhai, Huang, Weng, Li, Xiao, Fu, Zhang, Xiao, Fang\\nand Ke. This is an open-access article distributed under the terms of the Creative\\nCommons Attribution License (CC BY). The use, distribution or reproduction in\\notherforumsispermitted,providedtheoriginalauthor(s)a ndthecopyrightowner(s)\\nare credited and that the original publication in this journ al is cited, in accordance\\nwith accepted academic practice. No use, distribution or re production is permitted\\nwhichdoesnotcomplywiththeseterms.\\nFrontiers in Pediatrics | www.frontiersin.org 9 June 2020 | Volume 8 | Article 290', metadata={'source': 'papers/Qiu.pdf', 'page': 8})],\n",
       " [Document(page_content='RESEA RCH ARTICL E\\nMobile detection ofautism through machine\\nlearning onhome video: Adevelopment and\\nprospective validation study\\nQandeel Tariq ID\\n1,2,Jena Daniels ID\\n1,2,Jessey Nicole Schwartz ID\\n1,2,Peter Washington ID\\n1,2,\\nHaik Kalantarian1,2,Dennis Paul Wall ID\\n1,2*\\n1Department ofPediatrics, Divisio nofSystems Medicine, Stanfor dUniversity, Californi a,United States of\\nAmerica, 2Department ofBiomed icalData Science, Stanford University ,California, United States ofAmerica\\n*dpwall@ stanford.ed u\\nAbstract\\nBackground\\nThestandard approaches todiagnosing autism spectrum disorder (ASD) evaluate between\\n20and100behaviors andtakeseveral hours tocomplete. Thishasinpartcontributed to\\nlongwaittimes foradiagnosis andsubsequent delays inaccess totherapy. Wehypothesize\\nthattheuseofmachine learning analysis onhome video canspeed thediagnosis without\\ncompromising accuracy. Wehave analyzed item-level records from 2standard diagnostic\\ninstruments toconstruct machine learning classifiers optimized forsparsity, interpretability,\\nandaccuracy. Inthepresent study, weprospectively testwhether thefeatures from these\\noptimized models canbeextracted byblinded nonexpert raters from 3-minute home videos\\nofchildren withandwithout ASD toarrive atarapid andaccurate machine learning autism\\nclassification.\\nMethods andfindings\\nWecreated amobile webportal forvideo raters toassess 30behavioral features (e.g., eye\\ncontact, social smile) thatareused by8independent machine learning models foridentify-\\ningASD, each with>94% accuracy incross-validation testing andsubsequent independent\\nvalidation from previous work. Wethen collected 116short home videos ofchildren with\\nautism (mean age=4years 10months, SD=2years 3months) and46videos oftypically\\ndeveloping children (mean age=2years 11months, SD=1year 2months). Three raters\\nblind tothediagnosis independently measured each ofthe30features from the8models,\\nwithamedian timetocompletion of4minutes. Although several models (consisting ofalter-\\nnating decision trees, support vector machine [SVM], logistic regression (LR), radial kernel,\\nandlinear SVM) performed well, asparse 5-feature LRclassifier (LR5) yielded thehighest\\naccuracy (area under thecurve [AUC]: 92% [95% CI88%–97%]) across allages tested. We\\nused aprospectively collected independent validation setof66videos (33ASD and33non-\\nASD) and3independent rater measuremen tstovalidate theoutcome, achieving lower but\\ncomparable accuracy (AUC: 89% [95% CI81%–95%]) .Finally, weapplied LRtothe162-\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 1/20a1111111111\\na1111111111\\na1111111111\\na1111111111\\na1111111111\\nOPEN ACCESS\\nCitation: Tariq Q,Daniels J,Schwartz JN,\\nWashingto nP,Kalantaria nH,WallDP(2018)\\nMobile detection ofautism through machine\\nlearning onhome video: Adevelopm entand\\nprospecti vevalidation study. PLoS Med15(11):\\ne1002705. https://d oi.org/10.1371/j ournal.\\npmed.1002 705\\nAcademic Editor: Suchi Saria, Johns Hopkins\\nUniversity ,UNITED STATES\\nReceived: June 8,2018\\nAccepted: October 25,2018\\nPublished: November 27,2018\\nCopyright: ©2018 Tariq etal.Thisisanopen\\naccess article distributed under theterms ofthe\\nCreative Commons Attribution License, which\\npermits unrestricte duse,distribu tion,and\\nreproduction inanymedium, provided theoriginal\\nauthor andsource arecredited.\\nData Availabilit yStatement: Thede-identified data\\nhave been made available atthefollowing github\\nrepository andinclude theprimary dataset andthe\\nvalidation dataset :https://github .com/qandeelt/\\nvideo_phen otyping_autis m_plos/tree/ master/\\ndatasets. Thecode hasbeen made available atthe\\nfollowing github repository andinstructions on\\nhowtoruneach classifier have been provided:\\nhttps://github .com/qandee lt/video_phe notyping_\\nautism_plos', metadata={'source': 'papers/Tariq2018.pdf', 'page': 0}),\n",
       "  Document(page_content='video-feature matrix toconstruct an8-feature model, which achieved 0.93 AUC (95% CI\\n0.90–0.97) ontheheld-out testsetand0.86 onthevalidation setof66videos. Validation on\\nchildren withanexisting diagnosis limited theability togeneralize theperformance toundi-\\nagnosed populations.\\nConclusions\\nThese results support thehypothesis thatfeature tagging ofhome videos formachine learn-\\ningclassification ofautism canyield accurate outcomes inshort timeframes, using mobile\\ndevices. Further work willbeneeded toconfirm thatthisapproach canaccelerate autism\\ndiagnosis atscale.\\nAuthor summary\\nWhy wasthisstudy done?\\n•Autism hasrisen inincidence byapproximately 700% since 1996 and now impacts at\\nleast 1in59children intheUnited States.\\n•The current standard fordiagnosis requires adirect clinician-to-child observation and\\ntakes hours toadminister.\\n•The sharp riseinincidence ofautism, coupled with theun-scalable nature ofthestan-\\ndard ofcare (SOC), hascreated strain onthehealthcare system, and theaverage ageof\\ndiagnosis remains around 4.5years, 2years past thetime when itcould bereliably\\ndiagnosed.\\n•Mobile measures that scale could help toalleviate thisstrain onthehealthcare system,\\nreduce waiting times foraccess totherapy and treatment, and reach underserved\\npopulations.\\nWhat didtheresearchers doandfind?\\n•Weapplied 8machine learning models to162two-minute home videos ofchildren with\\nand without autism diagnosis totesttheability toreliably detect autism onmobile\\nplatforms.\\n•Three nonexpert raters measured 30behavioral features needed formachine learning\\nclassification bythe8models inapproximately 4minutes.\\n•Leveraging video ratings, amachine learning model with only 5features achieved 86%\\nunweighted average recall (UAR) on162videos and UAR =80% onadifferent and\\nindependently evaluated setof66videos, with UAR =83% onchildren atorunder 4.\\n•The above machine learning process ofrendering amobile video diagnosis quickly cre-\\nated anovel collection oflabeled video features and anew video feature–based model\\nwith>90% accuracy.\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 2/20Funding: Thework wassupported inpartbyfunds\\ntoDPW from NIH(1R01EB02 5025-01 &\\n1R21HD0915 00-01), TheHartwell Foundation, Bill\\nandMelinda Gates Foundation, Coulter Foundation,\\nLucile Packard Foundation, andprogram grants\\nfrom Stanford University’s Human Centere d\\nArtificial Intellig ence Program, Precision Health and\\nIntegrate dDiagnostics Center (PHIND), Beckman\\nCenter, Bio-X Center, Predicti vesandDiagnostic s\\nAccelerator ,andtheChild Health Researc h\\nInstitute. Wealsoreceived philanthr opicsupport\\nfrom Bobby Dekesye randPeter Sullivan. No\\nfunding bodies hadanyroleinstudy design, data\\ncollection andanalysis ,decision topublish, or\\npreparation ofthemanuscript.\\nCompeting interests :Ihave readthejournal’s\\npolicy andtheauthors ofthismanuscript have the\\nfollowing competing interests: DPW isthescientific\\nfounder ofCognoa, acompany focused ondigital\\npediatric healthcare; theapproach andfindings\\npresented inthispaper areindepende ntfrom/not\\nrelated toCognoa. Allother authors have declared\\nnocompeting interests exist.\\nAbbreviati ons:ADI-R, Autism Diagnost ic\\nInterview- Revised; ADOS, Autism Diagnost ic\\nObservation Schedule; ADTree, alterna tingdecision\\ntree;ADTree7, 7-feature alterna tingdecision tree;\\nADTree8, 8-feature alterna tingdecision tree;ASD,\\nautism spectrum disorder; AUC, areaunder the\\ncurve; AUC-ROC, areaunder thereceiver operating\\ncharacteristi ccurve; BID,Balanced Independent\\nDataset; IRA,interrater agreement ;LR,logistic\\nregressio n;LR10, 10-feature logistic regressio n\\nclassifier ;LR5, 5-feature logistic regressi on\\nclassifier ;LR9, 9-feature logistic regressi on\\nclassifier ;LR-EN-VF, logistic regressio nwithan\\nelastic netpenalty; ROC, receiver operating\\ncharacteristi c;SOC, standard ofcare; SVM,\\nsupport vector machine ;SVM10, 10-feature\\nsupport vector machine ;SVM12, 12-feature\\nsupport vector machine ;SVM5, 5-feature support\\nvector machine ;UAR, unweighte daverage recall.', metadata={'source': 'papers/Tariq2018.pdf', 'page': 1}),\n",
       "  Document(page_content='What dothese findings mean?\\n•Short home videos canprovide sufficient information torunmachine learning classifi-\\nerstrained todetect children with autism from those with either typical oratypical\\ndevelopment. Features needed bymachine learning models designed todetect autism\\ncanbeidentified and measured inhome videos onmobile devices bynonexperts in\\ntimeframes close tothetotal video length and under 6minutes.\\n•The machine learning models provide aquantitative indication ofautism risk that pro-\\nvides more granularity than abinary outcome toflaginconclusive cases, potentially add-\\ningvalue foruseinclinical settings, e.g., fortriage.\\n•The process ofmobile video analysis forautism detection generates agrowing matrix of\\nvideo features that canbeused toconstruct new machine learning models that may\\nhave higher accuracy forautism detection inhome video.\\n•Clinical prospective testing ingeneral pediatric settings onpopulations notyetdiag-\\nnosed willbeneeded. However, these results support thepossibility that mobile video\\nanalysis with machine learning may enable rapid autism detection outside ofclinics to\\nreduce waiting periods foraccess tocare and reach underserved populations inregions\\nwith limited healthcare infrastructure.\\nIntroduction\\nNeuropsychiatric disorders arethesingle greatest cause ofdisability duetononcommunicable\\ndisease worldwide, accounting for14% oftheglobal burden ofdisease [1].Asignificant con-\\ntributor tothismetric isautism spectrum disorder (ASD, orautism), which hasrisen ininci-\\ndence byapproximately 700% since 1996 [2,3] and now impacts 1in59children intheUnited\\nStates [4,5]. ASD isarguably oneofthelargest pediatric health challenges, assupporting an\\nindividual with thecondition costs upto$2.4 million during his/her lifespan intheUS[6]and\\nover $5billion annually inUShealthcare costs [6].\\nLike most mental health conditions, autism hasacomplex array ofsymptoms [7]that are\\ndiagnosed through behavioral exams. The standard ofcare (SOC) foranautism diagnosis uses\\nbehavioral instruments such astheAutism Diagnostic Observation Schedule (ADOS) [8]and\\ntheAutism Diagnostic Interview-Revised (ADI-R) [9].These standard exams aresimilar to\\nothers indevelopmental pediatrics [10] inthat they require adirect clinician-to-child observa-\\ntion and take hours toadminister [11–14]. The sharp riseinincidence ofautism, coupled with\\ntheunscalable nature oftheSOC, hascreated strain onthehealthcare system. Wait times fora\\ndiagnostic evaluation canreach orexceed 12months intheUS[15], and theaverage ageof\\ndiagnosis intheUSremains near 5years ofage[2,13], with underserved populations’ average\\nageatASD diagnosis ashigh as8years [16–18]. The high variability inavailability ofdiagnos-\\nticand therapeutic services iscommon tomost psychiatry and mental health conditions across\\ntheUS,with severe shortages ofmental health services in77% ofUScounties [19]. Behavioral\\ninterventions forASD aremost impactful when administered byorbefore 5years ofage\\n[12,20–23]; however, thediagnostic bottleneck that families face severely limits theimpact of\\ntherapeutic interventions. Scalable measures arenecessary toalleviate these bottlenecks,\\nreduce waiting times foraccess totherapy, and reach underserved populations inneed.\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 3/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 2}),\n",
       "  Document(page_content='Asastep toward enabling fastand accurate access tocare forASD, wehave used supervised\\nmachine learning approaches toidentify minimal setsofbehaviors that align with clinical diag-\\nnoses ofASD [24–30]. Weassembled and analyzed item-level outcomes from theadministra-\\ntion oftheADOS and ADI-R totrain and testtheaccuracy ofarange ofclassifiers. Forthe\\nADOS, wefocused ouranalysis onordinal outcome data from modules 1,2,and 3,which\\nassess children with limited ornovocabulary, with phrased speech, and with fluent speech,\\nrespectively. Each ofthe3ADOS modules uses approximately 10activities foraclinical obser-\\nvation ofthechild atriskand 28–30 additional behavioral measurements used toscore the\\nchild following theobservation. Our machine learning analyses focused onarchived records of\\nthecategorical and ordinal data generated from thescoring component ofthese ADOS exami-\\nnations. Similarly, theADI-R involves 93multiple-choice questions asked byaclinician ofthe\\nchild’s primary care provider during anin-clinic interview; aswith theADOS, wefocused our\\nclassification task ontheordinal outcome data that resulted from thetest’s administration.\\nThese preliminary studies focused onbuilding models optimized foraccuracy, sparsity, and\\ninterpretability that differentiate autism from non-autism while managing class imbalance.\\nWechose models with small numbers offeatures, with performance atornomore than 1stan-\\ndard error away from best testperformance, and with interpretable outcomes—for example,\\nscores generated byaboosted decision tree orlogistic regression (LR) approach. Inall,these\\nstudies have used score data from 11,298 individuals with autism (mixed with low-, medium-,\\nand high-severity autism) and 1,356 controls (including some children forwhom autism may\\nhave been suspected butwasruled out) and have identified thefollowing 8classifiers: a7-fea-\\nture alternating decision tree (ADTree7) [29], an8-feature alternating decision tree (ADTree8)\\n[30], a12-feature support vector machine (SVM12) [26], a9-feature LRclassifier (LR9) [26], a\\n5-feature support vector machine (SVM5) [27], a5-feature LRclassifier (LR5) [27], a10-fea-\\nture LRclassifier (LR10) [27], and a10-feature support vector machine (SVM10) [27].\\nTwo ofthese 8classifiers have been independently tested in4separate analyses. Inapro-\\nspective head-to-head comparison between theclinical outcome and ADTree7 (measured\\nprior totheclinical evaluation and official diagnosis) on222children (NASD=69;Ncontrols =\\n153; median age=5.8years), theperformance, measured astheunweighted average recall\\n(UAR [31]; themean ofthesensitivity and specificity), was84.8% [24]. Separately, Bone and\\ncolleagues [32] tested theADTree7 ona“Balanced Independent Dataset” (BID) consisting of\\nADI-R outcome data from 680participants (462 ASD, mean age=9.2years, SD=3.1years)\\nand 218non-ASD (mean age=9.4years, SD=2.9years) and found theperformance tobe\\nsimilarly high at80%. Duda and colleagues [25] tested theADTree8 with 2,333 individuals\\nwith autism (mean age=5.8years) and 283“non-autism” control individuals (mean age=6.4\\nyears) and found theperformance tobe90.2%. Bone and colleagues [32] also tested this\\nADTree8 model in1,033 participants from theBID—858 autism (mean age=5.2years,\\nSD=3.6years), 73autism spectrum (mean age=3.9years, SD=2.4years), and 102non-spec-\\ntrum (mean age=3.4years, SD=2.0years)—and found theperformance tobeslightly higher\\nat94%. These independent validation studies report classifier performance intherange ofthe\\npublished testaccuracy and lend additional support tothehypothesis that models using mini-\\nmal feature setsarereliable and accurate forautism detection.\\nOthers have runsimilar training and testing experiments toidentify top-ranked features\\nfrom standard instrument data, including Bone [33] and Bussu [34]. These approaches have\\narrived atsimilar conclusions, namely that machine learning isaneffective way tobuild objec-\\ntive, quantitative models with fewfeatures todistinguish mild-, medium-, and high-severity\\nautism from children outside oftheautism spectrum, including those with other developmen-\\ntaldisorders. However, thetranslation ofsuch models into clinical practice requires additional\\nsteps that have notyetbeen adequately addressed. Although some ofourearlier work has\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 4/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 3}),\n",
       "  Document(page_content='shown that untrained video annotators canmeasure autism behaviors onhome videos with\\nhigh interrater reliability and accuracy [35], thequestion ofwhat steps must betaken tomove\\nfrom minimal behavioral models into clinical practice remains.\\nThe present study builds onthisprior work toaddress thisquestion and thehypothesis that\\nfeatures represented inourminimal viable classifiers canbelabeled quickly, accurately, and\\nreliably from short home videos byvideo raters with noofficial training inautism diagnosis or\\nchild development. Wedeployed crowdsourcing and real-time video analysis forfeature label-\\ningtorunand evaluate theaccuracy ofthe8machine learning models trained todetect autism\\nin2independent home video repositories. This procedure enabled ustotesttheability to\\nreduce topractice theprocess ofrapid mobile video analysis asaviable method foridentifying\\nautism symptoms and screening. Inaddition, asthemobile feature tagging ofvideos automati-\\ncally generates arich feature matrix, itpresents theopportunity totrain anew artificial intelli-\\ngence model that haspotentially higher generalizability tothetask ofautomatic detection of\\nautism inshort video clips. Wetestthisrelated hypothesis byconstructing anovel video fea-\\nture classifier and comparing itsresults toalternative models inaheld-out subset oftheorigi-\\nnalvideo feature matrix and inanindependent external validation set.The results from this\\nwork support thehypothesis that autism detection canbedone from mobile devices outside of\\nclinical settings with high efficiency and accuracy.\\nMethods\\nSource classifiers forreduce-to-practice testing\\nWeassembled 8published machine learning classifiers totestviability foruseintherapid\\nmobile detection ofautism inshort home videos. Forallofthe8models, thesource oftraining\\nand validation data wasmedical records generated through theadministration ofoneoftwo\\ngold-standard instruments inthediagnosis ofautism, theADOS ortheADI-R. The ADOS has\\nseveral modules containing approximately 30features that correspond todevelopmental level\\noftheindividual under assessment. Module 1isused onindividuals with limited ornovocabu-\\nlary. Module 2isused onindividuals who usephrase speech butwho arenotfluent. Module 3\\nisused onindividuals who arefluent speakers. The ADI-R isaparent-directed interview that\\nincludes>90 elements each asked oftheparent, with multiple choices foranswers. Each\\nmodel wastrained onitem-level outcomes from theadministration ofeither theADOS and\\nADI-R and optimized foraccuracy, sparsity offeatures, and interpretability.\\nForthepurpose ofbrevity without omission ofdetail, weopted tocreate anabbreviation\\nforeach model using abasic naming convention. This abbreviation took theform of“model_-\\ntype”-“number offeatures.” Forexample, weused ADTree8 torefer totheuseofanalternating\\ndecision tree (ADTree) with 8features developed from medical data from theadministration\\nofthediagnostic instrument ADOS Module 1,and LR5 torefer totheLRwith 5behavioral\\nfeatures developed from analysis ofADOS Module 2medical record data, and soon.\\nADTree7. We(Wall and colleagues [29]) applied machine learning toelectronic medical\\nrecord data recorded through theadministration oftheADI-R inthediagnostic assessment of\\nchildren atrisk forautism. Weused an80%:20% training and testing split and performed\\n10-fold cross-validation forasample of891children with autism and 75non-autism control\\nparticipants with anADTree model containing 7features. The ADTree uses boosting toman-\\nageclass imbalance [36,37]. Wealso performed up-sampling through 1,000 bootstrap permu-\\ntations tomanage class imbalance. The model wasvalidated inaclinical trial on222\\nparticipants [24] and inaBID consisting of680individuals (462 with autism) [32]. The lowest\\nsensitivity and specificity exhibited were 89.9 and 79.7, respectively (UAR =84.8%).\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 5/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 4}),\n",
       "  Document(page_content='ADTree8. We[30] used adataset ofscore sheets from ADOS Module 2for612children\\nwith ASD and 15non-autism control participants with a90%:10% training and testing split\\nand 10-fold cross-validation totrain and testanADTree model with 8ofthe29Module 2fea-\\ntures. The ADTree uses boosting and hasinherent robustness toclass imbalance [36,37]. We\\nalso performed up-sampling through 1,000 bootstrap permutations totestthesensitivity of\\nmodel performance toclass imbalance. This 8-feature ADTree model wasindependently\\ntested on446individuals with autism byWall and colleagues [30], on2,333 individuals with\\nautism and 238without autism byDuda and colleagues [25], and on1,033 individuals (858\\nautism, 73autism spectrum, 102non-spectrum) byBone and colleagues [32]. The lowest sensi-\\ntivity and specificity reported were 97.1% and 83.3%, respectively (UAR =90.2%).\\nLR9. We[26] performed training with ADOS Module 2records on362individuals with\\nautism and 282individuals without autism with backward feature selection and iterative\\nremoval ofthesingle lowest-ranked feature across 10folds each with a90%:10% class split.\\nClasses were weighted inversely proportional toclass sizetomanage imbalance. The model\\nwith thehighest sensitivity and specificity and lowest number offeatures, LRwith L1regulari-\\nzation and 9features, wasselected fortesting. Wetested themodel onindependent data from\\n1,089 individuals with autism and 66individuals with noautism diagnosis. The lowest sensitiv-\\nityand specificity identified were 98.8% and 89.4%, respectively (UAR =94.1%).\\nSVM12. We[26] used score sheets from ADOS Module 3generated bytheevaluation of\\n510children with ASD and 93non-ASD control participants. These data were split into a90%\\ntraining and 10% testing set.Training and parameter tuning were performed with stepwise\\nbackward feature selection and iterative removal ofthesingle lowest-ranked feature across 10\\nfolds. Classes were weighted inversely proportional toclass sizetomanage imbalance. Several\\nmodels were fittoeach ofthefeature cross-validation folds. The model with thehighest sensi-\\ntivity and specificity and lowest number offeatures, aSupport Vector Machine (SVM) with a\\nradial basis function, wasthen applied tothetestsettomeasure generalization error. Wetested\\nthemodel on1,924 individuals with autism and 214individuals who didnotqualify foran\\nautism diagnosis. The lowest sensitivity and specificity identified onthetestsetwere 97.7%\\nand 97.2%, respectively (UAR =97.5%).\\nLR5 and SVM5. Inthisexperiment, we[27] used medical records generated through the\\nadministration ofADOS Module 2for1,319 children with autism and 70non-autism control\\nparticipants. The dataset wassplit 80%:20% into train and testsets, with thesame proportion\\nforparticipants with and without ASD ineach set.Class imbalance wasmanaged bysetting\\nclass weights inversely proportional totheclass sizes. A10-fold cross-validation wasused to\\nselect features, and aseparate 10-fold cross-validation wasrunforhyperparameter tuning\\nprior totesting theperformance. AnSVM and anLRmodel with L1regularization showed the\\nhighest testperformance with 5features. The lowest sensitivity and specificity exhibited onthe\\ntestsetforSVM5 were 98% and 58%, respectively, (UAR =78%) and 93% and 67%, respec-\\ntively, (UAR =80%) forLR5.\\nLR10 and SVM10. Inthisexperiment, we[27] used medical records generated through\\ntheadministration ofADOS Module 3for2,870 children with autism and 273non-autism\\ncontrol participants. The dataset wassplit 80%:20% into train and testsets, with thesame pro-\\nportion forparticipants with and without ASD ineach set.Class imbalance wasmanaged by\\nsetting class weights inversely proportional totheclass sizes. A10-fold cross-validation was\\nused toselect features, and aseparate 10-fold cross validation wasrunforhyperparameter tun-\\ningprior totesting theperformance. AnSVM and anLRmodel with L1regularization showed\\nthehighest testperformance with 10features. The lowest sensitivity and specificity exhibited\\nontheindependent testsetforSVM10 were 95% and 87%, respectively, (UAR =91%) and\\n90% and 89%, respectively, (UAR =89.5%) forLR10.\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 6/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 5}),\n",
       "  Document(page_content='Accounting foroverlap inthefeatures selected, these 8models measure 23unique features\\nintotal. The testaccuracy foreach model was>90%. Allmodels contain approximately 90%\\nfewer questions than theADI-R and 70%–84% fewer questions than thetotal features mea-\\nsured within theADOS. Anadditional 7features were chosen fortheir potential diagnostic\\nvalue and scored byvideo raters toassess their suitability forscoring home videos, creating a\\ntotal of30features forthemobile video rating process described below (Fig 1).\\nRecruitment and video collection\\nUnder anapproved Stanford University IRB protocol, wedeveloped amobile portal tofacili-\\ntatethecollection ofvideos ofchildren with ASD, from which participants electronically con-\\nsented toparticipate and upload their videos. Participants were recruited viacrowdsourcing\\nmethods [38–41] targeted atsocial media platforms and listservs forfamilies ofchildren with\\nautism. Interested participants were directed toasecure and encrypted video portal website to\\nconsent toparticipate. Werequired participants tobeatleast 18years ofageand theprimary\\ncare provider(s) forachild with autism between theages of12months and 17years. Partici-\\npants provided videos either through direct upload totheportal orviareference toavideo\\nalready uploaded toYouTube together with age, diagnosis, and other salient characteristics.\\nWeconsidered videos eligible ifthey (1)were between 1and 5minutes inlength, (2)showed\\ntheface and hands ofthechild, (3)showed clear opportunities forordirect social engagement,\\nand (4)involved opportunities fortheuseofanobject such asautensil, crayon, ortoy.\\nFig1.Feature- to-classifie rmapping. Video analysts scored each video with 30features. This matrix shows which feature\\ncorrespond stowhich classifier. Darker colored features indicate higher overlap, and lighter colors indicate lower overlap\\nacross themodels. The features arerank ordered according totheir frequency ofuseacross the8classifiers. Further details\\nabout theclassifiers areprovid edinTable 1.The bottom 7features were notpart ofthemachine learning process butwere\\nchosen because oftheir potential relations hipwith theautism phenotype and foruseinfurther evaluation ofthemodels’\\nfeature setswhen constructi ngavideo feature–sp ecific classifier. ADTree 7,7-feature alternating decision tree; ADTree8,\\n8-feature alternatin gdecision tree; LR5, 5-feature logistic regressio nclassifier; LR10, 10-feature logistic regressio nclassifier;\\nSVM5, 5-feature suppor tvector machine; SVM10, 10-feat uresupport vector machine; SVM12, 12-feature support vector\\nmachine.\\nhttps://doi. org/10.1371/j ournal.pmed. 1002705.g001\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 7/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 6}),\n",
       "  Document(page_content='Werelied onself-reported information provided bytheparents concerning thechild’s offi-\\ncialdiagnosis ofautism ornon-autism, theageofthechild when thevideo wassubmitted, and\\nadditional demographic information forvideos that were submitted directly totheweb portal.\\nForvideos that were provided viaYouTube URLs, weused YouTube metatags toconfirm the\\nageand diagnosis ofthechild inthevideo. Ifavideo didnotinclude ametatag fortheageof\\nthechild inthevideo, theagewasassigned following fullagreement among theestimates\\nmade by3clinical practitioners inpediatrics. Toevaluate theaccuracy oftheparents’ self-\\nreport and tosafeguard against reporting biases, wecommissioned apracticing pediatric spe-\\ncialist certified toadminister theADOS toreview arandom selection of20videos. Wealso\\ncommissioned adevelopmental pediatrician toreview anonoverlapping random selection of\\n10additional videos. These clinical experts classified each video as“ASD” or“non-ASD.”\\nFeature tagging ofvideos torun machine learning models\\nWeemployed atotal of9video raters who were either students (high school, undergraduate,\\norgraduate-level) orworking professionals. None hadtraining orcertification fordetection or\\ndiagnosis ofautism. Allwere given instructions onhow totagthe30questions and were asked\\ntoscore 10example videos before performing independent feature tagging ofnew videos.\\nTable 1.Eight machine learning classifier sused forvideo analysis and autism detection .The models were constructe dfrom ananalysis ofarchived medical records\\nfrom theuseofstandard instruments, including theADOS and theADI-R. All8models identified asmall, stable subset offeatures incross-validat ionexperime nts.The\\ntotal numbers ofaffected and unaffected control participants fortraining and testing areprovided together with measures ofaccuracy onthetestset.Four models were\\ntested onindependen tdatasets and have been mentioned inaseparate “Test” categor y.The remaining 4,indicated with “Train/test ,”used thegiven dataset with an\\n80%:20% train:test split tocalculate testaccuracy onthe20% held-out testset.The naming convention oftheclassifiers is“model type”-“numb eroffeatures”.\\nClassifier Medical record\\nsource#features NASD Nnon-ASD Mean age\\n(SD)%Male (N) Test\\nsensitivi tyTest\\nspecificityTest\\naccuracy\\nADTree8 [30] ADOS\\nModule 18 Train: 612 Train:15 6.16\\n(4.16)76.8%\\n(N=2,009)100% 100% 100%\\nTest [30]: 446 Test [30]: 0\\nTest [25]:\\n2,333Test [25]:\\n238\\nTest [32]: 931 Test [32]:\\n102\\nADTree7 [29] ADI-R 7 Train: 891 Train: 75 8.5(3.3) 65%\\n(N=628)100% 1.13% 99.9%\\nTest [24]: 222 Test [24]: 0\\nTest [32]: 462 Test [32]:\\n218\\nSVM with L1norm\\n(SVM5) [27]ADOS\\nModule 25 Train/test :\\n1,319Train/t est:\\n706.92\\n(2.83)80%\\n(N=1,101)98% 58% 98%\\nLRwith L2norm (LR5)\\n[27]ADOS\\nModule 25 Train/test :\\n1,319Train/t est:\\n706.92\\n(2.83)80%\\n(N=1,101)93% 67% 95%\\nLRwith L1norm (LR9)\\n[26]ADOS\\nModule 29 Train: 362 Train: 282 11.75\\n(10)76.4%\\n(N=1,375)98.81% 89.39% 98.27%\\nTest: 1,089 Test: 66\\nRadial kernel SVM\\n(SVM12) [26]ADOS\\nModule 312 Train: 510 Train: 93 16.25\\n(11.58)76.4%\\n(N=2,094)97.71% 97.2% 97.66%\\nTest: 1,924 Test: 214\\nLinear SVM (SVM10) [27] ADOS\\nModule 310 Train/test :\\n2,870Train/t est:\\n2739.08\\n(3.08)81%\\n(N=2,557)95% 87% 97%\\nLR(LR10) [27] ADOS\\nModule 310 Train/test :\\n2,870Train/t est:\\n2739.08\\n(3.08)81%\\n(N=2,557)90% 89% 94%\\nAbbreviation s:ADI-R, Autism Diagnostic Interview -Revised; ADOS, Autism Diagnostic Observation Schedule; ADTr ee7, 7-feature alternating decision tree; ADTree8,\\n8-feature alternating decision tree; LR,logistic regression ;LR5, 5-feature LRclassifier; LR10, 10-feature LRclassifier; SVM, support vector machine; SVM5, 5-feature\\nSVM; SVM10, 10-feature SVM; SVM12, 12-feature SVM.\\nhttps://do i.org/10.1371/j ournal.pm ed.1002705.t0 01\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 8/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 7}),\n",
       "  Document(page_content='After training, weprovided theraters with unique usernames and passwords toaccess the\\nsecure online portal towatch videos and answer 30questions foreach video needed bythefea-\\nture vectors torunthe8machine learning classifiers (Table 1).Features were presented tothe\\nvideo raters asmultiple-choice questions written atanapproximately seventh-grade reading\\nlevel. The raters, who remained blind todiagnosis throughout thestudy, were tasked tochoose\\noneofthetags foreach feature that best described thechild’s behavior inthevideo. Each\\nresponse toafeature wasthen mapped toascore between 0and 3,with higher scores indicat-\\ningmore severe autism features inthemeasured behavior, or8toindicate that thefeature\\ncould notbescored. The behavioral features and theoverlap across themodels areprovided in\\nFig1.\\nTotesttheviability offeature tagging videos forrapid machine learning detection and diag-\\nnosis ofautism, weempirically identified aminimum number ofvideo raters needed toscore\\nparent-provided home videos. Weselected arandom subset ofvideos from thefullsetofvid-\\neoscollected through ourcrowdsourced portal and rantheADTree8 [30] model onfeature\\nvectors tagged byall9raters. Wechose torunonly ADTree8 forefficiency reasons and\\nbecause thismodel hasbeen previously validated in2independent studies [25,32]. Weused a\\nsample-with-replacement permutation procedure tomeasure accuracy asafunction ofmajor-\\nityrater agreement with thetrue diagnostic classification. Weincrementally increased the\\nnumber ofvideo raters pertrial by1rater, starting with 1and ending with 9,drawing with\\nreplacement 1,000 times pertrial. When considering only 2raters, werequired perfect class\\nagreement between theraters. With anodd number ofraters, werequired astrict majority\\nconsensus. When aneven number ofraters disagreed onclassification, weused anindepen-\\ndent and randomly chosen rater’s score tobreak thetie.\\nAfter determining theminimally viable number ofvideo raters, weused that minimum to\\ngenerate thefullsetof30-feature vectors onallvideos. Seven ofthemodels were written in\\nPython 3using thepackage scikit-learn, and onewaswritten inR.Weranthese 8models on\\nourfeature matrices after feature tagging onvideos. Wemeasured themodel accuracy through\\ncomparison oftheraters’ majority classification result with thetrue diagnosis. Weevaluated\\nmodel performance further byagecategories:�2years,>2to�4years,>4years to�6years,\\nand>6years. Foreach category, wecalculated accuracy, sensitivity, and specificity.\\nWecollected timed data from each rater foreach video, which began when avideo rater\\npressed “play” onthevideo and concluded when avideo rater finished scoring byclicking\\n“submit” onthevideo portal. Weused these time stamps tocalculate thetime spent annotating\\neach video. Weapproximated thetime taken toanswer thequestions byexcluding thelength\\nofthevideo from thetotal time spent toscore avideo.\\nBuilding avideo feature classifier\\nThe process ofvideo feature tagging provides anopportunity togenerate acrowdsourced col-\\nlection ofindependent feature measurements that arespecific tothevideo ofthechild aswell\\nasindependent rater impressions ofthat child’s behaviors. This inturn hastheability togener-\\nateavaluable feature matrix todevelop models that include video-specific features rather than\\nfeatures identified through analysis onarchived data generated through administration ofthe\\nSOC (asisthecase forallclassifiers contained inTable 1).Tothisend, and following thecom-\\npletion oftheannotation onallvideos bytheminimum number ofraters, weperformed\\nmachine learning onourvideo feature set.Weused LRwith anelastic netpenalty [42] (LR-\\nEN-VF) topredict theautism class from thenon-autism class. Werandomly split thedataset\\ninto training and testing, reserving 20% forthelatter while using cross-validation onthetrain-\\ningsettotune forhyperparameters. Weused cross-validation formodel hyperparameter\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 9/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 8}),\n",
       "  Document(page_content='tuning byperforming agrid search with different values ofalpha (varying penalty weights)\\nand L1ratio (the mixing parameter determining how much weight toapply toL1versus L2\\npenalties). Based ontheresulting area under thecurve (AUC) and accuracy from each combi-\\nnation, weselected thetop-performing pair ofhyperparameters. Using thispair, wetrained\\nthemodel using LRand balanced class weights toadjust weights inversely proportional to\\nclass frequencies intheinput data. After determining thetop-ranked features based onthe\\ntrained model and theresulting coefficients, wevalidated themodel onthereserved testset.\\nIndependent test setforvalidation ofvideo phenotyping processes\\nWeused ourvideo portal and crowdsourcing approaches togenerate anindependent collec-\\ntion ofvideos forevaluation and feature tagging by3different raters than those used inthepri-\\nmary analysis. These raters hadsimilar characteristics totheoriginal group (age, education, no\\nclinical certifications indevelopmental pediatrics) and were trained forvideo tagging through\\nthesame procedures.\\nEthics statement\\nThis study wasconducted under approval byStanford University’s IRB under protocol IRB-\\n31099. Informed and written consent wasobtained from allstudy participants who submitted\\nvideos tothestudy.\\nResults\\nAllclassifiers used fortesting thetime and accuracy ofmobile video rating hadaccuracies\\nabove 90% (Table 1).The union offeatures across these 8classifiers (Table 1)was23(Fig 1).\\nThese features plus anadditional 7chosen forclinical validity testing were loaded into a\\nmobile video rating portal toenable remote feature tagging bynonclinical video raters.\\nWecollected atotal of193videos (Table 2)with average video length of2minutes 13sec-\\nonds (SD =1minute 40seconds). Ofthe119ASD videos, 72were direct submissions made by\\nTable 2.Demographi cinformation onchildr eninthecollected home videos. WecollectedN=193(119 ASD, 74non-ASD) home videos foranalysis. Weexcluded 31\\nvideos because ofinadequate labeling orvideo quality. Weused arandoml ychosen 25autism and 25non-autism videos toempirical lydefine anoptimal number ofraters.\\nVideo feature tagging formachine learning wasthen done on162home videos.\\nVideos NNASDNnon-\\nASDMean age\\n(SD)�2years>2years and\\n�4years>4years and\\n�6years>6years Percent male,\\nASDPercent male,\\nnon-ASD\\nTotal 193 119 74 4years 4\\nmonths\\n(2years 1\\nmonth)24.4%\\n(n=47)33.7% (n=65) 25.9% (n=50) 15.5%\\n(n=31)39.38%\\n(n=76)23.32% (n=45)\\nExcluded 31 3 28 3years 8\\nmonths\\n(1year 11\\nmonths )32.3%\\n(n=10)29.0% (n=9) 25.8% (n=8) 9.6%\\n(n=4)3.23% (n=1) 58.06% (n=18)\\nTotal videos used foranalysis ofall\\n8classifie rs162 116 46 4years 4\\nmonths\\n(2years 2\\nmonths )22.8%\\n(n=37)34.5% (n=56) 25.9% (n=42) 16.7%\\n(n=27)67.2%\\n(n=78)56.5% (n=26)\\nSubset ofvideos used tofind\\nminimally viable number ofraters50 25 25 4years 6\\nmonths\\n(2years 4\\nmonths )28%\\n(n=14)34% (n=17) 18% (n=9) 20%\\n(n=10)48%\\n(n=12)44% (n=11)\\nAbbreviation :ASD, autism spectrum disorder.\\nhttps://do i.org/10.1371/j ournal.pm ed.1002705.t0 02\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 10/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 9}),\n",
       "  Document(page_content='theprimary caregiver ofthechild, and 47were links toanexisting video onYouTube. Ofthe\\n74non-ASD videos, 46non-ASD videos were links toexisting YouTube videos, and 28were\\ndirect submissions from theprimary caregiver. Weexcluded 31videos because ofinsufficient\\nevidence forthediagnosis (n=25)orinadequate video quality (n=6),leaving 162videos (116\\nwith ASD and 46non-ASD) which were loaded into ourmobile video rating portal forthepri-\\nmary analysis. Tovalidate self-reporting ofthepresence orabsence ofanASD diagnosis, 2\\nclinical staff trained and certified inautism diagnosis evaluated arandom selection of30videos\\n(15with ASD and 15non-ASD) from the162videos. Their classifications hadperfect corre-\\nspondence with thediagnoses provided through self-report bytheprimary caregiver.\\nWerandomly selected 50videos (25ASD and 25non-ASD) from thetotal 162collected\\nvideos and had9raters feature tagallinaneffort toevaluate thepotential foranoptimal num-\\nberofraters, with optimal being defined through abalance ofscalability and information con-\\ntent. The average video length ofthisrandom subset was1minute 54seconds (SD =46\\nseconds) fortheASD class and 2minutes 36seconds (SD =1minute 15seconds) forthenon-\\nASD class. Wethen rantheADTree8 (Table 1)model onthefeature vectors generated bythe\\n9raters. Wefound thedifference inaccuracy tobestatistically insignificant between 3raters—\\ntheminimum number tohave amajority consensus ontheclassification with noties—and 9\\nFig2.Accurac yacross different permutation sof9raters for50videos. Weperformed theanalysis todetermine theoptimal number (the minim umnumber toreach\\naconsensus onclassificati on)ofvideo raters needed tomaintain accuracy without lossofpower. Nine raters analyzed and genera tedfeature tags forasubset ofn=50\\nvideos (n=25ASD,n=25non-ASD) onwhich werantheADTree 8classifier (Table 1).The increase inaccuracy conferre dbytheuseof3versus 9raters wasnot\\nsignificant .Wetherefore settheoptimal rater number to3forsubsequ entanalyse s.ADTree8, 8-feature alternating decision tree; ASD, autism spectru mdisorder.\\nhttps://doi.o rg/10.1371/j ournal.pmed.1 002705.g002\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 11/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 10}),\n",
       "  Document(page_content='raters (Fig 2).Wetherefore elected tousearandom selection of3raters from the9tofeature\\ntagall162crowdsourced home videos.\\nModel performance\\nThree raters performed video screening and feature tagging togenerate vectors foreach ofthe\\n8machine learning models forcomparative evaluation ofperformance (Fig 3).Allclassifiers\\nhadsensitivity>94.5%. However, only 3ofthe8models exhibited specificity above 50%. The\\ntop-performing classifier wasLR5, which showed anaccuracy of88.9%, sensitivity of94.5%,\\nand specificity of77.4%. The next-best-performing models were SVM5 with 85.4% accuracy\\n(54.9% specificity) and LR10 with 84.8% accuracy (51% specificity).\\nLR5 exhibited high accuracy onallageranges with theexception ofchildren over 6years\\nold(although note that wehadlimited examples ofnon-ASD [n=1]class inthisrange). This\\nmodel performed best onchildren between theages of4and 6years, with sensitivity and speci-\\nficity both above 90% (Fig 4,Table 3).SVM5 and LR10 showed anincrease inperformance on\\nchildren ages 2–4years, both with 100% sensitivity and theformer with 66.7% and thelatter\\nwith 58.8% specificity. The 3raters agreed unanimously on116outof162videos (72%) when\\nusing thetop-performing classifier, LR5. The interrater agreement (IRA) forthismodel was\\nabove 75% inallageranges with theexception oftheyoungest agegroup ofchildren, those\\nFig3.Overall procedure forrapid and mobile classificat ionofASD versus non-ASD and performa nceofmodels from Table 1.Participa ntswere recruited to\\nparticipate viacrowdsourci ngmethods and provid edvideo bydirect upload orviaapreexisti ngYouTub elink. The minimum formajority rules of3video raters\\ntagged allfeatures, genera ting feature vectors toruneach ofthe8classifiers automatica lly.The sensitivity and specificit ybased onmajority outcome generated bythe\\n3raters on162(119 with autism) videos areprovided. Highligh tedinyellow isthebest performing model, LR5. ADTree7 ,7-feature alternatin gdecision tree;\\nADTree8, 8-feature alternating decision tree; ASD, autism spectru mdisorder; LR5, 5-feature logistic regression classifier; LR9, 9-feature logistic regression classifier;\\nLR10, 10-feature logistic regressio nclassifier; SVM5, 5-feature support vector machine; SVM10, 10-feature support vector machine; SVM12, 12-feature suppor t\\nvector machine.\\nhttps://doi.org/10 .1371/journal.p med.1002705 .g003\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 12/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 11}),\n",
       "  Document(page_content='under 2years, forwhich there wasagreater frequency ofdisagreement. The numbers ofnon-\\nASD representatives were small fortheolder ageranges evaluated (Table 3).\\nThe median time forthe3raters towatch and score avideo was4minutes (Table 4).\\nExcluding thetime spent watching thevideo, raters required amedian of2minutes 16seconds\\ntotagall30features intheanalyst portal. Wefound asignificant difference (p=0.0009)\\nbetween theaverage time spent toscore thevideos ofchildren with ASD and theaverage time\\nspent toscore thenon-ASD videos (6minutes 36seconds compared with 5minutes 8\\nseconds).\\nIndependent validation\\nTovalidate thefeasibility and accuracy ofrapid feature tagging and machine learning onshort\\nhome videos, welaunched asecond effort forcrowdsourcing videos ofchildren with and with-\\noutautism togenerate anindependent replication dataset. Wecollected 66videos, 33ofchil-\\ndren with autism and 33non-ASD. This setofvideos wascomparable totheinitial setof162\\nvideos interms ofgender, age, and video length. The average ageforchildren with ASD was4\\nyears 5months (SD =1year 9months), and theaverage agefornon-ASD children was3years\\n11months (SD =1year 7months). Forty-two percent (n=14)ofthechildren with ASD were\\nmale and 45% (n=15)ofthenon-ASD children were male. The average video length was3\\nminutes 24seconds, with anSDof45seconds. Forthisindependent replication, weused 3dif-\\nferent raters, each with noofficial training orexperience with developmental pediatrics. The\\nraters required amedian time of6minutes 48seconds forcomplete feature tagging. LR5 again\\nyielded thehighest accuracy, with asensitivity of87.8% and aspecificity of72.7%. Atotal of13\\nofthe66videos were misclassified, with 4false negatives.\\nFig4.Performance forLR5 byage. LR5 exhibited thehighest classifier performan ce(89% accuracy) outofthe8classifiers tested (Table 1).This model\\nperformed best onchildren between theages of2and 6years. (A)shows theperforman ceofLR5 across 4ageranges, and (B)provides theROC curve forLR5’s\\nperforman ceforchildren ages 2to6years. Table 3provides addition aldetails, includin gthenumber ofaffected and unaffected control participants within each\\nagerange. AUC, area under thecurve; LR5, 5-feature logistic regression classifier; ROC, receiver operating characte ristic.\\nhttps://do i.org/10.1371/j ournal.pm ed.1002705. g004\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 13/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 12}),\n",
       "  Document(page_content='Given thehigher average time forvideo evaluation, wehypothesized that thevideos con-\\ntained challenging displays ofautism symptoms. Therefore, weexamined theprobabilities\\ngenerated bytheLR5 model forthe13misclassified videos. Two ofthe4false negatives and 4\\nofthe9false positives hadborderline probabilities scores between 0.4and 0.6.Weelected to\\ndefine aprobability threshold between 0.4and 0.6toflagvideos asinconclusive cases. Twenty-\\nsixofthe66videos fellwithin thisinconclusive group when applying thisthreshold. When we\\nexcluded these 26from ouraccuracy analysis, thesensitivity and specificity increased to91.3%\\nand 88.2%, respectively.Table 3.Model performan cebyage. This table details theaccuracy ,sensitivity, specificity, precision, andrecall for8classifiers (Table 1)and for4ageranges found in\\nevaluation of162home videos with anaverage length of2minutes. Wealso provide theIRA, which indicates thefrequency with which themodel results from all3raters’\\nfeature tags agreed onclass. The top-perform ingclassifier wasLR5, which yielded anaccuracy of88.9%, sensitivity of94.5%, andspecificity of77.4%. Other notable classifi-\\nerswere SVM5 and LR10, which yielded 85.4% and 84.8% accurac y,respective ly.These 3best-perfo rming classifiers showed improved classification power within certain\\nageranges.\\nAge group Statistic ADTree 8 ADTree7 SVM5 LR5 LR9 SVM1 2 SVM10 LR10\\nOverall\\n(116 ASD, 46non-ASD, 64.4% male)Sensitivity 100% 94.5% 100% 94.5% 100% 100% 100% 100%\\nSpecificity 22.4% 37.3% 54.9% 77.4% 31.4% 0% 17.6% 51.0%\\nAccuracy 76.1% 76.3% 85.4% 88.9% 78.4% 71.6% 73.9% 84.8%\\nIRA 76.1% 67.5% 68.4% 71.7% 75.9% 71.6% 79.5% 70.9%\\nPrecision 74.3% 76.3% 82.3% 89.7% 76.0% 71.6% 72.4% 82.0%\\nUAR 61.2% 65.9% 77.5% 86.0% 65.7% 50% 58.8% 75.5%\\n�2years\\n(17ASD, 20non-ASD, 56.8% male)Sensitivity 100% 100% 100% 93.3% 100% 100% 100% 100%\\nSpecificity 14.2% 18.2% 38.1% 77.3% 22.7% 0% 14.3% 47.6%\\nAccuracy 50% 51.4% 62.9% 83.8% 54.1% 40.5% 50.0% 96.2%\\nIRA 53% 48.6% 45.7% 51.4% 48.6% 100% 66.7% 100%\\nPrecision 45.5% 45.5% 51.9% 73.7% 46.9% 45.9% 45.4% 57.7%\\nUAR 57.1% 59.1% 69.1% 85.3% 61.4% 50.0% 57.2% 73.8%\\n>2years and�4years\\n(39ASD, 17non-ASD, 66.1% male)Sensitivity 100% 97.3% 100% 91.8% 100.0% 100% 100% 100%\\nSpecificity 23.6% 50% 66.7% 73.7% 38.9% 0% 22.2% 58.8%\\nAccuracy 76.4% 50% 88.9% 85.7% 80.4% 66.7% 74.5% 86.8%\\nIRA 74.5% 81.8% 63.0% 75.0% 76.8% 100% 85.4% 77.4%\\nPrecision 74.5% 80.0% 85.7% 87.2% 77.6% 69.6% 72.5% 83.7%\\nUAR 61.8% 73.7% 83.4% 82.8% 69.5% 50.0% 61.1% 79.4%\\n>4years and�6years\\n(34ASD, 8non-ASD, 61.9% male)Sensitivity 100% 96.8% 100% 96.9% 100.0% 100% 100% 100%\\nSpecificity 40.0% 60% 72.7% 90.9% 40.0% 0% 18.2% 50.0%\\nAccuracy 85.4% 87.8% 92.9% 95.3% 85.7% 74.4% 79.1% 88.1%\\nIRA 85.4% 78.0% 78.6% 79.1% 85.7% 93.0% 76.7% 71.4%\\nPrecision 83.8% 88.2% 91.2% 96.9% 84.2% 80.9% 78.0% 86.5%\\nUAR 70.0% 78.4% 86.4% 93.9% 70.0% 50.0% 59.1% 75.0%\\n>6years (26ASD, 1non-ASD, 74.1% male) Sensitivity 100% 84.6% 100% 96.2% 100% 100% 100% 100%\\nSpecificity 0% 0% 0% 0% 0% 0% 0% 0%\\nAccuracy 96.2% 81.5% 96.2% 92.6% 96.2% 96.2% 96.2% 96.3%\\nIRA 96.2% 70.4% 96.2% 81.5% 96.2% 100% 100% 85.2%\\nPrecision 96.3% 95.7% 96.3% 96.2% 96.3% 96.3% 96.3% 96.3%\\nUAR 50.0% 42.3% 50.0% 48.1% 50.0% 50.0% 50.0% 50.0%\\nAbbreviation s:ADTree7, 7-feature alternatin gdecision tree; ADTree8, 8-feature alternat ingdecision tree; ASD, autism spectrum disorder; IRA, interrater agreemen t;\\nLR5, 5-feature logistic regression classifier; LR9, 9-feature logistic regression classifier; LR10, 10-feature logistic regression classifier; SVM5, 5-feature support vector\\nmachine; SVM10, 10-feature support vector machin e;SVM12, 12-feature support vector machine; UAR, unweighted average recall.\\nhttps://do i.org/10.1371/j ournal.pm ed.1002705.t0 03\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 14/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 13}),\n",
       "  Document(page_content='Training avideo feature–specific classifier\\nTobuild avideo feature–specific classifier, wetrained anLR-EN-VF model on528(3rat-\\ners×176videos) novel measures ofthe30video features used todistinguish theautism class\\nfrom theneurotypical cohort. Out ofthese 176videos (ASD =121, non-ASD =58), 162\\n(ASD =116, non-ASD =46)were from theanalysis set,and 14videos (ASD =5,non-\\nASD =12)were from thesetof66validation videos. Model hyperparameters (alpha and L1\\nratio) identified through 10-fold cross-validation were 0.01 and 0.6,respectively. Weused a\\nhigh L1ratio toenforce sparsity and todecrease model complexity and thenumber offeatures.\\nWehadsimilar proportions (0.60) fornon-ASD and ASD measures inthetraining setand\\nheld-out testset,which allowed ustocreate amodel that generalizes well without asignificant\\nchange insensitivity orspecificity onnovel data. The model hadanarea under thereceiver\\noperating characteristic curve (AUC-ROC) of93.3% and accuracy of87.7% ontheheld-out\\ntestset.Acomparison ofLR-EN-VF with LRL2penalty (nofeature reduction) revealed similar\\nresults (AUC-ROC: 93.8%, testaccuracy: 90.7%) (Fig 5).The top-8 features selected bythe\\nmodel consisted ofthefollowing, inorder ofhighest tolowest rank: speech patterns, commu-\\nnicative engagement, understands language, emotion expression, sensory seeking, responsive\\nsocial smile, stereotyped speech. One ofthese 8features—sensory seeking—was notpart ofthe\\nfullsetsofitems onthestandard instrument data used inthedevelopment and testing ofthe8\\nmodels depicted inTable 1.Wethen validated thisclassifier ontheremaining 52videos\\n(ASD =28,non-ASD =21)from thevalidation set,and theresults showed anaccuracy of\\n75.5% and anAUC-ROC of86.0%.\\nDiscussion\\nPrevious work [26–29] hasshown that machine learning models built onrecords from stan-\\ndard autism diagnoses canachieve high classification accuracy with asmall number offea-\\ntures. Although promising interms oftheir minimal feature requirements and ability to\\ngenerate anaccurate riskscore, their potential forimproving autism diagnosis inpractice hasTable 4.Time require dformobile tagging ofvideo features needed torun themachine learning models. Wehighlight theaverage length ofvideos (allparticipan ts,\\nonly participants with ASD, and only participants without ASD) aswell astheaverage time required towatch and score thevideos and theaverage time requir edfrom start\\ntoendofthescoring compone ntalone.\\nTotal time required forreview and feature\\ntaggingTotal time required forfeature tagging\\naloneVideo length\\nOverall Mean\\n(SD)6minutes 9seconds (5minutes 28seconds) 3minutes 36seconds (5minutes 52\\nseconds)2minutes 13seconds (1minute 40\\nseconds)\\nMedian 4minutes 0seconds 2minutes 16seconds 1minute 45seconds\\nRange 1minute 0seconds to\\n37minutes 0seconds0minutes 50seconds to\\n35minutes 42seconds0minutes 25seconds to8minutes 6\\nseconds\\nASD only Mean\\n(SD)6minutes 36seconds (5minutes 54seconds) 4minutes 22seconds (6minutes 20\\nseconds)2minutes 4seconds (1minute 40\\nseconds)\\nMedian 5minutes 0seconds 2minutes 40seconds 1minute 30seconds\\nRange 1minute 0seconds to\\n37minutes 0seconds0minutes 50seconds to\\n35minutes 42seconds0minutes 25seconds to8minutes 6\\nseconds\\nNon-ASD\\nonlyMean\\n(SD)5minutes 8seconds (4minutes 8seconds) 2minutes 18seconds (4minutes 22\\nseconds)2minutes 38seconds (1minute 34\\nseconds)\\nMedian 4minutes 0seconds 1minute 21seconds 2minutes 11seconds\\nRange 1minute 0seconds to\\n30minutes 0seconds0minutes 50seconds to\\n25minutes 42seconds0minutes 36seconds to6minutes 42\\nseconds\\nAbbreviation :ASD, autism spectrum disorder.\\nhttps://do i.org/10.1371/j ournal.pm ed.1002705.t0 04\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 15/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 14}),\n",
       "  Document(page_content='remained anopen question. The present study tested theability toreduce these models tothe\\npractice ofhome video evaluation bynonexperts using mobile platforms (e.g., tablets, smart-\\nphones). Independent tagging of30features by3raters blind todiagnosis enabled majority\\nrules machine learning classification of162two-minute (average) home videos inamedian of\\n4minutes at90% AUC onchildren ages 20months to6years. This performance wasmain-\\ntained at89% AUC (95% CI81%–95%) inaprospectively collected and independent external\\nsetof66videos each with 3independent rater measurement vectors. Taking advantage ofthe\\nprobability scores generated bythebest-performing model (L1-regularized LRmodel with 5\\nfeatures) toflaglow-confidence cases, wewere able toachieve a91% AUC, suggesting that the\\napproach could benefit from theuseofthescores onamore quantitative scale rather than just\\nasabinary classification outcome.\\nByusing amobile format that canbeaccessed online, weshowed that itispossible toget\\nmultiple independent feature vectors forclassification. This hasthepotential toelevate confi-\\ndence inclassification outcome atthetime ofdiagnosis (i.e., when 3ormore agree onclass)\\nwhile fostering thegrowth ofanovel matrix offeatures from short home videos. Inthesecond\\npart ofourstudy, wetested theability forthisvideo feature matrix toenable development ofa\\nnew model that cangeneralize tothetask ofvideo-based classification ofautism. Wefound\\nthat an8-feature LRmodel could achieve anAUC of0.93 ontheheld-out subset and 0.86 on\\ntheprospective independent validation set.One ofthefeatures used bythismodel, sensory\\nseeking, wasnotused bytheinstruments onwhich theoriginal models were trained, suggest-\\ningthepossibility that alternative features may provide added power forvideo classification.\\nThese results support thehypothesis that thedetection ofautism canbedone effectively at\\nscale through mobile video analysis and machine learning classification toproduce aquanti-\\nfied indicator ofautism risk quickly. Such aprocess could streamline autism diagnosis to\\nenable earlier detection and earlier access totherapy that hasthehighest impact during earlier\\nwindows ofsocial development. Further, thisapproach could help toreduce thegeographic\\nand financial burdens associated with access todiagnostic resources and provide more equal\\nFig5.ROC curve forLR-EN-VF showin gperforma nceontest data along with anROC forL2loss with nofeature\\nreduction .The former chose 8outof30video features. AUC, area under thecurve; LR-EN-VF, logistic regression\\nwith anelastic netpenalty; ROC, receiver operating characterist ic.\\nhttps://d oi.org/10.1371/j ournal.pm ed.1002705. g005\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 16/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 15}),\n",
       "  Document(page_content='opportunity tounderserved populations, including those indeveloping countries. Further test-\\ningand refinement should beconducted toidentify themost viable method(s) ofcrowdsourc-\\ningvideo acquisition and feature tagging. Inaddition, prospective trials inundiagnosed and in\\nlarger, more-balanced cohorts including examples ofchildren with non-autism developmental\\ndelays willbeneeded tobetter understand theapproach’s potential foruseinautism diagnosis.\\nSupporting information\\nS1Table. Results of8classifiers onindependent validation set.LR10, LR5, and ADTree7\\narethetop-3 best-performing classifiers onthevalidation set,which falls inlinewith the\\nresults observed onthetestdataset of162videos used earlier. LR5 stillperforms with thehigh-\\nestspecificity outofthe8models. ADTree7, 7-feature alternating decision tree; LR5, 5-feature\\nlogistic regression classifier; LR10, 10-feature logistic regression classifier.\\n(DOCX)\\nS1Text. Instructions forvideo raters.\\n(DOCX)\\nS1Checklist. The tripod checklist.\\n(DOCX)\\nAcknowledgmen ts\\nWewould liketothank Kaitlyn Dunlap, theparticipating families, and each ofourvideo raters\\nfortheir important contributions tothisstudy.\\nAuthor Contributions\\nConceptualization: Dennis Paul Wall.\\nData curation: Qandeel Tariq, Jena Daniels, Jessey Nicole Schwartz, Peter Washington, Haik\\nKalantarian, Dennis Paul Wall.\\nFormal analysis: Qandeel Tariq, Peter Washington, Haik Kalantarian, Dennis Paul Wall.\\nFunding acquisition: Dennis Paul Wall.\\nInvestigation: Qandeel Tariq, Jena Daniels, Jessey Nicole Schwartz, Dennis Paul Wall.\\nMethodology: Qandeel Tariq, Jena Daniels, Dennis Paul Wall.\\nProject administration: Jena Daniels, Jessey Nicole Schwartz, Dennis Paul Wall.\\nResources: Jena Daniels, Jessey Nicole Schwartz, Peter Washington, Haik Kalantarian, Dennis\\nPaul Wall.\\nSoftware: Qandeel Tariq, Dennis Paul Wall.\\nSupervision: Dennis Paul Wall.\\nValidation: Qandeel Tariq, Dennis Paul Wall.\\nVisualization: Qandeel Tariq, Jessey Nicole Schwartz, Dennis Paul Wall.\\nWriting –original draft: Qandeel Tariq, Jessey Nicole Schwartz, Dennis Paul Wall.\\nWriting –review &editing: Qandeel Tariq, Jena Daniels, Jessey Nicole Schwartz, Peter Wash-\\nington, Haik Kalantarian, Dennis Paul Wall.\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 17/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 16}),\n",
       "  Document(page_content='References\\n1.Prince M,Patel V,Saxena S,MajM,Maselk oJ,Phillips MR,etal.Global mental health 1-Nohealth\\nwithout mental health. Lancet. 2007; 370(9590) :859–77. https:/ /doi.org/10.10 16/S014 0-6736(07)\\n61238-0 PMID: 178040 63\\n2.Baio J,Wiggins L,Christensen DL,Maenner MJ,Daniels J,Warren Z,etal.Prevalence ofAutism Spec-\\ntrum Disorder Among Children Aged 8Years—Auti smandDevelopm ental Disabilities Monitori ngNet-\\nwork, 11Sites, United States, 2014. MMWR Surveillanc eSummarie s.2018; 67(6):1. https:// doi.org/10.\\n15585/mmw r.ss6706a 1PMID: 29701730. PMCID: PMC5 919599.\\n3.Hertz-Piccio ttoI,Delwiche L.TheRise inAutism andtheRole ofAgeatDiagnosis. Epidem iology.\\n2009; 20(1):84–9 0.https://doi.or g/10.1097/ED E.0b013e318 1902d1 5PMID: 19234401. PMCID:\\nPMC411360 0.\\n4.Christensen DL,Baio J,VanNaarden Braun K,Bilder D,Charles J,Constantino JN,etal.Preval ence\\nandCharacter istics ofAutism Spectru mDisorder Among Children Aged 8Years–Autism andDevelop-\\nmental Disabilities Monitoring Networ k,11Sites, United States, 2012. MMWR Survei llSumm. 2016; 65\\n(3):1–23. https://doi.or g/10.155 85/mmwr. ss6503a1 PMID: 27031587.\\n5.Christensen DL,Bilder DA,Zahoro dnyW,Pettygrove S,Durkin MS,Fitzgerald RT,etal.Prevalence\\nandcharacter istics ofautism spectrum disorde ramong 4-year-old children intheautism anddevelop -\\nmental disabilities monitori ngnetwork. Journal ofDevelop mental &Behavioral Pediatrics .2016; 37\\n(1):1–8. https://doi.or g/10.109 7/DBP.00 00000000000 235PMID: 26651088.\\n6.Buescher AV,Cidav Z,Knapp M,Mandell DS.Costs ofautism spectrum disorders intheUnited King-\\ndom andtheUnited States. JAMA Pediatr. 2014; 168(8):721 –8.https://doi.or g/10.1001 /jamapediatric s.\\n2014.210 PMID: 24911948.\\n7.McPartlan dJC,Reichow B,Volkmar FR.Sensitivity andspecific ityofpropose dDSM-5 diagno sticcrite-\\nriaforautism spectrum disorder. JAmAcad Child Adolesc Psych iatry. 2012; 51(4):368– 83.https://doi.\\norg/10.1016/ j.jaac.2012. 01.007 PMID: 22449643. PMCID: PMC3 424065.\\n8.Lord C,Rutter M,Goode S,Heemsb ergen J,Jordan H,Mawhood L,etal.Austism diagnostic observa-\\ntionschedule: Astandard izedobservatio nofcommunic ative andsocial behavio r.Journal ofautism and\\ndevelop mental disorders .1989; 19(2):185– 212. PMID: 2745388.\\n9.Lord C,Rutter M,LeCouteur A.Autism Diagnostic Interview-R evised: arevised version ofadiagnosti c\\ninterview forcaregivers ofindividuals withpossible pervasive developme ntaldisorders .Journal of\\nautism anddevelop mental disorde rs.1994; 24(5):659– 85.PMID: 7814313.\\n10. Association AP.Diagnostic andstatistic almanual ofmental disorders (DSM-5® ).Arlington, VA:Ameri-\\ncanPsychiat ricPub; 2013.\\n11. Bernier R,Mao A,YenJ.Psychopa thology, families, andculture: autism. Child Adolesc Psychiatr Clin\\nNAm.2010; 19(4):855– 67.https://doi.or g/10.1016/ j.chc.2010.0 7.005 PMID: 21056350.\\n12. Dawson G.Early behavioral intervention ,brain plasticity ,andthepreventi onofautism spectrum disor-\\nder.DevPsychopatho l.2008; 20(3):775– 803. https://doi.or g/10.101 7/S09545794 080003 70PMID:\\n18606031.\\n13. Mazurek MO,Handen BL,Wodka EL,Nowinski L,Butter E,Engelha rdtCR.Ageatfirstautism spec-\\ntrum disorde rdiagnosis: theroleofbirth cohort, demogr aphic factors, andclinical features. JDevBehav\\nPediatr. 2014; 35(9):561– 9.https://doi.or g/10.1097/ DBP.00000000 00000097 PMID: 25211371.\\n14. Wiggins LD,Baio J,Rice C.Examinati onofthetimebetween firstevaluation andfirstautism spectrum\\ndiagnos isinapopulation- based sample. Journal ofDevelopment alandBehavioral Pediatrics. 2006; 27\\n(2):S79–S 87.PMID: 166851 89.\\n15. Gordon-L ipkin E,Foster J,Peacock G.Whittling Down theWait Time: Exploring Models toMinimize the\\nDelay from Initial Concern toDiagno sisandTreatment ofAutism Spectru mDisord er.Pediatr ClinNorth\\nAm.2016; 63(5):851 –9.https://doi.or g/10.1016/j .pcl.2016. 06.007 PMID: 27565363. PMCID:PMC 5583718.\\n16. Howlin P,Moore A.Diagnosis inautism: Asurvey ofover 1200 patients intheUK.autism. 1997; 1\\n(2):135–62 .\\n17. Kogan MD,Stricklan dBB,Blumberg SJ,Singh GK,Perrin JM,vanDyck PC.ANational Profile ofthe\\nHealth Care Experiences andFamily Impact ofAutism Spectrum Disorder Among Children intheUnited\\nStates, 2005-2006. Pediatrics .2008; 122(6):E1 149–E58. https://d oi.org/10.154 2/peds.20 08-1057\\nPMID: 190472 16.\\n18. Siklos S,Kerns KA.Assessing thediagnos ticexperie nces ofasmall sample ofparents ofchildren with\\nautism spectrum disorde rs.ResDevDisabil. 2007; 28(1):9–22 .https://do i.org/10.1016 /j.ridd.2005. 09.\\n003PMID: 164422 61.\\n19. Thomas KC,EllisAR,Konrad TR,Holzer CE,Morrissey JP.County -level estimates ofmental health\\nprofession alshortage intheUnited States. Psychiatr Serv. 2009; 60(10):132 3–8. https://doi.or g/10.\\n1176/ps.2 009.60.10 .1323 PMID: 19797371.\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 18/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 17}),\n",
       "  Document(page_content='20. Dawson G,Jones EJH, Merkle K,Venema K,Lowy R,Faja S,etal.Early Behavioral Interventi onIs\\nAssociated With Normalized Brain Activity inYoung Children With Autism. Journal oftheAmerican\\nAcademy ofChild andAdolesce ntPsychiatry. 2012; 51(11):115 0–9. https://doi.or g/10.101 6/j.jaac.\\n2012.08. 018PMID: 23101741. PMCID: PMC360742 7.\\n21. Dawson G,Rogers S,Munson J,Smith M,Winter J,Greenson J,etal.Randomized, controll edtrialof\\nanintervent ionfortoddlers withautism: theEarly Start Denver Model. Pediatrics. 2010; 125(1):e17 –23.\\nhttps://doi.or g/10.154 2/peds.20 09-0958 PMID: 19948568. PMCID: PMC4951 085.\\n22. Landa RJ.Efficacy ofearly interven tions forinfants andyoung children with, andatriskfor,autism spec-\\ntrum disorde rs.Internationa lReview ofPsychiatry .2018; 30(1):25–3 9.https://doi. org/10.1080/\\n09540261.2 018.1432574 PMID: 295373 31.PMCID: PMC603470 0.\\n23. Phillips DA,Shonko ffJP.From neurons toneighborho ods:Thescience ofearly childhood develop ment.\\nWashingto n,D.C.: National Academie sPress; 2000. https://doi.or g/10.17226 /9824 PMID: 25077268.\\n24. Duda M,Daniels J,Wall DP.Clinical Evaluati onofaNovel andMobile Autism Risk Assessme nt.J\\nAutism DevDisord. 2016; 46(6):1953 –61. https:// doi.org/10.10 07/s1080 3-016-271 8-4PMID:\\n26873142. PMCID: PMC4860 199.\\n25. Duda M,Kosmick iJA,Wall DP.Testing theaccuracy ofanobservation-ba sedclassifier forrapid detec-\\ntionofautism risk.Transl Psychiatry .2014; 4(8):e424. https://doi. org/10.1038/tp .2014.65 PMID:\\n25116834.\\n26. Kosmicki JA,Sochat V,Duda M,Wall DP.Searching foraminimal setofbehavio rsforautism detection\\nthrough feature selection- based machine learning. Translational Psychiatry .2015; 5(2):e514. https://\\ndoi.org/10.10 38/tp.2015.7 PMID: 25710120. PMCID: PMC444575 6.\\n27. Levy S,Duda M,Haber N,Wall DP.Sparsifyi ngmachine learning models identify stable subsets ofpre-\\ndictive features forbehavio raldetection ofautism. MolAutism. 2017; 8(1):65. https://doi.or g/10.1186/\\ns13229-017 -0180-6 PMID: 29270283. PMCID: PMC573553 1.\\n28. Wall DP,Kosmicki J,DeLuca TF,Harstad E,Fusaro VA.Useofmachine learning toshorten observa-\\ntion-based screening anddiagnos isofautism. Translati onalPsychiatry. 2012; 2(4):e100. https://doi.\\norg/10.1038/ tp.2012.10 PMID: 22832900. PMCID: PMC333707 4.\\n29. Wall DP,Dally R,Luyster R,Jung JY,Deluca TF.Useofartificial intelligence toshorten thebehavio ral\\ndiagnos isofautism. PLoS One. 2012; 7(8):e4385 5.https://doi.or g/10.1371/jour nal.pon e.0043855\\nPMID: 229527 89.\\n30. Wall DP,Kosmisck iJ,Deluca TF,Harstad L,Fusaro VA.Useofmachine learning toshorten observa-\\ntion-based screening anddiagnos isofautism. Translati onalPsychiatry. 2012; 2(e100). https://doi.or g/\\n10.1038/ tp.2012.10 PMID: 22832900. PMCID: PMC333707 4.\\n31. Schuller B,Vlasenko B,Eyben F,Wollmer M,Stuhlsatz A,Wendemuth A,etal.Cross-Co rpus Acoustic\\nEmotion Recognition :Variances andStrategies. Ieee Transact ionsonAffective Computing. 2010; 1\\n(2):119–31 .https://doi. org/10.1109/T- Affc.2010.8\\n32. Bone D,Goodwin MS,Black MP,LeeCC,Audhkh asiK,Narayanan S.Applying machine learning to\\nfacilitate autism diagnostic s:pitfalls andpromises. JAutism DevDisord. 2015; 45(5):1121 –36. https://\\ndoi.org/10.10 07/s10803-014 -2268-6 PMID: 252946 49.PMCID: PMC439040 9.\\n33. Bone D,Bishop SL,Black MP,Goodwi nMS,Lord C,Narayanan SS.Useofmachine learning to\\nimprove autism screening anddiagno sticinstrumen ts:effectivenes s,efficiency ,andmulti-instrum ent\\nfusion. Journal ofChild Psychology andPsychiatry .2016; 57(8):927– 37.https://do i.org/10.1111 /jcpp.\\n12559 PMID: 270906 13.PMCID: PMC495855 1.\\n34. Bussu G,Jones EJH, Charman T,Johnson MH,Buitelaar JK,Team B.Prediction ofAutism at3Years\\nfrom Behavio uralandDevelopment alMeasures inHigh-Risk Infants: ALongitu dinal Cross- Domain\\nClassifier Analysis. Journal ofAutism andDevelopmental Disorders. 2018; 48(7):2418 –33. https://doi.\\norg/10.1007/ s10803-018- 3509-x PMID: 29453709. PMCID: PMC599600 7.\\n35. Fusaro VA,Daniels J,Duda M,DeLuca TF,D’Angelo O,Tambure lloJ,etal.ThePotential ofAccelera t-\\ningEarly Detection ofAutism throug hContent Analysis ofYouTube Videos. Plos One. 2014; 9(4):\\ne93533. https:// doi.org/10.13 71/journal.p one.009 3533 PMID: 2474023 6.PMCID: PMC398917 6.\\n36. Freund Y,Schapire RE,editors. Experiments withanewboosting algorithm. Icml; 1996 July3,1996;\\nBari, Italy. SanFrancisco, CA,USA: Morgan Kaufman Publish ersInc.;1996.\\n37. Freund Y,Mason L,editors. Thealternating decision treelearning algorithm. icml; 1999 June 27,1999;\\nBled, Slovenia. SanFrancisc o,CA,USA: Morgan Kaufman nPublishe rsInc.\\n38. Behrend TS,Sharek DJ,Meade AW,Wiebe EN.Theviability ofcrowdsourcin gforsurvey research .\\nBehav ResMethods .2011; 43(3):800– 13.https://doi. org/10.3758/s 13428-011- 0081-0 PMID:\\n21437749.\\n39. David MM, Babineau BA,Wall DP.Canweaccelerate autism discoverie sthrough crowds ourcing?\\nResearch inAutism Spectrum Disorders. 2016; 32:80–3 .\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 19/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 18}),\n",
       "  Document(page_content='40. Ogunsey eS,Parsons J,editors. What Makes aGood Crowd? Rethinking theRelationshi pbetween\\nRecruitmen tStrategies andData Quality inCrowds ourcing. Proceedings ofthe16th AISSIGSAN D\\nSymposium ;2017 May19-20, 2017; Cincinnati, OH.\\n41. Swan M.Crowds ourced health research studies: animportant emergin gcomplement toclinical trials in\\nthepublic health research ecosystem. JMed Interne tRes. 2012; 14(2):e46. https:// doi.org/10.21 96/\\njmir.1988 PMID: 22397809. PMCID: PMC337650 9.\\n42. ZouH,Hastie T.Regular ization andvariable selection viatheelastic net.Journal oftheRoyal Statistical\\nSociety: Series B(Statistica lMethodolo gy).2005; 67(2):301– 20.https:// doi.org/10.11 11/j.146 7-9868.\\n2005.00503 .x\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 20/20', metadata={'source': 'papers/Tariq2018.pdf', 'page': 19})],\n",
       " [Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 0}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 1}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 2}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 3}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 4}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 5}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 6}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 7}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 8}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 9}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 10}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 11}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 12}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 13}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 14}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 15}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 16}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 17}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 18}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 19}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 20}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 21}),\n",
       "  Document(page_content='', metadata={'source': 'papers/Tariq_2019.pdf', 'page': 22})],\n",
       " [Document(page_content='A Video-Based Measure to Identify Autism Risk in Infancy\\nGregory S. Young, PhDa, John N. Constantino, MDb, Simon Dvorak, BSc, Ashleigh Belding, \\nMPHa, Devon Gangi, PhDa, Alesha Hill, BAa, Monique Hill, MAa, Meghan Miller, PhDa, \\nChandni Parikh, PhDa, AJ Schwichtenberg, PhDd, Erika Solis, BSa, Sally Ozonoff, PhDa\\naDepartment of Psychiatry & Behavioral Sciences, MIND Institute, University of California-Davis\\nbDepartment of Psychiatry, Washington University-St. Louis School of Medicine\\ncInformation and Educational Technology, University of California-Davis\\ndDepartment of Human Development & Family Studies, Purdue University\\nAbstract\\nBackground: Signs of autism are present in the first two years of life, but the average age of \\ndiagnosis lags far behind. Instruments that improve detection of autism risk in infancy are needed. \\nThis study developed and tested the psychometric properties of a novel video-based approach to \\ndetecting ASD in infancy.\\nMethods: A prospective longitudinal study of children at elevated or lower risk for autism \\nspectrum disorder was conducted. Participants were 76 infants with an older sibling with ASD and \\n37 infants with no known family history of autism. The Video-referenced Infant Rating System for \\nAutism (VIRSA) is a web-based application that presents pairs of videos of parents and infants \\nplaying together and requires forced-choice judgments of which video is most similar to the child \\nbeing rated. Parents rated participants on the VIRSA at 6, 9, 12, and 18 months of age. We \\nexamined split-half and test-retest reliability; convergent and discriminant validity; and sensitivity, \\nspecificity, and negative and positive predictive value for concurrent and 36-month ASD \\ndiagnoses.\\nResults: The VIRSA demonstrated satisfactory reliability and convergent and discriminant \\nvalidity. VIRSA ratings were significantly lower for children ultimately diagnosed with ASD than \\nchildren with typical development by 12 months of age. VIRSA scores at 18 months identified all \\nchildren diagnosed with ASD at that age, as well as 78% of children diagnosed at 36 months.\\nConclusions: This study represents an initial step in the development of a novel video-based \\napproach to detection of ASD in infancy. The VIRSA’s psychometric properties were promising \\nwhen used by parents with an older affected child, but still must be tested in community samples \\nwith no family history of ASD. If results are replicated, then the VIRSA’s low-burden, web-based \\nformat has the potential to reduce disparities in communities with limited access to screening.\\nKeywords\\nAutism; Screening; Infancy; Social Development\\nCorrespondence:  Sally Ozonoff, MIND Institute, UC Davis Health, 2825 50th Street, Sacramento CA 95817; 916-703-0259; \\nsozonoff@ucdavis.edu. \\nHHS Public Access\\nAuthor manuscript\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nPublished in final edited form as:\\nJ Child Psychol Psychiatry . 2020 January ; 61(1): 88–94. doi:10.1111/jcpp.13105.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 0}),\n",
       "  Document(page_content='Introduction\\nThe developmental course of autism spectrum disorder (ASD) involves the onset of \\nsymptoms in the first three years of life. Differences between children who will later receive \\nan ASD diagnosis and those with typical development emerge before the second birthday \\n(Gammer et al., 2015 ; Landa & Garrett-Mayer, 2006 ; Ozonoff et al., 2010 ; Zwaigenbaum et \\nal., 2005 ), with some studies documenting signs in the first year of life ( Maestro et al., 2002 ; \\nMiller et al., 2017 ; Werner, Dawson, Osterling & Dinno, 2000 ), and parents first expressing \\nconcerns at an average age of 14 months ( Chawarska et al., 2007 ). Despite advances in \\nknowledge about the earliest presentations of ASD, the mean age of diagnosis has \\nstubbornly remained over 4 years ( Baio et al., 2018 ) and has not declined over the last two \\ndecades, squandering years of potential intervention when the brain is most plastic. It is \\ncritical that further attempts are made to decrease the age of ASD diagnosis so that it better \\naligns with the age of first symptom emergence.\\nOne of the identified barriers to more prompt recognition of ASD is measurement ( Al \\nQabandi, Gorter & Rosenbaum, 2011 ). Over the last decade, much effort has gone into the \\ndevelopment of instruments for earlier detection of ASD ( Zwaigenbaum et al., 2015 ). The \\nmost feasible method for large-scale screening is parent report and most existing measures \\nuse this methodology. However, recent studies have demonstrated low agreement between \\nparent report and more objective measures of ASD symptoms ( Ozonoff et al., 2011 ), as well \\nas lower reliability for screening instruments when used in rural, low income, less educated, \\nand racially diverse samples ( Khowaja, Hazzard & Robins, 2015 ; Scarpa et al., 2013 ). A \\npopulation screening study of 10,479 twelve-month-olds ( Pierce et al., 2011 ) using a parent-\\nreport measure ( Wetherby, Brosnan-Maddox, Peace & Newton, 2008 ) identified 32 infants \\nwith ASD. This represents significant under-identification, even after accounting for cases \\nwith later onset ( Barger, Campbell & McDonough, 2013 ), since current prevalence studies \\nestimate that 170 of 10,000 children have ASD ( Baio et al., 2018 ).\\nThe lower sensitivity of early screening measures may be due to the subtlety of initial ASD \\nsymptoms and the difficulty of accurately conveying them to parents through written \\ndescriptions. Major sources of error in parent questionnaires include comprehension and \\ninterpretation problems ( Koriat, Goldsmith & Pansky, 2000 ; Krosnick & Presser, 2010 ), \\nsuch as limited understanding of the queried constructs, inadequate knowledge of \\ndevelopmental milestones, and bias due to post-event information (e.g., eventual diagnosis). \\nThe current study moves beyond verbal descriptions by employing video examples to reduce \\nsubjective interpretations. The use of videos has been shown to dramatically increase clarity \\nin other fields, from music instruction to motor vehicle repair ( Arguel & Jamet, 2009 ). \\nRecently, video was incorporated in ASD screening by Marrus and colleagues (2015) , who \\nhad parents complete ratings after watching a video of a socially competent toddler, in order \\nto “reduce discrepant interpretations of items by providing informants with a common \\nnaturalistic standard for comparison” (p. 1340).\\nHere we describe the development of a new instrument, the Video-referenced Infant Rating \\nSystem for Autism (VIRSA). It extends previous approaches ( Marrus et al., 2015 ) by Young et al. Page 2\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 1}),\n",
       "  Document(page_content='creating a large library of video clips depicting a wide range of social-communication ability \\nand relying solely on video in the ratings, with no written descriptions of behavior. We \\nhypothesized that the semantic clarity afforded by video would improve early discrimination \\nof infants at highest risk for ASD.\\nMethods\\nInstrument Development\\nThe VIRSA was developed using video from participants in a longitudinal infant sibling \\nstudy and then validated on an independent sample of infants. Videos used in the VIRSA \\nwere drawn from an archive of over 300,000 minutes of digitized video recorded in a clinical \\nlaboratory setting. Video depicted infants and parents playing together with age-appropriate \\ntoys. Segments were selected from a task that used a standardized toy set and instructed \\nparents to play with their child as they would at home ( Schwichtenberg, Kellerman, Miller, \\nYoung & Ozonoff, 2019 ). Video recordings utilized a consistent camera angle facing the \\nchild, with the parent in profile. All families gave both informed consent and legal \\nauthorization to include their videos in the VIRSA.\\nSocial behaviors, including smiles, vocalizations, and eye contact, were coded by research \\nassistants unaware of participant risk group or outcome, using a previously validated coding \\nscheme that is sensitive to the changes that occur during the onset of ASD symptoms as \\nearly as 6 months ( Gangi et al., 2019 ; Ozonoff et al., 2010 ). In order to include a broad \\nrange of behaviors in the VIRSA, candidate videos were ranked by frequencies of the coded \\nbehaviors. Twenty-second segments were then excised from the original video files, \\nresulting in a collection of over 3,000 video segments from 100 past participants between 6 \\nand 18 months of age. Next, video segments were rated by 9 clinical research staff on a scale \\nfrom 1 (least socially competent) to 10 (most socially competent). Each clinician rated a \\nrandomly selected set of 39 videos twice to establish test-retest reliability (mean=0.89, range \\n0.78–0.98). Inter-rater reliability was examined on a larger randomly selected set of 260 \\nvideo clips rated by all raters using a two-way random ICC model. The average measures \\nICC for absolute agreement was 0.92, with a lower bound of 0.86, suggesting strong inter-\\nrater reliability of the 10-point scale. Video segments were excluded for poor lighting or \\naudio quality, obscured video angles, or use of the child’s name. This resulted in a pool of \\nvideo comprising 1,132 individual 20-second clips, which was then constrained to insure \\nadequate representation across the 10-point rating scale within each age (6, 9, 12, and 18 \\nmonths). To limit the software overhead for the VIRSA app, 268 videos were then randomly \\nselected to create the final VIRSA video library. The final pool of video included segments \\nfrom 11 children with ASD, 23 children with non-ASD developmental concerns (e.g. \\nspeech-language delays), and 29 children with typical development, based on 36-month \\noutcome (63 children total). Thirty-eight (60.32%) of the children depicted in VIRSA videos \\nwere male and 43 (68.25%) were Non-Hispanic Caucasian. Analysis of ratings of VIRSA \\nvideos, using a generalized linear model with random effects for subjects and age, revealed a \\nsignificant group effect ( X2=6.76, df=2, p<.05), with the ASD videos rated an average of \\n4.18 (95% CI = 2.81 to 5.55), the videos of children with non-ASD developmental concerns \\nrated an average of 6.14 (95% CI = 5.19 to 7.09), and the videos of typically developing Young et al. Page 3\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 2}),\n",
       "  Document(page_content='participants rated an average of 6.35 (95% CI = 5.47 to 7.22). Simple comparisons indicated \\nthat the ratings of the ASD videos differed significantly from the videos of both the non-\\nASD developmental concerns ( t=2.31, p=.027) and typically developing cases ( t=2.62, p=.\\n013), who did not differ from one another ( t=0.31, p=0.76), as expected. Since the VIRSA \\nwas designed specifically to detect the social-communication behaviors relevant to ASD, but \\nnot broader developmental delays, this pattern of results provided further validation of the \\nfinal video pool.\\nThe library of video segments was incorporated into a web-based application that presented \\npairs of videos, depicting differing degrees of social competence, side by side, accompanied \\nby the prompt, “Which video is more like your child’s interaction with you on a typical \\nday?” On each trial, the video on the left played automatically, followed by the video on the \\nright, at which point the viewer selected the one most like the child. Presentation of video \\nfollowed an algorithm that always began with a pair of videos rated as 3 (less social) and 8 \\n(more social) on the 10-point scale. After each choice, the algorithm selected and displayed \\na second pair of videos with new scale values contingent upon the previously chosen video’s \\nranking. In each subsequent trial, the viewer’s video choice dictated the range of sociability \\nrepresented in the videos on the next trial, analogous to how optometrists help patients select \\neyeglass prescriptions. In this way, the algorithm presented videos of increasing similarity \\nover subsequent trials until the distance between videos reduced to 1 rating scale point on 2 \\nsubsequent trials, at which point the average rating of the last 2 trials was recorded as the \\nfinal score (see Figure S1 in online Appendix for examples).\\nThe VIRSA web application was designed with a brief introductory video that oriented \\nparents to the concepts and range of social behaviors depicted in the videos and provided \\nrating instructions. The VIRSA app also asked for confirmation of the child’s age in order to \\npresent videos from a matching age group. Since multiple video exemplars of each scale \\npoint (1 to 10) were available, videos were sampled from the pool without replacement.\\nThe UC Davis Institutional Review Board approved the study procedures. Parents signed an \\ninformed consent form prior to participation. They completed VIRSA ratings when their \\nchild was 6-, 9-, 12-, and 18-months-old and again two weeks later to examine test-retest \\nreliability. An automated email invited parents to the online VIRSA app, which could be \\naccessed by computer or mobile device (e.g., smartphones, tablets). VIRSA ratings were \\nalways done prior to in-person assessments, which were conducted at 6, 12, 18, 24, and 36 \\nmonths by examiners unaware of risk group or previous test results.\\nParticipants\\nThe VIRSA validation sample consisted of 110 infants (73 with an older sibling with ASD, \\n37 with no known family history of autism), none of whom supplied videos used in \\ninstrument development. Twenty-one children in the familial high-risk (HR) group received \\na diagnosis of ASD, whereas none of the low-risk (LR) children did. ASD diagnoses were \\nmade at any age that a child met DSM-5 criteria, based on all information available. One \\nchild was diagnosed with ASD at 12 months, 7 were diagnosed at 18 months, 7 at 24 \\nmonths, and 6 at 36 months. All children diagnosed before 36 months retained the ASD \\ndiagnosis at the final visit. The rest of the sample was classified as Non-ASD and then Young et al. Page 4\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 3}),\n",
       "  Document(page_content='stratified by familial risk to yield the HR Non-ASD and LR Non-ASD comparison groups. \\nDescriptive statistics are shown in Table 1.\\nMeasures\\nMullen Scales of Early Learning  (MSEL; Mullen, 1995 ) is an assessment of cognitive, \\nmotor, and language development for children aged 1 to 68 months. MSEL scores were used \\nto describe the sample. Scores on the Fine Motor and Visual Reception subtests were used to \\nevaluate discriminant validity since these developmental domains are theoretically unrelated \\nto the social-communicative focus of the VIRSA.\\nAutism Diagnostic Observation Schedule, 2nd edition  (ADOS-2; Lord et al., 2012 ) is an \\nobservational measure that assesses ASD symptoms through semi-standardized interactions \\nbetween the clinician and child. It is comprised of five modules appropriate for various ages \\nand language levels; the Toddler module was administered at 18 and 24 months and either \\nmodule 1 or 2 was used at 36 months, depending upon the child’s verbal level. Analyses \\nutilized the overall total algorithm (Social Affect + Restricted Repetitive Behavior or \\nSARRB) score. Scores on the ADOS at 18 months were also used to examine convergent \\nvalidity with VIRSA ratings at 18 months.\\nAnalysis Plan\\nPsychometric properties of the VIRSA were examined in several ways. Split-half reliability \\nwas analyzed by comparing the first half of the ratings within a given session to the second \\nhalf. Test-retest reliability  was analyzed by comparing initial VIRSA ratings to those \\nobtained two weeks later. Parents were shown the same series of paired videos they had seen \\ntwo weeks earlier, instead of video pairs dictated by the VIRSA algorithm, permitting \\nexamination of the reliability of individual trial choices. Convergent validity  was examined \\nthrough correlations at 18 months with concurrent SARRB algorithm scores on the ADOS-2 \\nToddler module. Discriminant validity  was examined by correlating VIRSA scores with \\nconcurrent MSEL fine motor and visual reception age equivalents. Predictive validity  was \\nassessed by examining diagnostic outcome group differences on the VIRSA scores at each \\nage, using a mixed model with group, age, and their interaction included as fixed effects and \\nVIRSA score as the time-varying dependent variable. We also used ROC analysis to assess \\nthe VIRSA’s sensitivity, specificity, positive predictive value (PPV), and negative predictive \\nvalue (NPV)  in predicting ASD diagnosis. Area under the curve (AUC)  was computed as a \\nmeasure of the ability to distinguish between groups. Sample sizes initially projected were \\n90 high-risk and 45 low-risk infants. With an anticipated ASD outcome rate of \\napproximately 20% in the high-risk group, power was estimated to be .82 to detect an AUC \\nvalue of .70 on the ROC analyses. All analyses were conducted in R, version 3.5.0.\\nResults\\nVIRSA trials (selections between paired videos) took an average of 56.49 seconds \\n(range=38 to 133, SD=11.49). The average number of trials before a final score was reached \\nwas 7.67 (range=6 to 14, SD=1.26), with completion of the VIRSA taking an average of \\n7.21 minutes (range=3.82 to 15.57, SD=1.61). Split-half reliability was moderate, at r=.48, Young et al. Page 5\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 4}),\n",
       "  Document(page_content='and test-retest reliability relatively strong, with 72% of the same video choices made two \\nweeks later, which was significantly greater than chance ( t=19.58, df=156, p<.0001). \\nConvergent validity correlation with concurrent ADOS-2 SARRB algorithm scores at 18 \\nmonths was r=−.36, which was significantly stronger than discriminant validity correlations \\nwith concurrent MSEL fine motor ( r=.05, Fisher’s z=2.50, p<.05) and visual reception ( r=.\\n10, Fisher’s z=2.11, p<.05) age equivalent scores.\\nFigure 1 shows the modeled parent VIRSA scores for each group between 6 and 18 months. \\nExamination of the model revealed a main effect for group ( χ2=10.32, df=2, p<.01). The \\nmain effect for age and the interaction between age and group were not significant. Planned \\ncontrasts revealed no significant differences in VIRSA scores between the ASD and \\ncomparison groups at 6 months (HR Non-ASD: t=0.71, p=0.48; LR Non-ASD: t=0.89, \\np=0.38) and 9 months (HR Non-ASD: t=1.75, p=.08; LR Non-ASD: t=1.46, p=0.14). At 12 \\nmonths, VIRSA scores were significantly lower in the ASD group than both the HR Non-\\nASD ( t=3.15, p=.002) and the LR Non-ASD groups ( t=2.11, p=.04). At 18 months, VIRSA \\nscores were significantly lower in the ASD group than in the HR Non-ASD group ( t=3.29, \\np=.002) and marginally lower than the LR Non-ASD group ( t=1.73, p=.08).\\nROC analyses were conducted on VIRSA scores at each age, predicting a binary 36-month \\noutcome (see Table 2). The threshold/cutoff at which ROC analyses best separated ASD and \\nNon-ASD cases was defined as the VIRSA score closest to the theoretical limit of maximum \\nspecificity and sensitivity. The VIRSA performed best at 18 months. Table 3 presents ROC \\nmodels that examined how well VIRSA scores at 18 months predicted concurrent 18-month \\ndiagnoses ( n=8 diagnosed with ASD at that age). Sensitivity was 100% (no false negatives), \\nbut specificity and positive predictive value were low.\\nDiscussion\\nWe hypothesized that employing video examples within a screening tool would enable \\ndetection of ASD in infancy. Starting at 12 months of age, VIRSA ratings were significantly \\nlower for the group eventually diagnosed with ASD than for the comparison groups. \\nSensitivity of 18-month VIRSA scores in predicting 36-month diagnosis was approximately \\n0.80. This compares quite favorably to a study reporting sensitivity of 18-month clinical \\ndiagnostic assessment in predicting 36-month diagnosis of only 0.37 ( Ozonoff et al., 2015 ). \\nIn fact, VIRSA scores had better sensitivity even at 6–12 months of age than that reported \\nfor clinical diagnosis at 18 months in Ozonoff et al. (2015) . We hypothesize that the use of \\nvideo allowed parents to “see” differences in their child that preceded the full onset of \\nsymptoms. The VIRSA’s sensitivity is especially impressive given the extended time course \\nof development of ASD symptoms. Multiple previous studies have demonstrated that \\nsymptoms slowly unfold over the first two years of life and many children who are \\nultimately diagnosed with ASD do not show overt signs before the first birthday ( Gammer et \\nal., 2015 ; Landa & Garrett-Mayer, 2006 ; Ozonoff et al., 2010 ; Zwaigenbaum et al., 2005 ). \\nChildren who are not identified by the VIRSA may not yet be showing signs of ASD for \\nparents to rate.Young et al. Page 6\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 5}),\n",
       "  Document(page_content='We also examined the VIRSA’s ability to index concurrent symptoms. ROC analyses of \\nVIRSA ratings at 18 months identified all eight children who had been diagnosed with ASD \\nby that age, with no false negatives. A recent meta-analysis of the accuracy of ASD \\nscreeners between 14 and 36 months of age ( Sanchez-Garcia et al., 2019 ) reported a pooled \\nsensitivity of 0.72 and specificity of 0.98. The sensitivity of the VIRSA at 18 months is thus \\ncomparable to or better than existing measures and suggests that it may be a useful adjunct \\nin identifying toddlers in need of referral for an ASD evaluation. Its specificity and positive \\npredictive value, however, were lower than recommended standards ( Cicchetti et al., 1995 ), \\nresulting in over-identification of risk. For this reason, the present results do not support use \\nof the VIRSA as a stand-alone ASD screener in infancy yet. Future studies could examine \\nwhether using the VIRSA as an initial step in a two-stage screening process improves \\naccuracy.\\nIn addition to the predictive validity of the VIRSA, we examined a number of other \\npsychometric properties. Test-retest reliability was strong, with parents selecting over 70% \\nof the same videos when they retook the VIRSA two weeks later. VIRSA scores at 18 \\nmonths were significantly correlated with ADOS-2 scores and correlations were \\nsignificantly higher than with measures of divergent abilities (e.g., fine motor and visual \\nreception skills).\\nThe majority of participants, and all the children who developed ASD, came from the high-\\nrisk group (e.g., had older siblings with ASD). It is imperative, prior to recommending the \\nVIRSA for clinical use, to examine its psychometric properties when used by parents who \\nare naïve to ASD. The positive predictive value of an instrument is dependent upon the base \\nrate of the condition in the population ( Clark & Harrington, 1999 ; Grimes & Schulz, 2002 ) \\nand thus the VIRSA’s predictive ability may be reduced in a community-based sample that \\nhas a lower prevalence of ASD than in high-risk families. Studies are currently underway in \\nour laboratory to determine whether the present results generalize to low-risk samples.\\nDespite these limitations, the VIRSA makes several contributions to the literature. First, it \\ndemonstrates that it is possible to develop a parent report instrument capable of identifying \\nASD risk in the first year of life. Second, it demonstrates that video can be used to clarify \\ndevelopmental phenomena and improve parent reporting of early development. Finally, an \\ninnovation of the VIRSA is its web-based, mobile-optimized application. Over 90% of \\nAmerican adults of childbearing age own a smartphone, with rates over 65% even in lower \\nincome, rural, and minority communities ( Pew Research Center, 2018 ). It is vital that \\nscreening procedures keep pace with such advances in technology and society’s increasingly \\ninternet-based preferences for information acquisition and communication. Thus, this study \\nprovides an initial step in the proof of principle of video- and web-based screening for ASD. \\nWith further development, the VIRSA, with its low-burden, quick, online ratings, has \\npotential to reduce disparities in communities with limited access to screening and provide \\nthe possibility of initiating intervention before the full symptom set of ASD has emerged.\\nSupplementary Material\\nRefer to Web version on PubMed Central for supplementary material.Young et al. Page 7\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 6}),\n",
       "  Document(page_content='Acknowledgments\\nThis work was supported by NIH grants R01 MH099046 (Ozonoff) and U54 HD079125 (Abbeduto; MIND \\nInstitute Intellectual and Developmental Disabilities Research Center) and Autism Speaks grant 8370 (Ozonoff). \\nWe are deeply grateful to the parents who authorized the use of their child’s video in the development of the VIRSA \\nand the children and families who participated in the validation study.\\nDisclosures: Dr. Constantino receives royalties from Western Psychological Services for the commercial \\ndistribution of the Social Responsiveness Scale. Dr. Miller has received research grant funding from the National \\nInstitutes of Health and travel reimbursement and/or honoraria from the Society for Clinical Child and Adolescent \\nPsychology and the Help Group. Dr. Ozonoff has received research grant funding from the National Institutes of \\nHealth and Autism Speaks, travel reimbursement and honoraria for editorial activities from Autism Speaks, Autism \\nScience Foundation, and Wiley, and book royalties from Guilford Press and American Psychiatric Press, Inc. Dr. \\nSchwichtenberg has received grant funding from the National Institutes of Health and travel support from Autism \\nSpeaks and the Autism Science Foundation. All other authors report no financial disclosures or potential conflicts \\nof interest.\\nAbbreviations:\\nASD autism spectrum disorder\\nVIRSA Video-referenced Infant Rating System for Autism\\nReferences\\nAl-Qabandi M, Gorter JW, Rosenbaum P. (2011). Early autism detection: Are we ready for routine \\nscreening? Pediatrics, 128(1), e211–e217. [PubMed: 21669896] \\nArguel A, Jamet E. (2009). Using video and static pictures to improve learning of procedural contents. \\nComputers in Human Behavior, 25, 354–359.\\nBaio J, Wiggins L, Christensen DL, Maenner MJ, Daniels J, Warren Z, Kurzius-Spencer M, Zahorodny \\nW, Robinson-Rosenberg C, White T, Durkin MS, Imm P, Nikolaou L, Yeargin-Allsopp M, Lee LC, \\nHarrington R, Lopez M, Fitzgerald RT, Hewitt A, Pettygrove S, Constantino JN, Vehorn A, \\nShenouda, Hall-Lande J, Van Naarden-Braun K, Dowling NF. (2018). Prevalence of autism \\nspectrum disorder among children aged 8 years: autism and developmental disabilities monitoring \\nnetwork, 11 sites, United States 2014. MMWR Surveillance Summary, 67(6), 1–23.\\nBarger BD, Campbell JM, McDonough JD. (2013). Prevalence and onset of regression within autism \\nspectrum disorders: A meta-analytic review. Journal of Autism and Developmental Disorders, 43(4), \\n817–828. [PubMed: 22855372] \\nChawarska K, Paul R, Klin A, Hannigen S, Dichtel LE, V olkmar F. (2007). Parental recognition of \\ndevelopmental problems in toddlers with autism spectrum disorders. Journal of Autism and \\nDevelopmental Disorders, 37(1), 62–72. [PubMed: 17195921] \\nCicchetti DV , V olkmar F, Klin A, Showalter D. (1995). Diagnosing autism using ICD-10 criteria: A \\ncomparison of neural networks and standard multivariate procedures. Child Neuropsychology, 1, \\n26–37.\\nClark A, & Harrington R (1999). On diagnosing rare disorders rarely: Appropriate use of screening \\ninstruments. Journal of Child Psychology and Psychiatry, 40(2), 287–290. [PubMed: 10188711] \\nGammer I, Bedford R, Elsabbagh M, Garwood H, Pasco G, Tucker L, V olein A, Johnson MH, \\nCharman T, BASIS Team. (2015). Behavioral markers for autism in infancy: Scores on the Autism \\nObservational Scale for Infants in a prospective study of at-risk siblings. Infant Behavior and \\nDevelopment, 38, 107–115. [PubMed: 25656952] \\nGangi DN, Boterberg S, Schwichtenberg AJ, Solis E, Young GS, Iosif A, & Ozonoff S (2019). Use of \\nprospective longitudinal gaze measurements in defining regression Paper presented at the annual \\nmeeting of the International Society for Autism Research, Montreal, 5.\\nGrimes DA, & Schulz KF (2002). Uses and abuses of screening tests. Lancet, 359, 881–884. [PubMed: \\n11897304] Young et al. Page 8\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 7}),\n",
       "  Document(page_content='Khowaja MK, Hazzard AP, Robins DL. (2015). Sociodemographic barriers to early detection of \\nautism: Screening and evaluation using the M-CHAT, M-CHAT-R, and follow-up. Journal of \\nAutism and Developmental Disorders, 45(6), 1797–1808. [PubMed: 25488122] \\nKoriat A, Goldsmith M, Pansky A. (2000). Toward a psychology of memory accuracy. Annual Review \\nof Psychology, 51, 481–537.\\nKrosnick JA, Presser S. (2010). Question and questionnaire design In Marsden PV , Wright JD (Eds.), \\nHandbook of Survey Research, 2nd edition (pp. 263–314). London: Emerald.\\nLanda R, Garrett-Mayer E. (2006). Development in infants with autism spectrum disorders: A \\nprospective study. Journal of Child Psychology and Psychiatry, 47(6), 629–638. [PubMed: \\n16712640] \\nLord C, Rutter M, DiLavore PC, Risi S, Gotham K, Bishop SL. (2012). Autism Diagnostic \\nObservation Schedule Manual, 2nd edition Torrance, CA: Western Psychological Services.\\nMaestro S, Muratori F, Cavallaro MC, Pei F, Stern D, Golse B, Palacio-Espasa F. (2002). Attentional \\nskills during the first six months of age in autism spectrum disorder. Journal of the American \\nAcademy of Child and Adolescent Psychiatry, 41(10), 1239–1245. [PubMed: 12364846] \\nMarrus N, Glowinski AL, Jacob T, Klin A, Jones W, Drain CE, Holzhauer KE, Hariprasad V , \\nFitzgerald RT, Mortensen EL, Sant SM, Cole L, Siegel SA, Zhang Y , Agrawal A, Heath AC, \\nConstantino JN. (2015). Rapid video-referenced ratings of reciprocal social behavior in toddlers: A \\ntwin study. Journal of Child Psychology and Psychiatry, 56(12), 1338–1346. [PubMed: 25677414] \\nMiller M, Iosif A, Hill M, Young GS, Schwichtenberg AJ, Ozonoff S. (2017). Response to name in \\ninfants developing autism spectrum disorder: A prospective study. Journal of Pediatrics, 183, 141–\\n146. [PubMed: 28162768] \\nMullen EM. (1995). Mullen Scales of Early Learning. Circle Pines, MN: AGS Publishing.\\nOzonoff S, Iosif AM, Baguio F, Cook IC, Hill MM, Hutman T, Rogers SJ, Rozga A, Sangha S, Sigman \\nM, Steinfeld MB, Young GS. (2010). A prospective study of the emergence of early behavioral \\nsigns of autism. Journal of the American Academy of Child and Adolescent Psychiatry, 49(3), \\n256–266. [PubMed: 20410715] \\nOzonoff S, Iosif A, Young GS, Hepburn S, Thompson M, Colombi C, Cook IC, Werner E, Goldring S, \\nBaguio F, Rogers S. (2011). Onset patterns in autism: Correspondence between home video and \\nparent report. Journal of the American Academy of Child and Adolescent Psychiatry, 50(8), 796–\\n806. [PubMed: 21784299] \\nOzonoff S, Young GS, Landa RJ, Brian J, Bryson S, Charman T. Chawarska K, Macari SL, Messinger \\nD, Stone WL, Zwaigenbaum L, Iosif AM. (2015). Diagnostic stability in young children at risk for \\nautism spectrum disorder: A baby siblings research consortium study. Journal of Child Psychology \\nand Psychiatry, 56(9), 988–998. [PubMed: 25921776] \\nPew Research Center. (2018). Demographics of mobile device ownership and adoption in the United \\nStates. http://www.pewinternet.org/fact-sheet/mobile/ . Accessed March 15, 2019.\\nPierce K, Carter C, Weinfeld M, Desmond J, Hazin R, Bjork R, Gallagher N. (2011). Detecting, \\nstudying, and treating autism early: The one-year well-baby check-up approach. Journal of \\nPediatrics, 159(3), 458–465. [PubMed: 21524759] \\nSanchez-Garcia AB, Galindo-Villardon P, Nieto-Librero AB, Martin-Rodero H, & Robins DL (2019). \\nToddler screening accuracy for autism spectrum disorder: A meta-analysis of diagnostic accuracy. \\nJournal of Autism and Developmental Disorders, 49(5), 1837–1852. [PubMed: 30617550] \\nScarpa A, Reyes NM, Patriquin MA, Lorenzi J, Hassenfeldt TA, Desai VJ Kerkering KW. (2013). The \\nmodified checklist for autism in toddlers: Reliability in a diverse rural American sample. Journal \\nof Autism and Developmental Disorders, 43(10), 2269–2279. [PubMed: 23386118] \\nSchwichtenberg AJ, Kellerman A, Miller M, Young GS, Ozonoff S. (2019). Mothers of children with \\nautism spectrum disorder: Play behaviors with infant siblings and social responsiveness. Autism \\n[epub ahead of print]. doi: 10.1177/1362361318782220.\\nWerner E, Dawson G, Osterling J, Dinno N. (2000). Recognition of autism spectrum disorder before \\none year of age: A retrospective study based on home videotapes. Journal of Autism and \\nDevelopmental Disorders, 30(2),157–162. [PubMed: 10832780] Young et al. Page 9\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 8}),\n",
       "  Document(page_content='Wetherby AM, Brosnan-Maddox S, Peace V , Newton L. (2008). Validation of the infant-toddler \\nchecklist as a broadbrand screener for autism spectrum disorders from 9 to 24 months of age. \\nAutism, 12(5), 487–511. [PubMed: 18805944] \\nZwaigenbaum L, Bryson S, Rogers T, Roberts W, Brian J, Szatmari P. (2005). Behavioral \\nmanifestations of autism in the first year of life. International Journal of Developmental \\nNeuroscience, 23(2–3), 143–152. [PubMed: 15749241] \\nZwaigenbaum L, Bauman ML, Fein D, Pierce K, Buie T, Davis PA, Newschaffer C, Robins DL, \\nWetherby A, Choueiri R, Kasari C, Stone WL, Yirmiya N, Estes A, Hansen RL, McPartland JC, \\nNatowicz MR, Carter A, Granpeesheh D, Mailloux Z, Smith-Roley S, Wagner S. (2015). Early \\nscreening of autism spectrum disorder: Recommendations for practice and research. Pediatrics, \\n136(Suppl 1), S41–S59. [PubMed: 26430169] Young et al. Page 10\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 9}),\n",
       "  Document(page_content='Key Points\\n1. Signs of ASD are present in the first two years of life, but the average age of \\ndiagnosis lags far behind. Instruments that improve detection of autism risk in \\ninfancy are needed.\\n2. We hypothesized that employing video examples within a screening tool \\nwould improve detection of ASD in infancy.\\n3. A newly developed video-based screening tool had high sensitivity at 18 \\nmonths in concurrently identifying the toddlers diagnosed with ASD at that \\nage, as well as predicting ASD at 36 months.\\n4. Employing video examples within a screening tool may be helpful in \\nidentifying ASD in infancy. A brief, low-burden, web-based screening tool \\ncould help reduce disparities in communities with limited access to care.Young et al. Page 11\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 10}),\n",
       "  Document(page_content='Figure 1: \\nVIRSA ratings by group from 6 to 18 months.Young et al. Page 12\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscript', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 11}),\n",
       "  Document(page_content='Author Manuscript Author Manuscript Author Manuscript Author ManuscriptYoung et al. Page 13\\nTable 1:\\nSample Descriptives.\\nASD HR Non-ASD LR Non-ASD\\nSample size 21 52 37\\nSex (% male) 61.91%a40.39%a59.46%a\\nRace/Ethnicity\\n % Non-Hispanic Caucasian 36.84%a42.00%a62.16%a\\n % Non-White Race or Multiracial 21.05%a38.00%a27.03%a\\n % Hispanic 42.11%a20.00%a10.81%b\\nMaternal education\\n % Graduate degree 14.29%a48.08%b35.14%ab\\n % College degree 57.14%a40.39%a59.46%a\\n % High school or V ocational training 19.05%a9.62%a5.41%a\\nHousehold income\\n % $60k or less 19.05%a7.69%a16.22%a\\n % $61k to $100k 33.33%a25.00%a18.92%a\\n % $101k or higher 23.81%a46.15%a51.35%a\\nAge at outcome (months) 36.39 (0.72)a36.92 (1.82)a36.61 (0.84)a\\nADOS-2 SARRB* algorithm score at 36 months 14.95 (6.25)a3.08 (2.31)b2.24 (2.02)b\\nMSEL outcome fine motor age eq 26.35 (6.76)a33.86 (4.79)b37.11 (5.48)c\\nMSEL outcome visual reception age eq 29.45 (8.93)a41.58 (7.58)b42.89 (6.57)b\\nMSEL outcome expressive language age 26.25 (9.53)a38.14 (4.97)b39.54 (4.60)b\\nMSEL outcome receptive language age eq 25.60 (9.55)a35.66 (5.68)b37.51 (4.78)b\\nNote:\\nValues with different subscripts are significantly different at p<.05\\n*Social Affect + Restrictive Repetitive Behavior overall total\\nMSEL outcome = Mullen Scales of Early Learning at 36 months of age\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 12}),\n",
       "  Document(page_content='Author Manuscript Author Manuscript Author Manuscript Author ManuscriptYoung et al. Page 14\\nTable 2:\\nROC analyses with 36-month ASD diagnostic classification.\\n6 months 9 months 12 months 18 months\\nTrue positives 7 10 8 14\\nFalse positives 11 37 31 31\\nTrue negatives 36 29 51 35\\nFalse negatives 6 6 8 4\\nAUC .62 .42 .59 .71\\nSpecificity (95% CI) .77 (.65 to .89) .44 (.32 to .56) .62 (.52 to .73) .53 (.41 to .65)\\nSensitivity (95% CI) .54 (.26 to .81) .63 (.39 to .86) .50 (.26 to .75) .78 (.59 to .97)\\nNegative Predictive Value (95% CI) .86 (.75 to .96) .83 (.70 to .95) .86 (.78 to .95) .90 (.80 to .99)\\nPositive Predictive Value (95% CI) .39 (.16 to .61) .21 (.10 to .33) .21 (.08 to .33) .31 (.18 to .45)\\nThreshold 5.25 8.25 6.75 8.25\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 13}),\n",
       "  Document(page_content='Author Manuscript Author Manuscript Author Manuscript Author ManuscriptYoung et al. Page 15\\nTable 3:\\nROC analyses for 18-month VIRSA with concurrent 18-month diagnosis.\\nVIRSA (18 months)\\nTrue positives 8\\nFalse positives 34\\nTrue negatives 38\\nFalse negatives 0\\nAUC .78\\nSpecificity .53\\nSensitivity 1.00\\nNegative Predictive Value 1.00\\nPositive Predictive Value .19\\nThreshold 7.75\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.', metadata={'source': 'papers/Young_Behavior.pdf', 'page': 14})],\n",
       " [Document(page_content='Research and development of\\nautism diagnosis information\\nsystem based on deep convolution\\nneural network and facial\\nexpression data\\nWang Zhao and Long Lu\\nSchool of Information Management, Wuhan University, Wuhan, China\\nAbstract\\nPurpose –Facial expression provides abundant information for social interaction, and the analysis and\\nutilization of facial expression data are playing a huge driving role in all areas of society. Facial expression data\\ncan reflect people ’s mental state. In health care, the analysis and processing of facial expression data can\\npromote the improvement of people ’s health. This paper introduces several important public facial expression\\ndatabases and describes the process of facial expression recognition. The standard facial expression database\\nFER2013 and CK þwere used as the main training samples. At the same time, the facial expression image data\\nof 16 Chinese children were collected as supplementary samples. With the help of VGG19 and Resnet18\\nalgorithm models of deep convolution neural network, this paper studies and develops an information system\\nfor the diagnosis of autism by facial expression data.\\nDesign/methodology/approach –The facial expression data of the training samples are based on the\\nstandard expression database FER2013 and CK þ. FER2013 and CK þdatabases are a common facial\\nexpression data set, which is suitable for the research of facial expression recognition. On the basis of FER2013and CK þfacial expression database, this paper uses the machine learning model support vector machine\\n(SVM) and deep convolution neural network model CNN, VGG19 and Resnet18 to complete the facial\\nexpression recognition.\\nFindings –In this study, ten normal children and ten autistic patients were recruited to test the accuracy of the\\ninformation system and the diagnostic effect of autism. After testing, the accuracy rate of facial expression\\nrecognition is 81.4 percent. This information system can easily identify autistic children. The feasibility ofrecognizing autism through facial expression is verified.\\nResearch limitations/implications –The CK þfacial expression database contains some adult facial\\nexpression images. In order to improve the accuracy of facial expression recognition for children, more facial\\nexpression data of children will be collected as training samples. Therefore, the recognition rate of the\\ninformation system will be further improved.\\nOriginality/value –This research uses facial expression data and the latest artificial intelligence technology,\\nwhich is advanced in technology. The diagnostic accuracy of autism is higher than that of traditional systems,so this study is innovative. Research topics come from the actual needs of doctors, and the contents and\\nmethods of research have been discussed with doctors many times. The system can diagnose autism as early as\\npossible, promote the early treatment and rehabilitation of patients, and then reduce the economic and mental\\nburden of patients. Therefore, this information system has good social benefits and application value.\\nKeywords Facial expression data, FER2013, CK þ, Deep convolution neural network, VGG19, Resnet18,\\nAutism, Diagnostic information system\\nPaper type Research paper\\n1. Introduction\\nFacial expression recognition is an important social cognitive skill. Emotions are expressed\\nby facial expressions. Therefore, recognition and understanding of facial expressions is theFacial\\nexpressions for\\nautism\\ndiagnosis\\nThis research has been possible thanks to the support of projects: National Natural Science Foundation\\nof China (No. 61772375) and Independent Research Project of School of Information ManagementWuhan University (No: 413100032).The current issue and full text archive of this journal is available on Emerald Insight at:\\nhttps://www.emerald.com/insight/0737-8831.htm\\nReceived 31 August 2019\\nRevised 16 December 2019\\nAccepted 23 January 2020\\nLibrary Hi Tech\\n© Emerald Publishing Limited\\n0737-8831\\nDOI 10.1108/LHT-08-2019-0176', metadata={'source': 'papers/zhao2020.pdf', 'page': 0}),\n",
       "  Document(page_content='basis of communication and interpersonal relationships with others. Abnormal expression is\\na prominent manifestation of autism, and it is also one of the criteria for the diagnosis of\\nautism. Doctors can diagnose autism by responding to abnormal facial expressions in\\nchildren.\\nAutism, also known as autism or autism disorders, is a representative disease of\\ngeneralized developmental disorders. In recent years, the incidence of autism in children has\\nbecome higher and higher, experiencing a transition from rare diseases to epidemics. At\\npresent, research on autism is still in its infancy at home and abroad, and research methods\\nand tools are still developing.\\nThe main symptoms of autism include impaired social and interpersonal communication,\\nlanguage retardation, repetitive behavior and sensory dysfunction. It is difficult for autistic\\npatients to correctly recognize faces and explain facial emotions. They have different\\nemotional expressions from ordinary people, and they cannot correctly perceive and\\nunderstand some basic expressions such as anger ( Yan, 2008 ).\\nAt present, the diagnostic methods for autism spectrum disorders include: traditional\\nstandard DSM-IV-TR ( Segal, 2010 ) and ICD-10 ( Organization W H, 1992 ), various autism\\ndiagnostic assessment scales such as “Childhood Autism Rating Scale (CARS) ”,“the autism\\nchild behavior scale (ABC) ”and autism behavior rating scale and questionnaire interviews\\n(Wang and Lu, 2015 ). Most of these methods rely on doctors ’direct observation of the\\npatient ’s expression, speech and behavior based on their experience. Diagnostic results are\\neasily disturbed by external factors such as hospital level, physician ’s subjective level,\\npatient ’s education level, age and so on. There are relatively large subjective factors, resulting\\nin a certain degree of missed diagnosis and misdiagnosis. It takes about 1 –2 h for each autistic\\npatient to diagnose, so doctors have a lot of work to do. The best period of treatment for\\nautistic patients is before the age of six. Early diagnosis is of great significance for the\\nrehabilitation of autistic patients.\\nThe purpose of our research and design is to train the model and make a facial expression\\nrecognition system based on the normal expression, so as to verify the abnormal expression.\\nThis system can test the facial expression of autistic children and judge the difference\\nbetween autistic children and normal children.\\nIn this study, FER2013 and CK þwere used as the main facial expression training\\nsamples. At the same time, we collected the facial expression image data of 16 Chinese\\nchildren as a supplementary sample of facial expression. With the help of VGG19 andResnet18 algorithm models of deep convolution neural network, according to the hospital\\nautism diagnosis scale and diagnosis process, this paper studies and designs an information\\nsystem for the diagnosis of autism by facial expression data. After the actual test of recruiting\\ntesters, the recognition rate of the system is 81.4 percent. It can effectively distinguish\\nwhether the expression of children is normal or not. It provides a practical information system\\nfor the diagnosis of autism. This paper will continue to collect more children ’s facial\\nexpression data from different countries and regions as training samples to further improve\\nthe recognition rate of facial expressions.\\nThe autism diagnosis information system designed in this study has the following\\nimportant significance:\\n(1) Autism can be diagnosed as early as possible by using this system. The best time to\\ntreat autism is before the age of six. The earlier the diagnosis of autism is made, the\\nless the treatment cost and the higher the probability of recovery. Early diagnosis is\\nof great value in alleviating the burden on families and society of autistic patients.\\nThe system can be published in the form of app or web pages and disseminated\\nthrough the Internet. The system can be installed and used on different devices, such\\nas computers, mobile phones, tablets, etc. It has good applicability. Through thisLHT', metadata={'source': 'papers/zhao2020.pdf', 'page': 1}),\n",
       "  Document(page_content='system, autism can be diagnosed conveniently, and time can be saved for the early\\ntreatment of autism patients, especially those in underdeveloped areas.\\n(2) It can make the diagnosis of autism more objective. The whole diagnosis process is\\ncompleted by the system. Because artificial intelligence technology is used to\\nrecognize facial expressions without human intervention, the diagnosis results are\\nobjective and accurate.\\n(3) Reduce the intensity of doctors ’work. Before the system was used, it took an hour for\\ndoctors to diagnose an autistic patient. By using this system, doctors can save a lot of\\ntime and pay attention to the treatment of autism.\\n(4) The facial expression database used in the training of this system contains different\\nraces in the world. Therefore, this system can not only diagnose children in different\\ncountries and regions but also diagnose suspected autism patients all over the world.\\n(5) This research designs the system according to the actual business. The early design\\nof the system adopts the suggestions of several doctors, so it is designed and\\nmanufactured according to the actual needs of doctors. Although there are some\\npapers on autism diagnosis by facial expressions at home and abroad, there are still\\nfew autism diagnosis systems developed which can be used in practice.\\n(6) This paper uses the latest in-depth learning technology to improve the accuracy of\\nfacial expression recognition. Previous traditional techniques and methods have low\\nrecognition rate of facial expressions. In recent years, with the development of\\nartificial intelligence technology and the improvement of computing speed, the\\nconvolutional neural network has greatly improved the accuracy of facial expression\\nrecognition, which is the innovation of this research in technology.\\n2. Facial expression database and its recognition technology\\n2.1 Facial expression database\\nFacial expression is an important way for peopl e to express their emotions. In the social\\nprocess, facial expression is an important way to judge the attitude and inner feelings of\\nthe other party ( Lanlan, 2018 ).Mehrabian (2008) found that in a conversation, the change\\nof facial expression played the most important role. Of these, 55 percent are facial\\nexpressions, 38 percent are voic e and only 7 percent are words ( Mei and Hu, 2015 ).\\nCompared with voice, expression can convey more abundant information. Recognition\\nand understanding of facial expressions i s very important for communicating with\\nothers ( Shen et al. , 2013 ). In 1972, Ekman demonstrated through empirical research that\\nhuman beings have six basic facial expression s: happiness, sadness, anger, fear, disgust\\nand surprise ( Ekman, 1992 ). In subsequent studies, neutral expression has also been\\nadded to the basic expression, and it is gen erally believed that there are seven basic\\nexpressions in facial expression.\\nWith the continuous development of computer software and hardware technology, people\\nhave a deeper understanding of facial expression recognition technology. In order to better\\nstudy facial expression recognition technology, many international research institutions\\nhave established standard facial expression databases, the main facial expression databases\\nare as follows:\\n(1) JAFFE\\nThe database stores facial expression data of Japanese women. It contains 213 facial images\\nof ten Japanese women. There are seven types of facial expressions, namely neutral, happy,Facial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020.pdf', 'page': 2}),\n",
       "  Document(page_content='sad, surprise, anger, disgust and fear. The resolution of each image is 256 3256 pixels.\\nEveryone has seven kinds of pictures of facial expressions.\\n(2) CKþ\\nThe expression database was collected under laboratory conditions. It includes African\\nAmericans, Asians and South Americans. The resolution of each image is 640*480 pixels. It\\ncontains 593 expression sequences of 123 people, 69 percent of whom are female and 31\\npercent are male. Each sequence begins and ends with neutral expression, which includes the\\nprocess from calm to strong expression. CK þis a facial expression data set with many\\napplications. The reliability of various facial expression evaluation experiments using this\\ndatabase is very high. It includes seven types of facial expressions: anger, contempt, disgust,\\nfear, happy, sadness and surprise.\\n(3) FER2013\\nThere are 35,887 facial images in the library, and there are seven facial expression types:\\nangry, disgust, fear, happy, sad, surprise and neutral. The resolution of each image is 48*48\\npixels. All the images are gray images. There are three sample sets: 28,709 images in the\\ntraining set; 3,589 images in the validation set and 3,589 images in the test set.\\n(4) MMI\\nThe expression database can be divided into two parts: one is a dynamic data set composed of\\nmore than 2,900 video sequences. The other part is a static data set consisting of a large\\nnumber of high resolution images. There are seven types of expression in the library.\\n(5) AFEW\\nAll the facial images in the database are edited from the movies and contain seven basic facial\\nexpressions.\\n(6) SFEW\\nThe expression library is a static frame image extracted from the AFEW data set, which\\ncontains seven basic expressions.\\n2.2 Facial expression recognition process\\nThe process of facial expression recognition includes two stages as shown in Figure 1 : One is\\nthe training stage and the other is the recognition stage. The training and recognition stages\\ncan be divided into three parts: the pretreatment of facial expression images, the extraction of\\nfacial expression features and the classification of facial expressions. The training stage is to\\ntrain the model in order to achieve the purpose that the model can be used. The recognition\\nstage is to recognize and classify the expression of the test image ( Du, 2018 ).\\nThe two stages of expression recognition process include the following processes: First,\\nface detection is carried out on the image in the expression database, including the location,\\nFigure 1.\\nFacial expression\\nrecognition processLHT', metadata={'source': 'papers/zhao2020.pdf', 'page': 3}),\n",
       "  Document(page_content='alignment and clipping of the face area. This is the basis of the follow-up process. Only when\\nthe expression area is accurately obtained, the following series of work will be more accurate.\\nAfter the face area is detected, the image needs to be preprocessed in order to eliminate the\\nnoise caused by the influence of acquisition equipment and environment and avoid the\\ninterference of feature extraction. Then it is the feature extraction step, which aims to extract\\nthe features that can represent the essence of expression from the preprocessed facial images.\\nIn this process, in order to avoid the high dimension of feature extraction and affect the\\nefficiency of the algorithm, we need to reduce the dimension of extracted features in order to\\nextract the most representative expression features. Finally, the extracted facial features are\\nclassified to determine which type of facial expression is.\\n2.3 Facial expression recognition technology\\nFacial expression recognition technology mainly includes traditional machine learning\\ntechnology and deep learning technology. The two technologies have similarities and\\ndifferent characteristics.\\n(1) Traditional machine learning technology\\nFacial expression recognition algorithm based on traditional machine learning includes three\\nsteps: image preprocessing, facial expression feature extraction and feature classification.\\nFirst, for the convenience of feature extraction, it is necessary to preprocess the image,\\nwhich can effectively avoid the interference of various noises and leave the key information\\nneeded by the face. The pretreatment process includes image gray processing, face\\nalignment, face size tailoring, data enhancement, brightness, pose normalization, etc. ( Li and\\nDeng, 2018 ).\\nSecond, the traditional feature extraction methods include directional gradient histogram\\nfeature, Gabor filter feature, local directional pattern feature and enhanced local binary\\nalgorithm. Because these methods are artificial design, time-consuming and laborious, and\\nhave certain limitations and often have better effect in feature extraction in small sample\\nimage set, most of the current studies are based on deep learning feature extraction method.\\nThere are many basic machine learning methods for expression classification, such as\\nsupport vector machine (SVM), hidden Markov model (HMM) and k-nearest classification\\nalgorithm.\\n(2) Deep learning technology\\nFacial expression recognition algorithm based on deep learning also needs image\\npreprocessing. The difference is that it often combines feature extraction and feature\\nclassification into an end-to-end model, which greatly simplifies the process of facial\\nexpression recognition. In addition to end-to-end learning, deep learning algorithm can be\\nused to extract facial expression features, and then other independent classifiers can be used.\\nFor example, SVM or random forest algorithm is used to process the extracted features and\\nclassify them.\\nIn this paper, we construct a facial expression recognition model based on deep learning\\ntechnology, extract facial expression feature data of children and classify them into groups,\\nso as to diagnose autism.\\n2.4 Driving role of facial expression data\\nResearch on facial expression recognition has been applied in a series of life scenarios. In\\nchildren ’s education, advanced human-computer interaction, medical diagnosis and other\\naspects have played an important role ( Cai, 2018 ).\\nIn distance education or classroom teaching, teachers can better improve students ’\\nlearning quality by observing students ’emotional changes in the classroom and adjustingFacial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020.pdf', 'page': 4}),\n",
       "  Document(page_content='teaching plans in time. Advanced human-computer interaction can make human-computer\\ninteraction more harmonious. For example, intelligent robots can automatically respond to\\nthe facial expressions of their interlocutors. In medical diagnosis, facial expressions also play\\nan important role in the prevention and diagnosis of diseases. For example, this article is to\\ndiagnose autism by analyzing children ’s facial expressions.\\n3. Autism and facial expression diagnosis\\n3.1 Autism and its development\\nAutism is a neurodevelopmental disorder, which is collectively referred to as autism\\nspectrum disorder ( Duan et al., 2015 ).\\nSince Kanner, an American child psychiatrist, first reported autism in 1943, the incidence\\nof autism has risen rapidly worldwide. In the 1980s, about 3 –5 out of every 10,000 people\\nsuffered from the disease, while in 2000, 6.7 out of every 1,000 children suffered from the\\ndisease ( Vismara and Rogers, 2008 ). According to the National Center for Health Statistics,\\nthe probability of autism among children aged 3 –14 in the United States reached 2.76 percent\\nin 2016 ( Zablotsky et al., 2017 ).\\nThere is no statistical survey on autistic children in China. However, according to the data\\nof the report on the development of China ’s autism education and rehabilitation industry II,\\nthe number of people with autism in China is estimated to exceed 10 million, of which 2 million\\nare autistic children. At the same time, it is growing at the rate of nearly 200,000 annually\\n(Beijing Wucai Deer Autism Research Institute, 2017 ).\\nAutism brings serious financial burden to both society and family. Families with autistic\\nchildren, on the one hand, spend a lot of time caring for their children, while working hours are\\nreduced so that work income is reduced. On the other hand, the cost of family rehabilitation\\ntreatment for autistic children is huge, which increases the family ’s financial burden ( Wu and\\nChen, 2018 ). According to the survey on the occupational and economic burden of preschool\\nautistic children ’s families, 33 percent of parents of autistic children reported that their\\ncaregiving problems seriously affected their careers, and their annual income was\\nsignificantly lower than that of ordinary families, with an average loss of income of 30,957\\nyuan per year. Meanwhile, the average annual cost of autistic children ’s families for children ’s\\neducation and training is significantly higher than that of ordinary families ( Yang and Wang,\\n2014). The society and the government also need to invest a lot of money in the rehabilitation\\neducation of autistic children. At the same time, autism also brings high subjective load and\\ndepression to the families of patients, which has a negative impact on their quality of life\\n(Singh et al., 2017 ;Wang et al., 2018 ). It can be seen that the incidence of autism in children is\\nrelatively serious, and the harm to society and family is enormous.\\n3.2 Diagnosis of autism through facial expressions\\n3.2.1 Facial expression recognition disorder. Autistic children have facial expression\\nrecognition obstacles, which are mainly manifested in their inability to recognize facial\\nexpressions ( Liuet al., 2015 ). It is easy to distinguish autistic children from normal children by\\nobserving their facial expressions. Therefore, we combine facial expression recognition\\ntechnology to extract facial expression response feature vectors and use artificial intelligence\\ntechnology to distinguish normal group and autistic group based on these facial features.\\n3.2.2 The principle of diagnosing autism through facial expressions. A large number of\\nstudies have pointed out that autistic patients have deficiencies in facial expression\\nrecognition and understanding. This is the core source of impaired social function in\\nautistic patients ( Yang et al. , 2017 ). Autistic children are more difficult to identify other\\npeople ’s emotional behavior, and it is difficult to make appropriate judgment and responseLHT', metadata={'source': 'papers/zhao2020.pdf', 'page': 5}),\n",
       "  Document(page_content='(Shen et al. , 2013 ). Overseas research on facial expression recognition ability of autistic\\npatients has been carried out not only in children but also in adults. Most studies believe that\\nthe ability of facial expression recognition of autistic patients is low. Baron-Cohen et al. (1997)\\nused standard facial expression maps to study the recognition of different emotional types in\\nautistic adults. It was found that autistic adults had better recognition of some basic facial\\nexpressions, such as happiness, but relatively complex facial expressions such as surprise\\nrecognition were difficult to recognize.\\nAt present, the main diagnostic criteria of autism are: IDC-10, DSM-IV, the autism child\\nbehavior scale (ABC), the children autism rating scale (CARS) and the Clancy behavior scale\\n(CABS) ( Wang, 2007 ).\\nAfter consulting a large number of literatures and investigating the actual situation of the\\nhospital, now the hospital mainly uses CABS (filled by parents), ABC (filled by parents) and\\nCARS (filled by doctors) to diagnose autism. After a detailed review of the test items of the\\nthree scales, these scales all contain the test items to judge autism through children ’s facial\\nexpressions. There were 14 items in the CABS scale, of which the seventh item was\\ninexplicable laughter and the tenth item was not looking at each other ’s face. Avoiding eye\\ncontact was related to expression. There were 57 items in the ABC scale, of which the seventh\\nitem was non-communicative smile, the seventeenth item did not respond to other people ’s\\nfacial expressions, and the twenty-fourth item was active avoidance of eye contact with\\nothers. Fifteen items of the CARS scale, the third of which is emotional response, pleasure and\\nunhappiness and interest, are expressed by changes in facial expression and posture. These\\nscales basically include the items of autism detection by children ’s facial expressions, which\\nshow that the diagnosis of autism can be more accurate by facial expressions. With the\\nprogress of artificial intelligence technology, facial expression recognition technology can\\nobjectively and effectively reflect the mental health of children and can be used in early\\ndiagnosis of autism ( Yanbin et al., 2018 ).\\nWe also communicated with doctors of Hubei Maternal and Child Health Hospital, Wuhan\\nChildren ’s Hospital and Guangzhou Women and Children ’s Medical Center many times, and\\nactually checked the process of using the above autism diagnostic scale to diagnose children.\\nThe doctor observes the tester ’s reaction to determine whether the tester is autistic after\\nrequesting the tester to make the corresponding expression. Doctors point out that facial\\nexpression is an important part of autism diagnosis. In terms of system design, they put\\nforward requirements and suggestions for the process of diagnosing autism through facialexpression.\\n4. Research and development of autism diagnosis information system\\n4.1 Facial expression database selection\\nThe expression databases in this study mainly come from two public expression databases\\nCKþand FER2013. In addition, 16 Chinese children ’s expression data were collected as\\nsupplementary samples. The two public expression databases are standard and international\\nand have been widely used, including facial expression data of adults and children. Each\\nsample in the database contains seven expressions: angry, disgust, fear, happy, sad, surprise\\nand neutral. Because children ’s facial expressions are different from adults, in order to\\nimprove the recognition rate of children ’s facial expressions, we collected facial expression\\ndata of 16 children aged 5 to 8 in China. Seven expressions were collected from each child. We\\ncombine Chinese children ’s facial expression data and public expression database as our\\nsystem ’s facial expression database.\\n4.1.1 FER2013 facial expression database. The reason for choosing FER2013 expression\\ndatabase is that it has more samples and is more mature than other expression databases. It\\nhas advantages in model training. At the same time, it has been used in many studies (see\\nPlate 1 ).Facial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020.pdf', 'page': 6}),\n",
       "  Document(page_content='4.1.2 CK þFacial expression database. CKþfacial expression database was selected\\nbecause it was collected in the laboratory, so its accuracy is relatively high ( Lucey et al., 2010 )\\n(seePlate 2 ).\\n4.1.3 Facial expression data of Chinese children. At present, the mature facial expression\\ndatabases at home and abroad are mainly based on adult male or female facial expression\\nimages. Therefore, it is urgent to establish a facial expression database for children.\\nFacial images of children are quite diffe rent from those of adults. Children have\\nrounder faces, larger eyes and less prominent bones. Because of these differences,\\nchildren ’s facial features are less obvious and more difficult to recognize than adults.\\nBecause of the particularity of children, it is very difficult to collect children ’sf a c i a l\\nimages. In order to improve the recognition rate of children ’s facial expressions, we\\ncooperated with Amy Education School in Zhengzhou. Sixteen healthy children as\\nvolunteers were recruited to collect facial expression data. Each of them collected seven\\nkinds of expressions, totaling 112 pictures. These children are between 5 and 8 years old,\\nincluding 8 boys and 8 girls. The acquisition environment is quiet and there is no\\nexternal interference. High-d efinition cameras are used t o collect facial expression\\nimages, which are processed professionally. Bef ore collecting facial expression data,\\nparents have been informed of the purpose o f collecting facial expression data. After\\nquestioning with parents, all the children who participated in the collection of facial\\nexpression data had no history of autism.\\nWe loaded the expression data into the training sample library. The purpose of collecting\\nChinese children ’s facial expression data is to increase the number of Chinese children ’s facial\\nexpression samples in training samples and improve the recognition rate of the system for\\nchildren ’s facial expression. The collection process and the collected children ’s facial\\nexpression data are shown in Plate 3 .\\n4.2 Network topology\\nAccording to the network environment and equipment of the information service platform,\\nthe network topology can be divided into four levels. The network topology diagram is shown\\ninFigure 2 .\\nThe first layer is the application layer, which consists of users, computers and various\\nsmart devices. Smart devices include smart tablet computer, smartphones and other\\nelectronic devices. Users access and use the information service platform through computers\\nand various smart devices.\\nPlate 1.\\nFER2013 facialexpression database\\nPlate 2.CKþFacial expression\\ndatabaseLHT', metadata={'source': 'papers/zhao2020.pdf', 'page': 7}),\n",
       "  Document(page_content='The second layer is the communication layer, mainly based on the internet network\\nenvironment, providing access channels for users and systems.\\nThe third layer is the application server layer, which is composed of firewall and\\napplication server and has an ontology display system for autism. The application server\\nmanages various business functions, handles various business requests submitted by users\\nand can access the database server for various data exchange.\\nThe fourth layer is the database server layer, which stores all kinds of data and knowledge\\nresources of the information service platform.\\n4.3 System architecture\\nThe smart diagnosis system of autism adopts client/server architecture. The client includes\\ndifferent versions of programs suitable for computers and smartphones. The system\\narchitecture diagram is shown in Figure 3 .\\nThe client includes three main modules: user interaction, image acquisition and face\\ndetection. User interaction module is responsible for human-computer interaction. According\\nto the requirements of the autism diagnostic scale, users who diagnose are required to make\\nappropriate expressions and feedback by prompting pictures and voice guidance. Through\\nthe camera, the image acquisition module can dynamically capture facial expression images.\\nAt the right time, the system will collect facial expression images and transmit them to the\\nface detection module. Face detection module recognizes the valid face features and\\ncompresses the image and transfers it to the server through the internet or mobile Internet.\\nThe server includes six main modules: image processing, feature extraction, group\\nclassification, automatic diagnosis, training model and data management. The image\\nprocessing module can receive the expression image transmitted by the client and then\\nprocess the expression image and transfer it to other modules on the server side. The feature\\nextraction module receives the facial expression images provided by the image processing\\nmodule and extracts the facial expression features. The group classification module is\\nFigure 2.\\nNetwork topology\\ndiagramPlate 3.\\nCollection of facial\\nexpression data of\\nChinese childrenFacial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020.pdf', 'page': 8}),\n",
       "  Document(page_content='responsible for group classification and correctly classifies the expression images into the\\nmost matching expressions among the seven kinds of expressions. The automatic diagnosis\\nmodule gives the diagnosis of autism by comparing the facial expressions that the tester is\\nrequired to imitate and the facial expressions that the tester actually makes. Model training\\nmodule is the core module of the system, which is responsible for recognizing and processing\\nthe newly collected facial expression images. The data management module mainly manages\\nfacial expression data, including storing and reading facial expression images transmitted by\\nthe client.\\nThe system server stores facial expression feature files, which are formed by feature\\nextraction of facial expression database. The expression feature file is HDF5 file format. The\\nexpression recognition system running on the server can read the expression feature file at\\nany time. If new facial expression samples are collected, the model can be retrained and the\\nfacial expression feature files can be updated.\\nThe client collects the tester ’s facial expression data by high-definition camera and\\ntransmits the facial expression data to the server by JSON file according to TCP communication\\nprotocol. The facial expression recognition system running on the server processes the collected\\nfacial expression data and then feeds the recognition results back to the tester through the\\nnetwork and stores the recognition results and facial expression data in the server database.\\nFacial data and diagnostic system are stored on a server, and the recognition results and facial\\ndata are stored in the SQL Server database. The diagnostic system reads data from the\\ndatabase through SQL structured query language. The response time of the whole database\\noperation and communication process should not exceed 5 s.\\n4.4 System architecture\\n4.4.1 VGG19 model. Researchers from the Oxford University and the Google Brain have\\njointly developed the convolutional neural network VGG. VGNet consists of 11, 13, 16 and 19\\nlayers of neural networks [20].VGNet constructs 16 –19 layers of neural networks by stacking\\nsmall convolution cores of 3 33 and maximum pooling layers of 2 32 repeatedly. VGGNet\\nFigure 3.\\nSystem architectureLHT', metadata={'source': 'papers/zhao2020.pdf', 'page': 9}),\n",
       "  Document(page_content='has strong scalability and greatly reduces the error rate when extending. When migrating to\\nother image data, it has good generalization ability and simple structure.\\n4.4.2 ResNet18 model. ResNet was proposed by Kaiming He and others of Microsoft\\nResearch institute. They have successfully trained 152 layers of neural networks by using\\nResNet unit. The structure of ResNet can accelerate the training of the neural network, and\\nthe accuracy of the model has been greatly improved.\\n4.4.3 Graphic of deep learning framework. The deep learning framework used in this paper\\nfor facial expression recognition is shown in Plate 4 .\\nThe whole process includes image input, image preprocessing, model building, model\\ntraining, model testing and output of expression recognition results. There are two kinds of\\ndeep learning algorithms used in this paper: VGG19 and ResNet18. ResNet18 solves the\\nproblem of network performance degradation caused by the high depth of VGG19. By\\ntraining the two models and synthesizing the two convolutional neural network models, the\\nfacial expression features of autistic children can be extracted accurately.\\n4.4.4 Image preprocessing. The purpose of image preprocessing is to achieve uniform\\nnormalization of the final input image. The process is shown in Figure 4 .\\nConverting an image to a grayscale image can reduce the computational complexity of the\\nlatter pixel level and also reflect the overall and local distribution and characteristics of the\\nimage. Then, image transformation is used to enhance data by zooming, rotating, cutting and\\ntranslating, and the image is located in the center of the window. The contrast and brightness\\nof the image can be improved by histogram equalization to reduce the influence of\\nillumination on expression feature learning. In order to make the image uniform, it is planned\\nto transform the image size into the same size by normalizing the image size. Finally, the\\nmask is used to remove the occlusion of non-face areas.\\n4.4.5 Model training. Before model training, we need to enhance the image data. We choose\\nSGD random gradient descent algorithm as the optimization method. The batch size is still\\n128 by default, and the learning rate is set to 0.01 initially. In addition, the initialization of\\nPlate 4.\\nFramework of deep\\nlearning\\nFigure 4.\\nImage preprocessingFacial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020.pdf', 'page': 10}),\n",
       "  Document(page_content='Plate 5.\\nFacial expression\\nrecognition resultsLHT', metadata={'source': 'papers/zhao2020.pdf', 'page': 11}),\n",
       "  Document(page_content='network parameters is also very important. We have adopted a random initialization method\\nto train the two network algorithms. The core code of Python is as follows:\\n# Model training\\ndeftrain (epoch ):\\nifepoch >learning_rate_decay_start andlearning_rate_decay_start >=0:\\nfrac =(epoch -learning_rate_decay_start )/ /learning_rate_decay_every\\ndecay_factor =learning_rate_decay_rate **frac\\ncurrent_lr =opt.lr *decay_factor\\nutils .set_lr (optimizer ,current_lr )# set the decayed rate\\nelse :\\ncurrent_lr =opt.lr\\nforbatch_idx ,(inputs ,targets )inenumerate (trainloader ):\\nifuse_cuda :\\ninputs ,targets =inputs .cuda (),targets .cuda ()\\noptimizer .zero_grad ()\\nutils .clip_gradient (optimizer ,0.1)\\noptimizer .step ()\\ncorrect +=predicted .eq(targets .data ).cpu().sum ()\\n4.4.6 Recognition results. Through the trained model, we use some children ’s facial\\nexpressions pictures and videos to test, and get the probability of various expressions and the\\nfinal prediction results of the model. As shown in Plate 5 , the histogram shows the probability\\nof each type of facial expression, and the histogram of maximum probability is the final\\nrecognized facial expression. After testing, the recognition rate of children ’s facial expression\\nreaches 81.4 percent, which can effectively distinguish whether children ’s facial expression is\\nnormal or not.\\n5. System validation\\n5.1 Testing environment\\nIn this study, two kinds of mobile phones, personal computers and servers are selected as test\\nenvironments. The hardware and software environments are shown in Table I .\\n5.2 Diagnostic procedure and interface of diagnostic system\\nThe diagnostic process is shown in Figure 5 . First, the system randomly displays one of the\\nseven kinds of facial expressions for the tester to imitate. The system will prompt the tester\\nTesting equipment Hardware environment Software environment\\nOPPO R17 mobile phone CPU:SDM670 RAM:8GB Android\\nIPhone 8 mobile phone CPU:A11 RAM:2GB iOSPersonal computer CPU:Intel i7 RAM:16GB Windows 10Server CPU:Intel W2133 RAM:16GB Windows Server 2019\\nFigure 5.\\nAutomatic diagnostic\\nprocedureTable I.\\nTesting environmentFacial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020.pdf', 'page': 12}),\n",
       "  Document(page_content='to imitate the facial expression by pictures a nd sounds. For example, the system displays\\nhappy cartoon smiling faces, plays happy children ’s songs and induces children to make\\nhappy expressions. The system displays t he same expression example three times and\\ncollects the tester ’se x p r e s s i o nd a t aa tt h es a m et i m e .T h e nt h es y s t e mc o m p a r e st h e\\nexpression examples and the actual collect ed expression data and gives the diagnosis\\nresults.\\n5.2.1 Diagnostic procedure. The diagnostic process is shown in Figure 5 . First, the system\\nrandomly displays one of the seven kinds of facial expressions for the tester to imitate. The\\nsystem will prompt the tester to imitate the facial expression by pictures and sounds. For\\nexample, the system displays happy cartoon smiling faces, plays happy children ’s songs and\\ninduces children to make happy expressions. The system displays the same expression\\nexample three times and collects the tester ’s expression data at the same time. Then the\\nsystem compares the expression examples and the actual collected expression data and gives\\nthe diagnosis results.\\n5.2.2 Interface of diagnostic system. The system diagnostic interface is designed according\\nto the diagnostic process (see Plate 6 ).\\n5.3 System testing\\n5.3.1 Test sample. We recruited ten normal children and ten autistic children and divided\\nthem into normal children group and autistic children group for comparative verification.\\nThe accuracy of the system is verified by the actual test of the autism diagnosis information\\nsystem.\\nThe normal group of children was provided by Amy Education School in Zhengzhou,\\nwhich cooperated with us. Ten healthy children as volunteers were recruited as the normal\\nPlate 6.\\n(a) The main interface\\nof the autism smart\\ndiagnosis informationsystem, including\\nsystem introduction,\\nknowledgeintroduction of autism\\nand other functions. (b)\\nThe facial expressionthat the system\\nprompts the tester to\\nsimulate after starting\\nthe diagnostic process.\\n(c) The expressionanalysis after\\ndiagnosis. (d) The\\nresult given by thesystem after three\\ndiagnosesLHT', metadata={'source': 'papers/zhao2020.pdf', 'page': 13}),\n",
       "  Document(page_content='group for testing. These children were between 5 and 8 years old, including 5 boys and 5 girls.\\nParents were informed of the purpose and content of the experiment before the experiment.\\nChildren who participated in the experiment had no history of autism after being asked by\\ntheir parents.\\nThe autistic children were provided by Guangzhou Children ’s Care Center, which\\ncooperated with us. Ten volunteers of autistic children were recruited as the autistic children\\ngroup for testing. These children were aged between 3 and 6 years old, including 5 boys and 5\\ngirls. Parents were informed of the purpose and content of the experiment before the\\nexperiment. The selected children with autism were diagnosed by a professional physician.\\n5.3.2 Test environment and process. All the tests were conducted in quiet classrooms\\nwithout noise and external factors. Through our autism diagnosis information system, each\\nchild was prompted by pictures and sounds to imitate seven kinds of facial expressions and\\nprompted to make corresponding facial responses according to the facial expressions on the\\npictures. At this time, the camera will capture their facial expressions, and after system\\nanalysis, they will be saved in the form of pictures in the computer of the test system (see\\nPlate 7 ).\\n5.3.3 Test result. We used the system to test the normal combination and autistic children\\nrespectively. Finally, we compared the recognition rate of the two groups.\\nFrom Table II , the average recognition rate of each expression is angry 80 percent, disgust\\n70 percent, fear 80 percent, happy 100 percent, sad 80 percent, surprise 70 percent and neutral\\n90 percent.\\nTest child 1 only had a disgusting expression recognition error, and other facial\\nexpression recognition was correct, then the average recognition rate of the seven facial\\nFacial expressionNumber of test children\\nCorrect identification (Y 5Yes, No 5N)\\nAverage recognition rate % 1234567891 0\\nAngry Y Y Y N Y Y Y Y Y N 80\\nDisgust N Y Y Y N Y N Y Y Y 70Fear Y Y Y N Y Y Y Y N Y 80Happy Y Y Y Y Y Y Y Y Y Y 100S a d YYNYYYYYYN 8 0\\nSurprise Y N Y N Y Y N Y Y Y 70\\nNeutral Y Y Y Y Y Y Y Y Y N 90Plate 7.\\n(a) Test environment\\nfor normal children\\ngroup. (b) Test\\nenvironment for\\nautistic children group\\nTable II.\\nTest results in normal\\nchildren groupFacial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020.pdf', 'page': 14}),\n",
       "  Document(page_content='expressions of test child 1 was 85.7 percent. According to this method, the average\\nrecognition rates of the seven expressions from test child 1 to test child 10 were 85.7 percent,\\n85.7 percent, 85.7 percent, 57.1 percent, 85.7 percent, 100 percent, 71.4 percent, 100 percent,\\n85.7 percent and 57.1 percent, respectively. The average recognition rate is 81.4 percent.\\nJudging by the 60 percent threshold, there are two test children ’s facial expression\\nrecognition rates at 57.1 percent. This shows that in real environment, the algorithm of the\\nsystem is affected by the environment and light, and the accuracy will be affected to a certain\\nextent. However, according to the accuracy of 81.4 percent, it can basically meet the\\npreliminary diagnostic requirements of whether the expression is abnormal or not. In the\\nfuture, more real samples will be added to further improve the accuracy of the system\\nalgorithm.\\nThe experimental results show that the errors mainly concentrate on the expressions of\\ndisgust and surprise. The main reasons are as follows:\\n(1) Disgust and surprise have only minor local changes in the faces of the two kinds of\\nexpressions, and there is no significant distinguishing feature.\\n(2) Some of the participants had little change in the two facial expressions, did not have\\nthe obvious features of the corresponding categories, approached neutral expressions\\nand were easy to confuse.\\nFrom Table III , the average recognition rate of each expression is angry 50 percent, disgust 10\\npercent, fear 30 percent, happy 60 percent, sad 60 percent, surprise 20 percent and neutral 10\\npercent.\\nThe average recognition rates of the seven expressions from test child 1 to test child 10\\nwere 28.5 percent, 28.5 percent, 28.5 percent, 42.9 percent, 42.9 percent, 57.1 percent, 28.5\\npercent, 42.9 percent, 28.5 percent and 14.3 percent, and the average recognition rate is 34.3\\npercent.\\nFacial expressionNumber of test children\\nCorrect identification (Y 5Yes, No 5N)\\nAverage recognition rate % 1234567891 0\\nAngry Y N N N Y Y N Y Y N 50\\nDisgust N N N Y N N N N N N 10\\nFear N N Y N Y Y N N N N 30Happy N Y Y Y N Y N Y N Y 60S a d NYNYYYNYYN 6 0Surprise Y N N N N N Y N N N 20Neutral N N N N N N Y N N N 10\\nFacial expression Normal children group Autistic children group\\nAngry 80% 50%\\nDisgust 70% 10%Fear 80% 30%Happy 100% 60%Sad 80% 60%Surprise 70% 20%\\nNeutral 90% 10%\\nAverage recognition rate % 81.4% 34.3%Table III.\\nTest results in autistic\\nchildren group\\nTable IV.\\nComparisons of two\\ngroups of children ’s\\nfacial expression\\nrecognition rateLHT', metadata={'source': 'papers/zhao2020.pdf', 'page': 15}),\n",
       "  Document(page_content='The experimental results show that the recognition rate of happiness and sadness is\\nhigher in the seven expressions. Testing children showed difficulty in identifying complex\\nfacial expressions such as neutrality and aversion (see Table IV ).\\nThe experimental results showed that the recognition rate of facial expressions in autistic\\nchildren was significantly lower than that in normal children. All the autistic children who\\nparticipated in the test had a facial recognition rate of less than 60 percent. Therefore, if the\\naccuracy rate of facial expression diagnosis by the system was less than 60 percent, the tester\\nwould have a tendency to suffer from autism. The lower the recognition rate, the higher the\\ntendency of autism.\\n6. Conclusion\\nIn the era of rapid development of information technology, the processing of a large number\\nof health data has brought new opportunities and challenges to medical research. The\\nincidence of autism is increasing, which has attracted more and more attention from all\\naspects of society. The use of information technology, especially artificial intelligence\\ntechnology, to build an autism diagnosis system has become an urgent need for doctors and\\npatients. In this paper, an autism diagnosis system based on deep convolution neural network\\nand expression data is constructed. After testing, it can meet the design requirements of\\nautism diagnosis. The public can download and use the system through the network to\\ndiagnose autism conveniently. In addition, we will expand the function of the system,\\nincrease the recognition of children ’s physical movement and realize the diagnosis of autism\\nfrom multiple perspectives.\\nBecause the average age of children using and collecting facial expression data is between\\n3 and 8 years old, the system can recognize children aged 3 –6 years old. Therefore, through\\nthis system, autism can be diagnosed as soon as possible. The earlier the diagnosis and\\ntreatment of autism is, the better the rehabilitation effect. Therefore, it is of great significance\\nfor the treatment of autism.\\nBecause the training samples of the system adopt the international open facial expression\\ndatabase, which contains the facial expression data of children and adults in different\\ncountries and regions, the system can diagnose autism for children and adults in different\\ncountries and regions.\\nOf course, the system also needs to be improved through practical use. Next, we will\\narrange for the system to be tested in a large number of cooperative hospitals. Next, there are\\ntwo main tasks to be done. The first is to collect more data of normal and autistic children ’s\\nfacial expressions, improve the recognition effect of the system on children ’s facial\\nexpressions and establish a special database of children ’s facial expressions. The second is to\\nimprove the system function, according to the results of facial expression diagnosis of autistic\\nchildren for a detailed classification, to distinguish between severe, moderate and mild autism\\npatients, in order to facilitate the treatment of doctors.\\nThis study hopes to be helpful to the diagnosis of autism in remote and underdeveloped\\nareas, so as to promote the early diagnosis and treatment of autistic children and reduce the\\nmedical costs and burdens of autistic families and society. Therefore, this study has more\\nimportant social significance and application value.\\nReferences\\nBaron-Cohen, S., Wheelwright, S., Jolliffe, T. (1997), “Is there a “language of the eyes ”? Evidence from\\nnormal adults, and adults with autism or asperger syndrome ”,Visual Cognition , Vol. 4 No. 3,\\npp. 311-331.\\nBeijing Wucai Deer Autism Research Institute (2017), Report on the Development of Autism Education\\nand Rehabilitation Industry in China 2 , Huaxia Publishing House, Beijing.Facial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020.pdf', 'page': 16}),\n",
       "  Document(page_content='Cai, Y. (2018), “Facial tracking and facial expression recognition based on in-depth learning ”\\nSoutheast University.\\nDu, J. (2018), “Research on face expression recognition based on Kernel relieff ”Zhengzhou University.\\nDuan, Y., Wu, X. and Jinfeng (2015), “Research progress on etiology and treatment of autism ”,Chinese\\nScience: Life Science , Vol. 9, pp. 820-844.\\nEkman, P. (1992), “An argument for basic emotions ”,Cognition and Emotion , Vol. 6 Nos 3-4,\\npp. 169-200.\\nLanlan (2018), “Research on facial expression recognition method based on multi-feature fusion ”, Jilin\\nUniversity.\\nLi, S. and Deng, W. (2018), “Deep facial expression recognition: a survey ”arXiv preprint arXiv:\\n1804.08348.\\nLiu, Y., Huo, W. and Hu, X. (2015), “Summary of research on facial expression recognition of autistic\\nchildren ”,Modern Special Education , Vol. 8, pp. 35-39.\\nLucey, P., Cohn, J.F., Kanade, T., Saragih, J. and Ambadar, Z. (2010), “The extended cohn-kanade\\ndataset (ckþ): a complete dataset for action unit and emotion-specified expression ”,2010 IEEE\\nComputer Society Conference on Computer Vision and Pattern Recognition-Workshops ,\\n2010, IEEE.\\nMehrabian, A. (2008), “Communication without words ”,Communication Theory , Vol. 6, pp. 193-200.\\nMei, J. and Hu, B. (2015), “Research and implementation of real-time face expression recognition\\nmethod ”,Information and Technology , Vol. 44 No. 4, pp. 145-148.\\nOrganization W H (1992), The ICD-10 Classification of Mental and Behavioural Disorders: Clinical\\nDescriptions and Diagnostic Guidelines , World Health Organization, Geneva.\\nSegal, D.L. (2010), “Diagnostic and statistical manual of mental disorders (DSM-IV-TR) ”,The Corsini\\nEncyclopedia of Psychology , Vol. 1 No. 16, pp. 1-3.\\nShen, X., He, Z. and Ding, X. (2013), “Computer facial expression recognition training to improve the\\nfacial expression recognition ability of autistic children ”,Sci-tech Horizon , Vol. 25, pp. 12-13.\\nSingh, P., Ghosh, S. and Nandi, S. (2017), “Subjective burden and depression in mothers of children\\nwith autism spectrum disorder in India: moderating effect of social support ”,Journal of Autism\\nand Developmental Disorders , Vol. 47 No. 10, pp. 3097-3111.\\nVismara, L.A. and Rogers, S.J. (2008), “The early start denver model ”,Journal of Early Intervention ,\\nVol. 31 No. 1, pp. 91-108.\\nWang, H. (2007), “Psychological and behavioral characteristics, diagnosis and evaluation of autistic\\nchildren ”,Chinese Journal of Rehabilitation Medicine , Vol. 22 No. 9, pp. 853-856.\\nWang, G. and Lu, M. (2015), “Research on educational games for children with autism spectrum\\ndisorders ”,Modern Special Education , Vol. 14, pp. 38-40.\\nWang, Y., Xiao, L. and Chen, R. (2018), “Social impairment of children with autism spectrum disorder\\naffects parental quality of life in different ways ”,Psychiatry Research , Vol. 2018 No. 266,\\npp. 168-174.\\nWu, X. and Chen, S. (2018), “Research progress on quality of life and its influencing factors of primary\\ncaregivers for autistic children ”,General Nursing , Vol. 16 No. 18, pp. 2206-2208.\\nYan, S. (2008), “Experimental study on facial expression processing of autistic children ”, East China\\nNormal University.\\nYanbin, H., Fuxing, W., Heping, X., Jing, A., Yuxin, W. and Huashan, L. (2018), “Facial processing\\ncharacteristics of autism spectrum disorders: meta-analysis of eye movement research ”,\\nProgress in Psychological Science , Vol. 1, pp. 26-41.\\nYang, Y. and Wang, M. (2014), “Employment and financial burdens of families with preschool-aged\\nchildren with autism ”,Chinese Journal of Clinical Psychology , Vol. 22 No. 2, pp. 295-297, 361.LHT', metadata={'source': 'papers/zhao2020.pdf', 'page': 17}),\n",
       "  Document(page_content='Yang, J., Xing, H., Shao, Z. and Yuan, J. (2017), “Facial expression sensitivity deficits in patients with\\nautism spectrum disorder: impact of task nature and implications for intervention ”,Chinese\\nScience: Life Science , Vol. 47 No. 4, pp. 443-452.\\nZablotsky, B., Black, L.I. and Blumberg, S.J. (2017), “Estimated prevalence of children with diagnosed\\ndevelopmental disabilities in the United States, 2014-2016 ”,NCHS Data Brief , Vol. 291, pp. 1-8.\\nCorresponding author\\nWang Zhao can be contacted at: creativesoft@sohu.com\\nFor instructions on how to order reprints of this article, please visit our website:\\nwww.emeraldgrouppublishing.com/licensing/reprints.htmOr contact us for further details: permissions@emeraldinsight.comFacial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020.pdf', 'page': 18})]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing -----> papers/15_Nazneen_clean.txt\n",
      "processed -----> papers/15_Nazneen_clean.txt\n",
      "processing -----> papers/1_Ramırez-Duque__clean.txt\n",
      "processed -----> papers/1_Ramırez-Duque__clean.txt\n",
      "processing -----> papers/22_Ouss_ASD_clean.txt\n",
      "processed -----> papers/22_Ouss_ASD_clean.txt\n",
      "processing -----> papers/Abbas_2018_clean.txt\n",
      "processed -----> papers/Abbas_2018_clean.txt\n",
      "processing -----> papers/Abbas_2020_clean.txt\n",
      "processed -----> papers/Abbas_2020_clean.txt\n",
      "processing -----> papers/Asd_Cry_patterns_clean.txt\n",
      "processed -----> papers/Asd_Cry_patterns_clean.txt\n",
      "processing -----> papers/carpenter2020 (1)_clean.txt\n",
      "processed -----> papers/carpenter2020 (1)_clean.txt\n",
      "processing -----> papers/Dawson_clean.txt\n",
      "processed -----> papers/Dawson_clean.txt\n",
      "processing -----> papers/LEE_clean.txt\n",
      "processed -----> papers/LEE_clean.txt\n",
      "processing -----> papers/Patten_Audio_clean.txt\n",
      "processed -----> papers/Patten_Audio_clean.txt\n",
      "processing -----> papers/Qiu_clean.txt\n",
      "processed -----> papers/Qiu_clean.txt\n",
      "processing -----> papers/Tariq2018_clean.txt\n",
      "processed -----> papers/Tariq2018_clean.txt\n",
      "processing -----> papers/Tariq_2019_clean.txt\n",
      "processed -----> papers/Tariq_2019_clean.txt\n",
      "processing -----> papers/Young_Behavior_clean.txt\n",
      "processed -----> papers/Young_Behavior_clean.txt\n",
      "processing -----> papers/zhao2020_clean.txt\n",
      "processed -----> papers/zhao2020_clean.txt\n"
     ]
    }
   ],
   "source": [
    "#chunks \n",
    "\n",
    "import os \n",
    "files=os.listdir('papers')\n",
    "files\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "import re\n",
    "texts=[]\n",
    "for file in files:\n",
    "    if file.endswith(\".txt\"):\n",
    "        files_path=os.path.join('papers',file)\n",
    "        print('processing ----->',files_path)\n",
    "        loader=TextLoader(files_path)\n",
    "        document=loader.load()\n",
    "        texts.append(document)\n",
    "        print('processed ----->',files_path)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(page_content='', metadata={'source': 'papers/15_Nazneen_clean.txt'})],\n",
       " [Document(page_content='https://doi.org/10.1007/s10846-018-00975-y\\nRobot-AssistedAutismSpectrumDisorderDiagnosticBased\\nonArtiﬁcialReasoning\\nAndr´esA.Ram ´ırez-Duque1·AnselmoFrizera-Neto1·TeodianoFreireBastos1\\nReceived:25April2018/Accepted:20December2018\\n©SpringerNatureB.V.2019\\nAbstract\\nAutism spectrum disorder (ASD) is a neurodevelopmental disorder that affects people from birth, whose symptoms are\\nfound in the early developmental period. The ASD diagnosis is usually performed through several sessions of behavioral\\nobservation, exhaustive screening, and manual coding behavior. The early detection of ASD signs in naturalistic behavioral\\nobservation may be improved through Child-Robot Interaction (CRI) and technological-based tools for automated behavior\\nassessment. Robot-assisted tools using CRI theories have been of interest in intervention for children with Autism Spectrum\\nDisorder (CwASD), elucidating faster and more significant gains from the diagnosis and therapeutic intervention when\\ncompared to classical methods. Additionally, using computer vision to analyze child’s behaviors and automated video coding\\nto summarize the responses would help clinicians to reduce the delay of ASD diagnosis. In this article, a CRI to enhance\\nthe traditional tools for ASD diagnosis is proposed. The system relies on computer vision and an unstructured and scalable\\nnetwork of RGBD sensors built upon Robot Operating System (ROS) and machine learning algorithms for automated face\\nanalysis. Also, a proof of concept is presented, with participation of three typically developing (TD) children and three\\nchildren in risk of suffering from ASD.\\nKeywords Child-Robot interaction ·Autism spectrum disorder ·Convolutional neural network ·Robot reasoning model ·\\nStatistical shape modeling\\n1Introduction\\nResearch in Child-Robot Interaction (CRI) aims to provide\\nthe necessary conditions for the interaction between a\\nchild and a robotic device taking into account some\\nfundamental features, such as child’s neurophysical and\\nphysical condition, and the child’s mental health [ 1].\\nThat is how Robot-Assisted Therapies (RAT) using CRI\\ntheories have been of interest as an intervention for\\nCwASD, elucidating faster and more significant gains from\\nthe therapeutic intervention when compared to traditional\\ntherapies [ 2–4].\\nASD is a neurodevelopmental disorder that affects people\\nfrom birth, and its symptoms are found in the early\\n/envelopebackAndr ´es A. Ram ´ırez-Duque\\naaramirezd@gmail.com\\n1Universidade Federal do Espir ´ıto Santo., Av. Fernando Ferrari,\\n514 (29075-910), Vitoria, Brazildevelopmental period. Individuals suffering from ASD\\nexhibit persistent deficits in social communication, social\\ninteraction and repetitive patterns of behavior, interests, or\\nactivities [ 5]. Some of the ASD signs may be observed\\nbefore the age of 10 months, although a reliable diagnosis\\ncan only be performed at 18 months of age, according to [ 6],\\nor 24 months according to [ 7].\\nThe use of computer vision to analyze the child’s\\nbehaviors, and automated video coding to summarize the\\ninterventions, can help the clinicians to reduce the delay\\nof ASD diagnosis, providing the CwASD with access\\nto early therapeutic interventions. In addition, CRI-based\\nintervention can transform traditional diagnosis methods\\nthrough a robotic device to systematically elicit child’s\\nbehaviors that exhibit ASD signs [ 8].\\nSome of the first systems developed to assist ASD\\ntherapists and make diagnosis based on robotic devices\\nhave primarily been open loop and remotely operated sys-\\ntems. However, these approaches are unable to perform\\nautonomous feedback to enhance the interaction [ 9–11].JournalofIntelligent&RoboticSystems(2019)96:267–281\\n/Publishedonline:29 2019March\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.Nevertheless, different systems are able to modify the\\nbehavior of the robot according to environmental interac-\\ntions and the child’s response, using a closed-loop and\\nartificial cognition approaches [ 12–16]. These systems have\\nbeen hypothesized to offer technological mechanisms for\\nsupporting more flexible and potentially more naturalistic\\ninteraction [ 17]. In fact, literature reports that automatic\\nrobot’s social behaviors modulation according to specifics\\nscenarios has a strong effect on child’s social behavior\\n. However, despite the increase of positive evidence,\\nthis technology has rarely been applied to specific ASD\\ndiagnosis.\\nThis work aims to present a robot-assisted framework\\nusing an artificial reasoning module to assist clinicians with\\nthe ASD diagnostic process. The framework is composed of\\na responsive robotic platform, a flexible and scalable vision\\nsensor network, and an automated face analysis algorithm\\nbased on machine learning models. In this research we take\\nadvantage of some neural models available as open sources\\nprojects to build a completely new pipeline algorithm for\\nglobal recognition and tracking of child’s face among many\\nfaces present in a typical unstructured clinical intervention,\\nin order to estimate the child’s visual focus of attention\\nalong the time. The proposed system can be used in different\\nbehavioral analysis scenarios typical of an ASD diagnostic\\nprocess. In order to illustrate the feasibility of the proposed\\nsystem, in this paper an experimental trial to assess joint-\\nattention behavior is presented employing an in-clinic setup\\n(unstructured environment).\\nThe main contributions of this paper are: (i) the\\ndevelopment of a new artificial reasoning module upon\\na flexible and scalable ROS-based vision system using\\nstate-of-the-art machine learning neural models; (ii) the\\nproposal and implementation of a supervised CRI (child-\\nrobot interaction) based on an open source social robotic\\nplatform to enhance the traditional tools for ASD diagnosis\\nusing an in-clinic setup protocol. For the best of our\\nknowledge, there are no open source projects available for\\nface analysis based on a multi-camera approach using ROS\\nwith the characteristics described in our research.\\n2RelatedWork\\nRecent researches have shown the acceptance and efficiency\\nof technologies used as auxiliary tools for therapy\\nand teaching of individuals with ASD [ 18–21]. Such\\ntechnologies may also be useful for people surrounding\\nASD individuals (therapists, caregivers, family members).\\nFor example, the use of artificial vision systems to measure\\nand analyze the child’s behavior can lead to alternative\\nscreening and monitoring tools that help the clinicians to\\nget feedback from the effectiveness of the intervention [ 22].Additionally, social robots have great potential for aid in\\nthe diagnosis and therapy of children with ASD [ 18,23].\\nA higher degree of control, prediction and simplicity may\\nbe achieved in interactions with robots, impacting directly\\non frustration and reducing the anxiety of these individuals\\n.\\nRespect to the use of computer vision techniques,\\nprevious studies already analyzed child’s behaviors, such\\nas visual attention, eye gaze, eye contact, smile events, and\\nvisual exploration using cameras and eye trackers [ 25,26]\\nand RGBd cameras [ 27,28]. These studies have shown\\nthe potential of vision systems in improving the behavioral\\ncoding in ASD therapies. However, these studies did not\\nimplement techniques of CRI to enhance the intervention.\\nOn the other hand, studies about how CwASD respond\\nto a robot mediator compared to human mediator have\\nbeen reported, such as intervention scenarios with imitation\\ngames [ 29,30], telling stories [ 9] and free play tasks\\n[12,31]. These works used features, such as proxemics,\\nbody gestures, visual contact and eye gaze as behavioral\\ndescriptors, whereas the behavior analysis was estimated\\nusing manual video coding.\\nResearchers of Vanderbilt University published a series\\nof research showing an experimental protocol to assess joint\\nattention (JA) tasks defined as the capacity for coordinated\\norientation of two people toward an object or event [ 6].\\nThe protocol consisted of directing the attention of the\\nchild towards objects located in the room through adaptive\\nprompts [ 32]. Bekele et al. inferred the participant’s eye\\ngaze by the head pose, which was calculated in real-time by\\nan IR camera array [ 17]. In their last works, Zheng et al. and\\nWarren et al. used a commercial eye tracker to estimated the\\nchildren’s eye gaze around the robot and manual behavioral\\ncoding for global evaluation [ 10,33]. However, eye tracker\\ndevices require pre-calibration and may limit the movement\\nof the individual. The results of these works showed that the\\nrobot attracted children’s attention and that CwASD reached\\nall JA task. Nevertheless, developing JA tasks is more\\ndifficult with a robot than with humans [ 10]. Anzalone et al.\\ndeveloped a CRI scenario using the NAO robot to perform\\nJA tasks, in which the authors used an RGBD camera\\nto estimate only body and head movements. The results\\nshowed that JA performance of children with ASD was\\nsimilar to the performance of TD children when interacting\\nwith the human mediator, however, with a robot mediator,\\nthe children with ASD presented a lower performance than\\nthe TD children, i.e, the children with ASD needed more\\nsocial cues to finalize the task [ 34]. Chevalier et al. analyzed\\nin their study, some features, such as proprioceptive and\\nvisual integration in CwASD, using an RGBD sensor\\nto record the interventions sessions and manual behavior\\ncoding to analyzed the participants’ performance [ 35]. In\\nnone of the previous works, a closed-loop subsystem wasJIntellRobotSyst(2019)96:267–281 268\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.implemented to provide some level of artificial cognition to\\nenable automated robot behavior.\\nIn contrast with the aforementioned researches, other\\nworks implemented automated face analysis and artificial\\ncognition through robot-mediator and computer vision,\\nwhich analyzed child’s engagement [ 36,37], emotions\\nrecognition capability [ 13,15,38] and child’s intentions\\n[14,16]. In these works, two different strategies were\\nimplemented, where the most common is based on mono-\\ncamera approach using an external RGB or RGBd sensor\\n[15,36,37] or using on-board RGB cameras mounted\\non the robotic-platform [ 13,16]. Other strategies are\\nbased on a highly structured environment composed of\\nan external camera plus an on-board camera [ 38]o ra\\nnetwork of vision sensors attached to a small table [ 14].\\nThese strategies based on multi-camera methods improve\\nthe system’s performance, but remain constrained in relation\\nto desired features, such as flexibility, scalability, and\\nmodularity. Thus, despite the potential that these techniques\\nhave shown, achieving automated child’s behavior analysis\\nin a naturalistic way into unstructured clinical-setups with\\nrobots that interact accordingly remains a challenge in CRI.\\n3SystemArchitectureOverview\\nThe ROS system used in this work is a flexible and\\nscalable open framework for writing modular robot-\\ncentered systems. Similar to a computing operating system,\\nROS manages the interface between robot hardware and\\nsoftware modules and provides common device drivers, data\\nstructures and tool-based packages, such as visualization\\nand debugging tools. In addition, ROS uses an interface\\ndefinition language (IDL) to describe the messages sent\\nbetween process or nodes, this feature facilitates the multi-\\nlanguage (C++, Python and Lisp) development [ 39].\\nThe overall system developed here was built using a\\nnode graph architecture, taking advantages of the principal\\nROS design criteria. As with ROS, our system consists of\\na number of nodes to local video processing together a\\nrobot’s behavior estimation, distributed around a number\\nof different hosts and connected at runtime in a peer-to-\\npeer topology. The inter-node connection is implemented\\nas a hand-shaking and occurs in XML-RPC protocol\\nalong with a web-socket communication for robot’s web-\\nbased node (/ONO node, see Fig. 1). The node structure\\nis flexible, scalable and can be dynamically modified,\\ni.e., each node can be started and left running along an\\nexperimental session or resumed and connected to each\\nother at runtime. In addition, from a general perspective,\\nany robotic platform with web-socket communication can\\nbe integrated. The developed system is composed of two\\ninterconnected modules as shown in Fig. 1: an artificialreasoning module and a CRI-channel module. The module\\narchitectures are detailed in the following subsections.\\n3.1ArchitectureofReasoningModule\\nIn this module, a distributed architecture for local video\\nprocessing is implemented. The data of each RGBD sensor\\nin the multi-camera system are processed for two nodes,\\nin which the first is a driver level node and the second\\nis a processing node. The driver1node transforms the\\nstreaming data of the RGBD sensor into the ROS messages\\nformat. The driver addresses the data through a specialized\\ntransport provided by plugings to publishes images in a\\ncompressed representations while the receptor node only\\nsees sensor msgs/Image messages. The data processing\\nnode executes the face analysis algorithm. This node uses\\naimage transport subscriber and a ROS packages called\\nCvBridge to turn the data into a image format supported for\\nthe typical computer vision algorithms. Later, the same node\\npublishes the head pose and eye gaze direction by means of\\na ROS navigation message defined as nav msgs/Odometry .\\nAn additional node hosted in the most powerful\\nworkstation carries out a data fusion of all navigation\\nmessages that were generated in the local processing\\nstage. In addition to the fusion, this node computes the\\nvisual focus of attention (VFOA) and publishes this as a\\nstd msgs/Header , in which the time stamp and the target\\nname of the VFOA estimation are registered.\\n3.2ArchitectureofCRI-Channel\\nThe system proposed here has two bidirectional communi-\\ncation channels, a robot-device, and a web-based applica-\\ntion to interact with both the child and the therapist. The\\nrobot device can interact with the CwASD executing differ-\\nent physical actions, such as facial expression, upper limb\\nposes, and verbal communication. Thus, according to the\\nchild’s performance, the reasoning module can modify the\\nrobot’s behavior through automatic gaze shifting, chang-\\ning the facial expression and providing sound rewards. The\\nclient-side application was developed to allow the therapist\\nto control and register all step of the intervention proto-\\ncol. This interface was also used to supervise and control\\nthe robot’s behavior and to offer feedback to the therapist\\nabout the child’s performance along the intervention. This\\nApp has two channels of communication for interacting\\nwith the reasoning module. The first connection uses a web-\\nsocket protocol and a RosBridge suite package to support\\nthe interpretation of ROS messages, as well as, JSON-based\\ncommands in ROS. The second one uses a ROS module\\n1Tools for using the Kinect One (Kinect V2) in ROS, https://github.\\ncom/code-iai/iai kinect2 .JIntellRobotSyst(2019)96:267–281 269\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.Fig.1 Node graph architecture\\nof the proposed ROS-based\\nsystem. The system is composed\\nof two interconnected modules,\\nan artificial reasoning module\\nand a CRI-channel module. The\\nONO web server has two way of\\nbidirectional communication: a\\nwebsocket and a standard ROS\\nSubscriber\\ndeveloped in the server-side application to directly run a\\nROS node and communicate with standard ROS publishers\\nand subscribers.\\n4TheRoboticPlatformONO\\nThe CRI is implemented through the open source platform\\nfor social robotics (OPSORO),2which is a promising\\nand straightforward system developed for face to face\\ncommunication composed of a low-cost modular robot\\ncalled ONO (see Fig. 2) and web-based applications\\n. Some of the most important requirements and\\ncharacteristics that make ONO interesting for this CRI\\nstrategy are explained in the following sections.\\n4.1AppearanceandIdentity\\nThe robot is covered in foam and also fabric to have a more\\ninviting and huggable appearance to the children. The robot\\nhas an oversized head to make its facial expressions more\\nprominent and to highlight the importance for communica-\\ntion and emotional interaction. As a consequence of its size\\nand pose, children can interact with the robot at eye height\\nwhen the robot is placed on a table.\\nThe robot ONO has not a predefined identity, as the\\nonly element previously conceived is the name. Unlike other\\nrobots that have well-defined identities, such as Probo [ 9]\\nor Kaspar [ 41], in this work the ONO’s identity is built with\\nthe participation of the child through a co-creation process.\\nFor this reason, a neutral appearance is initially used. In the\\n2Open Source Platform for Social Robotics (OPSORO) http://www.\\nopsoro.com .intervention, the therapist can provide the child with clothes\\nand accessories to define the identity of ONO.\\n4.2MechanicsPlatform\\nAs the initial design of ONO is composed only of the\\nactuated face, in this work it was needed to provide the ONO\\nwith some body language. For this purpose, motorized arms\\nwere designed and implemented.\\nThe new design of ONO has a fully face and two\\narms actuated, giving a total of 17 Degrees of Freedom\\n(DOF). The ONO is able to perform facial expressions and\\nnonverbal cues, such as waving, shake hands and pointing\\ntowards objects, moving its arms (2 DOF x 2), eyes (2 DOF\\nx 2), eyelids (2 DOF x 2), eyebrows (1 DOF x 2), and mouth\\n(3 DOF). The robot has also a sound module that allows\\nexplicit positive feedback as well as reinforcement learning\\nthrough playing words, conversations and other sounds.\\n4.3SocialExpressiveness\\nIn order to improve social interaction with a child, the ONO\\nis able to exhibit different facial expressions. The ONO’s\\nexpressiveness is based on the Facial Action Coding System\\n(FACS) developed in [ 42]. Each DOF that composes the\\nONO’s face is linked with a set of Action Units (AU) defined\\nby the FACT, and each facial expression is determined for\\nspecific AU values. The facial expressions are represented as\\na 2D vector fe=(v, a) in the emotion circumplex model\\ndefined by valence and arousal [ 9]. In this context, the basic\\nfacial expressions are specified on a unit circle, where the\\nneutral expression corresponds to the origin of the space\\nfe0=(0,0). The relation between the DOF position and\\nthe AU values is resolved through a lookup table algorithm\\nusing a predefined configuration file [ 40].JIntellRobotSyst(2019)96:267–281 270\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.Fig.2 ONO robot, developed\\nthrough the open source\\nplatform for social robotics\\n(OPSORO)\\n4.4AdaptabilityandReproducibility\\nThe application of the Do-It-Yourself (DIY) concept is\\nthe principal feature of ONO’s design, which facilitates\\nits dissemination and use in research areas other than\\nengineering as health care. These characteristics allow ONO\\nbuilding for any person without specialized engineering\\nknowledge. Additionally, it is possible to replicate ONO\\nwithout the need for high-end components or manufacturing\\nmachines [ 40]. The electronic system is based on a\\nRaspberry Pi single-board computer combined with a\\ncustom OPSORO module with circuitry to control up to\\n32 servos, drive speakers and touch sensors. Any sensor\\nor actuator compatible with the embedded communication\\nprotocols (UART, I2C, SPI) implemented on the Raspberry\\nPi can be used by this platform.\\n4.5ControlandAutonomy\\nWith the information delivered for the automated reasoning\\nmodule, it was possible to automate the ONO’s behavior\\nand, then, the robot can infer and interpret the children’s\\nintentions to react most accurately to the action performed\\nby them, thus enabling a more efficient and dynamic\\ninteraction with ONO. In this work, the automated ONO’s\\nbehavior is partially implemented, i.e., the framework can\\nmodify some physical actions of ONO using the feedback\\ninformation about the child’s behavior. The actions suitable\\nto be modified are gaze shift toward the child in specifics\\nevents, changing from neutral to positive facial expression\\nwhen the child looks toward the target, and providing\\nsound rewards. Also, an Aliveness Behavior Module (ABM)\\nis implemented to improve the CRI, which consist of\\nblinking the robot’s eyes and changing its arms amongsome predefined poses. Also, the robot can be manually\\noperated through a remote controller hosted in the client-\\nside application.\\n5ReasoningModule:MachineLearning\\nMethodsforChild’sFaceAnalysis\\nThe automated child’s face analysis consists of monitoring\\nnonverbal cues, such as head and body movements, head\\npose, eye gaze, visual contact and visual focus of attention.\\nIn this work, a pipeline algorithm is implemented using\\nmachine learning neural models for face analysis. The\\nchosen methods were developed using state-of-art trained\\nneural models, available by Dlib3 and OpenFace4\\n. Some modification such as, turn the neural model an\\nattribute of the ROS node class and evaluate this in each\\ntopic callback, were needed to run the neural models into a\\ncommon ROS node.\\nThe algorithm proposed for child’s face analysis involves\\nface detection, recognition, segmentation and tracking,\\nlandmarks detection and tracking, head pose, eye gaze and\\nvisual focus of attention (VFOA) estimation. In addition, the\\narchitecture proposed here also implement new methods for\\nasynchronous matching and fusion of all local data, visual\\nfocus of attention estimation based on Hidden Markov\\nModel (HMM) and direct connection with the CRI-channel\\nto influence the robot’s behaviors. A scheme of the pipeline\\nalgorithm is shown in Fig. 3.\\n3Dlib C++ Library http://dlib.net/ .\\n4A Open Source Facial Behavior Analysis https://github.com/\\nTadasBaltrusaitis/OpenFace .JIntellRobotSyst(2019)96:267–281 271\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.Fig.3 Pipeline algorithm of the\\nautomated child’s face analysis\\n5.1Child’sFaceDetectionandRecognition\\nThe in-clinic setup requires differentiate the child’s face\\nfrom other faces detected and found in the scene. For this\\nreason, a face recognition process was also implemented in\\nthis work. First, the face detection is executed to initialize\\nthe face recognition process and, subsequently, initialize\\nthe landmarks detection. In this work, both detection and\\nrecognition are implemented using deep learning models,\\nwhich are described in this section.\\nIn the detection process, a Convolutional Neural Network\\n(CNN) based face detector with a Max-Margin Object\\nDetection (MMOD) as loss layer is used [ 45]. The CNN\\nconsist first of a block composed of three downsampling\\nlayers, which apply convolution with a 5x5 filter size and\\n2×2 stride to reduce the size of the image up to eight\\ntimes its original size and generate a feature map with 16\\ndimensions. Later, the result are processed for one more\\nblock composed of four convolutional layers to get the final\\noutput of the network. The three first layers of the last\\nblock have 5 ×5 filter size and 1x1 stride, but, the last layer\\nhas only 1 channel and a 9 ×9 filter size. The values in\\nthe last channel are large when the network thinks it has\\nfound a face at a particular location. All convolutional block\\nabove are implemented with two additional layers among\\nconvolutional layers, pointwise linear transformation, and\\nRectified Linear Units (RELU) to apply the non-saturating\\nactivation function f( x)=max(0,x). The training dataset\\nused to create the model is composed of 6975 faces and is\\navailable at Dlib’s homepage.5\\nThe face recognition algorithm used in this work is\\ninspired on the deep residual model from [ 46]. The\\n5http://dlib.net/files/data/dlib face detection dataset-2016-09-30.tar.\\ngz.residual network (ResNet) model developed by He et. al\\nreformulates the convolutional layers to learn a residual\\nfunctions F(x) :=H(x) −xwith reference to the layer\\ninputs x, instead of learning unreferenced functions. In the\\npractical implementation, the previous formulation means\\ninserting shortcut connections, which turn the network into\\nits counterpart residual version [ 46]. The CNN model then\\ntransforms each face detected to a 128D vector space in\\nwhich images from the same person will be close to each\\nother, but faces from different people will be far apart.\\nFinally, the faces are classified as child’s face, caregiver’s\\nface and therapist’s face.\\nBoth detection and recognition CNN model were\\nimplemented and trained from [ 43] and released in Dlib\\n19.6.\\n5.2FaceAnalysis,Landmarks,HeadPoseandEye\\nGaze\\nThis work uses the technique for landmarks detection, head\\npose and eye gaze estimation developed by Baltru ˇsaitis et\\nal., named Conditional Local Neural Fields (CLNF) [ 47].\\nThis technique is an extension of the Constrained Local\\nModel (CLM) algorithm using specialized local detectors\\nor patch experts. CNLF model consists of a statistical\\nshape model, which its learned from data examples and is\\nparametrized for mcomponents of linear deformation to\\ncontrol the possible shape variations of the non-rigid objects\\n. Approaches based on CLM [ 49,50]a n dC L N F[ 47]\\nmodel the object appearance in a local fashion, i.e, each\\nfeature point has its own appearance model to describe the\\namount of misalignment.\\nCLNF-based landmark detection consists of three main\\nparts: the shape model, the local detectors or patch experts,\\nand the fitting algorithm, which are detailed below.JIntellRobotSyst(2019)96:267–281 272\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.5.2.1ShapeModel\\nThe CLNF technique uses a linear model to describe non-\\nrigid deformations called Point Distribution Model (PDM).\\nThe PDM is used to estimate the likelihood of the shapes\\nbeing in a specific class, given a set of feature points [ 48].\\nThis is important for model fitting and shape recognition.\\nThe shape of a face that has nlandmark points can be\\ndescribed as:\\nX=[X1,X2,...,X n,Y1,Y2,...,Y n,Z1,Z2,...,Z n],\\n(1)\\nand the class that describes a valid instance of a face using\\nPDM can be represented as:\\nX=¯X+/Phi1q, (2)\\nwhere ¯Xis the mean shape of the face, /Phi1described\\nthe principal deformation modes of the shape, and q\\nrepresent the non-rigid deformation parameters. Both ¯X\\nand/Phi1are learned automatically from labeled data using\\nPrincipal Component Analysis (PCA). The probability\\ndensity distribution of the instances into the shape class is\\nexpressed as a zero mean Gaussian with Covariance matrix\\n/Lambda1=([λ1;...;λm])evaluated at q:\\np(q)=N(q;0;/Lambda1)=1√(2π)m|/Lambda1|exp{\\n−1\\n2(qT/Lambda1−1q)}\\n(3)\\nOnce the model is defined, it is necessary to place the 3D\\nPDM in an image space. The following equation is used\\nto transform between 3D space to image space using weak\\nperspective projection [ 49]:\\nxi=s·R2D·(¯Xi+/Phi1iq)+t, (4)\\nwhere ¯Xi=[ ¯xi,¯yi,¯zi]Tis the mean value of the ith\\nlandmark. The instance of the face in an image is, therefore,\\ncontrolled using the parameter vector p=[s,w,t,q ],\\nwhere qrepresents the local non-rigid deformation, sis a\\nscaling term, wis the rotation term that controls the 2 ×3\\nmatrix R2D,a n dtis the translation term.\\nThe global parameters are used to estimate the head\\npose in reference to the camera space using orthographic\\ncamera projection and solving the Perspective-n-Point\\n(PnP) problem respect to the detected landmarks. The PDM\\nused in [ 44] was trained on two public datasets [ 51,52].\\nThis result in a model with 34 non-rigid (Principal modes)\\nand 6 rigid shape parameters.\\n5.2.2PatchExperts\\nThe patch experts scheme is the main novelty implemented\\nin the CLNF model. The new Local Neural Field (LNF)patch expert takes advantage of the non linear relationship\\nbetween pixel values and the patch response maps. The LNF\\ncaptures two kinds of spatial characteristics between pixels,\\nsuch as similarity and sparsity [ 47].\\nLNF patch expert can be interpreted as a three layer\\nperceptron with a sigmoid activation function followed by\\na weighted sum of the hidden layers. It is also similar to\\nthe first layer of a Convolutional Neural Network [ 44].\\nThe new LNF patch expert is able to learn from multiple\\nilluminations and retain accuracy. This becomes important\\nwhen creating landmark detectors and trackers that are\\nexpected to work in unseen environments and on unseen\\npeople.\\nThe learning and inference process is developed using\\na gradient-based optimization method to help in finding\\nlocally optimal model parameters faster and more accu-\\nrately.\\nIn the CLNF model implemented in [ 44], 28 set in total\\nof LNF patch experts were trained for seven views and\\nfour scales. The framework uses patch experts specifically\\ntrained to recognize the eyelids, iris and the pupil, in order\\nto estimate the eye gaze [ 44].\\n5.2.3FittingAlgorithm\\nFor each new image or video frame, the fitting algorithm\\nof CLNF-based landmark detection process attempts to\\nfind the value of the local and global deformable model\\nparameters pthat minimizes the following function [ 49]:\\nE(p)=R(p)n∑\\ni=1Di(xi;I), (5)\\nwhereRis a weight to penalize unlikely shapes, which\\ndepends on the shape model, and Drepresents the\\nmisalignment of the ithlandmark in the image I,w h i c h\\nis function of both the parameters pand the patch experts.\\nUnder the probabilistic point of view, the solution of ( 5)i s\\nequivalent to maximize the a posteriori probability (MAP)\\nof the deformable model parameters p:\\np(\\np|{li=1}n\\ni=1,I)\\n∝p((p))n∏\\ni=1p(li=1|xi,I),(6)\\nwhere, li∈{1,−1}is a discrete random variable indicating\\nwhether the ithlandmark is aligned or misaligned, p(p)\\nis the prior probability of the deformable parameters p,\\nandp(li=1|xi,I)is the probability of a landmark being\\naligned at a particular pixel location xi, which is quantified\\nfrom the response maps created by patch. Therefore, the\\nlast term in ( 6) represents the joint probability of the patch\\nexpert response maps.\\nThe MAP problem is solved using a optimization strategy\\ndesigned specifically for CLNF fitting called non-uniformJIntellRobotSyst(2019)96:267–281 273\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.regularized landmark mean shift (NU-RLMS) [ 47], which\\nuses two step process. The first step evaluates each of\\nthe patch experts around the current landmark using a\\nGaussian Kernel Density Estimator (KDE). The second step\\niteratively updates the model parameters to maximize ( 6).\\nThe NU-RLMS uses expectation maximization algo-\\nrithm, where the E-step involves evaluating the posterior\\nprobability over the candidates, and the M-step finds the\\nparameter updated through the mean shift vector v.T h e\\nmean shift vector points in the direction where the feature\\npoint should go, but the motion is restricted by the statisti-\\ncal shape model and the R(p). This interpretation leads to\\nthe new update function:\\nargmin\\n/Delta1p{\\n∥J/Delta1 p−v∥2\\nW+r∥p+/Delta1p∥2\\n˜/Lambda1−1}\\n, (7)\\nwhere ris a regularization term, Jis the Jacobian, which\\ndescribe how the landmarks location are changing based\\non the infinitesimal changes of the parameters p,˜/Lambda1−1=\\ndiag([0;0;0;0;0;0;λ−1\\n1;...;λ−1\\nm]),a n d Wallows for\\nweighting of mean-shift vectors. Non-linear least squares\\nleads to the following update rule:\\n/Delta1p=−(\\nJTWJ+r/Lambda1−1)(\\nr/Lambda1−1p−JTWv)\\n.( 8 )\\nTo construct W, the performance of patch experts on training\\ndata is used.\\n5.3DataFusion\\nThe fusion of the local results for the head pose estimation\\nis done applying a consensus over the rotation algorithm\\n. This algorithm consists of calculating the weighted\\naverage pose between each camera estimation and its\\nimmediate sensors’ estimation neighbors using the axis-\\nangle representation. The local pose is penalized by two\\nweights: the alignment confidence of landmarks detection\\nprocedure and the Mahalanobis distances between the head\\npose and a neutral pose.\\n5.4FieldofView(FoV)andVisualfocusofAttention\\n(VFOA)\\nThe VFOA estimation model is implemented as a dynamic\\nBayesian network through a Hidden Markov Model\\n(HMM). The model assumes a specific set of child’s\\nattention attractors or targets F. The estimation process\\ndecodes the sequence of child’s head poses Ht=\\n(Hyaw\\nt,Hpitch\\nt)∈R2in terms of VFOA states Ft∈F\\nat time t. The probability distribution of the head\\nposes in reference to a given VFOA target is represented\\nby a Gaussian distribution, whereas the transitions amongthese targets are represented by the transition matrix A.T h e\\nHMM equations can then be written as follows:\\nP(H t|Ft=f,μh\\nt)=N(Ht|μh\\nt(f ), /Sigma1 H(f )) (9)\\np(Ft=f|Ft−1=ˆf)=Afˆf. (10)\\nThe Gaussian covariances is defined manually to reflect\\ntarget sizes and head pose estimation variability. Moreover,\\nthe Gaussian means corresponding to each specific target\\nμh\\ntis calculated through a gaze model that sets this\\nparameter as a fixed linear combination of the target\\ndirection and the head reference direction [ 55]:\\nμh\\nt(f )=α⋆μ t(f )+(12−α) ⋆ R t, (11)\\nwhere ⋆denotes the component wise product 1 2=(1,1),\\nα=(αyaw,αpitch)=(0.7,0.5)are adjustable constants\\nthat describe the fraction of the gaze shift that corresponds\\nto the child’s head rotation, μt∈(R2)Kis the directions\\nof the given K targets, and Rt∈R2represents the\\nreference direction, which is the average head pose over\\na time window WR. The above assumption describes the\\nbody orientation behavior of any child who tends to orient\\nhimself/herself towards the set of gaze targets to make more\\ncomfortable to rotate his/her head towards different targets\\n.\\nRt=1\\nWRt∑\\ni=t−WRHi. (12)\\nFinally, for the estimation of the VFOA sequence a classic\\nViterbi algorithm of HMM is implemented [ 54].\\n6CaseStudy\\nFor the case study, the vision system is composed of\\nthree Kinect V2 sensors. Each sensor is connected to a\\nworkstation equipped with a processor of Intel Core i5\\nfamily and a GeForce GTX GPU board (two workstation\\nwith GTX960 board, and one workstation with GTX580\\nboard). All workstation are connected through a local area\\nnetwork synchronized using the NTP protocol.6The sensors\\nwere intrinsically and extrinsically calibrated through a\\nconventional calibration process using a standard black-\\nwhite chessboard.7\\n6.1In-clinicSetup\\nA multidisciplinary team of psychologists, doctors and\\nengineers developed a case study using a psychology room\\nequipped with a unidirectional mirror to perform behavioral\\n6Network Time Protocol Homepage, http://www.ntp.org .\\n7Tools for using the Kinect One (Kinect V2) in ROS, https://github.\\ncom/code-iai/iai kinect2 .JIntellRobotSyst(2019)96:267–281 274\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.Fig.4 Representation of the\\ninterventions room of in-clinic\\nsetup\\nobservation appropriately. The room was prepared with a\\ntable and three chairs: one for the child, another for the\\ncaregiver and a third one for the therapist. The robot was\\nplaced on the table, and the following toys, a helicopter,\\na truck and a train, were attached to room’s walls. The\\nRGBD sensors were located close to the walls, and no\\nadditional camera was placed on the robot or the table, so\\nas not to attract the child’s attention. A representation of the\\ninterventions room of in-clinic setup is shown in Fig. 4.\\n6.2InterventionProtocol\\nIn this work, a technology-based system was used as a\\ntool in various stages of the ASD diagnostic process.\\nThe framework can be implemented to extract differentbehavioral features to be assessed, e.g., eye contact,\\nstereotyped movements of the head, concentration and\\nexcessive interest in objects or events. However, for the\\nscope of this research, a specific clinical setup intervention\\nto assess Joint Attention (JA) behaviors is presented. The\\nintervention aims to evaluate the capacity of JA; which can\\nbe divided into three classes: initiation of joint attention\\n(IJA), responding to joint attention bids (RJA), and initiation\\nof request behavior (IRB) [ 6]. The therapist guides the\\nintervention all the time and leverages the robot device as\\nan alternative channel of communication with the child,\\nfor the above, both the specialist and the robot remained\\nin the room during the intervention. The children were\\naccompanied throughout the session by a caregiver who\\nwas oriented not to help the child in the execution of the\\nFig.5 The child’s nonverbal\\ncues elicited by the CRI, to look\\ntowards the therapist, towards\\nthe robot, point and self\\nocclusion\\nJIntellRobotSyst(2019)96:267–281 275\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.Fig.6 Performance of the\\nchild’s face analysis pipeline for\\nthe case study. Face detection\\nand recognition, landmarks\\ndetection, head pose and eye\\ngaze estimation were executed\\ntasks. The exercise developed aimed to direct the attention\\nof the child towards objects located in the room through\\nstimuli, such as, look at, point and speak. The stimuli were\\ngenerated first only by the therapist and later just by the\\nrobot.\\n6.3Subjects\\nThree children without confirmed ASD diagnosis, but with\\nevidence of risk factors, and three typically developing\\n(TD) children as the control group participated in the\\nexperiments. All volunteers participated with their parent’s\\nconsent, which were five boys (3 ASD, 2 TD) and\\none TD girl, between 36 months to 48 months. Each\\nvolunteer participated in one single session. The goal was to\\nanalyze the based-line of the child’s behavior and establish\\ndifferences in the behavioral reaction between TD and ASDchildren for stimuli generated through CRI and leverage the\\nnovelty effect raised by the robot mediator.\\n7ResultsandDiscussion\\nThe child’s nonverbal cues elicited by the CRI can be\\no b s e r v e di nF i g . 5. Some examples of children’s behavior\\ntagged to perform the behavioral coding are shown in the\\nsix pictures. The tagged behaviors were: to look towards\\nan object, towards the robot, and towards the therapist, to\\npoint and, to respond to a prompt of both mediators and self\\nocclusion. Typical occlusion problem, as occlusion by hair,\\nhands and the robot were detected.\\nThe performance of video processing in the proof of\\nconcept session is reported in Fig. 6. In the case study\\nsessions, the child’s face detection and recognition, the\\nFig.7 Evolution over time of\\nthe child’s head/neck rotation\\n(yaw rotation) for a TD group\\nJIntellRobotSyst(2019)96:267–281 276\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.Fig.8 Evolution over time of the child’s head/neck rotation (yaw rotation) for a TD volunteer and VFOA estimation results\\nlandmarks detection, head pose and eye gaze estimation for\\ndifferent viewpoints are shown in Fig. 3. The recognition\\nprocess was able to detect all faces in the session\\nsuccessfully in most cases.\\nThe child’s head pose was captured throughout the\\nsession and analyzed automatically to estimate the evolution\\nover time of child’s head and the VFOA. Along the\\nsession, the child’s neck right/left rotation movement was\\npredominant (Yaw axis), while the neck flexion/extension\\n(Pitch axis) and neck R/L lateral flexion movements (Roll\\naxis) remained approximately constant. The Yaw rotation of\\nthe TD children group is reported in Fig. 7. The vertical light\\nblue stripe indicates the intervention period with therapist-\\nmediator, and the vertical light green stripe represents\\nthe period with robot-mediator. The continuous blue line\\nrepresents the raw data recorded, and the continuous red line\\ndescribes the average data trend. From the observation of the\\nthree plot, the TD children started the intervention looking\\ntowards the robot, evidently, the robot was a naturalistic\\nattention attractor. Subsequently, when the therapist begins\\nthe protocol explaining the tasks, the children attention\\nshifts towards the therapist. The children remained this\\nbehavior until that the therapist introduced the robot-\\nmediator. In this transition, the children’s behaviors, such as,RJA and IJA toward the therapist were observed. Once the\\ntherapist changed the mediation with the robot, the children\\nturned his/her attention to the robot and the objects in the\\nroom.\\nA more detailed analysis of one of the TD volunteers is\\nshown in Fig. 8. The plot (A) shows the overall intervention\\nsession; the plot (B) and plot (C) are a zoom of the period\\nwith therapist and robot mediator, respectively. The colors\\nconvention in the three plots of Fig. 8describes the results\\ngenerated by the automated estimation of VFOA. From\\nthese scenarios, some essential aspects already emerge. In\\nthe therapist-mediator interval, the child responded to JA\\ntask using only one repetition for all prompt level. The\\nchild’s behavior of RJA was according to the protocol,\\ni.e., the child looked towards the therapist to wait for\\ninstructions, rapidly the child searched in the target, and\\nnext looked again toward the therapist (Color sequence:\\nlight blue - yellow - light blue - orange - light blue - red).\\nThis behavior was the same for all prompts. In contrast,\\nwith the robot-mediator, the child did not look toward\\nthe robot among indications at consecutive targets (Color\\nsequence: light green - yellow - orange - red - orange -\\nyellow). The above happened because, in the protocol, both\\nmediators executed the instructions in the same order, andJIntellRobotSyst(2019)96:267–281 277\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.Fig.9 Evolution over time of the child’s head/neck rotation (yaw rotation) for a ASD group\\nthe child memorized the commands and the object’s position\\nuntil the robot mediator interval. This fact did not affect\\nthe intervention’s aim, as the robot mediator succeeded to\\nelicit the child’s behaviors of RJA and IJA. In addition,\\nas highlighted in the plot (A) in Fig. 8, when the session\\nfinalized and the robot mediator said goodbye, again, RJA\\nand IJA behaviors were perceived. The pictures (a-d) show\\nthese events: first the child said goodbye towards the robot,\\nthen, he looked the therapist to confirm that the session\\nended and looked again towards the robot, finally the child\\ntook the robot’s hand.\\nFrom the analysis of the three TD volunteers, the same\\nreported behaviors were perceived. However, the analysis\\nof the children in the ASD group showed different behavior\\npatterns concerning comfort, visual contact and novelty\\nstimulus effect during the sessions. The evolution over time\\nof the child’s head/neck rotation (yaw rotation) for an ASD\\ngroup is shown in Fig. 9. On the one hand, the three children\\nin the ASD group maintained more visual contact with\\nthe robot compared to the therapist and exhibited more\\ninterest in the robot platform compared to the TD children.\\nHowever, the performance of the children in the activities\\nof JA did not improve significantly when the robot executed\\nthe prompt. On the other hand, the clinicians manifested that\\nin all cases the first visual contact toward them occurred\\nin the instant that the robot entered the scene and startedinteracting, i.e., the ONO mediation elicited behaviors of\\nIJA towards the therapist. In addition, the CwASD exhibited\\nless discomfort regarding the session, from the first moment\\nwhen the robot initiated mediation in the room and, in some\\ncases, when showed appearance of verbal and non-verbal\\npro-social behaviors. These facts did not arise with the TD\\nchildren, because the first visual contact with the therapist\\noccurred when they entered the room. Additionally, TD\\nchildren showed the ability to divide the attention between\\nthe robot and the therapist from the beginning to the end\\nof the intervention, exhibiting comfort in every moment.\\nThe behavior modulation of CwASD is observed in Fig. 9.\\nBefore the period with robot-mediator the children exhibited\\ndiscomfort (unstable movements of their head), and after\\nof this period, the head movement tended to be more\\nstable.\\nThe novelty of a robot-mediator at diagnostic session\\ncan be analyzed as an additional stimulus of the CRI.\\nAccordingly, in this case study the children of the ASD\\ngroup showed more behavior modification (attention and\\ncomfort) produced by the robot interaction at the beginning\\nof the CRI, remaining until the end of the session. On the\\nother hand, the children of the TD group responded to the\\nnovelty effect of the robot mediator from the time the child\\nentered the room and saw the robot, until the beginning of\\nthe therapist presentation. For the above, despite the noveltyJIntellRobotSyst(2019)96:267–281 278\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.of the stimuli effect, these did not seem to affect the social\\ninteraction between the TD children and the therapist, and\\nin contrast, these stimuli seemed to enhance the CwASD\\nsocial interaction with the therapist along the intervention.\\nThese results are impressive, since they show the\\npotential of CRI intervention to systematically elicit\\ndifferences between the pattern of behavior on TD and ASD\\nchildren. We identified RJA and IJA toward the therapist at\\nthe beginning of the intervention, at the transition between\\ntherapist to robot mediator, and at the end for all TD\\nchildren. In contrast, we only identified IJA towards the\\ntherapist in the transition between mediators, for ASD\\nchildren. This fact shows a clear difference of behavior\\npattern between CwASD and TD children, which can be\\nanalyzed using a JA task protocol. In fact, these pattern\\ndifferences can be used as evidence to improve the ASD\\ndiagnosis.\\n8Conclusions\\nThis work presented a Robot-Assisted tool to assist and\\nenhance the traditional practice of ASD diagnosis. The\\ndesigned framework combines a vision system with the\\nautomated analysis of nonverbal cues in addition to a\\nrobotic platform; both developed upon open source projects.\\nThis research contributes to the state-of-the-art with an\\ninnovative flexible and scalable architecture capable to\\nautomatically register events of joint attention and patterns\\nof visual contact before and after of a robot-based mediation\\nas well as the pattern of behavior related to comfort or\\ndiscomfort along the ASD intervention.\\nIn addition, an artificial vision pipeline based on a multi-\\ncamera approach was proposed. The vision system performs\\nface detection, recognition and tracking, landmark detection\\nand tracking, head pose, gaze and estimation of visual focus\\nof attention was proposed, with its performance considered\\nsuitable for use into conventional ASD intervention. At\\nleast one camera captured the child’s face in each sample\\nframe. Furthermore, the feedback information about the\\nchild’s performance was successfully used to modulate the\\nsupervised behavior of ONO, improving the performance of\\nthe CRI and the visual attention of the children. Regarding\\nthe VFOA estimation, the algorithm was able to estimate\\nthe target into the FoV in different situations recurrently.\\nAlso, the robot was able to react according to the estimation.\\nHowever, the algorithm only failed when occlusion by the\\nchild’s hands is generated. On the other hand, the occlusion\\nby the therapist and the robot was compensated using the\\nmulti-camera approach. The child’s face recognition system\\nshowed to be imperative to analyze the child’s behavior in\\nthe clinical setup implemented in this work, which required\\nthe caregiver’s attention in the room.Despite the limited number of children of this study,\\npreliminary results of this case study showed the feasibility\\nof identifying and quantify differences in the patterns of\\nbehavior of TD children and CwASD elicited by the CRI\\nintervention. Through the proof of concept, it is evidenced\\nhere the system ability to improve the traditional tools used\\nin ASD diagnosis. As future works, it is recommended a\\nstudy to replicate the protocol proposed in this paper with\\nten CwASD and ten TD children. Another suggestion is to\\nquantify other kinds of behaviors in addition to that assessed\\nin this paper, such as verbal utterance patterns, physical\\nand emotional engagement, object or event preferences and\\ngather more evidence to improve the assistance to therapists\\nin ASD diagnosis processes.\\nAcknowledgements This work was supported by the Google Latin\\nAmerica Research Awards (LARA) program. The first author scholar-\\nship was supported in part by the Coordenac ¸˜ao de Aperfeic ¸oamento de\\nPessoal de N ´ıvel Superior - Brasil (CAPES) - Finance Code 001.\\nDisclosurestatement No potential conflict of interest was reported by\\nthe authors.\\nReferences\\n1. Belpaeme, T., Baxter, P.E., de Greeff, J., Kennedy, J., Read, R.,\\nLooije, R., Neerincx, M., Baroni, I., Zelati, M.C.: Child-Robot\\ninteraction: perspectives and challenges. In: 5th International\\nConference, ICSR 2013, pp. 452–459. Springer International\\nPublishing, Bristol (2013)\\n2. Diehl, J.J., Schmitt, L.M., Villano, M., Crowell, C.R.: The clinical\\nuse of robots for individuals with autism spectrum disorders: A\\ncritical review. Res. Autism Spectr. Disord. 6(1), 249–262 (2012)\\n3. Scassellati, B., Admoni, H., Maja, M.: Robots for use in autism\\nresearch. Annu. Rev. Biomed. Eng. 14(1), 275–294 (2012)\\n4. Pennisi, P., Tonacci, A., Tartarisco, G., Billeci, L., Ruta, L.,\\nGangemi, S., Pioggia, G.: Autism and social robotics: A\\nsystematic review (2016)\\n5. American Psychiatric Association: DSM-5 diagnostic classifica-\\ntion. In: Diagnostic and Statistical Manual of Mental Disorders.\\nAmerican Psychiatric Association, 5 (2013)\\n6. Eggebrecht, A.T., Elison, J.T., Feczko, E., Todorov, A., Wolff, J.J.,\\nKandala, S., Adams, C.M., Snyder, A.Z., Lewis, J.D., Estes, A.M.,\\nZwaigenbaum, L., Botteron, K.N., McKinstry, R.C., Constantino,\\nJ.N., Evans, A., Hazlett, H.C., Dager, S., Paterson, S.J., Schultz,\\nR.T., Styner, M.A., Gerig, G., Das, S., Kostopoulos, P., Schlaggar,\\nB.L., Petersen, S.E., Piven, J., Pruett, J.R.: Joint attention and brain\\nfunctional connectivity in infants and toddlers. Cerebral Cortex\\n27(3), 1709–1720 (2017)\\n7. Steiner, A.M., Goldsmith, T.R., Snow, A.V ., Chawarska, K.:\\nDisorders in infants and toddlers. J. Autism Dev. Disord. 42(6),\\n1183–1196 (2012)\\n8. Belpaeme, T., Baxter, P.E., Read, R., Wood, R., Cuay ´ahuitl, H.,\\nKiefer, B., Racioppa, S., Kruijff-Korbayov ´a, I., Athanasopoulos,\\nG., Enescu, V ., Looije, R., Neerincx, M., Demiris, Y ., Ros-\\nEspinoza, R., Beck, A., Canamero, L., Hiolle, A., Lewis, M.,\\nBaroni, I., Nalin, M., Cosi, P., Paci, G., Tesser, F., Sommavilla, G.,\\nHumbert, R.: Multimodal child-robot interaction: building social\\nbonds. Journal of Human-Robot Interaction 1(2), 33–53 (2012)JIntellRobotSyst(2019)96:267–281 279\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.9. Vanderborght, B., Simut, R., Saldien, J., Pop, C., Rusu, A.S.,\\nPintea, S., Lefeber, D., David, D.O.: Using the social robot probo\\nas a social story telling agent for children with ASD. Interact. Stud.\\n13(3), 348–372 (2012)\\n10. Warren, Z.E., Zheng, Z., Swanson, A.R., Bekele, E., Zhang,\\nL., Crittendon, J.A., Weitlauf, A.F., Sarkar, N.: Can robotic\\ninteraction improve joint attention skills? J. Autism Dev. Disord.\\n45(11), 3726–3734 (2015)\\n11. Wood, L.J., Dautenhahn, K., Lehmann, H., Robins, B., Rainer,\\nA., Syrdal, D.S.: Robot-mediated interviews: Do robots pos-\\nsess advantages over human interviewers when talking to chil-\\ndren with special needs? Lecture Notes in Computer Sci-\\nence (including subseries Lecture Notes in Artificial Intelli-\\ngence and Lecture Notes in Bioinformatics) 8239 LNAI , 54–63\\n(2013)\\n12. Feil-Seifer, D., Mataric, M.J.: b3IA A control architecture for\\nautonomous robot-assisted behavior intervention for children\\nwith Autism Spectrum Disorders. In: ROMAN 2008 The 17th\\nIEEE International Symposium on Robot and Human Interactive\\nCommunication, pp. 328–333 (2008)\\n13. Leo, M., Del Coco, M., Carcagn ´ı, P., Distante, C., Bernava,\\nM., Pioggia, G., Palestra, G.: Automatic emotion recognition in\\nRobot-Children interaction for ASD treatment. In: Proceedings\\nof the IEEE International Conference on Computer Vision, 2015-\\nFebru(c), pp. 537–545 (2015)\\n14. Esteban, P.G., Baxter, P.E., Belpaeme, T., Billing, E., Cai, H.,\\nCao, H.-L., Coeckelbergh, M., Costescu, C., David, D., De Beir,\\nA., Fang, Y ., Ju, Z., Kennedy, J., Liu, H., Mazel, A., Pandey,\\nA., Richardson, K., Senft, E., Thill, S., Van De Perre, G.,\\nVanderborght, B., Vernon, D., Hui, Y ., Ziemke, T.: How to build\\na supervised autonomous system for Robot-Enhanced therapy\\nfor children with autism spectrum disorder. Paladyn Journal of\\nBehavioral Robotics 8(1), 18–38 (2017)\\n15. Pour, A.G., Taheri, A., Alemi, M., Ali, M.: Human–Robot\\nfacial expression reciprocal interaction platform: case studies\\non children with autism. Int. J. Soc. Robot. 10(2), 179–198\\n(2018)\\n16. Feng, Y ., Jia, Q., Wei, W.: A control architecture of Robot-\\nAssisted intervention for children with autism spectrum disorders.\\nJ. Robot. 2018 , 12 (2018)\\n17. Bekele, E., Crittendon, J.A., Swanson, A., Sarkar, N., Warren,\\nZ.E.: Pilot clinical application of an adaptive robotic system for\\nyoung children with autism. Autism: The International Journal of\\nResearch and Practice 18(5), 598–608 (2014)\\n18. Huijnen, C.A.G.J., Lexis, M.A.S., Jansens, R., de Witte, L.P.:\\nMapping robots to therapy and educational objectives for children\\nwith autism spectrum disorder. J. Autism Dev. Disord. 46(6),\\n2100–2114 (2016)\\n19. Aresti-Bartolome, N., Begonya, G.-Z.: Technologies as support\\ntools for persons with autistic spectrum disorder: s systematic\\nreview. Int. J. Environ. Res. Public Health 11(8), 7767–7802\\n(2014)\\n20. Boucenna, S., Narzisi, A., Tilmont, E., Muratori, F., Pioggia, G.,\\nCohen, D., Mohamed, C.: Interactive technologies for autistic\\nchildren: a review. Cogn. Comput. 6(4), 722–740 (2014)\\n21. Grynszpan, O., Patrice, L., Weiss, T., Perez-Diaz, F., Gal, E.:\\nInnovative technology-based interventions for autism spectrum\\ndisorders: a meta-analysis. Autism 18(4), 346–361 (2014)\\n22. Rehg, J.M., Rozga, A., Abowd, G.D., Goodwin, M.S.: Behavioral\\nimaging and autism. IEEE Pervasive Comput. 13(2), 84–87, 4\\n(2014)\\n23. Cabibihan, J.J., Javed, H., Ang, M., Aljunied, S.M.: Why robots?\\na survey on the roles and benefits of social robots in the\\ntherapy of children with autism. Int. J. Soc. Robot. 5(4), 593–618\\n(2013)24. Sartorato, F., Przybylowski, L., Sarko, D.K.: Improving therapeu-\\ntic outcomes in autism spectrum disorders: enhancing social com-\\nmunication and sensory processing through the use of interactive\\nrobots. J. Psychiatr. Res. 90, 1–11 (2017)\\n25. Chong, E., Chanda, K., Ye, Z., Southerland, A., Ruiz, N., Jones,\\nR.M., Rozga, A., Rehg, J.M.: Detecting gaze towards eyes in\\nnatural social interactions and its use in child assessment. Proc.\\nACM Interact. Mob. Wearable Ubiquitous Technol. 1(3), 43:1–\\n43:20 (2017)\\n26. Ness, S.L., Manyakov, N.V ., Bangerter, A., Lewin, D., Jagannatha,\\nS., Boice, M., Skalkin, A., Dawson, G., Janvier, Y .M., Goodwin,\\nM.S., Hendren, R., Leventhal, B., Shic, F., Cioccia, W., Gahan,\\nP.: JAKE® Multimodal data capture system: Insights from an\\nobservational study of autism spectrum disorder. Frontiers in\\nNeuroscience 11(SEP) (2017)\\n27. Rehg, J.M., Abowd, G.D., Rozga, A., Romero, M., Clements,\\nM.A., Sclaroff, S., Essa, I., Ousley, O.Y ., Li, Y ., Kim, C., Rao,\\nH., Kim, J.C., Lo Presti, L., Zhang, J., Lantsman, D., Bidwell,\\nJ., Ye, Z.: Decoding children’s social behavior. In: 2013 IEEE\\nConference on Computer Vision and Pattern Recognition, pp.\\n3414–3421 (2013)\\n28. Adamo, F., Palestra, G., Crifaci, G., Pennisi, P., Pioggia, G.,\\nRuta, L., Leo, M., Distante, C., Cazzato, D.: Non-intrusive\\nand calibration free visual exploration analysis in children\\nwith autism spectrum disorder. In: Computational Vision and\\nMedical Image Processing V - Proceedings of 5th Eccomas\\nThematic Conference on Computational Vision and Medical\\nImage Processing, VipIMAGE 2015, pp .201–208 (2016)\\n29. Michaud, F., Salter, T., Duquette, A., Mercier, H., Lauria, M.,\\nLarouche, H., Larose, F.: Assistive technologies and Child-Robot\\ninteraction. American Association for Artificial Intelligence ii(3),\\n8–9 (2007)\\n30. Duquette, A., Michaud, F., Mercier, H.: Exploring the use of\\na mobile robot as an imitation agent with children with low-\\nfunctioning autism. Auton. Robot. 24(2), 147–157 (2008)\\n31. Simut, R.E., Vanderfaeillie, J., Peca, A., Van de Perre, G., Bram,\\nV .: Children with autism spectrum disorders make a fruit salad\\nwith probo, the social robot: an interaction study. J. Autism Dev.\\nDisord. 46(1), 113–126 (2016)\\n32. Bekele, E., Lahiri, U., Swanson, A.R., Crittendon, J.A., Warren,\\nZ.E., Nilanjan, S.: A step towards developing adaptive robot-\\nmediated intervention architecture (ARIA) for children with\\nautism. IEEE Trans. Neural Syst. Rehabil. Eng. 21(2), 289–299\\n(2013)\\n33. Zheng, Z., Zhang, L., Bekele, E., Swanson, A., Crittendon, J.A.,\\nWarren, Z.E., Sarkar, N.: Impact of robot-mediated interaction\\nsystem on joint attention skills for children with autism. In: IEEE\\nInternational Conference on Rehabilitation Robotics (2013)\\n34. Anzalone, S.M., Tilmont, E., Boucenna, S., Xavier, J., Jouen,\\nA.L., Bodeau, N., Maharatna, K., Chetouani, M., Cohen, D.: How\\nchildren with autism spectrum disorder behave and explore the\\n4-dimensional (spatial 3D + time) environment during a joint\\nattention induction task with a robot. Res. Autism Spectr. Disord.\\n8(7), 814–826 (2014)\\n35. Chevalier, P., Martin, J.C., Isableu, B., Bazile, C., Iacob,\\nD.O., Adriana, T.: Joint attention using human-robot interaction:\\nimpact of sensory preferences of children with autism. In: 25th\\nIEEE International Symposium on Robot and Human Interactive\\nCommunication, RO-MAN 2016, pp. 849–854 (2016)\\n36. Lemaignan, S., Garcia, F., Jacq, A., Dillenbourg, P.: From real-\\ntime attention assessment to “with-me-ness” in human-robot\\ninteraction. In: ACM/IEEE International Conference on Human-\\nRobot Interaction, 2016-April, pp. 157–164 (2016)\\n37. Del Coco, M., Leo, M., Carcagni, P., Fama, F., Spadaro, L.,\\nRuta, L., Pioggia, G., Distante, C.: Study of mechanisms ofJIntellRobotSyst(2019)96:267–281 280\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.social interaction stimulation in autism spectrum disorder by\\nassisted humanoid robot. IEEE Transactions on Cognitive and\\nDevelopmental Systems 8920 (c), 1–1 (2017)\\n38. Palestra, G., Varni, G., Chetouani, M., Esposito, F.: A multimodal\\nand multilevel system for robotics treatment of autism in children.\\nIn: Proceedings of the International Workshop on Social Learning\\nand Multimodal Interaction for Designing Artificial Agents - DAA\\n’16, pp. 1–6. ACM Press, New York (2016)\\n39. Quigley, M., Gerkey, B., Conley, K., Faust, J., Foote, T., Leibs,\\nJ., Berger, E., Wheeler, R., Ng, A.: ROS : an open-source robot\\noperating system. In: ICRA workshop on open source software,\\nnumber 3.2, pp. 5 (2009)\\n40. Vandevelde, C., Saldien, J., Ciocci, C., Vanderborght, B.: The\\nuse of social robot ono in robot assisted therapy. In: International\\nConference on Social Robotics, Proceedings, m (2013)\\n41. Dautenhahn, K.: A paradigm shift in artificial intelligence: why\\nsocial intelligence matters in the design and development of robots\\nwith human-like intelligence. 50 Years of Artificial Intelligence,\\npp. 288–302 (2007)\\n42. Ekman, P., Friesen, W.: Facial Action Coding System. Consulting\\nPsychologists Press (1978)\\n43. King, D.E.: Dlib-ml: a machine learning toolkit. J. Mach. Learn.\\nRes. 10, 1755–1758 (2009)\\n44. Baltru ˇsaitis, T., Robinson, P., Morency, L.-P.: OpenFace: an open\\nsource facial behavior analysis toolkit. IEEE Winter Conference\\non Applications of Computer Vision (2016)\\n45. King, D.E.: Max-Margin Object Detection. 1 (2015)\\n46. He, K., Zhang, X., Ren, S., Jian, S.: Deep residual learning for\\nimage recognition. In: 2016 IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), pp. 770–778. IEEE, 6\\n47. Baltru ˇsaitis, T., Robinson, P., Morency, L.P.: Constrained local\\nneural fields for robust facial landmark detection in the wild. In:\\nProceedings of the IEEE International Conference on Computer\\nVision, pp. 354–361 (2013)\\n48. Cristinacce, D., Cootes, T.F.: Feature detection and tracking with\\nconstrained local models. In: Proceedings of the British Machine\\nVision Conference 2006, pp. 1–95 (2006)\\n49. Saragih, J.M., Lucey, S., Cohn, J.F.: Deformable model fitting\\nby regularized landmark mean-shift. Int. J. Comput. Vis. 91(2),\\n200–215 (2011)\\n50. Baltru ˇsaitis, T., Robinson, P., Morency, L.P.: 3D constrained local\\nmodel for rigid and non-rigid facial tracking. In: Proceedings of\\nthe IEEE Computer Society Conference on Computer Vision and\\nPattern Recognition, pp. 2610–2617 (2012)\\n51. Belhumeur, P.N., Jacobs, D.W., Kriegman, D.J., Neeraj, K.:\\nLocalizing parts of faces using a consensus of exemplars. IEEE\\nTrans. Pattern Anal. Mach. Intell. 35(12), 2930–2940 (2013)\\n52. Le, V ., Brandt, J., Lin, Z., Bourdev, L., Huang, T.S.: Interactive\\nFacial Feature Localization, pp. 679–692. Springer, Berlin (2012)\\n53. Jorstad, A., Dementhon, D., Jeng Wang, I., Burlina, P.: Distributed\\nconsensus on camera pose. IEEE Trans. Image Process. 19(9),\\n2396–2407 (2010)\\n54. Ba, S.O., Odobez, J.-M.: Multi-Person visual focus of attention\\nfrom head pose and meeting contextual cues. IEEE Trans. Pattern\\nAnal. Mach. Intell. 33(August), 1–16 (2008)55. Sheikhi, S., Jean-Marc, O.: Combining dynamic head pose-\\ngaze mapping with the robot conversational state for attention\\nrecognition in human-robot interactions. Pattern Recogn. Lett. 66,\\n81–90 (2015)\\nPublisher’s Note Springer Nature remains neutral with regard to\\njurisdictional claims in published maps and institutional affiliations.\\nAndr´esA.Ram´ ırez-Duque received his bachelor’s degree in Mecha-\\ntronics Engineering from the Universidad Nacional de Colombia,\\nBogot ´a, Colombia, in 2009, and his Industrial Automation Master\\ndegree from the Universidad Nacional de Colombia, Bogot ´a, Colom-\\nbia, in 2011. He is currently working toward a Ph.D. degree in the\\nAssistive Technology Center, Federal University of Esp ´ırito Santo,\\nVit´oria, Brazil. He won a Google Latin America Research Award 2017.\\nHis current research interests include Child-Robot interaction, cloud\\nparallel computing, high performance computing, smart environments\\nand serious games applied to Children with development impairments.\\nAnselmo Frizera-Neto received his bachelor’s degree in Electrical\\nEngineering (2006) from the Federal University of Esp ´ırito Santo\\n(UFES) in Brazil and his doctorate in Electronics (2010) at the\\nUniversity of Alcal ´a, Spain. From 2006 to 2010 he was a researcher of\\nthe Bioengineering Group of the Consejo Superior de Investigaciones\\nCient ´ıficas (Spain) where he carried out research related to his\\ndoctoral thesis. He is currently a permanent professor and adjunct\\ncoordinator of the Graduate Program in Electrical Engineering at\\nUFES. He has authored or co-authored more than 250 papers in\\nscientific journals, books and conferences in the fields of electrical\\nand biomedical engineering. He has conducted or co-directed master’s\\nand doctoral theses in research institutions from Brazil, Argentina,\\nItaly and Portugal. His research is aimed at rehabilitation robotics, the\\ndevelopment of advanced strategies of human-robot interaction and the\\nconception of sensors and measurement technologies with applications\\nin different fields of electrical and biomedical engineering. Along with\\nAndr ´es Ram ´ırez-Duque, he won a Google Latin America Research\\nAward 2017.\\nTeodiano Freire Bastos received his B.Sc. degree in Electrical\\nEngineering from Universidade Federal do Esp ´ırito Santo (Vit ´oria,\\nBrazil) in 1987, his Specialist degree in Automation degree from\\nInstituto de Autom ´atica Industrial (Madrid, Spain) in 1989, and\\nhis Ph.D. degree in Physical Science (Electricity and Electronics)\\nfrom Universidad Complutense de Madrid (Spain) in 1994. He made\\ntwo postdocs, one at the University of Alcal ´a (Spain, 2005) and\\nanother at RMIT University (Australia, 2012). He is currently a\\nfull professor at Universidade Federal do Esp ´ırito Santo (Vit ´oria,\\nBrazil), teaching and doing research at the Postgraduate Program of\\nElectrical Enginneering, Postgraduate Program of Biotechnology and\\nRENORBIO Ph.D. Program. His current research interests are signal\\nprocessing, rehabilitation robotics and assistive technology for people\\nwith disabilitiesJIntellRobotSyst(2019)96:267–281 281\\nContent courtesy of Springer Nature, terms of use apply. Rights reserved.1.\\n2.\\n3.\\n4.\\n5.\\n6.Terms and Conditions \\nSpringer Nature journal content, brought to you courtesy of Springer Nature Customer Service Center GmbH (“Springer Nature”). \\nSpringer Nature supports a reasonable amount of sharing of  research papers by authors, subscribers and authorised users (“Users”), for small-\\nscale personal, non-commercial use provided that all copyright, trade and service marks and other proprietary notices are maintained. By\\naccessing, sharing, receiving or otherwise using the Springer Nature journal content you agree to these terms of use (“Terms”). For these\\npurposes, Springer Nature considers academic use (by researchers and students) to be non-commercial. \\nThese Terms are supplementary and will apply in addition to any applicable website terms and conditions, a relevant site licence or a personal\\nsubscription. These Terms will prevail over any conflict or ambiguity with regards to the relevant terms, a site licence or a personal subscription\\n(to the extent of the conflict or ambiguity only). For Creative Commons-licensed articles, the terms of the Creative Commons license used will\\napply. \\nWe collect and use personal data to provide access to the Springer Nature journal content. We may also use these personal data internally within\\nResearchGate and Springer Nature and as agreed share it, in an anonymised way, for purposes of tracking, analysis and reporting. We will not\\notherwise disclose your personal data outside the ResearchGate or the Springer Nature group of companies unless we have your permission as\\ndetailed in the Privacy Policy. \\nWhile Users may use the Springer Nature journal content for small scale, personal non-commercial use, it is important to note that Users may\\nnot:  \\nuse such content for the purpose of providing other users with access on a regular or large scale basis or as a means to circumvent access\\ncontrol;\\nuse such content where to do so would be considered a criminal or statutory offence in any jurisdiction, or gives rise to civil liability, or is\\notherwise unlawful;\\nfalsely or misleadingly imply or suggest endorsement, approval , sponsorship, or association unless explicitly agreed to by Springer Nature in\\nwriting;\\nuse bots or other automated methods to access the content or redirect messages\\noverride any security feature or exclusionary protocol; or\\nshare the content in order to create substitute for Springer Nature products or services or a systematic database of Springer Nature journal\\ncontent. \\nIn line with the restriction against commercial use, Springer Nature does not permit the creation of a product or service that creates revenue,\\nroyalties, rent or income from our content or its inclusion as part of a paid for service or for other commercial gain. Springer Nature journal\\ncontent cannot be used for inter-library loans and librarians may not upload Springer Nature journal content on a large scale into their, or any\\nother, institutional repository. \\nThese terms of use are reviewed regularly and may be amended at any time. Springer Nature is not obligated to publish any information or\\ncontent on this website and may remove it or features or functionality at our sole discretion, at any time with or without notice. Springer Nature\\nmay revoke this licence to you at any time and remove access to any copies of the Springer Nature journal content which have been saved. \\nTo the fullest extent permitted by law, Springer Nature makes no warranties, representations or guarantees to Users, either express or implied\\nwith respect to the Springer nature journal content and all parties disclaim and waive any implied warranties or warranties imposed by law,\\nincluding merchantability or fitness for any particular purpose. \\nPlease note that these rights do not automatically extend to content, data or other material published by Springer Nature that may be licensed\\nfrom third parties. \\nIf you would like to use or distribute our Springer Nature journal content to a wider audience or on a regular basis or in any other manner not\\nexpressly permitted by these Terms, please contact Springer Nature at  \\nonlineservice@springernature.com ', metadata={'source': 'papers/1_Ramırez-Duque__clean.txt'})],\n",
       " [Document(page_content='Ouss et al. Translational Psychiatry           (2020) 10:54 \\nhttps://doi.org/10.1038/s41398-020-0743-8 Translational Psychiatry\\nARTICLE Open Access\\nBehavior and interaction imaging at 9 months of\\nage predict autism/intellectual disability in high-riskinfants with West syndrome\\nLisa Ouss1,G i u s e p p eP a l e s t r a2, Catherine Saint-Georges2,3, Marluce Leitgel Gille1, Mohamed Afshar4,\\nHugues Pellerin2, Kevin Bailly2, Mohamed Chetouani2, Laurence Robel1,B e r n a r dG o l s e1, Rima Nabbout5,\\nIsabelle Desguerre5, Mariana Guergova-Kuras4and David Cohen2,3\\nAbstract\\nAutomated behavior analysis are promising tools to overcome current assessment limitations in psychiatry. At\\n9 months of age, we recorded 32 infants with West syndrome (WS) and 19 typically developing (TD) controls during astandardized mother –infant interaction. We computed infant hand movements (HM), speech turn taking of both\\npartners (vocalization, pause, silences, overlap) and motherese. Then, we assessed whether multimodal social signals\\nand interactional synchrony at 9 months could predict outcomes (autism spectrum disorder (ASD) and intellectualdisability (ID)) of infants with WS at 4 years. At follow-up, 10 infants developed ASD/ID (WS +). The best machine\\nlearning reached 76.47% accuracy classifying WS vs. TD and 81.25% accuracy classifying WS +vs. WS−. The 10 best\\nfeatures to distinguish WS +and WS −included a combination of infant vocalizations and HM features combined with\\nsynchrony vocalization features. These data indicate that behavioral and interaction imaging was able to predict ASD/ID in high-risk children with WS.\\nIntroduction\\nBehavior and interaction imaging is a promising domain\\nof affective computing to expl ore psychiatric conditions1–3.\\nRegarding child psychiatry, many researchers haveattempted to identify reliable indicators of neurodevelop-mental disorders (NDD) in high-risk populations (e.g., sib-\\nlings of children with autism) during the ﬁrst year of life to\\nrecommend early interventions\\n4,5. However, social signals\\nand any alterations of them are very dif ﬁcult to identify at\\nsuch a young age6. In addition, exploring the quality and\\ndynamics of early interactions is a complex endeavor. Itusually requires (i) the perception and integration of mul-timodal social signals and (ii) an understanding of how twointeractive partners synchronize and proceed in turn\\ntaking\\n7,8.\\nAffective computing offers the possibility to simulta-\\nneously analyze the interaction of several partners whileconsidering the multimodal nature and dynamics of socialsignals and behaviors\\n9. To date, few seminal studies have\\nattempted to apply social signal processing to\\nmother –infant interactions with or without a speci ﬁc\\ncondition, and these studies have focused on speech turns(e.g., Jaffe et al.\\n10), motherese11, head movements12, hand\\nmovements13, movement kinematics2, and facial\\nexpressions3.\\nHere, we focused on West syndrome (WS), a rare epi-\\nleptic encephalopathy with early onset (before age 1 year)\\nand a high risk of NDD outcomes, including one-third of\\nWS children showing later autism spectrum disorder(ASD) and/or intellectual disability (ID). We recruited 32infants with WS and 19 typically developing (TD) controlsto participate in a standardized early mother –infant\\n© The Author(s) 2020\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 Internat ional License, which permits use, sharing, adaptation, distribution and reproduction\\nin any medium or format, as long as you give appropriate credit to the origina l author(s) and the source, provide a li n kt ot h eC r e a t i v eC ommons license, and indicate if\\nchanges were made. The images or other third party material in this article are included in the article ’s Creative Commons license, unless indicated otherwise in a credit line to the material. If\\nmaterial is not included in the article ’s Creative Commons license and your intended use is not permitted by sta tutory regulation or exceeds the permitted use, you will need to obtain\\npermission directly from the copyright hol der. To view a copy of this license, visit http://creativecommons .org/licenses/by/4.0/ .Correspondence: Lisa Ouss ( lisa.ouss@aphp.fr ) or David Cohen\\n(david.cohen@aphp.fr )\\n1Service de Psychiatrie de l ’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres,\\n75015 Paris, France\\n2Institut des Systèmes Intelligents et de Robotique, CNRS, UMR 7222, Sorbonne\\nUniversité, 4 Place Jussieu, 75252 Paris Cedex, FranceFull list of author information is available at the end of the article1234567890():,;1234567890():,;\\n1234567890():,;\\n1234567890():,;interaction protocol and followed infants with WS to\\nassess outcomes at 4 years of age. We aim to explorewhether multimodal social signals and interpersonal\\nsynchrony of infant –mother interactions at 9 months\\ncould predict outcomes.\\nMaterials and methods\\nDesign, participants, and clinical measures\\nWe performed a prospective follow-up study of infants\\nwith WS14. The Institutional Review Board ( Comité de\\nProtection des Personnes from the Groupe-Hospitalier\\nNecker Enfants Malades ) approved the study, and both\\nparents gave written informed consent after they receivedverbal and written information on the study. They wereasked to participate to a follow-up study to assess out-come of WS taking into account development, earlyinteraction, genetics and response to pharmacologicaltreatment\\n14. The study was conducted from November\\n2004 to March 2010 in the Neuro-pediatrics Department\\nCenter for Rare Epilepsia of Necker Enfants-Malades\\nHospital, Paris. Of the 41 patients screened during thestudy period, we enrolled all but two cases ( N=39) with\\nWS. Seven patients dropped out before the age of 3leading to a sample of 32 patients with detailed follow-updata. Typical developing infants ( N=19) were recruited\\nfrom Maternal and Infant Prevention institutions, inpediatric consultations, or by proxy.To assess neurodevelopmental outcomes, we focused on\\nID and ASD. ID was assessed through the Brunet-Lézine\\nDevelopmental Examination , performed for all children at\\nthe age of 3 years. The Brunet-Lézine Developmental\\nExamination estimates a developmental quotient (DQ)based upon normative data available for 3-year-oldFrench toddlers\\n15. The diagnosis of autism was based\\nupon several measurements and an expert assessmentthat was blind to other variables: (i) At 3 years of age, allparents completed the Autism Diagnostic Interview-\\nRevised (ADI-R) to assess autism signs by dimensions\\nand developmental delay\\n16. (ii) At 2 and 3 years of age, all\\npatients were assessed with the Children ’s Autism Rating\\nScale (CARS)17. (iii) An expert clinician (LR) who was\\nblind to child history assessed autism and ID from 20-minvideotapes of child/mother play at 2 years of age. Finally,diagnoses of ASD and/or ID at age 4 were based upon aconsensus approach using direct assessment of the childby a clinician with expertise in autism (LO) as well as by\\nclinical information from the CARS, ADI-R, and DQ.\\nVideo recordings\\nInfant –mother interactions were assessed between 9\\nand 12 months of age during a play session (Fig. 1). Two\\nsynchronized cameras (face and pro ﬁle; Fig. S1A) recor-\\nded the movements in two dimensions while the infantwas sitting in a baby chair. Audio interactions were also\\nAround 9 months emordnys tseWn erldihc\\n(N=32)lacipyTg nipoleved\\ncontrols (N=19)Lab recording of \\ninfant-mother\\ninterac/g415on\\nMotherInfant\\nFree interac/g415on\\nInterac/g415on\\nwith the giraﬀe\\nMother singing3 sequences\\nof interac/g415on\\nWS with\\nID/ASD (N=10)WS without\\nID/ASD (N=22)Typical developing\\ncontrols (N=19)At 4 years\\nMachine \\nlearning\\nclassiﬁca/g415on\\nWest \\nsyndrome\\nvs.TDWS with\\nID/ASD\\nvs. WS \\nwithout\\nID/ASD Audio features extrac/g415on\\nInfant : typical vocaliza/g415on, \\natypical vocaliza/g415on, pause\\nMother : vocaliza/g415on, \\nmotherese, pause\\nSynchrony : overlap, silence, \\ninfant synchrony ra/g415o\\nVideo features extrac/g415on\\nInfant’s hand movement: \\nvelocity, accelera/g415on, \\ncurvature, spa/g415al, pause\\nFig. 1 Pipeline of our machine learning approach to classify WS vs. TD.Ouss et al. Translational Psychiatry           (2020) 10:54 Page 2 of 7recorded. The standardized situation encompassed three\\nsequences of 3 min: (sequence 1) free play after instruct-ing the mother to interact “as usual ”without any toy;\\n(sequence 2) free play using the help of a toy (Sophie the\\ngiraffe); (sequence 3) mother singing to her baby. Due tothe position of the baby chair on the ﬂoor and the\\nmother ’s seated position, the mother was positioned\\nslightly higher in all of the recordings. The mother ’s\\nindicated position was on the left of the child as shown onthe picture, but exceptions were sometimes observedduring the recordings. For infant hand movement (HM)\\nfeatures, 1 min was extracted from each 3-min video and\\nall recordings, according to two criteria: the child ’s hands\\nshould be visible for at least part of the sequence (e.g., themother is not leaning on the child), and the minuterepresented the greatest amount of interaction betweenthe mother and the child. For audio and speech turn-taking computing, we only used the 3-min audiorecording of sequence 1.\\nVision computing (Fig. S1B, vision computing panel)\\nTo process infant hand movements (HM), we used the\\nmethods developed in Ouss et al.13. Here, we summarize\\nthe successive steps to calculate HM features. In step 1(hand trajectory extraction and data processing), the two-dimensional coordinates of the hand were extracted fromeach of the video recordings by tracking a wristband on\\nthe right hand (yellow in Fig. S1A, video-audio recording\\npanel). The tracking framework comprised three steps:prediction, observation, and estimation as proposed inref.\\n18. As the hand motion was highly nonlinear, we\\ndeveloped an approach using a bootstrap-based particleﬁlter with a ﬁrst-order model to address abrupt changes in\\ndirection and speed\\n19,20. To address hand occlusion, we\\nimplemented an approach combining tracking with\\ndetection by adding a boolean variable to the state vector\\nassociated with each particle18.\\nEach extracted trajectory consisted of 1500 pairs of x\\nandycoordinates (25 frames per second, generating 1500\\npairs of coordinates in the 60 s; see Fig. S1 left panel,vision computing). The frames where the hand was notvisible were clearly indicated in each trajectory as missingcoordinates for these time points. To account for differ-\\nences in the camera zoom parameters, the trajectories\\nobtained were normalized using a ﬁxed reference system\\npresent in the settings of each video recording. The nor-malization was performed on all trajectories, and 95% ofthe normalization factors ranged between 0.8 and 1.22with a few outlier trajectories that required greater cor-rection. Forty-one percent of the trajectories required <5%correction. Although the recordings between the two\\ncameras were synchronized and in principle allowed 3D\\nreconstruction of the trajectory, the accumulation ofmissing data prevented such reconstruction. However, 2Dmotion capture with appropriately de ﬁned movement\\ndescriptors can be powerful for detecting clinically rele-vant changes\\n21, thereby justifying the independent ana-\\nlysis of the 2D-trajectory videos (see Fig. S1B, vision\\ncomputing, 2d panel on the left).\\nIn step 2, the descriptors of the HM were calculated\\nfrom the planar trajectories (Fig. S1B, table shown in thevision computing panel). Descriptors covered thosealready reported in the literature as important in char-acterizing infants ’HM\\n21. (1) To describe the space\\nexplored by the hand, we calculated the maximum dis-\\ntance observed on the two axes (xRange, yRange) and the\\nstandard deviation of the X and Y coordinates observedduring the 60 s (xSd, ySd). We also calculated the max-imum distance between any two points of the trajectoryusing the FarthestPair java library ( http://algs4.cs.\\nprinceton.edu/code/ ) (Fig. S1B, vision computing panel,\\nred line in the third panel from the left). (2) To evaluateHM dynamics, we calculated the velocity and accelera-\\ntion. (3) Also related to HM dynamics, we calculated HM\\npauses de ﬁned as part of the trajectory in which the\\nvelocity was lower than a speci ﬁc threshold for a mini-\\nmum duration of 4 s. (4) Finally, the curvature of thetrajectories was calculated using a standard de ﬁnition of\\nthe curvature ( κ) of plane curves in Cartesian coordinates\\nasγ(t)=(x(t),y(t)). The curvature calculated at each point\\nof the trajectory is presented in the right panel of Fig. S1B\\n(video computing), where the ﬁrst 1.2 s of the trajectory\\nare plotted and the associated calculated curvatures ateach point (and respective time, indicated on the axis) arepresented as columns.\\nAudio computing (Fig. S1C, audio computing)\\nWe extracted two types of audio social signals from the\\naudio channel of the mother –infant interaction: speech\\nturn taking (STT) and motherese. For STT extraction, we\\nfollowed the methods developed by Weisman et al.22and\\nBourvis et al.23(Fig. S1, audio computing). First, we used\\nELAN to segment the infants ’and mothers ’speech turns\\nand annotate the dialog acts. Mothers ’audio interactions\\nwere categorized as mother vocalization (meaningfulvocalizations, laugh, singing, animal sounds) or othernoise (clap hands, snap ﬁngers or snap the tongue, mouth\\nnoise, etc.). Similarly, infants ’audio production was\\ndeﬁned as infant vocalization (babbling vocalizations,\\nlaugh, and cry) or atypical vocalization (other noise suchas “rale ”). The infant ’s and mother ’s utterances were\\nlabeled by two annotators (blind to group status). Cohen ’s\\nkappa between the two annotators was calculated for eachdyad, each task and each item of the grid. For all items, thekappa values were between 0.82 and 1.\\nFrom the annotation, we extracted all the speech turns\\nof the infant and the mother. A speech turn is a con-tinuous stream of speech with <150 ms of silence. WeOuss et al. Translational Psychiatry           (2020) 10:54 Page 3 of 7obtained a list of triples: speaker label (infant or mother),\\nstart time, and duration of speech turn. From these triples,we also deduced the start time and duration of the time\\nsegments when the mother or the infant were not\\nspeaking (pauses). Therefore, we extracted M other\\nVocalizations ;Mother Other Noise ;Infant Vocalizations ;\\nInfant Atypical Vocalizations ;Mother Pauses ;Infant\\nPauses . We also extracted three dyadic features: (1)\\nSilence deﬁned as sequences of time during which neither\\nparticipant was speaking for more than 150 ms; (2)Overlap Ratio deﬁned as the duration of vocalization\\noverlaps between mothers and infants divided by the\\nduration of the total interaction. This ratio measures theproportion of interactional time in which both partici-pants were simultaneously vocalizing; (3) Infant Syn-\\nchrony Ratio deﬁned as the number of infants ’responses\\nto their mother ’s vocalization within a time limit of 3 s\\ndivided by the number of mother vocalizations during thetime paradigm. The 3-s window was based on the avail-\\nable literature on synchrony\\n7,24.\\nFrom the mother vocalizations, we also computed\\naffective speech analysis, as previous work has shown thatmotherese may shape parent-infant interactions\\n25. The\\nsegments of mother vocalizations were analyzed using acomputerized classi ﬁer for categorization as “motherese ”\\nor “non-motherese/other speech ”initially developed to\\nanalyze home movies\\n11. The system exploits the fusion of\\ntwo classi ﬁers, namely, segmental and suprasegmental26.\\nConsequently, the utterances are characterized by bothsegmental (Mel frequency cepstrum coef ﬁcients) and\\nsuprasegmental/prosodics (e.g., statistics with regard tofundamental frequency, energy, and duration) features.The detector used the GMM (Gaussian mixture model)classi ﬁer for both segmental and suprasegmental features\\n(M, number of Gaussians for the GMM Classi ﬁer:M=12\\nand 15, respectively, and λ=weighting coef ﬁcient used in\\nthe equation fusion: λ=0.4). For the purpose of the\\ncurrent study, we explored the performance of ourmotherese classi ﬁer in French mothers. We analyzed\\n200 sequences from French mothers (100 motherese vs.100 other speech) that were blindly validated by twopsycholinguists. We calculated the Intraclass correlation(ICC) between the two raters (the expert and the algo-\\nrithm) and found a good and very signi ﬁcant ICC (ICC =\\n0.79 (95% CI: 0.59 –0.90), p< 0.001). This level of predic-\\ntion made it suitable for further analysis of the entiredata set.\\nBased on this automatic detection of motherese, we\\ncreated two subclasses for mother vocalizations: mother-ese vs. non-motherese. Two variables were derived:Motherese Ratio (duration of motherese vocalization/\\nduration of interaction) and Non-motherese Ratio (dura-\\ntion of non-motherese vocalization/duration of interac-tion). We also derived two synchrony ratios: SynchronyMotherese Ratio and Synchrony Non-motherese Ratio ,\\nwhich re ﬂect the ratio of time during which the infant\\nvocalizes in response to his/her mother motherese and\\nother speech (non-motherese).\\nPrediction of the outcome using machine learning\\nThe pipeline of our approach is shown in Fig. 1. First, a\\ndata quality analysis was performed to ensure the validityof the data. As expected, all data were available for audioanalysis. However, a substantial proportion of the datawere discarded due to video recording or vision com-\\nputing issues. We ﬁnally kept 18 video recordings for the\\nWS and 17 videos for the TD groups. Second, given thenumber of features (21 infant HM for each camera andeach sequence; 16 STT) compared with the data set (32WS and 19 TD), we reduced our data using principalcomponent analysis (PCA). Third, we tested severalalgorithms to classify WS vs. TD based on the whole dataset available for both vision and audio computing features\\n(leave one out) (Table S1). The best algorithm was deci-\\nsion stump\\n27. All results presented here are based on the\\nclassi ﬁcation with a decision stump algorithm. We also\\nanalyzed WS with ID/ASD (WS +) vs. WS without ID/\\nASD (WS −). For each classi ﬁcation, we also extracted a\\nconfusion matrix and explored which individual featurescontributed the most to a given classi ﬁcation using\\nPearson correlations.\\nResults\\nTable S2 summarizes the demographic and clinical\\ncharacteristics of children with WS. At follow-up, 10infants out of 32 children with WS developed ASD/ID(WS+). Eight children had ASD and ID, whereas 2 had\\nonly ID. As expected, all variables related to ASD and IDwere signi ﬁcantly different in WS +compared with WS −.\\nFigure 2a summarizes the best classi ﬁcation models\\nusing the decision stump algorithm (leave one out). Asshown, multimodal classi ﬁcation outperformed unimodal\\nclassi ﬁcation to distinguish WS and TD. Therefore, we\\nonly used the multimodal approach to classify WS +vs.\\nWS−. The best model reached 76.47% accuracy classify-\\ning WS vs. TD and 81.25% accuracy classifying WS +vs.\\nWS−based on multimodal features extracted during early\\ninteractions. Interestingly, the confusion matrices (Fig.\\n2b) show that when classifying WS vs. TD, all errors came\\nfrom TD being misclassi ﬁed as WS ( N=12); when clas-\\nsifying WS +vs. WS−, most errors came from WS +being\\nmisclassi ﬁed as WS −(N=5).\\nTable 1lists the best features for each multimodal\\nclassi ﬁcation based on the Pearson correlation values. The\\nbest features to distinguish WS and TD included four\\ninfant HM features, 1 mother audio feature. In contrast,\\nthe best features to distinguish WS +and WS −included a\\ncombination of infant vocalization features ( N=2),Ouss et al. Translational Psychiatry           (2020) 10:54 Page 4 of 70102030405060708090\\nMu/g415modal Video AudioWest vs. TD (N=51)\\nWest with ID/ASD vs. \\nWest with out ID/ASD \\n(N=32)Classiﬁed\\nas West TD\\nWest 32 0\\nTD 12 7\\nClassiﬁed\\nas West with\\nID/ASDWest with\\noutID/ASD\\nWest with\\nID/ASD5 5\\nWest with\\noutID/ASD1 21Machine learning classiﬁca/g415on\\nDecision stump (leave one out)a\\nConfusion\\nmatricesb\\nFig. 2 Machine learning classi ﬁcation of WS vs. TD and WS +vs. WS−based on uni- and multimodal features extracted during early infant –mother\\ninteraction.\\nTable 1 Best features for classi ﬁcation (based on signi ﬁcant Pearson ’s correlation between feature and class).\\nFeature characteristics Pearson rp -value\\nWest vs. Typical developing\\nRatio of all maternal audio intervention during free interaction Audio, mother 0.35 0.012\\nTotal number of infant HM pauses (side view camera) during free interaction Video, infant 0.34 0.014\\nTotal number of infant HM pauses (side view camera) when the mother is singing Video, infant 0.32 0.023\\nVertical amplitude of the giraffe (front view camera) Video, infant −0.30 0.032\\nMovement acceleration max (side view camera) during free interaction Video, infant 0.29 0.034\\nWest with ASD/ID vs. West without ASD/ID\\nTotal number of all infant vocalization during free interaction Audio, infant −0.56 <0.001\\nSynchrony ratio (infant response to mother) Audio, synchrony −0.55 <0.001\\nRatio of all infant vocalization during free interaction Audio, infant −0.55 0.001\\nMotherese synchrony ratio (infant response to motherese) Audio, synchrony −0.54 0.002\\nNon-motherese synchrony ratio (infant response to non-motherese) Audio, synchrony −0.48 0.005\\nHM acceleration SD (front view camera) during the giraffe interaction Video, infant −0.46 0.008\\nHM acceleration max (side view camera) during the giraffe interaction Video, infant −0.45 0.01\\nHM velocity SD (front view camera) during the giraffe interaction Video, infant −0.43 0.014\\nCurvature max (side view camera) during the giraffe interaction Video, infant −0.37 0.039\\nRelative time spent motionless (pause) (front view camera) during free interaction Video, infant 0.36 0.04\\nHMhand movement, ASD autism spectrum disorder, IDintellectual disability, SDstandard deviation.Ouss et al. Translational Psychiatry           (2020) 10:54 Page 5 of 7synchrony vocalization features ( N=3) and infant HM\\nfeatures ( N=5), the last of which showed lower correla-\\ntion scores.\\nDiscussion\\nTo the best of our knowledge, this is the ﬁrst study to\\napply multimodal social signal processing tomother –infant interactions in the context of WS. Com-\\nbining speech turns and infant HM during aninfant –mother interaction at 9 months signi ﬁcantly pre-\\ndicted the development of ASD or severe to moderate ID\\nat 4 years of age in the high-risk children with WS.\\nConfusion matrices showed that the classi ﬁcation errors\\nwere not random, enhancing the interest of the compu-tational method proposed here. In addition, the bestcontributing features for the performed classi ﬁcations\\ndiffered when classifying WS vs. TD and WS +vs. WS −.\\nInfant HMs were the most signi ﬁcant features to distin-\\nguish WS versus TD, probably re ﬂecting the motor\\nimpact due to acute WS encephalopathy. For classifying\\nWS+vs. WS −, the contribution of infant audio features\\nand synchrony features became much more relevantcombined with several HM features.\\nWe believe that the importance of synchrony and\\nreciprocity during early interactions is in line withrecent studies that have investigated the risk of ASD orNDD during the ﬁrst year of life from home movies\\n(e.g., refs.\\n11,24), from prospective follow-up of high-risk\\ni n f a n t ss u c ha ss i b l i n g s( e . g . ,r e f s .4,28) or infants with\\nWS (e.g., ref.14), and from prospective studies assessing\\ntools to screen risk for autism (e.g., ref.29). In the ﬁeld of\\nASD, synchrony, reciprocity, parental sensitivity, and\\nemotional engagement are now proposed as targets ofearly interventions\\n30, which could prevent early inter-\\nactive vicious circl es. Parents of at-risk infants try to\\ncompensate for the lack of interactivity of their child by\\nmodifying their stimulation and therefore sometimesreinforcing the dysfunctional interactions\\n24.E a r l y\\nidenti ﬁcation of these interactive targets is especially\\nuseful among babies with neurological comorbiditiesbecause delays in developmental milestones andimpairments in early social interactions are not suf ﬁ-\\ncient to predict ASD.\\nSimilarly, we believe that the importance of HM in\\ndistinguishing WS vs. TD on one hand, and WS +vs.\\nWS−on the other hand, is also in line with the studies\\nthat investigated the importance of non-social behaviorsfor investigating the risk of ASD or NDD during the ﬁrst\\nyear of life. For example, studying home movies, Purpuraet al. found more bilateral HM and ﬁnger movements in\\ninfants who will later develop ASD\\n31. Similarly, several\\nprospective follow-up studies of high-risk siblings32–35or\\nretrospective studies on home movies36,37reported spe-\\nciﬁc motor atypical repertoire in infants with ASD.In ASD, early social signals have previously been\\nassessed with automatized and computational procedures,focusing on eye tracking at early stages\\n38–40, vocal pro-\\nductions41, analysis of acoustics of ﬁrst utterances or cry\\nepisodes42, but none was done in an interactive setting.\\nOur study proposed a paradigm shift from the assessmentof infant behavior to dyadic assessment of interactions, aspreviously achieved in retrospective approaches usinghome movies\\n24. The aim is not to implement studies of\\nsocial signal processing in routine clinical work but ratherto decompose clinical intuitions and signs and validate the\\nmost relevant cues of these clinical features. From clinical\\nwork, back to clinics, social signal processing is a rigorousstep to help clinicians better identify and assess earlytargets of interventions.\\nGiven the exploratory nature of both our approach and\\nmethod, our results should be interpreted with cautiontaking into account strengths (prospective follow-up,automatized multimodal social signal processing, and\\necological standardized assessment) and limitations.\\nThese limitations include (1) the overall sample sizeknowing that WS is a rare disease; (2) the high rate ofmissing data during video recording due to the ecologicalconditions of the infant –mother interaction (mothers\\ninterposing between the camera and the infant); the ﬁnal\\nsample size of WS +(N=10) that limited the power of\\nmachine learning methods.\\nWe conclude that the method proposed here combining\\nmultimodal automatized assessment of social signal pro-cessing during early interaction with infants at risk forNDD is a promising tool to decipher clinical features thatremain dif ﬁcult to identify and assess. In the context of\\nWS, we showed that such a method we proposed to label‘behavioral and interaction imaging ’was able to sig-\\nniﬁcantly predict the development of ASD or ID at 4 years\\nof age in high-risk children who had WS and were\\nassessed at 9 months of age.\\nAcknowledgements\\nThe authors thank all of the patients and families who participated in thisstudy. The study was funded by the EADS foundation (PILE), by the Agence\\nNationale de la Recherche (ANR-12-SAMA-006-1) and the Groupement de\\nRecherche en Psychiatrie (GDR-3557). It was partially performed in the Labex\\nSMART (ANR-11-LABX-65), which is supported by French state funds andmanaged by the ANR in the Investissements d ’Avenir program under reference\\nANR-11-IDEX-0004-02. The sponsors had no involvement in the study design,\\ndata analysis, or interpretation of the results.\\nAuthor details\\n1Service de Psychiatrie de l ’Enfant, AP-HP, Hôpital Necker, 149 rue de Sèvres,\\n75015 Paris, France.2Institut des Systèmes Intelligents et de Robotique, CNRS,\\nUMR 7222, Sorbonne Université, 4 Place Jussieu, 75252 Paris Cedex, France.\\n3Département de Psychiatrie de l ’Enfant et de l ’Adolescent, AP-HP, Hôpital\\nPitié-Salpêtrière, 47-83, Boulevard de l ’Hôpital, 75651 Paris, Cedex 13, France.\\n4Ariana Pharmaceuticals, Research Department, Paris, France.5Service de\\nNeuropédiatrie, AP-HP, Hôpital Necker, 136, Rue de Vaugirard, 75015 Paris,FranceOuss et al. Translational Psychiatry           (2020) 10:54 Page 6 of 7Conﬂict of interest\\nThe authors declare that they have no con ﬂict of interest.\\nPublisher ’s note\\nSpringer Nature remains neutral with regard to jurisdictional claims in\\npublished maps and institutional af ﬁliations.\\nSupplementary Information accompanies this paper at ( https://doi.org/\\n10.1038/s41398-020-0743-8 ).\\nReceived: 7 December 2019 Revised: 13 January 2020 Accepted: 16 January\\n2020\\nReferences\\n1. Spodenkiewicz, M. et al. Distinguish se lf- and hetero-perceived stress through\\nbehavioral imaging and ph ysiological features. Prog. Neuropsychopharmacol.\\nBiol. Psychiatry 82,1 0 7–114 (2018).\\n2. Leclere, C. et al. Interaction and behaviour imaging: a novel method to\\nmeasure mother-infant interaction using video 3D reconstruction. Transl.\\nPsychiatry 6, e816 (2016).\\n3 . M e s s i n g e r ,D .S . ,M a h o o r ,M .H . ,C h o w ,S .M .&C o h n ,J .F .A u t o m a t e dm e a -\\nsurement of facial expression in infant-mother interaction: a pilot study.Infancy 14,2 8 5–305 (2009).\\n4. Wan, M. W. et al. Parent-infant interact ion in infant siblings at risk of autism. Res\\nDev. Disabil. 33,9 2 4–932 (2012).\\n5 . R o g e r s ,S .J .e ta l .A u t i s mt r e a t m e n ti nt h e ﬁrst year of life: a pilot study of\\ninfant start, a parent-implemented intervention for symptomatic infants. J.\\nAutism Dev. Disord. 44,2 9 8 1 –2995 (2014).\\n6. Zwaigenbaum, L., Bryson, S. & Garon, N. Early identi ﬁcation of autism spectrum\\ndisorders. Behav. Brain Res 251, 133–146 (2013).\\n7. Feldman, R. Parent-infant synchrony and the construction of shared timing;\\nphysiological precursors, developmental outcomes, and risk conditions. J.\\nChild Psychol. Psychiatry 48,3 2 9–354 (2007).\\n8. Delaherche, E. et al. Interpersonal synchrony: a survey of evaluation methods\\nacross disciplines. IEEE Trans. Affect Comput 3,3 4 9–365 (2012).\\n9. Vinciarelli, A., Pantic, M. & Bourlard, H. Social signal processing: survey of an\\nemerging domain. Image Vis. Comput 27,1 7 4 3 –1759 (2009).\\n10. Jaffe, J., Beebe, B., Feldstein, S., Crown, C. L. & Jasnow, M. D. Rhythms of\\ndialogue in infancy: coordina ted timing in development. Monogr. Soc. Res\\nChild Dev. 66,1–132 (2001).\\n11. Cohen, D. et al. Do parentese prosody and fathers ’involvement in interacting\\nfacilitate social interaction in in fants who later develop autism? PLoS ONE 8,\\ne61402 (2013).\\n12. Hammal, Z., Cohn, J. F. & Messinger, D. S. Head movement dynamics during\\nplay and perturbed mother-infant interaction. IEEE Trans. Affect Comput. 6,\\n361–370 (2015).\\n13. Ouss, L. et al. Developmental trajectories of hand movements in typical infants\\nand those at risk of developmental disorders: an observational study ofkinematics during the ﬁ\\nrst year of life. Front Psychol. 9,8 3( 2 0 1 8 ) .\\n14. Ouss, L. et al. Taking into account infant ’s engagement and emotion during\\nearly interactions may help to determine the risk of autism or intellectual\\ndisability in infants with West syndrome. Eur. Child Adolesc. Psychiatry 23,\\n143–149 (2014).\\n15. Josse, D. Le manuel BLR-C, “Brunet-Lézine Révisé: Echelle de Developpement\\nPsychomoteur de la Premiere Enfance (EAP, Paris, 1997).\\n16. Lord, C., Rutter, M. & Le Couteur, A. Autism diagnostic interview-revised: a\\nrevised version of a diagnostic interview for caregivers of individuals withpossible pervasive dev elopmental disorders. J. Autism Dev. Disord. 24,6 5 9–685\\n(1994).\\n17. Schopler, E., Reichler, R. J., DeVellis , R. F. & Daly, K. Toward objective classi ﬁ-\\ncation of childhood autism: Childhood Autism Rating Scale (CARS). J. Autism\\nDev. Disord. 10,9 1–103 (1980).\\n18. Czyz, J., Ristic, B. & Macq, B. A color-based particle ﬁlter for joint detection and\\ntracking of multiple objects. in Proceedings (ICASSP ‘05) IEEE InternationalConference on Acoustics, Speech, and Signal Processing (IEEE, Philadelphia, PA,\\n2005).\\n19. Hue, C. Méthodes Séquentielles de Monte Carlo pour le Filtrage non Linéaire\\nMulti-Objets dans un Environnement Bruité. Applications au Pistage Multi-Cibleset à la Trajectographie d ’E n t i t é sd a n sd e sS é q u e n c e sd ’Images 2D .P h DT h e s i s ,\\nUniversité de Rennes I, Rennes, France (2003).\\n20. Isard, M. & Blake, A. Condensation —conditional density propagation for visual\\ntracking. Int J. Comput Vis. 29,5–28 (1998).\\n2 1 . M a r c r o f t ,C . ,K h a n ,A . ,E m b l e t o n ,N .D . ,T r e n e l l ,M .&P l o t z ,T .M o v e m e n t\\nrecognition technology as a method of assessing spontaneous generalmovements in high risk infants. Front Neurol. 5, 284 (2014).\\n22. Weisman, O. et al. Dynamics of non-verbal vocalizations and hormones during\\nfather-infant interaction. IEEE Trans. Affect Comput 7,3 3 7–345 (2016).\\n23. Bourvis, N. et al. Pre-linguistic infan ts employ complex communicative loops\\nto engage mothers in social exchanges and repair interaction ruptures. R. Soc.\\nOpen Sci. 5, 170274 (2018).\\n24. Saint-Georges, C. et al. Do parents recognize autistic deviant behavior long\\nbefore diagnosis? Taking into account interaction using computational\\nmethods. PLoS ONE 6, e22393 (2011).\\n25. Saint-Georges, C. et al. Motherese in interaction: at the cross-road of emotion\\nand cognition? (A systematic review). PLoS ONE 8, e78103 (2013).\\n26. Mahdhaoui, A. et al. Computerized home video detection for motherese may\\nhelp to study impaired interaction between infants who become autistic and\\ntheir parents. Int J. Methods Psychiatr. Res. 20,e 6–e18 (2011).\\n27. Iba, W. & Langley, P. Induction of one-level decision trees. in Machine Learning:\\nProceedings of the Ninth International Workshop (eds Sleeman, D. & Edwards, P.)\\n233–240 (Morgan Kaufmann, San Mateo, CA, 1992).\\n28. Wan, M. W. et al. Quality of interaction between at-risk infants and caregiver at\\n12–15 months is associated with 3-year autism outcome. J. Child Psychol.\\nPsychiatry 54,7 6 3–771 (2013).\\n29. Olliac, B. et al. Infant and dyadic as sessment in early community-based\\nscreening for autism spectrum disorder with the PREAUT grid. PLoS ONE 12,\\ne0188831 (2017).\\n30. Green, J. et al. Parent-mediated interve ntion versus no intervention for infants\\nat high risk of autism: a parallel, single-blind, randomised trial. Lancet Psychiatry\\n2,1 3 3–140 (2015).\\n31. Purpura, G. et al. Bilateral patterns of repetitive movements in 6- to 12-\\nmonth-old infants with autism spectrum disorders. Front Psychol. 8, e1168\\n(2017).\\n3 2 . L o h ,A .e ta l .S t e r e o t y p e dm o t o rb e h a v i ors associated with autism in high-risk\\ninfants: a pilot videotape an alysis of a sibling sample. J. Autism Dev. Disord. 37,\\n25–36 (2007).\\n33. Morgan, L., Wetherby, A. M. & Barber, A. Repetitive and stereotyped move-\\nments in children with autism spectru m disorders late in the second year of\\nlife.J. Child Psychol. Psychiatry 49,8 2 6–837 (2008).\\n34. Elison, J. T. et al. Repetitive beh avior in 12-month-olds later classi ﬁed with\\nautism spectrum disorder. J. Am. Acad. Child Adolesc. Psychiatry 53,1 2 1 6 –1224\\n(2014).\\n35. Wolff, J. J. et al. Longitudinal patterns of repetitive behavior in toddlers with\\nautism. J. Child Psychol. Psychiatry 55,9 4 5–953 (2014).\\n36. Phagava, H. et al. General movements in infants with autism spectrum dis-\\norders. Georgian Med. N. 156,1 0 0–105 (2008).\\n3 7 . L i b e r t u s ,K . ,S h e p e r d ,K .A . ,R o s s ,S .W .&L a n d a ,R .J .L i m i t e d ﬁne motor and\\ngrasping skills in 6-month-old infants at high risk for autism. Child Dev. 85,\\n2218–2231 (2014).\\n38. Bedford, R. et al. Precursors to social and communication dif ﬁculties in infants\\nat-risk for autism: gaze following and attentional engagement. J. Autism Dev.\\nDisord. 42, 2208 –2218 (2012).\\n3 9 . E l s a b b a g h ,M .e ta l .W h a ty o us e ei sw h a ty o ug e t :c o n t e x t u a lm o d u l a t i o no f\\nface scanning in typical and atypical development. Soc. Cogn. Affect Neurosci.\\n9,5 3 8–543 (2014).\\n4 0 . J o n e s ,W .&K l i n ,A .A t t e n t i o nt oe y e s is present but in decline in 2-6-month-\\nold infants later diagnosed with autism. Nature 504,4 2 7–431 (2013).\\n4 1 . P a u l ,R . ,F u e r s t ,Y . ,R a m s a y ,G . ,C h a w a r s k a ,K .&K l i n ,A .O u to ft h em o u t h so f\\nbabes: vocal production in infan t siblings of children with ASD. J. Child Psychol.\\nPsychiatry 52,5 8 8–598 (2011).\\n4 2 . S h e i n k o p f ,S .J . ,I v e r s o n ,J .M . ,R i n a l d i ,M .L .&L e s t e r ,B .M .A t y p i c a lc r ya c o u s t i c s\\nin 6-month-old infants at risk for autism spectrum disorder. Autism Res. 5,\\n331–339 (2012).Ouss et al. Translational Psychiatry           (2020) 10:54 Page 7 of 7', metadata={'source': 'papers/22_Ouss_ASD_clean.txt'})],\n",
       " [Document(page_content='Research and Applications\\nMachine learning approach for early detection of autism\\nby combining questionnaire and home video screening\\nHalim Abbas,1Ford Garberson,1Eric Glover,2and Dennis P Wall1,3,4\\n1Cognoa Inc., Palo Alto, CA, USA www.linkedin.com/in/halimabbas,2eric_g@ericglover.com,3Department of Pediatrics, Stanford\\nUniversity, Stanford, CA, USA,4Department of Biomedical Data Science, Stanford University, Stanford, CA, USA\\nCorrespondence to: Cognoa Inc., Palo Alto, CA, USA; halim@cognoa.com\\nReceived 19 September 2017; Revised 16 March 2018; Editorial Decision 25 March 2018; Accepted 2 April 2018\\nABSTRACT\\nBackground: Existing screening tools for early detection of autism are expensive, cumbersome, time- intensive,\\nand sometimes fall short in predictive value. In this work, we sought to apply Machine Learning (ML) to goldstandard clinical data obtained across thousands of children at-risk for autism spectrum disorder to create alow-cost, quick, and easy to apply autism screening tool.\\nMethods: Two algorithms are trained to identify autism, one based on short, structured parent-reported ques-\\ntionnaires and the other on tagging key behaviors from short, semi-structured home videos of children. A com-bination algorithm is then used to combine the results into a single assessment of higher accuracy. To over-come the scarcity, sparsity, and imbalance of training data, we apply novel feature selection, featureengineering, and feature encoding techniques. We allow for inconclusive determination where appropriate inorder to boost screening accuracy when conclusive. The performance is then validated in a controlled clinicalstudy.\\nResults: A multi-center clinical study of n¼162 children is performed to ascertain the performance of these\\nalgorithms and their combination. We demonstrate a signiﬁcant accuracy improvement over standard screen-ing tools in measurements of AUC, sensitivity, and speciﬁcity.\\nConclusion: These ﬁndings suggest that a mobile, machine learning process is a reliable method for detection\\nof autism outside of clinical settings. A variety of confounding factors in the clinical analysis are discussed alongwith the solutions engineered into the algorithms. Final results are statistically limited and will beneﬁt from fu-ture clinical studies to extend the sample size.\\nKey words : supervised machine learning, autism spectrum disorder, diagnostic techniques and procedures, mobile applications\\nINTRODUCTION\\nDiagnosis within the first few years of life dramatically improves the\\noutlook of children with autism, as it allows for treatment while the\\nchild’s brain is still rapidly developing.1,2Unfortunately, autism is\\ntypically not diagnosed earlier than age 4 in the United States, withapproximately 27% of cases remaining undiagnosed at age 8.\\n3This\\ndelay in diagnosis is driven primarily by a lack of effective screening\\ntools and a shortage of specialists to evaluate at-risk children. Theuse of higher accuracy screening tools to prioritize children to be\\nseen by specialists is therefore essential.Most autism screeners in use today are based on questions for\\nthe parent or the medical practitioner, that produce results by com-paring summed answer scores to predetermined thresholds. Notableexamples are the Modified Checklist for Autism in Toddlers, Re-vised (M-CHAT),\\n4a checklist-based screening tool for autism that is\\nintended to be administered during developmental screenings forchildren between the ages of 16 and 30 months, and the Child Be-havior Checklist (CBCL).\\n5Both are parent-completed screening\\ntools. For both instruments, responses to each question are summedwith each question given equal weighting, and if the total is above apre-determined threshold the child is considered to be at high risk of\\nVCThe Author(s) 2018. Published by Oxford University Press on behalf of the American Medical Informatics Association.\\nAll rights reserved. For permissions, please email: journals.permissions@oup.com\\n1000Journal of the American Medical Informatics Association , 25(8), 2018, 1000–1007\\ndoi: 10.1093/jamia/ocy039\\nAdvance Access Publication Date: 7 May 2018\\nResearch and Applicationsautism. In the case of CBCL there are multiple scales based upon dif-\\nferent sets of questions corresponding to different conditions. The“Autism Spectrum Problems” scale of CBCL is used when comparingits performance to the performances of our algorithms in this paper.\\nIn this paper, we present two new machine learning screeners that\\nare reliable, cost-effective, short enough to be completed in minutes,and achieve higher accuracy than existing screeners on the same agespan as existing screeners. One is based on a short questionnaire about\\nthe child, which is answered by the parent. The other is based on iden-\\ntification of specific behaviors by trained analysts after watching twoor three short videos of the child within their natural environmentthat are captured by parents using a mobile device.\\nThe parent questionnaire screener keys on behavioral patterns\\nsimilar to those probed by a standard autism diagnostic instrument,the Autism Diagnostic Interview – Revised (ADI-R).\\n6This clinical\\ntool consists of an interview of the parent with 93 multi-part ques-tions with multiple choice and numeric responses which are deliv-ered by a trained professional in a clinical setting. While thisinstrument is considered a gold-standard, and gives consistentresults across examiners, the cost and time to administer it can beprohibitive in a primary care setting. In this paper, we present ourapproach to using clinical ADI-R instrument data to create ascreener based on a short questionnaire presented directly to parentswithout supervision.\\nThe video screener keys on behavioral patterns similar to those\\nprobed in another diagnostic tool, the Autism Diagnostic Observa-tion Schedule (ADOS).\\n7ADOS is widely considered a gold standard\\nand is one of the most common behavioral instruments used to aidin the diagnosis of autism.\\n8It consists of an interactive and struc-\\ntured examination of the child by trained clinicians in a tightly con-trolled setting. ADOS is a multi-modular diagnostic instrument,with different modules for subjects at different levels of cognitive de-velopment. In this paper, we present our approach to mining ADOSclinical records, with a focus on younger developmental age, to cre-ate a video-based screener that relies on an analyst evaluating shortvideos of children filmed by their parents at home.\\nThe use of behavioral patterns commonly probed in ADI-R and\\nADOS scoresheets as inputs to train autism screening classifiers wasintroduced, studied, and clinically validated in previous work.\\n9–12\\nThere are several new aspects in this paper. First, the algorithms de-\\ntailed in the present study have been designed to be more accurate\\nand more robust against confounding biases between training and\\napplication data. Next, this paper focuses considerable attention onthe impact of confounding factors on machine learning algorithmsin this context. Examples of these confounding biases will be dis-cussed below and highlighted in Table 2 . Labeled data usually origi-\\nnates from tightly controlled clinical environments and is, hence,clean but sparse, unbalanced, and of a different context to the dataavailable when applying the screening techniques in a less formal en-vironment. This paper also presents a combination between thealgorithms for a more powerful single screener. Lastly, this papergeneralizes the algorithms to be non-binary, sometimes resulting inan “inconclusive” determination when presented with data frommore challenging cases. This allows higher screening accuracy forthose children who do receive a conclusive screening, while still pre-senting a clinically actionable inconclusive outcome in the morechallenging cases.\\nThese classifiers of this paper were applied to screen children in\\na clinical study using the Cognoa\\n13App. To date, Cognoa has been\\nused by over 250 000 parents in the US and internationally. The ma-jority of Cognoa users are parents of young children between 18 and30 months. The clinical study consisted of 162 at-risk children who\\nhad undergone full clinical examination and received a clinical diag-nosis at a center specialized in neurodevelopmental disorders.\\nMETHODS\\nIt is not feasible to amass large training sets of children who havebeen evaluated by the mobile screeners and who also have received aprofessional medical diagnosis. Our approach is to start with histor-ical medical instrument records of previously diagnosed subjects,and use those as training data for screeners that will rely on informa-tion acquired outside the clinical setting. Expected performance deg-radation from applying the algorithms into a less controlled settingwould result in inaccurate screeners if conventional machine learn-ing methods were used. Much of this paper outlines the details ofcreative machine learning methods designed to overcome this chal-lenge and create reliable screeners in this setting.\\nTraining data were compiled from multiple repositories of\\nADOS and ADI-R score-sheets of children between 18 and 84months of age including Boston Autism Consortium, Autism Ge-\\nnetic Resource Exchange, Autism Treatment Network, Simons Sim-\\nplex Collection, and Vanderbilt Medical Center. Since suchrepositories are highly imbalanced with very few non-autisticpatients, the controls across the datasets were supplemented withbalancing data obtained by conducting ADI-R interviews by atrained clinician on a random sample of children deemed at low riskfor autism from Cognoa’s user base. For both algorithms a smallerset of optimal features was selected using methods that will be dis-cussed below. Details about the final selected features are given intheSupplementary Material .\\nThe clinical validation sample consists of 230 children who pre-\\nsented to one of three autism centers in the United States between 18and 72 months of age. All participants were referred through theclinics’ typical referral program process, and only those withEnglish-speaking parents were considered for the study. The threeclinical centers were approved on a multisite IRB (project number2202803). Every child received an ADOS as well as standard screen-ers like M-CHAT and CBCL as appropriate, and a diagnosis was ul-timately ascertained by a licensed health care provider. For 162 ofthose children, the parents also used their mobile devices to com-plete the short parental questionnaire and submit the short videosrequired for the screeners discussed in this paper. The sample break-down by age group and diagnosis for both the training and clinicalvalidation datasets is shown in Table 1 .\\nApproach\\nWe trained two independent ML classifiers and combined their out-\\nputs into a single screening assessment. The parent questionnaireclassifier was trained using data from historical item-level ADI-Rscore-sheets with labels corresponding to established clinical diagno-ses. The video classifier was trained using ADOS instrument score-sheets and diagnostic labels. In each case, progressive sampling wasused to verify sufficient training volume as detailed in the Supple-\\nmentary Materials . Multiple machine learning algorithms were eval-\\nuated including ensemble techniques on the training data. A numberof algorithms performed well. Random Forests were chosen becauseof robustness against overfitting.\\nADI-R and ADOS instruments are designed to be administered by\\ntrained professionals in highly standardized clinical settings and typi-cally take hours. In contrast, our screening methods are deliberatelyJournal of the American Medical Informatics Association , 2018, Vol. 25, No. 8 1001designed to be administered at home by parents without expert super-\\nvision, and to take only minutes to complete. This change of environ-ment causes significant data degradation and biases resulting in anexpected loss of screening accuracy. For each classifier, we presentmindful adjustments to ML methodology to mitigate these issues.These biases and efforts to mitigate them are discussed below.\\nDifferences Between Training and Application\\nEnvironments\\nThe screeners are trained on historical patient records that corre-\\nspond to controlled, lengthy clinical examinations, but applied viaweb or mobile app aimed at unsupervised parents at home. Table 2\\ndetails the various mechanisms by which confounding biases mayconsequently creep into the application data. Note that inaccuraciesintroduced by such biases cannot be probed by cross- validation orsimilar analysis of the training data alone.\\nHyperparameter Optimization\\nFor each parental questionnaire and video model that will be dis-cussed below, model hyperparameters were tuned with a boot-strapped grid search. In all cases, class labels were used to stratifythe folds, and (age, label) pairs were used to weight-balance the sam-ples. More details can be found in the Supplementary Materials .Parent Questionnaire\\nMultiple model variants representing incremental improvements\\nover a generic ML classification approach are discussed below.\\nGeneric ML Baseline Variant\\nA random forest was trained over the ADI-R instrument data. Each\\nof the instrument’s 155 data columns was treated as a categoricalvariable and one-hot encoded. The subject’s age and gender were in-cluded as features as well. Of the resulting set of features, the top 20were selected using feature-importance ranking in the decisionforest.\\nRobust Feature Selection Variant\\nDue to the small size and sparsity of the training dataset, generic fea-ture selection was not robust, and the selected features (along withthe performance of the resulting model) fluctuated from run to rundue to the stochastic nature of the learner’s underlying bagging ap-proach. Many ADI-R questions are highly correlated, leading tomultiple competing sets of feature selection choices that were seem-ingly equally powerful during training, but which had different per-formance characteristics when the underlying sampling bias wasexposed via full bootstrapped cross-validation. This resulted in awide performance range of the variant of the Generic ML baselinemethod as shown in Table 3 .Table 1. Dataset Breakdown by Age Group and Condition Type for Each of the Sources of Training Data and for the Clinical Validation\\nSample. The Negative Class Label Includes Normally Developing (i.e. neurotypical) Children as Well as Children with Developmental Delaysand Conditions other than Autism\\nNumber of samples\\nAge Condition Classification type Questionnaire Video Clinical validation\\n(years) training training\\n<4 Autism þ 414 1445 84\\n<4 Other condition /C0 133 231 18\\n<4 Neurotypical /C0 74 308 3\\n/C214 Autism þ 1885 1865 37\\n/C214 Other condition /C0 154 133 11\\n/C214 Neurotypical /C0 26 277 9\\nTable 2. Differences Between Training and Application Environments. These Differences are Expected to Cause Bias that Cannot be Cap-\\ntured by Cross-validation Studies\\nAspect Training Setting Application Setting\\nSource ADI-R and ADOS instrument administered\\nby trained professionals during clinicaleval-uationsShort parent questionnaires displayed on smartphone, and behavior tagging by\\nana-lysts after observing two or three 1-minute home videos uploaded byparents\\nProctor Highly trained medical professionals Parents answering the questionnaires are un-trained, and the analysts evaluating\\nthe home videos are only minimally trained. As a result, their answers may not\\nbe as consistent, objective, or reliable\\nSetting Clinic setting with highly standardized and\\nsemi-structured interactionsAt home. Not possible to recreate the structured clinical environment, resulting in\\nan undesired variability of the output signals. Subjects might also behave differ-\\nently at the clinic than at home, further amplifying the bias\\nDuration The ADI-R can take up to 4 hours to com-\\nplete; The ADOS can take up to 45\\nminutes of direct observation by trainedprofessionalsUnder 10 minutes to complete the parent questionnaire, and a few minutes of\\nhome video. As a result, some symptoms and behavioral patterns might be pre-\\nsent but not observed. Also causes big uncertainty about the severity and fre-quency of observed symptoms\\nQuestionnaires Sophisticated language involving psycholog-\\nical concepts, terms, and subtleties unfa-\\nmiliar to nonexpertsSimpliﬁed questions and answer choices result in less nuanced, noisier inputs1002 Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8Robust feature selection overcame that limitation using a two-\\nstep approach. First, a 100-count bootstrapped feature selectionwas run, with a weight balanced 90% random sample selected ineach iteration. The top 20 features were selected each time, and arank-invariant tally was kept for the number of times each featuremade it to a top-20 list. Next, the top 30 features in the tally werekept as candidates and all other features were discarded. A finalfeature-selection run was used to pick the best subset of these can-didate features. This approach was found to be more robust to sta-tistical fluctuations, usually selecting the same set of features whenrun multiple times. A minimal subset of maximally performant fea-tures was chosen and locked for clinical validation, totaling 17 fea-tures for the young children and 21 features for the old. Detailsabout these selected features are available in the Supplementary\\nMaterial .\\nAge Silo Variant\\nThis variant built upon the improvements of the robust feature selec-\\ntion method, by exploiting of the dichotomy between pre-phrasal\\nand fully-phrasal language capability in at-risk children. Languagedevelopment is significant in this domain as it is known to affect thenature in which autism presents, and consequently the kinds of be-havioral clues to look for in order to screen for it.\\nThis variant achieved better performance by training separate\\nclassifiers for children in the younger and older age groups ofTable 1 . The age dichotomy of <4,/C214 was chosen to serve as the\\nbest proxy for language ability. Feature selection, model parameter-tuning, and cross-validation were run independently for each agegroup classifier. Before siloing by age group, the classifier was lim-ited to selecting features that work well across children of both de-velopmental stages. Siloing enabled the classifiers to specialize onfeatures that are most developmentally appropriate within each agegroup.Severity-level Feature Encoding Variant\\nBuilding upon the method including age siloing above, this variant\\nachieved better performance by replacing one-hot feature encodingwith a more context-appropriate technique. One-hot encoding doesnot distinguish between values that correspond to increasing levels of\\nseverity of a behavioral symptom, and values that do not convey a clearconcept of severity. This is especially troublesome since a typical ADI-R instrument question includes answer choices from both types of val-ues. For example, ADI-R question 37, which focuses on the child’s ten-dency to confuse and mix up pronouns, allows for answer codes 0, 1,2, 3, 7, 8, and 9. Among those choices, 0 through 3 denote increasingdegrees of severity in pronominal confusion, while 7 denotes any othertype of pronominal confusion not cove red in 0-3 regardless of severity.\\nCodes 8 and 9 denote the non-applicability of the question (for exam-ple, to a child still incapable of phrasal speech) or the lack of an answer(for example, if the question was skipped) respectively. When codingthe answers to such questions, generic one-hot encoding would allowfor non-symptomatic answer codes to be selected as screening featuresbased on phantom correlations present in the dataset.\\nSeverity-level encoding converts all answer codes that do not con-\\nvey a relevant semantic concept to a common value, thereby reducing\\nthe chance of useless feature selection, and reducing the number of fea-tures to choose from. In addition, severity-level encoding condenses thesignal according to increasing ran ges of severity. For example, the\\nencoding of ADI-R question 37 would map its responses to new fea-tures with 1s in the following cases (all other new features would bezero): (0 !“¼0,” 1!“1,” 2![“1,” “2”], 3 ![“1,” “2,” “3”], 7 !\\n“¼7,” 8, 9 !None ). This more closely resembles the way medical prac-\\ntitioners interpret such answer choi ces, and helps alleviate the problem\\nof sparsity over each of the one-hot encoded features in the dataset.\\nAggregate Features Variant\\nBuilding upon the method including severity level encoding above,this variant achieved better performance by incorporating aggregateTable 3. Performance of Increasingly Effective Classiﬁer Variants Based on the Training Data for the Parent Questionnaire. Results in the\\nTop Table are Based on Cross-validated Training Performance. Results in the BottomTable (which are only available for variants using theoptimally selected features) are Based on Actual Clinical Results\\nAUC Sensitivity Specificity\\nAll ages <4 years >¼4 years All ages <4 years >¼4 years All ages <4 years >¼4 years\\nTraining scenario\\nGeneric ML baseline 0.932 to\\n0.9500.928 to\\n0.9530.928 to\\n0.9530.976 to\\n0.9820.975 to\\n0.9840.975 to\\n0.9840.628 to\\n0.6450.625 to\\n0.6480.625 to\\n0.648\\nRobust feature selection\\nvariant0.958 0.958 0.958 0.982 0.982 0.982 0.624 0.624 0.624\\nAge silo variant 0.953 0.939 0.961 0.962 0.939 0.977 0.777 0.774 0.779\\nSeverity-level feature\\nencoding variant0.965 0.950 0.974 0.962 0.912 0.993 0.748 0.833 0.692\\nAggregate features variant 0.972 0.987 0.963 0.992 0.988 0.994 0.754 0.894 0.661\\nWith inconclusive\\nallowance [up to 25\\\\%]0.991 0.997 0.983 1.000 1.000 1.000 0.939 0.977 0.881\\nApplication scenario\\nAge silo variant 0.62 0.68 0.54 0.65 0.62 0.52 0.48 0.46 0.24\\nSeverity-level feature\\nencoding variant0.67 0.69 0.64 0.64 0.62 0.58 0.48 0.46 0.33\\nAggregate features variant 0.68 0.73 0.68 0.68 0.69 0.65 0.57 0.62 0.48\\nWith inconclusive\\nallowance [up to 25\\\\%]0.72 0.72 0.73 0.70 0.72 0.67 0.67 0.71 0.53Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8 1003features such as the minimum, maximum, and average severity level,\\nas well as number of answer choices by severity level across thequestions corresponding to the 20 selected features. These new fea-tures were especially helpful due to the sparse, shallow, and wide na-ture of the training set, whereupon any semantically meaningfulcondensation of the signal can be useful to the trained classifier.\\nInconclusive Results Variant\\nChildren with more complex symptom presentation are known topose challenges to developmental screening. These children oftenscreen as false positives or false negatives, resulting in an overalldegradation of screening accuracy that is observed by all standardmethods and has become acceptable in the industry. Given that ourlow-cost instruments do not rely on sophisticated observations todifferentiate complex symptom cases, our approach was to avoidassessing them altogether, and to try instead to spot and label themas “inconclusive.”\\nBuilding upon the method including feature engineering, two\\nmethods to implement this strategy were devised. The first was totrain a binary classifier with a continuous output score, then replacethe cutoff threshold with a cutoff range, with values within the cut-off range considered inconclusive. A grid search was used to deter-mine the optimal cutoff range representing a tradeoff betweeninconclusive determination rate and accuracy over conclusive sub-jects. The second approach was to train and cross-validate a simplebinary classifier, label the correctly and incorrectly predicted sam-ples as conclusive or inconclusive respectively, and then build a sec-\\nond classifier to predict whether a subject would be incorrectly\\nclassified by the first classifier. At runtime, the second classifier wasused to spot and label inconclusives. The conclusives were sent forclassification by a third, binary classifier trained over the conclusivesamples only. Both methods for labeling inconclusive results yieldedsimilar performance. Therefore, the simpler method of using athreshold range in the machine learning output was used to reportinconclusive results for this paper.\\nThe inconclusive rate is a configurable model parameter that\\ncontrols the tradeoff between coverage and accuracy. Throughoutthis paper, the inconclusive rate for this variant was set to 25%.\\nVideo\\nThe second of our two-method approach to autism screening is anML classifier that uses input answers about the presence and severityof target behaviors among subjects. This information was providedby an analyst upon viewing two or three 1-minute home videos ofchildren in semi-structured settings that are taken by parents ontheir mobile phones. The classifier was trained on item-level data\\nfrom two of the ADOS modules (module 1: preverbal, module 2:\\nphrased speech) and corresponding clinical diagnosis.\\nTwo decision forest ML classifiers were trained corresponding to\\neach ADOS module. For each classifier, 10 questions were selectedusing the same robust feature selection method, and the same allow-ance for inconclusive outcomes was made as for the parental ques-tionnaire classifier. Each model was independently parameter-tunedwith a bootstrapped grid search. Class labels were used to stratifythe cross-validation folds, and (age, label) pairs were used to weight-balance the samples.\\nProblems related to the change of environment from training to\\napplication are especially significant in the case of video screeningbecause ADOS involves a 45 minute direct observation of the childby experts, whereas our screening was based on unsupervised shorthome videos. Specifically, we expect the likelihood of inconclusive\\nor unobserved behaviors and symptoms to be much higher in the ap-plication than in the training data, and the assessed level of severityor frequency of observed symptoms to be less reliable in the applica-tion than in the training data. The following improvements weredesigned to help overcome these limitations.\\nPresence of Behavior Encoding\\nTo minimize potential bias from a video analyst misreading the se-verity of a symptom in a short cell phone video, this encodingscheme improves feature reliability at the expense of feature infor-mation content by collapsing all severity gradations of a questioninto one binary value representing the presence vs absence of the be-havior or symptom in question. Importantly, a value of 1 denotesthe presence of behavior, regardless of whether the behavior is indic-ative of autism or of normalcy. This rule ensures that a value of 1corresponds to a reliable observation, whereas a 0 does not necessar-ily indicate the absence of a symptom but possibly the failure to ob-serve the symptom within the short window of observation.\\nMissing Value Injection to Balance the Nonpresence of Features for\\nthe Video Screener Training DataWhile collapsing severity gradations into a single category over-comes noisy severity assessment, it does not help with the problemof a symptom not present or unnoticeable in a short home video.For this reason, it is important that the learning algorithm treat avalue of 1 as semantically meaningful, and a value of 0 as inconse-\\nquential. To this end, we augmented the training set with duplicate\\nsamples that had some feature values flipped from 1 to 0. The injec-tion of 0s was randomly performed with probabilities such that thesample-weighted ratio of positive to negative samples for which thevalue of any particular feature is 0 is about 50%. Such ratios ensurethat the trees in a random forest will be much less likely to drawconclusions from the absence of a feature.\\nCombination\\nIt is desirable to combine the questionnaire and video screeners toachieve higher accuracy. However, the needed overlapping trainingset was not available. Instead, the clinical validation dataset itselfwas used to train the combination model.\\nThe numerical responses of each of the parent questionnaire and\\nvideo classifiers were combined using L2-regularized logistic regres-sion, which has the advantage of reducing the concern of overfitting,particularly given the logistic model has only three free parameters.Bootstrapping and cross -validation studies showed that any overfit-ting that may be present from this procedure is not detectable withinstatistical limitations. Since each of the individual methods wassiloed by age, separate combination algorithms were trained per agegroup silo. For each combination algorithm, optimal inconclusiveoutput criteria were chosen using the logistic regression response,using the same techniques as for the parental questionnaire andvideo classifiers. The performance characteristics of the overallscreening process compared to standard alternative screeners are\\nshown below.\\nRESULTS\\nParent Questionnaire Performance on Training Data\\nBootstrapped cross-validation performance metrics for the optimally\\nparameter-tuned version of each of the variants of the parental1004 Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8questionnaire are reported in the top of Table 3 . The results for\\nbaseline variant are reported as a range rather than a single value,\\nbecause the unreliability of generic feature selection leads to differ-ent sets of features selected from run to run, with varying perfor-mance results.\\nParents of children included in the clinical study answered short,\\nage-appropriate questions chosen using the robust feature selectionmethod discussed above. The clinical performance metrics for each ofthe classification variants that build upon that feature selection schemea r es h o w ni nt h eb o t t o mo f Table 3 . The difference in performance be-\\ntween the training and validation datasets is driven by the differencesthat are emphasized in Table 2 . See below and the results of Table 4\\nfor a discussion of the statistical significance of these results.\\nROC curves in Figure 1 show how our parent questionnaire clas-\\nsification approach outperforms some of the established screeningtools like MCHAT and CBCL on the clinical sample. Since clinicalcenters are usually interested in screening tools with a high sensitiv-ity, we have drawn shaded regions between 70% and 90% sensitiv-ity to aid the eye.\\nCombination Screening Performance on Clinical Data\\nROC curves in Figure 2 show how combining the questionnaire and\\nvideo classifiers into a single assessment further boosted perfor-mance on the clinical study sample. When up to 25% of the mostchallenging cases are allowed to be determined, inconclusive the per-formance on the remaining cases is shown in Figure 3 . Note that the\\nROC curves in these figures for M-CHAT contain only younger chil-dren (mostly under four years of age) due to the fact that this instru-\\nment is not intended for older children. A same-sample comparison\\nbetween M-CHAT and the ML screeners can be seen in the agebinned figures ( Figures 4 and5).\\nResults for Young Children\\nYoung children are of particular interest given the desire to identify\\nautism as early as possible. Results restricted to only children lessthan four years old are shown in Figures 4 and5.Statistical Significance\\nFor the training data, sample sizes are large enough that statistical\\nlimitations are minimal. However, results reported for the clinicaldata have significant statistical limitations. In this section we com-pare the performance of the screening algorithms on the clinicaldata that we have discussed in this paper: (1) the questionnaire-based algorithm of,\\n13(2) M-CHAT, (3) CBCL, (4) the\\nquestionnaire-based algorithm of this paper, and (5) the combined\\nquestionnaire plus video algorithm of this paper. Direct comparisonsin performance between many of these algorithms are reportedalong with statistical significances in Table 4 .\\nDISCUSSION\\nWe have introduced a novel machine learning algorithm based on a\\nparental questionnaire and another based on short home videosrecorded by parents and scored by a minimally trained analyst. Wehave discussed pitfalls such as data sparsity, and the mixed ordinaland categorical nature of the questions in our training data. Wehave also identified several important confounding factors that arisefrom differences between the training and application settings of the\\nalgorithms. We have shown novel feature encoding, feature selec-\\ntion, and feature aggregation techniques to address these challenges,and have quantified their benefits. We have shown the benefits ofallowing some subjects with lower certainty output from the algo-rithms to be classified as inconclusive. We have also shown the bene-fits of combining the results of the two algorithms into a singledetermination.\\nBy specializing the machine learning models on a dichotomy of\\nage groups, we found that the screener for younger children capital-ized on non-verbal behavioral features such as eye contact, gestures,and facial expressions, while the screener for older children focusedmore on verbal communication and interactions with other children.For more details please refer to the Supplementary Material .\\nThe methods and resulting improvements shown in this paper\\nare expected to translate well into other clinical science applicationsTable 4. Performance Comparisons Between Various Algorithms on Clinical Data\\nBase model Model from this paper AUC improvement Mean recall improvement\\n2012 publication Questionnaire 0.07, [ /C00.03, 0.17] 0.1, [0.02, 0.17]\\nM-CHAT Questionnaire 0.01, [ /C00.11, 0.12] 0.06, [ /C00.04, 0.17]\\nCBCL Questionnaire 0.06, [ /C00.04, 0.17] 0.11, [0.03, 0.2]\\n2012 publication Questionnaire & video 0.16, [0.07, 0.25] 0.12, [0.04, 0.2]\\nM-CHAT Questionnaire & video 0.08, [ /C00.03, 0.19] 0.1, [ /C00.01, 0.21]\\nCBCL Questionnaire & video 0.15, [0.04, 0.26] 0.14, [0.04, 0.24]\\n2012 publication Questionnaire þinconclusive 0.16, [0.02, 0.28] 0.09, [ /C00.02, 0.2]\\nM-CHAT Questionnaire þinconclusive /C00.01, [/C00.39, 0.31] 0.08, [ /C00.18, 0.29]\\nCBCL Questionnaire þinconclusive 0.15, [0.01, 0.29] 0.11, [ /C00.02, 0.24]\\n2012 publication Questionnaire & video þinconclusive 0.21, [0.1, 0.32] 0.19, [0.1, 0.28]\\nM-CHAT Questionnaire & video þinconclusive 0.09, [ /C00.05, 0.23] 0.15, [0.04, 0.27]\\nCBCL Questionnaire & video þinconclusive 0.2, [0.09, 0.32] 0.2, [0.09, 0.31]\\nQuestionnaire Questionnaire & video 0.09, [0.02, 0.15] 0.03, [ /C00.04, 0.09]\\nQuestionnaire Questionnaire þinconclusive 0.09, [ /C00.01, 0.17] /C00.0, [/C00.09, 0.08]\\nQuestionnaire Questionnaire & video þinconclusive 0.14, [0.06, 0.23] 0.09, [0.01, 0.17]\\nQ. and video Questionnaire & video þinconclusive 0.06, [0.01, 0.11] 0.06, [0.0, 0.13]\\nEach row evaluates the improvement of one of the algorithms from this paper over a “Base model” algorithm for the AUC metric, and for the average between\\nthe autism and the non-autism recalls at a response threshold point that achieves approximately 80% sensitivity. Negative values would represent a w orsening of\\nperformance for a given algorithm compared to the base model. Both average values of the improvements and [5%, 95%] conﬁdence intervals are reported. Algo-\\nrithms that are labeled “inconclusive” allow up to 25% of the most difﬁcult samples to be discarded from the metric evaluation. Note that the M-CHAT ins tru-\\nment is intended for use on younger children. Therefore, older children were excluded when preforming comparisons to M-CHAT in this table.Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8 1005Figure 1. ROC curves on the clinical sample for various questionnaire based autism screening techniques, ordered from the least to most sophisticated. Note th at\\nunlike Figures 2 through 3and4, 168 children are included in this sample (six children did not have videos available).\\nFigure 2. ROC curves on the clinical sample for the questionnaire and the video based algorithms, separately and in combination. The established screening too ls\\nMCHAT and CBCL are included as baselines.\\nFigure 3. ROC curves on the clinical sample for the questionnaire and the video based algorithms, separately and in combination. Inconclusive determination i s\\nallowed for up to 25% of the cases. The established screening tools MCHAT and CBCL are included as baselines.\\nFigure 4. ROC curves on the clinical results for children under four years of age, for the questionnaire and the video based algorithms, as well as the combinatio n.\\nComparisons with the established (nonmachine learning) screening tools MCHAT and CBCL are also shown.1006 Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8including screening for cognitive conditions such as dementia for the\\nelderly and physical conditions such as concussions in adults. Fur-ther, we expect that these methods would apply well to any othersurvey based domain in which the application context is differentfrom the training context.\\nSignificant further improvements may be possible. Initial studies\\nhave identified probable improvements to the machine learningmethodology as well as improved methods for handling the biasesbetween the training data and application settings. A new clinicaltrial with larger sample sizes is underway that will make it possibleto validate new improvements resulting from these studies as well asto improve confidence in the high performance of our algorithms.\\nCONCLUSION\\nMachine learning can play a very important role in improving the ef-fectiveness of behavioral health screeners. We have achieved a sig-nificant improvement over established screening tools for autism inchildren as demonstrated in a multi-center clinical trial. We havealso shown some important pitfalls when applying machine learningin this domain, and quantified the benefit of applying proper solu-\\ntions to address them.\\nFUNDING\\nThis research received no specific grant from any funding agency in the pub-\\nlic, commercial or not-for-profit sectors.\\nCOMPETING INTERESTS\\nAll authors are affiliated with Cognoa Inc. in an employment and/or\\nadvisory capacity.\\nCONTRIBUTORS\\nAll listed authors contributed to the study design as well as the draft-ing and revisions of the paper. All authors approve of the final ver-sion of the paper to be published and agree to be accountable for allaspects of the work.SUPPLEMENTARY MATERIAL\\nSupplementary material is available at Journal of the American\\nMedical Informatics Association online.\\nREFERENCES\\n1. Durkin MS, Maenner MJ, Meaney FJ. Socioeconomic inequality in the\\nprevalence of autism spectrum disorder: evidence from a U.S. cross-\\nsectional study. PLoS One 2010; 5 (7): e11551.\\n2. Christensen DL, Baio J, Braun KV, et al. Prevalence and characteristics of\\nautism spectrum disorder among children aged 8 years—autism and devel-\\nopmental disabilities monitoring network, 11 sites, United States, 2012.\\nMMWR Surveill Summ 2016; 65 (3): 1–23.\\n3. Zwaigenbaum L, Bryson S, Lord C, et al . Clinical assessment and\\nmanagement of toddlers with suspected autism spectrum disorder:insights from studies of high-risk infants. Pediatrics 2009; 123 (5):\\n1383–91.\\n4. BernierMao RA, Yen J. Diagnosing autism spectrum disorders in primary\\ncare. Practitioner 2011; 255 (1745): 27–30.\\n5. Achenbach TM, Rescorla LA. Manual for the ASEBA School-Age Forms\\n& Proﬁles . Burlington, VT: University of Vermont, Research Center for\\nChildren, Youth, & Families. 2001.\\n6. Lord C, Rutter M, Le Couteur A. Autism diagnostic interview-revised: a\\nrevised version of a diagnostic interview for caregivers of individuals with\\npossible pervasive developmental disorders. J Autism Dev Disord 1994;\\n24 (5): 659–85.\\n7. Lord C, Rutter M, Goode S, et al. Autism diagnostic observation schedule:\\na standardized observation of communicative and social behavior. J Au-\\ntism Dev Disord 1989; 19 (2): 185–212.\\n8. Lord C, Petkova E, Hus V, et al. A multisite study of the clinical diagnosis\\nof different autism spectrum disorders. Arch Gen Psychiatry 2012; 69 (3):\\n306–13.\\n9. Wall DP, Dally R, Luyster R, et al. Use of artiﬁcial intelligence to shorten\\nthe behavioral diagnosis of autism. PLoS One 2012; 7 (8): e43855.\\n10. Duda M, Kosmicki JA, Wall DP, et al . Testing the accuracy of an\\nobservation-based classiﬁer for rapid detection of autism risk. Transl Psy-\\nchiatry 2014; 4 (8): e424.\\n11. DudaDaniels MJ, Wall DP. Clinical evaluation of a novel and mobile au-\\ntism risk assessment. J Autism Dev Disord 2016; 46 (6): 1953–1961.\\n12. Fusaro VA, Daniels J, Duda M, et al. The potential of accelerating early\\ndetection of autism through content analysis of youtube videos. PLoS One\\n2014; 16;9 (4): e93533.\\n13. Cognoa, Inc. Palo Alto: CA. https://www.cognoa.com/.\\nFigure 5. ROC curves on the clinical results for children under four years of age, for the questionnaire and the video based algorithms, as well as the combinatio n,\\nrestricted to the children who were not determined to have an inconclusive outcome (tuned to have at most 25% allowed to be inconclusive). Comparisons with\\nthe established (nonmachine learning) screening tools MCHAT and CBCL are also shown.Journal of the American Medical Informatics Association , 2018, Vol. 25, No. 8 1007', metadata={'source': 'papers/Abbas_2018_clean.txt'})],\n",
       " [Document(page_content='1 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreportsMulti-modular Ai Approach to \\nStreamline Autism Diagnosis in \\nYoung children\\nHalim Abbas  1, ford Garberson  1, Stuart Liu-Mayo  1, eric Glover1* & Dennis p . Wall  2\\nAutism has become a pressing healthcare challenge. the instruments used to aid diagnosis are time \\nand labor expensive and require trained clinicians to administer, leading to long wait times for at-risk \\nchildren. We present a multi-modular, machine learning-based assessment of autism comprising three \\ncomplementary modules for a unified outcome of diagnostic-grade reliability: A 4-minute, parent-report questionnaire delivered via a mobile app, a list of key behaviors identified from 2-minute, semi-structured home videos of children, and a 2-minute questionnaire presented to the clinician at the time of clinical assessment. We demonstrate the assessment reliability in a blinded, multi-site clinical \\nstudy on children 18-72 months of age (n = 375) in the United States. It outperforms baseline screeners \\nadministered to children by 0.35 (90% CI: 0.26 to 0.43) in AUC and 0.69 (90% CI: 0.58 to 0.81) in \\nspecificity when operating at 90% sensitivity. Compared to the baseline screeners evaluated on children less than 48 months of age, our assessment outperforms the most accurate by 0.18 (90% CI: 0.08 to 0.29 at 90%) in AUC and 0.30 (90% CI: 0.11 to 0.50) in specificity when operating at 90% sensitivity.\\nIdiopathic forms of Autism Spectrum Disorder (ASD)\\xa0have no known biological cause and may correspond to \\nmultiple conditions with similar symptoms. The incidence of ASD has increased in recent years, and it impacts 1 \\nin 59 children according to the latest studies\\n1. ASD is diagnosed from clinical observations according to standard \\ncriteria2 relating to the child’s social and behavioral symptoms. Autism is said to be on a spectrum due to the \\nvaried severities of symptoms, ranging from relatively mild social impairment to debilitating intellectual disabil-ities, inabilities to change routines and severe sensory reactions\\n2. Approximately 25–50%3 of autistic children are \\nnon-verbal and\\xa0have severe symptoms.\\nNotably, diagnosis within the first few years of life dramatically improves the outlook of children with autism, \\nas it allows for treatment during a key window of developmental plasticity4,5. Unfortunately, the latest studies \\nshow that although 85% of parents of children with autism reported developmental concerns about their chil-dren by 36 months of age, the median age of diagnosis in the United States is 52 months\\n1. The complexity of the \\ndiagnostic procedures and the shortage of trained specialists can result in children with ASD not\\xa0getting a diag-nosis\\xa0early enough to receive behavioral\\xa0therapies during the time when they are most effective.\\nDiagnosing autism in the United States generally takes two steps: developmental screening followed by com-\\nprehensive diagnostic evaluation if screened positive\\n6. Screening instruments typically use\\xa0questionnaires that \\nare answered by a parent, teacher or clinician7,8. They are\\xa0generally easy and inexpensive to administer and can \\nbe useful to flag some at-risk children, however, they are not always\\xa0accurate enough to help inform a diagnosis9. \\nStandard autism screeners can also have a high false positive rate, leading to unnecessary referrals and healthcare costs\\n10. Comprehensive diagnostic evaluation instruments, on the other hand, are more accurate but require long \\nand expensive\\xa0interactions with highly trained clinicians11,12.\\nIn this paper, we present improvements to\\xa0two previously published13 automated autism assessment modules \\nunderlying the Cognoa14 software. The first module is based on a brief questionnaire about the child presented \\ndirectly to parents without supervision. The second module is based on lightly\\xa0trained analysts evaluating short videos of children within their natural environment that are captured by parents using a mobile device. We also present a new, third module that is intended to be completed in a primary care setting such as a pediatrician’s office during a clinic visit. The third module is based upon a questionnaire that is answered by a clinician after \\nexamining the child and talking to the parent. We demonstrate that these three modules are as fast and easy to \\n1Cognoa Inc., Palo Alto, CA, USA. 2Departments of Pediatrics, Biomedical Data Science and Psychiatry and \\nBehavioral Sciences, Stanford University, Stanford, CA, USA. *email: eri_g@ericglover.comopen2 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/administer as most of the typical screening instruments, yet their combined assessment accuracy is shown in this \\nwork to be significantly higher, such that they may be used to aid in diagnosis of autism.\\nWe present our approach to selecting maximally predictive features for each of the modules. Both the parent \\nand the clinician questionnaire modules key on behavioral patterns similar to those probed by a standard autism diagnostic instrument, the Autism Diagnostic Interview - Revised (ADI-R)\\n11. ADI-R is administered by a trained \\nclinician, and typically gives consistent results across examiners. But its 93 point questionnaire often spanning 2.5 hours of the interviewer and parent’s time makes it largely impractical for the primary care setting\\n15. The \\nvideo assessment module keys on behavioral patterns similar to those probed in another diagnostic instrument, the Autism Diagnostic Observation Schedule (ADOS)\\n12. ADOS is a multi-modular diagnostic instrument, with \\ndifferent modules for subjects at different levels of cognitive development. It is widely considered a gold standard and is one of the most common behavioral instruments used to aid in the diagnosis of autism\\n16. It consists of an \\ninteractive and structured examination of the child by trained clinicians in a tightly controlled setting.\\nFor validation, the three modules are applied to assess children in a clinical study using the Cognoa14 software. \\nTo-date, Cognoa has been used by over 300,000 parents in the US and internationally. The majority of Cognoa users are parents of young children between 18 and 48 months. The clinical study underlying the validation results discussed in the results section consists of a total of 375 at-risk children who had undergone full clinical examination and received a clinical diagnosis at a center specialized in neurodevelopmental disorders\\n17. The \\noutputs of the assessment modules are compared to those of three screening instruments. The Modified Checklist for Autism in Toddlers, Revised (M-CHAT-R)\\n7 is a parent-completed questionnaire for autism that is intended \\nto be administered during developmental screenings for children between the ages of 16 and 30 months and is commonly used as an autism screening instrument. The Social Responsiveness Scale - Second Edition (SRS) is another standard ASD screener that is based upon a questionnaire filled out by an examiner\\n18–20. The SRS has a \\npreschool form intended for children of ages 30 months to 54 months, and a school age form intended for chil-dren of ages 48 months through 18 years of age. We use SRS “total score” scale as a baseline autism assessment. The Child Behavior Checklist (CBCL)\\n8 is a parent-completed questionnaire that provides risk assessments in \\nmany categories. We use the “ Autism Spectrum Problems” scale of CBCL for comparison. In all cases, the answers to the questions comprising the screeners are coded, then the codes are summed and the sum compared against a threshold to determine whether the child is at risk.\\nMethods\\nWe base our approach on de-identified\\xa0historical patient records. We collect medical instrument score sheet data pertaining to children tested for suspicion of autism, and process those into training sets for the predictive models underlying each of our three autism assessment modules.\\nSince we apply said predictive models in a significantly different setting than the clinics where the correspond-\\ning training data were generated, we expect a consequential performance degradation resulting in unacceptable \\ndiagnostic accuracy if conventional machine learning methods are used\\n13. To counteract that effect, we apply \\ncustom machine learning techniques as detailed in this section, building upon previous experimental work13. The \\nnew techniques discussed below are empirical post-hoc feature selection, training data noise injection, and an \\noverfitting-resilient probabilistic combination of module outcomes.\\nData. Training data were compiled from multiple repositories of de-identified\\xa0ADOS and ADI-R score sheets \\nof children between 18 and 84 months of age including Boston Autism Consortium, Autism Genetic Resource Exchange, Autism Treatment Network, Simons Simplex Collection, and Vanderbilt Medical Center. To counteract class imbalance, the sample set negative class was supplemented with 59 low risk children random-sampled from Cognoa’s user-base, and ADI-R was administered on those additional controls.\\nThe diagnostic accuracy of our modules was measured using data from a multi-site blinded clinical validation \\nstudy (reviewed and approved by Western IRB project number 2202803)\\n17. The study was performed in 2016 and \\n2017 at three tertiary care centers in the United States. Informed consent was obtained from guardians of each child, and all relevant regulations and guidelines were followed. Children enrolled in the study were 18 to 72 months of age, of English-speaking households, and were all referred through the typical referral process for sus-picion of autism. Every child was measured using autism assessment instruments (such as ADOS, M-CHAT-R, \\nand/or CBCL) as appropriate for his or her age. Diagnosis was ultimately ascertained by a licensed health care \\nprovider. Prior to the clinical assessment, parents used the Cognoa mobile app to complete the parent question-naire and video assessment modules, and starting in 2017, a clinician also completed the Cognoa clinician ques-tionnaire. The clinicians were blinded to the results of the assessment rendered by Cognoa. More details on the steps of the clinical study are shown in Fig.\\xa0 1.\\nThe enrollment process in 2016 yielded 162 validation samples, which were used to validate the parent ques-\\ntionnaire and video modules. This same clinical enrollment cohort was used as validation dataset in our previous publication on the subject\\n13. Given the learnings from this dataset, and prior to the extension of the study in 2017, \\nseveral improvements were made to the algorithms including tuning of model thresholds, training combination modules, and performing feature selection for the clinician module which was newly introduced in 2017. The enrollment process in 2017 yielded 213 additional validation participants, bringing the total N to 375 samples over the course of the two years.\\nThe sample breakdown by cohort, age group, and diagnosis for all data used for training and validation is \\nshown in Table\\xa0 1. In both the training and the validation datasets, the majority of the “Not autism” class label is \\ncomposed mostly of children who are diagnosed with an alternate developmental delay (e.g.,\\xa0ADHD or speech and\\xa0language disorder). Since these conditions share many symptoms with autism, this is a particularly challeng-ing sample for classification\\xa0. Only seven of the children in the validation sample are neurotypical, suggesting that this sample will be harder to perform correct classifications on than in the general population.3 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/Algorithm methodology.  In this section we explain important aspects of our machine learning methodol-\\nogy that are common to the classifiers underlying each of our three assessment modules.\\nTraining procedure.  Classifier training, feature selection, and optimization were done separately for children \\nunder four years of age and four years of age and over. The parent questionnaire and clinician questionnaire clas-\\nsifiers make predictions based off of answers to questions that probe similar concepts to the ADI-R questionnaire. They were trained using the answers to questions from historical item-level ADI-R score sheets with labels corre-sponding to established clinical diagnoses. The video module makes predictions based off of answers to questions that probe similar concepts to the ADOS instrument, as recorded by video analysts. It was trained using ADOS instrument score sheets and diagnostic labels. Progressive sampling was used to verify sufficient training volume as detailed in the supplementary materials. Gradient boosted decision trees were used for all three modules as they consistently performed better than other options that were considered such as neural networks, support \\nvector machines, and logistic regression. For all models, hyper-parameters were tuned with a bootstrapped grid \\nsearch. In all cases, true class labels (ASD or non-ASD) were used to stratify the folds, and (age, label) pairs were used to weight-balance the samples. More details can be found in the supplementary materials.\\nIn all cases, the machine learning models were trained using historical patient records that correspond to con-\\ntrolled clinical examinations, but focused on application in non-clinical\\xa0settings aimed for brevity, ease-of-use, and/or unsupervised parent usage at home. These differences introduce biases which can be significant enough to ruin the performance of an algorithm if not properly addressed, and which cannot be probed by cross validation. See the supplementary material for further details. New strategies to address these biases are discussed below that result in big improvements in accuracy compared to previous work\\n13.\\nFigure 1. Detailed steps performed during the clinical study described in this document.\\nAge (years) ConditionNumber of samples\\nParent/Clinician \\nmodule trainingVideo module trainingClinical validation 2016Clinical validation 2017\\n<4 Autism 414 1445 75 91\\n<4 Not autism 207 539 20 30\\n≥4 Autism 1885 1865 46 60\\n≥4 Not autism 180 410 21 32\\nTable 1. Dataset breakdown by age group and condition for each of the sources of training data and for the \\nclinical validation sample. Machine learning model training was stratified by age group. Clinical validation 2016 \\nand 2017 samples are used together to evaluate performance of the parent and video modules in this paper, while the clinician module was only available for the clinical 2017 dataset.4 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/Inconclusive outcomes.  Each of the three modules predicts one of three assessment outcomes: Positive, negative, \\nand inconclusive. As outlined in Fig.\\xa0 2, support for inconclusive determination is incorporated using a process \\nthat involves three separate machine learning training runs. The first model is trained to make predictions that are \\nused to label the samples in the training data that are the most likely to be misclassified. A second model is then trained using these labels to predict the likelihood of any new samples being misclassified. Finally, only samples \\nthat are likely to be classified correctly are used to train a final, binary autism classifier. Only the latter two models \\nare used at prediction time: the one to identify and filter out samples that should be labeled “inconclusive” , and the other to make a binary prediction of whether the child is autistic in those which are likely to be correctly labeled. More details about how these models are trained are\\xa0available in the supplementary material.\\nparental module.  Initial feature selection . The parental questionnaire probes for a minimal set of highly \\nrelevant child behavioral patterns that are maximally predictive of autism in combination. Care is taken to phrase the questions and answers such that the most reliable signal can be input from everyday parents undertaking the questionnaire via mobile app without clinical assistance.\\nTo that effect, a custom feature selection method is devised involving robust bootstrap-driven backwards \\nsubtraction, the details of which are discussed in a previous publication\\n13. Out of an initial set of 93 questions \\nunder consideration, this produces an optimal set of 17 novel\\xa0questions for children less than four years old, and 21 questions for children four and older.\\nEmpirical post-hoc feature selection refinement. Following the conclusion of the 2016 clinical validation study \\nenrollment, we studied differences in the distribution of answers to each question between the training data and \\nthe validation data that was collected in the 2016 clinical study. While some questions had quite good agree-ment, others show a strong bias towards higher (or lower) severity answer choices in the clinical data than in the training data. Questions for which the mean absolute severity difference was statistically greater than three standard errors (averaged over the autism and the non-autism samples) are rejected. This requirement results in \\nthe exclusion of 4 out of the 17 questions in the younger cohort, and 8 out of 21 questions in the older cohort, and \\nthe models are re-trained (with new hyper-parameter tuning) on the reduced feature set. This further refinement of the selected features minimizes the significant biases due to differences between the training and application environment. See the supplementary material for more details on these differences.\\nThis feature refinement leads to a larger boost in performance compared with\\n13 than any other improvement. \\nThe size of the performance improvement is validated on the held-out sample of children collected during 2017, where the new models show a statistically equivalent increase in performance compared with the 2016 sample.\\nVideo module. The video assessment module consists of a parent upload of 2 or 3 mobile videos, each 1 to \\n2 minutes in length, of the child during play or meal time at home. The underlying algorithm produces autism assessments based upon the responses of at least three minimally-trained analysts who watch the videos and then respond to a behavioral questionnaire.\\nThe data available for training the video module’s classification model are taken from ADOS sessions admin-\\nistered by clinicians in standardized clinical settings. Gradient boosted decision trees are trained keying off of the features identified in the analysis of\\xa0ADOS records. The questionnaires that the video analysts answer are then created to probe for similar behavioral features as those observed in the training data. A challenge of this method-ology is that the module must make predictions in the face of missing features that are not observable in the short videos uploaded by the parents. The video analysts are allowed to skip any questions if not answerable based on \\nFigure 2. An illustration of the methodology for training diagnostic assessment algorithms capable of \\noutputting one of three possible outcomes: “positive” , “negative” , or “inconclusive” . The first binary classifier is only used to assist in training and never at runtime. It is trained to predict binary “autism” vs “not autism” , and these labels are then compared with the true ASD results to label which samples are incorrectly classified. The samples with their “correct” and “incorrect” labels are\\xa0used to train the classifiers at runtime. A “indeterminate” \\nclassifier is trained to predict which samples will have their ASD diagnosis misclassified, which serves as a filter \\nto identify “inconclusive” cases at runtime, while only the predicted “correct” samples are used to train the final binary ASD diagnosis classifier.5 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/the posted videos. On average, analysts skipped questions 15% of the time, with big variations among particular \\nquestions. This effect, combined with the large discrepancy in the observation time window from the original clinical examination to the brief home-video version, would result in a big assessment-accuracy degradation unless steps are taken to correct for the bias and variance .\\nWe tackle this problem by introducing bias and variance to the training data in a manner designed to make it statis-\\ntically similar to the video analyst answers on which the assessments will be run. The data from the 2016 clinical study\\n13 \\nis used to develop this methodology, and the performance of the algorithm on the data from the children enrolled in 2017 is used to validate the generalizability of the improvements. Most children who participated in the clinical study are also administered a full ADOS, which provided paired ADOS and video data that we use to determine what\\xa0noise patterns to add. Using these paired data, we construct a probability map for each question-response set describing the \\nways video analysts are likely to respond for a given “true” ADOS response. We then use the mapping as a stochastic \\ntransform to build a new training data set that can be thought of as the results of a hypothetical experiment in which the technicians watch parent-supplied video of the children in the training data and respond accordingly.\\nThe addition of simulated “setting noise” to the classifier training data leads to a larger boost in performance \\ncompared with\\n13 than any other improvement13. Additionally, the optimal parameters for the resulting decision \\ntree models favor larger tree depth. This is as expected, since the new models are expected to make determina -\\ntions as to which features are reliable when present, as well as which features to fall back on when the best features are missing.\\nclinician module.  We introduce a module to screen for autism using questionnaire responses from a cli-\\nnician. A pediatrician might answer these questions during a regular checkup. The questions for the clinician were selected in a similar manner as used for the Parental Module (see the supplementary material for details). Responses from both the parent and the clinician are used in a machine learning module in the same manner as described for the parental questionnaire above. Some key behaviors are probed via questions directed at both the \\nparent and the clinician, but the clinician questions are more nuanced and allow for more subtle answer choices. \\nIn cases where the parent and the clinician give contradictory answers to the same question, the clinician’s answer overrides that of the parent. The clinician module was introduced to the clinical validation study beginning in 2017. Its results are therefore based on a smaller sample size than those of the other modules.\\nFeature selection. In order to create a brief clinician questionnaire appropriate for the primary care setting, \\nmultiple lists of candidate questions are each compiled and ordered using different strategies. The lists are then \\nintersected and prioritized, then the top features in the intersection set are shortlisted. The first list of candidate \\nquestions is prepared by considering those questions from the original medical instruments that had been excluded from the parental questionnaire because they were deemed too difficult for a parent to answer reliably. This list is ranked by feature importance values as measured and ranked by a dedicated offline machine learning training and cross validation run in the same manner as performed for initial parental module feature selection. The second list is prepared from the parental questionnaire questions by simulating the effects of parents over or underestimating answer severities on children with machine learning responses near a decision threshold. Children in the training data for whom the model response was between [0 and 0.1] above the ASD-vs-non ASD decision threshold had their question severities dropped one at a time by one severity value, while children who were between [0 and 0.1] \\nbelow the decision threshold had their question severities raised by one severity category. The questions in this list \\nare then ranked based upon the average size of the resulting shift in model responses. The procedure is repeated for children in the training data between [0.1 and 0.3] above or below the decision threshold. In each case the top 7 questions are selected (with significant overlap). This results to a total of 9 candidate questions for young chil-dren and 10 for older children. The third list is prepared by consulting domain experts for an assessment of the likelihood of each candidate question to benefit from a clinician’s input as a complement to the parent’s input. This method is conducted separately for each of the two age-silo groups, and results in an overall clinician questionnaire of 13 questions for children 18 through 47 month old, and 15 questions for children 4 to six years old.\\nModule combination. Due to limitations on available training data, it is not possible to train a single \\ncombined model that uses the input features from each of the parental, video, and clinician modules. Instead, responses from the modules are each considered to be a probability and combined mathematically\\n21 using the \\nequation:\\n=Σ ∗Σ−− −rI RI I () () (1) combTT 11 1\\nWhere rcomb is the result of the combination, I is a vector of 1s, R is a vector of responses for each module to be \\ncombined, and Σ  is the covariance matrix of the response residuals compared to the true diagnosis. The “training” \\nof the combination module consists of calculating the values of Σ to use in this equation, which is done using the \\nresponses of each module on data from the clinical study. For each child, the Σ  values in the rcomb equation were \\ncalculated with that child excluded. This process is similar to leave-one-out cross validation, and ensures that the results reported for our combination procedure do not suffer from overfitting.\\nSince Eq. (1) produces only a single model response, the determination of “inconclusive” outcomes proceeds \\nin a different manner than for the individual assessment modules. Both a lower and an upper threshold are \\napplied on the combined response. Children with a response less than both thresholds are considered to be \\nnon-ASD, children with a response in between the two thresholds are considered to be inconclusive, and children with a response greater than both thresholds are considered to have ASD. As in the single model cases, the two thresholds can be tuned independently to optimize the sensitivity, specificity, and model coverage.6 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/Results\\nEach of the individual Cognoa assessment modules, their combinations, as well as 3 baselines based on \\ncommonly-used autism screening instruments (CBCL, M-CHAT-R, and SRS) are evaluated on the data collected during a blinded clinical study. When the inconclusive determination feature is turned off and all samples are required to be assessed conclusively, the Cognoa assessment modules achieve ROC AUC up to 0.83 and sensi-tivity and specificity up to 80% and 75% respectively. Turning on the inconclusive determination feature with an \\nallowance of up to 30% inconclusive outcomes results in an accuracy improvement over the conclusive samples, \\nwith AUC up to 0.92 with sensitivity and specificity up to 90% and 83% respectively. This performance is shown to be a statistically significant improvement over each of the baselines used for comparison.\\nROC curves in Fig.\\xa0 3 show how the parent module performs individually, as well as in combination with the \\nvideo and clinician modules at a 30% inconclusive rate allowance. Figure\\xa0 4 shows a similar comparison with all \\nthe variants consistently restricted to children under four years of age. ROC curves corresponding to the assess-ment modules with the inconclusive allowance turned off can be found in the supplementary material.\\nStatistical model performance comparisons between assessment modules and baselines are shown in Table\\xa0 2. \\nFor each comparison, the subset of children for whom both screeners were administered are selected (n  in the \\ntable), and 10,000 bootstrapping experiments are run where n children are selected with replacement. The average \\nand [5%, 95%] confidence interval improvements in AUC and the specificity between the screeners are evaluated across all bootstrapping experiments. In the case of specificity the calculation of the improvement is performed using thresholds that are set to achieve 90% sensitivity. \\nTable\\xa0 2 shows that Cognoa modules show an improvement of at least 0.26 in AUC and at least 0.52 in spec-\\nificity compared with the CBCL and SRS-2 screeners at 95% confidence level. Due to the fact that M-CHAT-R screener is only evaluated on younger children the statistical uncertainty in the comparison is larger, however, it also shows an improvement of at least 0.08 in AUC and 0.11 in specificity at 95% confidence level. In these comparisons we allow Cognoa assessment modules to decide to hold aside up to 30% of the hardest cases as \\ninconclusive. The same comparisons when we force the classification on\\xa0all of the hardest cases can be found in \\nTable 3 of the supplementary material.\\nFigure 3. ROC curves on the clinical sample for the parent, video, and clinician modules, separately and in \\ncombination. Inconclusive determination is allowed for up to 30% of the cases. The established screening tools M-CHAT-R, SRS-2 and CBCL are compared as baselines. The ROC curve for the M-CHAT-R baseline instrument only includes children under four years of age since M-CHAT-R is not applicable for older children.\\nFigure 4. ROC curves on kids \\xa0 <\\xa04 years of age in the clinical sample for the parent, video, and clinician \\nmodules, separately and in combination. Inconclusive determination is allowed for up to 30% of the cases. The established screening tools M-CHAT-R, SRS-2 and CBCL are compared as baselines.7 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/time to completion comparisons.  A random sample of 529 Cognoa users was used in order to measure \\ntime to completion of each of the Cognoa autism assessment modules. The median time to completion of the \\nparent module was just under 4 minutes. The median time to completion of the clinician module was 1.2 minutes. The median time per video analysts to score a videos was 20 minutes. More details can be found in the supple-mentary material. The results indicate that the parent and clinician modules can be completed in as little time as \\nmost established autism screeners and in some cases much faster, while achieving significantly higher accuracy. \\nThe time required for a video analyst to score a video is more lengthy, however, the turnaround time is faster than for an ADOS administration\\n12 and can be performed by minimally trained analysts as opposed to certified \\nclinical practitioners.\\nDiscussion\\nWe presented a multi-modular assessment consisting of three machine learning modules for the identification of autism via mobile App as well as an evaluation of their performance and time-to-completion in a blinded clinical study. The assessment modules outperform conventional autism screeners, as shown in Table\\xa0 2 and Fig.\\xa0 3. The \\naccuracy of the combined assessment is similar to that of gold-standard instruments such as ADOS and ADI-R\\n22, \\nwithout requiring hours of time from certified clinical practitioners. This suggests the potential for the Cognoa assessment to be useful as an autism diagnostic. The high performance of these modules benefits from the use of the techniques described in this paper to identify and set aside up to 30% of the most challenging samples as inconclusive. The supplementary material of this paper shows that we outperform conventional autism screeners \\nwithout this technique as well.\\nImportant open questions remain. First, in all cases in this paper, the assessment modules were validated on \\nchildren who had been preselected as having high risk of autism. Children that are pre-selected in this way tend to \\nhave autism-like characteristics regardless of their true diagnosis, increasing the challenge of distinguishing true ASD cases. These modules are expected to perform better on a general population sample of children. Further work is needed to verify this hypothesis by conducting clinical studies on children from the general population. Second, the clinician module newly presented in this work appears promising, but so far it has only been applied in a secondary-care setting. Further testing in primary care clinics is needed to validate accuracy in that setting. In addition, two wider avenues of exploration are interesting as further steps. First, while these assessment mod-\\nules have been shown to be effective at identifying the presence or absence of autism, our goal is to extend them \\nto identify the severity of the condition (if present) as well. Second, the techniques presented in this paper could potentially be used to build algorithms for other child behavioral conditions than autism, as well as behavioral conditions affecting adults and seniors.\\nReceived: 12 March 2019; Accepted: 20 February 2020;\\nPublished: xx xx xxxxAge Group Baseline Screener Assessment Module ΔAUC, [0.05, 0.95] C.I.ΔSpecificity at 90% \\nsensitivity, [0.05, 0.95] C.I. n\\nAll ages CBCL Parent 0.17, [0.10, 0.23] 0.21, [0.13, 0.30] 370\\nAll ages SRS-2 Parent 0.20, [0.12, 0.28] 0.21, [0.12, 0.31] 307\\nAll ages CBCL Parent, Video 0.29, [0.22, 0.36] 0.41, [0.30, 0.52] 363\\nAll ages SRS-2 Parent, Video 0.32, [0.24, 0.40] 0.41, [0.30, 0.52] 302\\nAll ages CBCL Parent, Video, and Clinician 0.35, [0.26, 0.43] 0.69, [0.58, 0.81] 200\\nAll ages SRS-2 Parent, Video, and Clinician 0.42, [0.33, 0.50] 0.65, [0.52, 0.78] 175\\n1.5 to 3 y.o. M-CHAT-R Parent −0.01, [−0.10, 0.07] 0.09, [−0.02, 0.21] 209\\n1.5 to 3 y.o. CBCL Parent 0.12, [0.03, 0.22] 0.20, [0.07, 0.34] 214\\n1.5 to 3 y.o. SRS-2 Parent 0.15, [0.03, 0.27] 0.24, [0.12, 0.38] 161\\n1.5 to 3 y.o. M-CHAT-R Parent, Video 0.14, [0.06, 0.22] 0.20, [0.07, 0.34] 204\\n1.5 to 3 y.o. CBCL Parent, Video 0.28, [0.18, 0.38] 0.33, [0.18, 0.49] 209\\n1.5 to 3 y.o. SRS-2 Parent, Video 0.31, [0.20, 0.42] 0.37, [0.21, 0.55] 157\\n1.5 to 3 y.o. M-CHAT-R Parent, Video, and Clinician 0.18, [0.08, 0.29] 0.30, [0.11, 0.50] 107\\n1.5 to 3 y.o. CBCL Parent, Video, and Clinician 0.34, [0.22, 0.45] 0.46, [0.25, 0.67] 111\\n1.5 to 3 y.o. SRS-2 Parent, Video, and Clinician 0.40, [0.27, 0.53] 0.50, [0.28, 0.73] 91\\n1.5 to 3 y.o. CBCL Parent 0.21, [0.11, 0.30] 0.23, [0.11, 0.35] 156\\n1.5 to 3 y.o. SRS-2 Parent 0.25, [0.15, 0.36] 0.18, [0.06, 0.31] 146\\n4 to 6 y.o. CBCL Parent, Video 0.30, [0.20, 0.39] 0.49, [0.34, 0.64] 154\\n4 to 6 y.o. SRS-2 Parent, Video 0.33, [0.22, 0.44] 0.44, [0.29, 0.59] 145\\n4 to 6 y.o. CBCL Parent, Video, and Clinician 0.35, [0.23, 0.47] 0.93, [0.83, 1.00] 89\\n4 to 6 y.o. SRS-2 Parent, Video, and Clinician 0.43, [0.30, 0.55] 0.79, [0.66, 0.91] 84\\nTable 2. Statistical tests of performance improvements between models in this paper and standard baseline \\nscreening models. ΔAUC tells us the increase in AUC found in the screeners of this paper across bootstrapping \\nexperiments. ΔSpecificity tells us the increase in the specificity in the bootstrapping experiments at a threshold \\ndesigned to achieve 90% sensitivity. Each Δ calculation shows the average value of the improvement along with the [0.05,\\xa00.95] confidence interval.\\n8 Scientific  RepoRtS  |         (2020) 10:5014  | https://doi.org/10.1038/s41598-020-61213-w\\nwww.nature.com/scientificreports www.nature.com/scientificreports/References\\n 1. Baio, J. et al . Prevalence of autism spectrum disorder among children aged 8 years— autism and developmental disabilities \\nmonitoring network, 11 sites, united states, 2014. MMWR Surveill Summ  67(No. SS-6), 1–23, https://doi.org/10.15585/mmwr.\\nss6706a1 (2018).\\n 2.  Association., A. P . & Association., A. P . Diagnostic and statistical manual of mental disorders : DSM-5 (American Psychiatric \\nAssociation Arlington, V A, 2013), 5th ed. edn.\\n 3.  Patten, E., Ausderau, K. K., Watson, L. R. & Baranek, G. T. Sensory response patterns in nonverbal children with asd. Autism Res. Treat .2013, https://doi.org/10.1155/2013/436286 (2013).\\n 4. Dawson, G. & Bernier, R. A quarter century of progress on the early detection and treatment of autism spectrum disorder. Dev. \\nPsychopathol.  25, 1455–1472, https://doi.org/10.1017/S0954579413000710 (2013).\\n 5.  Dawson, G. et al . Randomized, controlled trial of an intervention for toddlers with autism: The early start denver model. Pediatrics  \\n125, e17–e23 https://doi.org/10.1542/peds.2009-0958, http://pediatrics.aappublications.org/content/125/1/e17.full.pdf (2010).\\n 6.  Gordon-Lipkin, E., Foster, J. & Peacock, G. Whittling down the wait time exploring models to minimize the delay from initial \\nconcern to diagnosis and treatment of autism spectrum disorder. Pediatr. clinics North Am. 63, 851–859 https://doi.org/10.1016/j.\\npcl.2016.06.007 (2016). Exported from https://app.dimensions.ai on 2018/10/19.\\n 7. Bernier, R., Mao, A. & Y en, J. Diagnosing autism spectrum disorders in primary care. Practitioner  255(1745), 27–30 (2011).\\n 8.  Achenbach, T. & Rescorla, L. Manual for the ASEBA preschool forms & profiles. Univ. Vermont, Res. Cent. for Child. Youth & Fam. (2000).\\n 9.  Norris, M. & Lecavalier, L. Screening accuracy of level 2 autism spectrum disorder rating scales: A review of selected instruments. Autism , 14, 263–284, https://doi.org/10.1177/1362361309348071 (2010). PMID: 20591956,\\n 10.  Charman, T. & Gotham, K. Measurement issues: Screening and diagnostic instruments for autism spectrum disorders - lessons \\nfrom research and practise. Child Adolesc. Mental Heal.18, 52–63, https://doi.org/10.1111/j.1475-3588.2012.00664.x.\\n 11. Lord, C., Rutter, M. & Le Couteur, A. Autism diagnostic interview-revised: a revised version of a diagnostic interview for caregivers \\nof individuals with possible pervasive developmental disorders. J. Autism Dev. Disord. 24, 659–685 (1994).\\n 12. Lord, C. et al. Autism diagnostic observation schedule: a standardized observation of communicative and social behavior. J Autism \\nDev Disord  19, 185–212 (1989).\\n 13.  Abbas, H., Garberson, F., Glover, E. & Wall, D. P . Machine learning approach for early detection of autism by combining \\nquestionnaire and home video screening. J. Am. Med. Informatics Assoc. ocy039, 10.1093_jamia_ocy039/1/ocy039 (2018).\\n 14.  Cognoa, Inc. 2390 El Camino Real St 220, Palo Alto, CA 94306 https://www.cognoa.com/.\\n 15.  Wall, D. P ., Dally, R. L., Luyster, R., Jung, J.-Y . & DeLuca, T. F. Use of artificial intelligence to shorten the behavioral diagnosis of autism. PLoS One, https://doi.org/10.1371/journal.pone.0043855. (2012).\\n 16. Lord, C. et al. A multisite study of the clinical diagnosis of different autism spectrum disorders. Arch. Gen. Psychiatry 69, 306–313 \\n(2012).\\n 17.  Kanne, S., Arnstein Carpenter, L. & Warren, Z. Screening in toddlers and preschoolers at risk for autism spectrum disorder: \\nEvaluating a novel mobile-health screening tool. Autism Res . (2017).\\n 18. Moody, E. J. et al . Screening for autism with the srs and scq: Variations across demographic, developmental and behavioral factors \\nin preschool children. J. Autism Dev. Disord. 47, 3550–3561 (2017).\\n 19. Aldridge, F. J., Gibbs, V . M., Schmidhofer, K. & Williams, M. Investigating the Clinical Usefulness of the Social Responsiveness Scale (SRS) in a Tertiary Level, Autism Spectrum Disorder Specific Assessment Clinic. Journal of Autism and Developmental Disorders  \\n42(8), 294–300 (2012).\\n 20. Schanding, G. T., Nowell, K. P . & Goin-Kochel, R. P . Utility of the Social Communication Questionnaire-Current and Social Responsiveness Scale as Teacher-Report Screening Tools for Autism Spectrum Disorders. Journal of Autism and Developmental \\nDisorders  42(8), 1705–1716 (2012).\\n 21. Jacobs, R. A. Methods for combining experts’ probability assessments. Neural Computation  7, 867–888, https://doi.org/10.1162/\\nneco.1995.7.5.867 (1995).\\n 22. Falkmer, T., Anderson, K., Falkmer, M. & Horlin, C. Diagnostic procedures in autism spectrum disorders: a systematic literature \\nreview. Eur. Child & Adolesc. Psychiatry 22, 329–340, https://doi.org/10.1007/s00787-013-0375-0 (2013).\\nAuthor contributions\\nHalim Abbas, Ford Garberson, and Stuart Liu-Mayo were each involved in all aspects of model development, \\noptimization, training, and validation of the models for each of the modules, as well as the writing of this paper. Eric Glover and Dennis Wall provided advice on the development of the algorithms. Halim Abbas, Ford Garberson, Stuart Liu-Mayo and Dennis Wall co-wrote the manuscript.\\ncompeting interests\\nAll authors are affiliated with Cognoa Inc. in an employment and/or advisory capacity.\\nAdditional information\\nSupplementary information is available for this paper at https://doi.org/10.1038/s41598-020-61213-w .\\nCorrespondence and requests for materials should be addressed to E.G.\\nReprints and permissions information is available at www.nature.com/reprints.Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \\ninstitutional affiliations.\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \\nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-ative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \\nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/. © The Author(s) 2020', metadata={'source': 'papers/Abbas_2020_clean.txt'})],\n",
       " [Document(page_content='RESEA RCH ARTICL E\\nEarly screening ofautism spectrum disorder\\nusing cryfeatures\\nAida Khozaei ID\\n1,Hadi Moradi ID\\n1,2*,Reshad Hosseini ID\\n1,Hamidreza Pouretemad3,\\nBahareh Eskandari3\\n1School ofElectrical andComput erEnginee ring, Univers ityofTehran, Tehran, Iran, 2Intelligent System s\\nResearc hInstitute, SKKU, Suwon, South Korea, 3Department ofPsychology, Shahid Behesh tiUniversity,\\nTehran, Iran\\n*moradih @ut.ac.ir\\nAbstract\\nTheincrease inthenumber ofchildren withautism andtheimportance ofearly autism inter-\\nvention hasprompted researchers toperform automatic andearly autism screening. Conse-\\nquently, inthepresent paper, acry-based screening approach forchildren withAutism\\nSpectrum Disorder (ASD) isintroduced which would provide both early andautomatic\\nscreening. During thestudy, werealized thatASD specific features arenotnecessarily\\nobservable inallchildren withASD andinallinstances collected from each child. Therefore,\\nweproposed anewclassification approach tobeabletodetermine such features andtheir\\ncorresponding instances. Totesttheproposed approach asetofdata relating tochildren\\nbetween 18to53months which hadbeen recorded using high-quality voice recording\\ndevices andtypical smartphones atvarious locations such ashomes anddaycares was\\nstudied. Then, after preprocessing, theapproach wasused totrain aclassifier, using data\\nfor10boys withASD and10Typically Developed (TD) boys. Thetrained classifier was\\ntested onthedata of14boys and7girlswithASD and14TDboys and7TDgirls. Thesensi-\\ntivity, specificity, andprecision oftheproposed approach forboys were 85.71%, 100%, and\\n92.85%, respectively. These measures were 71.42%, 100%, and85.71% forgirls, respec-\\ntively. Itwasshown thattheproposed approach outperforms thecommon classification\\nmethods. Furthermore, itdemonstrated better results than thestudies which used voice fea-\\ntures forscreening ASD. Topilotthepracticality oftheproposed approach forearly autism\\nscreening, thetrained classifier wastested on57participants between 10to18months.\\nThese 57participants consisted of28boys and29girlsandtheresults were veryencourag-\\ningfortheuseoftheapproach inearly ASD screening.\\nIntroduction\\nChildren with Autism Spectrum Disorder (ASD) aredefined bytheir abnormal orimpaired\\ndevelopment insocial interaction and communication, aswell asrestricted and repetitive\\nbehaviors, interests, oractivities .The rapid growth ofASD inthepast 20years hasinspired\\nmany research efforts toward thediagnosis and rehabilitation ofASD [2–5]. Inthefield of\\nPLOS ONE\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 1/21a1111111111\\na1111111111\\na1111111111\\na1111111111\\na1111111111\\nOPEN ACCESS\\nCitation: Khozaei A,Moradi H,Hosseini R,\\nPouretem adH,Eskandari B(2020) Early screening\\nofautism spectrum disorder using cryfeatures.\\nPLoS ONE15(12): e0241690. https://do i.org/\\n10.1371/ journal.pone. 0241690\\nEditor: Zhishun Wang, Columbia University,\\nUNITED STATES\\nReceived: November 26,2019\\nAccepted: October 19,2020\\nPublished: December 10,2020\\nPeer Review History: PLOS recognize sthe\\nbenefits oftranspar ency inthepeer review\\nprocess; therefore, weenable thepublication of\\nallofthecontent ofpeer review andauthor\\nresponse salongside final, published articles. The\\neditorial history ofthisarticle isavailable here:\\nhttps://doi.o rg/10.1371/jo urnal.pone.0 241690\\nCopyright: ©2020 Khozaei etal.Thisisanopen\\naccess article distributed under theterms ofthe\\nCreative Commons Attribution License, which\\npermits unrestricte duse,distribu tion,and\\nreproduction inanymedium, provided theoriginal\\nauthor andsource arecredited.\\nData Availabilit yStatement: Theoriginal and\\ncleaned voices andtheirextracted features (the\\ndataset)inthisresearch andtheimplementation\\ncodes oftheproposed method aredeposited inthe\\nfollowing repositories: CodeOcean 10.24433/ CO.diagnosis, there areseveral well-established manual methods todiagnose children over 18\\nmonths .However, thepractical average ageofdiagnosis isover 3years duetothelack of\\nknowledge about ASD and thelack ofexpertise fordiagnosing autism [7,8].Itisoftheutmost\\nimportance tohave early diagnosis/screening inorder toprovide early intervention which is\\nmore effective atthefirst fewyears oflifethan later on[7,9–11]. Itisshown that early inter-\\nvention improves thedevelopmental performance inchildren with ASD . Ithasalso been\\nreported that early interventions would becost saving forfamilies and thetreatment service\\nsystems [13, 14]. Consequently, there aretwomain questions: 1)canautism bescreened earlier\\nthan 18months toreduce thetypical diagnosis orintervention ageand 2)isitpossible to\\nemploy intelligent methods forthescreening ofautism toeliminate thewidespread need for\\nexperts? Itshould bementioned that ourgoal wastoanswer these questions with respect to\\nscreening allchildren who may nothave clear symptoms. The screened children should go\\nthrough adiagnosis procedure toacquire confirmation and/or becautiously worked with.\\nFortunately, there arestudies intheliterature showing that theageofdiagnosis canbe\\nlower than 18months. Forexample, Thabtah and Peebles  reviewed several questionnaire-\\nbased approaches that may beable toscreen ASD above 6months ofage. However, those\\napproaches, likeAutism Diagnostic Interview-Revised (ADI-R)  and Autism Diagnostic\\nObservation Schedule (ADOS)  which have been clinically proven tobeeffective and ade-\\nquate, aretime-consuming instruments  and need trained practitioners tousethem. To\\nreduce thedependency onthehuman expertise needed inusing such questionnaires ,sev-\\neralstudies proposed machine learning methods toclassify children with ASD [18, 19]using\\nquestionnaires. Their goal wastoautomate theprocess and/or find anoptimum subset of\\nquestions orfeatures. Forinstance, Abbas etal. proposed amulti-modular assessment sys-\\ntem combined ofthree modules, aparent questionnaire, aclinician questionnaire, and avideo\\nassessment module. Although theauthors used machine learning toautomate and improve\\nclassification process, theneed forhuman involvement stillexists inorder toanswer questions\\norassess videos.\\nOntheother hand, Emerson et.alshowed that fMRI  canbeused topredict thediagno-\\nsisofautism attheageof2inhigh-risk 6-month-old infants. Denisova and Zhao  used\\nmovement data from rs-fMRI from 1–2month-old infants topredict future atypical develop-\\nmental trajectories asbiological features. Furthermore, Bosl, Tager-Flusberg, and Nelson \\nsuggested that useful biomarkers canbeextracted from EEG signals forearly detection of\\nautism. Blood-based markers [24, 25]and prenatal immune markers  were also proposed\\ntodiagnose ASD that canbeused right after birth. Although these approaches suggest new\\ndirections towards early ASD diagnosis/screening, they arecostly, require expertness and dedi-\\ncated equipment, which would limit their usage. Furthermore, these methods arestillinthe\\nearly stages ofresearch and require further approval. Finally, approaches which involve meth-\\nodssuch asfMRI orEEG, aredifficult touseonchildren, especially onchildren with autism\\nwho may have trouble following instructions appropriately , have atypical behaviors ,\\norhave excessive head movements [29, 30].\\nThere arestudies that used vocalization-based analysis toscreen children with autism. For\\ninstance, Brisson etal. showed differences invoice features between children with ASD\\nand Typically Developing (TD) children. Several studies, like, used speech-related features\\nforthescreening ofchildren older than 2.Toreach thegoal ofearly ASD screening, vocaliza-\\ntions ofinfants under 2years ofagehave been investigated [33–35]. Santos etal. used\\nvocalizations, such asbabbling, toscreen ASD children attheageof18months. They collected\\ndata from 23and 20ASD and TDchildren, respectively. They reported high accuracy of\\naround 97% which canbeduetothefactthat they used k-fold cross-validation without consid-\\nering subject-wise hold outinorder tohave unseen subjects inthetestfold . Oller etal.\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 2/210622770.v1 Harvard Datave rse(Contain sonlyarar\\nfileofsounds): 10.7910/DVN/L STBQW\\nFunding: HMreceived asmall fund forcollecting\\ndataandfordiagnosing thesubjects. Grant\\nnumber 123Cognitive Sciences andTechnology\\nCouncil ofIrancogc.ir Thefunders hadnorolein\\nstudy design, datacollection andanalysis, decision\\ntopublish, orpreparation ofthemanuscript.\\nCompeting interests :Theauthors have declared\\nthatnocompeting interests exist. proposed another vocalization-based classification method inwhich they included age\\nand excluded crying. They applied themethod on106TDchildren and 77children with ASD\\nbetween 16to48months and reached 86% accuracy. Pokorny etal. extracted eGeMAPS\\nparameter set, which includes 88acoustic parameters, in10month oldchildren. This set\\nconsists ofstatistics calculated for25frequency-related, energy-related, and spectral low-level\\ndescriptors. They reached a75% accuracy onapopulation of10TDchildren and 10children\\nwith ASD.\\nEsposito, Hiroi, and Scattoni  showed that crying isapromising biomarker forthe\\nscreening ofASD children. Sheinkopf etal. and Orlandi etal. have shown that there\\naredifferences inthecryofchildren with ASD compared toTDchildren. Tothebest ofour\\nknowledge, ourown group’s preliminary study  wastheonly research that hasused cry\\nsounds forthescreening ofchildren with ASD. Weused adataset of5children with ASD and\\n4TDchildren older than twoyears. The accuracy oftheproposed method is96.17% using k-\\nfold cross validation without considering subject-wise hold out, which isashortcoming ofthis\\nstudy. Inother words, ithasbeen overfitted totheavailable data and may failtocorrectly clas-\\nsifynew samples. So,athorough examination using anunseen testsetoncryfeatures isneces-\\nsary toevaluate theresults. Itshould benoted that thedata from ourprevious study  could\\nnotbeused inthestudy presented inthispaper duetothedifferences indata collection\\nprocedures.\\nInalltheabove studies, itwasassumed that thespecific sound features, distinguishing chil-\\ndren with ASD from TDchildren, arecommon among alltheASD cases. However, thismay\\nnotbethecase forallthefeatures. Forinstance, tiptoe walking, which isoneoftherepetitive\\nbehaviors ofchildren with ASD, appears inapproximately 25% ofthese children . Conse-\\nquently, inthecurrent study, wepropose anew cry-based approach forscreening children\\nwith ASD. Our screening approach makes useoftheassumption that alldiscriminative charac-\\nteristics ofautism may notappear inallASD children. This assumption isincontrast with the\\nassumption putforward intheordinary instance-based machine learning methods, which\\nassumes that allinstances ofaclass include alldiscriminative features needed forclassification.\\nInourproposed method, atfirst, discriminable instances ofcries, which exist insubsets ofchil-\\ndren with ASD, arefound. Then ituses these instances toselect features todistinguish between\\nthese ASD instances from TDinstances. Itshould bementioned that thefinal selected features,\\ninthisstudy, arecommon among oursetofchildren with ASD between 18to53months of\\nage. These selected features support theexperiential knowledge ofourexperts stating that the\\nvariations inthecries ofchildren with ASD aremore than TDchildren. This approach isdif-\\nferent from theother approaches that either used adataset ofchildren with aspecific age[33,\\n35]orused ageinformation forclassification . The proposed approach hasbeen imple-\\nmented and tested on62participants. The results show theeffectiveness oftheapproach with\\nrespect toaccuracy, sensitivity, and specificity.\\nMethod\\nSince thisstudy wasperformed onhuman subjects, first, itwasapproved bytheethics commit-\\nteeatShahid Beheshti University ofMedical Sciences and Health Services. Alltheparents of\\ntheparticipants were informed about thestudy and signed anagreement form before being\\nincluded inthestudy.\\nParticipants\\nThere were 62participants aged between 18and 53months, who were divided into two\\ngroups, i.e.31ASD and 31TDwith 24boys and 7girls ineach group. Since weexpected to\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 3/21have different vocalization characteristics forboys and girls, thetraining setwasassembled of\\nonly boys, including 10TD, and 10ASD. Inother words, wewanted toeliminate thegender\\neffects onthefeature extraction and model training. Unfortunately, duetothelower number\\nofgirls with ASD intherealworld, notenough data forgirls with ASD could becollected.\\nNonetheless, themodel wasalso tested onthegirls toseehow itwould generalize even on\\nthem.\\nThe inclusion criteria oftheASD participants were: a)being very recently diagnosed with\\nASD based onDSM-5 with noother neurodevelopmental, mental, and intellectual disorder, b)\\nhaving noother known medical orgenetic conditions, orenvironmental factors, and c)not\\nhaving received anytreatment ormedication, orhaving received treatment inlessthan a\\nmonth. There were only twogirls who didnotfallinto these criteria since they hadbeen diag-\\nnosed more than ayear before. The participants’ average language development atthetime of\\nparticipation, which wasassessed based on[43–46], wasequal tochildren between 6to12\\nmonths old. The autism diagnosis procedure started with theGilliam Autism Rating Scale-Sec-\\nond Edition (GARS-2) questionnaire  which wasanswered bytheparents. Then the\\nparents were interviewed, based onDSM-5, while theparticipants were evaluated and\\nobserved bytwochild clinical psychologists with Ph.D. degrees. Inaddition, thediagnosis of\\nASD wasseparately confirmed byatleast onechild psychiatrist inadifferent setting. Itshould\\nbenoted that ADOS, which isavery common diagnostic tool isnotadministered widely in\\nIran since there isnoofficial translation ofADOS inFarsi. TDchildren were selected from\\nthose inanagerange similar totheASD participants from volunteer families from their\\nhomes and health centers. They hadnoevidence orofficial diagnosis ofanyneurological or\\npsychological disorder atthetime ofrecording their voices. The children with ASD were older\\nthan 20months with themean, standard deviation, and range of35.6, 8.8,and 33months\\nrespectively. The TDchildren were younger than 51months with themean, standard devia-\\ntion, and range ofabout 30.8, 10.3, and 33months respectively. Itshould bementioned that\\nthediagnosis ofthechildren under 3years wasmainly based onexperts’ evaluation, notthe\\nGARS score. Furthermore, allTDparticipants under 3years ofagehadafollow upstudy when\\nthey passed theageof3,tomake sure theinitial TDassignment wascorrect orstillvalid. Todo\\nso,weused asetofexpert-selected questions based on toassess them through interviews\\nwith parents.\\nTables 1and 2show thedetails oftheparticipants onthetraining and testsets, respectively.\\nIneach table, thenumber ofvoice instances from each participant and thetotal duration ofall\\nitsinstances inseconds areshown incolumns 3and 4,respectively. The recording device cate-\\ngory, i.e.ahigh-quality recorder (HQR) and typical cellphones (CP), isgiven inthedevice cat-\\negory column. The next twocolumns include GARS-2 scores and thelanguage developmental\\nmilestone oftheparticipants with ASD atthetime oftherecording. Insixcases, there were no\\nGARS score available atthetime ofstudy, demonstrated byND(No Data). The column\\nlabeled as‘Place’ shows thelocation oftherecording which canbeinhomes (H), autism cen-\\nters(C1, C2,and C3), and health centers (C4, C5,and C6). There wasatotal number of359\\nsamples forallchildren. 53.44% ofthesamples were from ASD participants and 46.56% were\\nfrom TDparticipants.\\nTwo groups of10TDand 10ASD children were selected fortraining theclassifiers such\\nthat twogroups were asbalanced aspossible with respect toageand therecording device.\\nThus, each child intheTDgroup hadacorresponding child intheASD group around the\\nsame age. Asaresult ofthisdata balancing, weobtained training participants with anage\\nbetween 20and 51months. The mean ages inthetraining setwere 32.7 and 35.2 months for\\nASD and TDparticipants, respectively. The standard deviations are9and 9.9months with the\\nrange of25and 30months forASD and TDparticipants, respectively.\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 4/21Although thisapproach wastrained andtested onchildren older than 18months, wetested the\\nproposed approach on57participants between 10to18months toinvestigate how itworks onchil-\\ndren under 18months. These 57participants consisted of28boys and29girls with themean age\\nof15.2 forboth andstandard deviations of2.8and2.9respectively. Allthese participants were eval-\\nuated atalater date attheageof3orolder, bythesame follow-up procedure, using ourexpert-\\nselected questionnaire. Atthetime ofinitial voice collection, 55ofthese participants hadnoevident\\nordiagnosed disorder. Two ofthem were referred toourexperts duetothepositive results of\\nscreening using ourmethod. The diagnosis orconcerns about thetwomentioned participants, as\\nwell astheparticipants with anyevidence ofhaving abnormality inthedevelopmental milestones\\nduring thefollow-up procedure aresummarized inTable 3.The summary ofdisorders given inthe\\nlastcolumn ofTable 3isbased ontheparental interviews andourexperts’ evaluation. Unfortu-\\nnately, Child5, Child6, andChild7’s parents didnotcooperate inobtaining expert evaluation.\\nData collection and preprocessing\\nAsmentioned earlier, thedata wasrecorded using high-quality devices and typical smart-\\nphones. The high-quality devices were aUX560 Sony voice recorder and aSony UX512F voice\\nrecorder. Tousetypical smartphones, avoice-recording and archiving application wasdevel-\\noped and used onvarious types ofsmartphones. Allvoices, through theapplication orthe\\nhigh-quality recorders, were recorded inwav format, 16bits, and with thesampling rate of\\n44.1 kHz. The reason forusing various devices wastoavoid biasing oftheapproach toaspe-\\ncific device. Similarly, theplace ofrecording wasnotrestricted tooneplace inorder tomake\\ntheresults applicable toallplaces.\\nThe parents and trained voice collectors were asked torecord thevoices inaquiet environ-\\nment. Furthermore, they were asked tokeep therecorders orsmartphones about 25cmfrom\\ntheparticipants’ mouth. Despite theproposed tworecommendations ,there were recordedTable 1.The training setdata ofparticipants .\\nID Age (month) #ofinstances Total duration(s ec) Device GARS score Language milesto ne(month) Place Reason forcryingASDASD1 20 9 7.8 CP 104 0–6 C1 Annoyed/U ncomfort able\\nASD2 24 3 1.5 HQR 83 0–6 C2 Unwillin g\\nASD3 26 5 2.1 HQR 120 0–6 C1 Annoyed/U ncomfort able\\nASD4 28 13 9.1 HQR 121 0–6 C2 Annoyed/U ncomfort able\\nASD5 29 14 26 HQR 89 6–12 C2 Unwillin g/Complain ing\\nASD6 31 4 2.4 HQR 87 0–6 C2 Unwillin g/Complain ing\\nASD7 36 11 11 HQR 87 6–12 C2 Unwillin g/Complain ing\\nASD8 43 2 0.7 CP ND ND C2 Unwillin g\\nASD9 45 3 2.6 CP 72 6–12 C2 Complaini ng\\nASD10 45 4 3.4 CP ND ND H SleepyTDTD1 21 11 14 HQR NA NA H Complaini ng\\nTD2 24 12 12 HQR NA NA C4 Scared/U nwilling\\nTD3 26 2 2.3 HQR NA NA C5 Unwillin g\\nTD4 28 6 13 CP NA NA C5 Scared/U nwilling\\nTD5 36 3 2.6 CP NA NA H Unwillin g/Complain ing\\nTD6 38 3 1.5 HQR NA NA C6 Complaini ng\\nTD7 41 3 2.4 HQR NA NA H Unwillin g\\nTD8 43 3 2.2 CP NA NA H Sleepy\\nTD9 44 2 1.2 CP NA NA H Complaini ng\\nTD10 51 2 1.7 CP NA NA H Complaini ng\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t001\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 5/21voices where therecommendations were notfollowed and didnothave therequired quality.\\nConsequently, those recordings were eliminated from thestudy. Also, allthecrysounds which\\nwere duetopain, hadbeen removed from thestudy since they were similar between theTD\\nand ASD groups.Table 2.The test setinformation .\\nID Age (month) #ofinstances Total duration(S) Device GARS score Language milestone (months) Place Reason forcryingASD\\nBoysASD11 28 12 7.2 HQR 102 0–6 C2 Unwilling/ Uncomfort able\\nASD12 30 18 17.1 HQR ND ND C3 Separation from mother\\nASD13 30 3 2.9 CP ND ND H Unwilling/Sl eepy\\nASD14 31 5 2.3 HQR 73 0–6 C2/HSeparation from mother/Hungr iness\\nASD15 33 3 2.5 HQR 91 0–6 C2 Unwillin g\\nASD16 33 2 2.5 HQR 104 0–6 C1 Annoyed/U ncomfort able\\nASD17 34 1 0.6 HQR 91 0–6 C2 Unwilling/ Complaining\\nASD18 35 2 1.7 HQR 81 ND C1 Annoyed/U ncomfort able\\nASD19 37 1 0.6 HQR 94 12–18 C2 Unwilling/ Complaining\\nASD20 40 19 14 HQR 91 0–6 C1 Annoyed\\nASD21 45 1 0.3 HQR 81 6–12 C2 Unwilling/ Complaining\\nASD22 48 2 1.6 HQR 100 6–12 C2 Annoyed/C omplaining\\nASD23 52 6 3.1 HQR 113 12–18 C2 Unwilling/ Complaining\\nASD24 53 7 5.2 HQR 78 6–12 C1 Annoyed/U ncomfort ableGirlsASD25 25 12 14 HQR 85 0–6 C2 Unwilling/ Complaining\\nASD26 26 5 2 CP 102 0–6 C1 Scared\\nASD27 31 3 1.7 HQR 94 0–6 C2 Unwilling/ Complaining\\nASD28 32 2 1.3 HQR 100 0–6 C2 Unwilling/ Complaining\\nASD29 41 8 3 HQR 102 0–6 C2 Unwilling/ Complaining\\nASD30 45 2 1.2 CP ND ND H Thirsty\\nASD31 49 7 12 CP ND ND H Unwilling/ ComplainingTD\\nBoysTD11 18 4 2 HQR NA NA C4 Scared\\nTD12 18 7 5.1 HQR NA NA C4 Scared/Un willing\\nTD13 19 7 4.2 HQR NA NA C5 Unwillin g\\nTD14 20 9 8 HQR NA NA C5 Unwilling/ Complaining\\nTD15 21 4 1.2 HQR NA NA H Complaini ng\\nTD16 24 3 2.7 HQR NA NA C5 Scared /Unwillin g\\nTD17 24 2 1.5 HQR NA NA C5 Scared/Un willing\\nTD18 24 6 5.1 HQR NA NA C4 Unwilling/ Complaining\\nTD19 24 4 2.4 HQR NA NA C5 Unwilling/ Complaining\\nTD20 24 5 4.2 HQR NA NA C5 Unwilling/ Complaining\\nTD21 29 11 10 HQR NA NA H Unwilling/ Complaining\\nTD22 30 4 2 HQR NA NA C5 Scared/Un willing\\nTD23 30 4 2 CP NA NA H Unwillin g\\nTD24 43 12 11 HQR NA NA H Complaini ngGirlsTD25 24 5 6 HQR NA NA C4 Unwilling/ Complaining\\nTD26 25 2 4.4 HQR NA NA C5 Scared\\nTD27 29 5 5 HQR NA NA C5 Scared\\nTD28 33 2 2.1 CP NA NA H Complaini ng\\nTD29 45 16 11 HQR NA NA H Unwilling/ Complaining\\nTD30 50 6 7 HQR NA NA H Complaini ng\\nTD31 51 2 0.7 CP NA NA H Unwillin g\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t002\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 6/21After data collection, there wasapreprocessing phase inwhich only pure crying parts ofthe\\nrecordings, with noother types ofvocalization, were selected. Toexplain more, theparts ofcry\\nsounds which were accompanied byscreaming, saying words/other vocalizations, orthat\\noccurred with closed/non-empty mouth were eliminated. Allsegmentations and eliminations\\nwere done manually using Sound Forge Pro11.0. From theselected cries, thebeginning and\\ntheend, which contained voice rises and fades, were removed inorder tojustkeep thesteady\\nparts ofthecries; thisprevents having toomuch variation inthevoice which canlead to\\nunsuitable statistics. Also, theuvular/guttural parts ofthecries were removed. The reason for\\nthiswasthat webelieve these parts distort thefeature values ofthesteady parts ofavoice. Each\\nremaining continuous segment ofthecries wasconsidered and used asasample (instance) in\\nthisstudy. Finally, since thebasic voice features were extracted from 20milliseconds frames\\n, togenerate statistical features ofthebasic features, theminimum length ofthecryseg-\\nments were setto15frames, i.e.300milliseconds. Thus, anycrysamples below 300millisec-\\nonds were eliminated from thestudy. Inthisstudy, thefinal prepared samples were between\\n320milliseconds to3seconds.\\nFeature extraction\\nPrevious studies working onvoice features fordiscriminating ASD children used different setsof\\nfeatures. These methods share several common features likeF0,i.e.thefundamental frequency\\nofavoice, andMel-Frequency Cepstral Coefficients (MFCC), i.e.coefficients which represent\\ntheshort-term power spectrum ofasound . F0hasbeen oneofthemost common features\\nused [31,32,39].However, since ageisanimportant factor affecting F0, thisfeature isuseful\\nwhen participants have asimilar age. Ontheother hand, MFCC coefficients andseveral related\\nstatistical values have been reported tobeuseful features inseveral studies [35,41,53].Consider-\\ningtheuseful features reported inprevious studies andthespecifications ofthecurrent study,\\nseveral features were selected tobeused inthiswork that areexplained inthefollowing.\\nInthisstudy, each instance wasdivided into 20milliseconds frames, toextract basic voice\\nfeatures. Weused several features proposed byMotlagh, Moradi, and Pouretemad  and by\\nBelalca ´zar-Bolaños etal.. The features used byMotlagh, Moradi, and Pouretemad \\ninclude certain statistics likemean and covariance oftheframe-wise basic features, such as\\nMFCC coefficients, over avoice segment. They also used themean and variance offrame-wise\\ntemporal derivative [55, 56]ofthebasic features. The frame-wise temporal derivative meansTable 3.The participants with anabnormali tyinthefollow-up.\\nID Gender Age (inmonths) Disorder\\natrecording time atfollowing -up time\\nChild1 M 11 11 Developm ental delaya,signs ofgenetic diseases\\nChild2 M 17 17 UNDDb\\nChild3 M 12 40 ASDb\\nChild4 M 12 36 Sensory processi ngdisorderc,several ADHD symptomsb\\nChild5 M 18 40 Languag edelay\\nChild6 M 15 46 Development aldelay symptoms\\nChild7 M 12 43 Development aldelay symptoms\\nUNDD, Unspecif iedNeurodev elopmental Disorder .\\naClinical observa tion byourexpert based on.\\nbClinical observation byourexpert based on\\ncClinical observa tion byourexpert based on.\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t003\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 7/21thedifference between twoconsecutive frames, which inasense istherate ofchange ofafea-\\nture value inoneframe step. Wemodified thespectral flatness features byincluding therange\\nof125–250 Hzbeside the250–500 Hzrange. This range wasadded tocover awider frequency\\nrange than thenormal children frequency range, which showed tobenecessary intheprocess\\noffeature extraction and selection. Each range isdivided into 4octaves and thespectral flatness\\niscomputed forthose octaves.\\nWeremoved alluninformative and noisy features ofthesetwhich areexplained inthefol-\\nlowing. The mean offrame-wise temporal derivative ofthebasic features isremoved because it\\nisnotameaningful feature and isequal totaking thedifference between thevalue ofthelast\\nand thefirst frames. There aremeans ofthefeatures related totheenergy, such astheaudio\\npower, total loudness, SONE, and thefirst coefficient ofMFCC, that were removed tomake\\ntheclassifier independent oftheloudness/power inchildren’s voices. Zero crossing rate (ZCR)\\nwasomitted too, duetoitsdependency onthenoise intheenvironment.\\nThe second setoffeatures used inthisstudy wasfrom Belalca ´zar-Bolaños etal. because\\nithasphonation features, likejitter andshimmer. Jitter andshimmer, which have been reported\\ntobediscriminative forASD, arelinked toperceptions ofbreathiness, hoarseness, andrough-\\nness . Other features used from Belalca ´zar-Bolaños etal. include glottal features related\\ntovocal quality andtheclosing velocity ofthevocal folds . The mean oflogarithmic energy\\nfeature wasomitted forthesame reason asother energy-related features. Asummary ofthefea-\\ntures, added toorremoved from thesetsby and, ispresented inTable 4.\\nThe proposed subset instance classifier\\nToexplain theproposed classifier, itwasassumed that there isatarget group ofparticipants\\nthat wewant todistinguish from therestoftheparticipants, called therest. Furthermore, each\\nparticipant inthetarget group may have several instances that may beused todistinguish the\\ntarget group from therest. Fig1Ashows asituation inwhich allinstances ofallparticipants of\\nthetarget group aredifferentiable using common classifiers that wecallWhole SetInstance\\n(WSI) classifiers. Inthisfigure, thecircles represent ourtarget group andthetriangles represent\\ntherest. The color coding isused todifferentiate between theinstances ofeach participant in\\neach group. Incontrast tothesituation inFig1A,inFig1Bthetarget group cannot easily bedis-\\ntinguished from therest. Insuch asituation, there areinstances oftwoparticipants inthetarget\\ngroup, i.e.theredandbrown circles that arenoteasily separable from theinstances intherest\\n(Case 1).Furthermore, there isaparticipant with noinstances, i.e.theorange circles, easily\\nTable 4.The features and statisti cswhich were added orremoved tothetwo feature sets.\\nFeature removing /adding Reason\\nSecond\\nsetlogarithm icenergy Mean statistic isremoved Classificat iondependen cyonloudness/powe rofcries\\nFirst set Audio power\\nTotal loudness\\nSONE\\nFirst MFCC coefficient\\nZCR The basic feature isremoved The feature’s depende ncyonenvironm ental noise\\nAllbasic features\\napplicab lemean offrame-wise tempora lderivative ofthebasic\\nfeatures isremovedNomeaning forthefeature\\nMFCC Coefficients of14–24 areadded Having higher-order coefficients forvocal cords information as\\nwell asvocal tract\\nSpectral flatness Arange of125–250 Hzisadded Coverin gthelow-frequ ency range ofhuman voice\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t004\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 8/21separable from therest(Case 2).Anexample ofCase 1istiptoe walking inchildren with ASD,\\nwhich iscommon inabout 25% ofthese children  who doitmost ofthetime. Anexample\\nofCase 2ischildren with ASD who donottiptoe walk. Inother words, there arechildren with\\nASD who cannot bedistinguished from TDchildren using thetiptoe walking behavior factor.\\nApplying anyWSI classifier may failforthedata type shown inFig1B.Consequently, we\\nproposed SubSet Instance (SSI) classifier that first finds differentiable instances and then trains\\naclassifier onthese instances. Asanexample, theproposed SSIclassifier first tries tofind the\\ncircles ontheleftofthelineinFig1B,using aclustering method. Then, ituses these circles, as\\nexclusiveinstances having aspecific feature common inasubset ofthetarget group, totrain a\\nclassifier separating asubset ofthetarget group.\\nThe steps ofcommon WSI classifiers areshown inFig2A.The steps ofourproposed SSI\\nclassifier areshown inFig2B.IntheSSIclassification approach, after thefeature extraction\\nand clustering steps, foreach cluster, aclassifier istrained toseparate itsexclusive instances\\nfrom theinstances oftherestoftheparticipants. Inthetesting phase, anyparticipant with only\\noneinstance classified inthetarget group (positive instance), isclassified asatarget group’s\\nparticipant. The pseudo-code fortheproposed approach isgiven inAlgorithms 1and 2.\\nAlgorithm 1.Training SSIclassifiers\\nT:set ofall target group instances\\nR:set ofall the rest instances\\nF:set ofall classifiers\\nρ:threshold for the number ofsamples inacluster\\ns:the number ofminimum samples needed inacluster tobeable to\\ntrain aclassifier for it\\nCj:The jthcluster\\nn:number ofclusters\\nF=;\\n1:While9j|Cj| >ρ;while there isacluster bigger than athreshold\\norn=1\\n2: n=n+1;increase the number ofclusters\\n3: Cluster the T+Rinto nclusters Cj,j=1,...,n\\n4: EC={Cj�T}; the set ofclusters ofonly exclusive instances,\\ni.e. exclusive clusters\\nFig1.Two different hypothetic altypes oftwo-dimens ional data ofthetarget group and therest. The instances shown bythewarm-colo redcircles and\\nthecool-color edtriangles areforthetarget group and therest, respective ly.Allinstances belonging toaparticipa nthave thesame color. In(a),allthetarget\\ngroup participan ts’instances aredistinguis hable using aclassifier. In(b),only some instances ofthetarget group participant sareseparable from theother\\ninstances byaclassifier.\\nhttps://d oi.org/10.1371/j ournal.pon e.0241690.g0 01\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 9/215: IfEC6¼;;check ifthere isany exclusive cluster\\n6: For all CjinECwith |Cj| >s\\n7: Train aclassifier using positive labels c�Cjand negative\\nlabels r�R\\n8: Add the classifier toF\\n9:T¼T\\x00P\\nCj�ECCj;remove the instances ofthe exclusive clusters from\\ntarget group instances\\n10: n=1;set 1tore-start clustering intwo groups onthe remain-\\ning instances\\nAlgorithm 2.Testing SSIclassifiers\\nF:set oftrained classifiers\\nA:set ofsubject instances\\n1:For all instances aofA\\n2: P={a2A|9f, classifies aaspositive instance}\\n3: IfP6¼;\\n4: The participant isfrom the target group\\n5: Else\\n6: The participant isfrom the rest\\nIntheproposed training algorithm oftheSSIapproach, thegoal istofind clusters contain-\\ningtheASD instances only. Then aclassifier istrained using theinstances ofthese clusters and\\nFig2.Anoverall view ofWSI and SSImethods. (a)InWSI method, after feature extraction, aclassifier istrained onallinstances and\\nmajor itypooling (MP) isusually used inthetesting phase. Inthisstudy Best-chan cethreshold Pooling (BP), which isathreshold -based\\npooling with thethreshold giving thebest accuracy onthetestset,isalso used togive thebest chance toWSI classifier. (b)Inthe\\npropos edSSIclassifier, after feature extraction, clustering isapplied tofind and select exclusive instances containing instances ofthe\\ntarget group participa ntsonly. Then classifiers aretrained using exclusive instances, andaparticipant isclassified inthetarget group in\\nthetesting phase ifanyclassifier detects apositive instance forit.\\nhttps:// doi.org/10.137 1/journal.pone. 0241690.g002\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 10/21added toalistofalltrained classifiers (lines 7and 8ofAlgorithm 1).Asshown intheloop of\\nthealgorithm, starting atline1,thedata isclustered starting with twoclusters. Then thenum-\\nberofclusters isincreased until acluster, containing only thetarget group instances, emerges.\\nThe exclusive instances insuch acluster areremoved from thesetofalltarget group’s\\ninstances, and theloop isrestarted. Before restarting theloop, ifthenumber ofinstances in\\nthiscluster ismore than athreshold, anew classifier using these instances istrained and this\\nclassifier isadded tothesetofalltrained classifiers. The loop stops when thenumber ofsam-\\nples ineach cluster islessthan athreshold.\\nFortesting theparticipants, using thetrained classifiers, alltheinstances ofeach participant\\nareclassified onebyoneusing allthetrained classifiers (line 2ofAlgorithm 2).Asubject would\\nbeclassified inthetarget group ifatleast oneofitsinstances isclassified inthetarget group at\\nleast byoneoftheclassifiers (lines 3and4ofAlgorithm 2).Otherwise, ifthere isnoinstance\\nclassified among thetarget group, theparticipant isclassified astherest(lines 5and6).\\nDetails oftheimplementations\\nThe classifiers were implemented inPython using scikit-learn library.\\nWSI classifiers. Wehave tested several common WSI classifiers, butwereport only the\\nresult ofSVM with RBF kernel and with nofeature selection, which gives thebest average\\naccuracy. Itshould benoted that several feature selection approaches, likeL1-SVM and back-\\nward elimination, were tested butthey only reduced theaccuracy. Weused group 5-fold cross-\\nvalidation fortuning hyper-parameters. Group K-fold means that allinstances ofeach partici-\\npant areplaced inonly oneofthefolds. This prevents having thesame participant’s instances\\ninthetrain and validation folds simultaneously. Ineach fold, there were twoASD and twoTD\\nparticipants. Itshould bementioned that before applying thealgorithms, webalanced the\\nnumber ofinstances ofthetwogroups using upsampling.\\nTwo approaches were exploited tocombine thedecisions ondifferent samples ofapartici-\\npant intheWSI approach. The first approach wasmajority pooling which classifies apartici-\\npant asASD, ifthenumber ofinstances classified asASD aremore than 50percent ofall\\ninstances. The second approach wasthreshold-based pooling which issimilar tothefirst\\napproach except that athreshold other than 50isused.\\nSSIclassifiers. Before applying thealgorithm, webalanced thenumber ofinstances ofthe\\ntwogroups byupsampling. The threshold fortheminimum number ofsamples, needed ina\\ncluster, tobeable totrain aclassifier issetto10.Itshould bementioned that agglomerative\\nclustering and decision tree arethemethods used forclustering and classification parts of\\nAlgorithm 1,respectively.\\nTraining theSSIclassifiers. After running Algorithm 1onourdata, twoexclusive clus-\\nterswith enough instances, i.e.atleast 10instances inourstudy, were found. Then twoclassifi-\\nerswere trained corresponding toeach cluster. One ofthese exclusive clusters had11\\ninstances from 4ASD participants (Table 1).These 11instances consisted of6outof9\\ninstances ofASD1, 2outof4instances ofASD10, 1outof2instances ofASD8, and 2outof4\\ninstances ofASD6. Asexplained inthealgorithm, foreach cluster, adecision tree classifier was\\ntrained using theASD instances inthecluster versus allTDinstances. Interestingly, only one\\nfeature wasenough todiscriminate instances inthecluster from allTDinstances. Among\\nthose features that candiscriminate thecluster’s instances, weselected theVariance ofFrame-\\nwise Temporal Derivative (VFTD) ofthe7thMFCC coefficient asthefeature which candis-\\ncriminate more ASD participants from thesetofallparticipants with asimple threshold. The\\nclassifier obtained bysetting athreshold based onthisfeature wasthefirst classifier. This fea-\\nture supports ourexpert’s report regarding thehigher variations inthecrysounds ofASD\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 11/21children than TDchildren. From 10ASD children, 8ofthem canbediscriminated using this\\nfeature. Foreach participant, thenumber ofinstances found bythisclassifier isshown inthe\\n2ndcolumn ofTable 5.\\nAfter excluding theASD samples from thefirst classifier, thesecond classifier wastrained\\nbased onthesecond exclusive cluster. This cluster included allinstances ofparticipant ASD4.\\nThe only feature used forclassifying thiscluster wasVFTD ofthe6thSONE coefficient. SONE\\nisaunit ofloudness which isasubjective perception ofsound pressure . Having higher\\nVFTD ofthe6thSONE coefficient confirms theexperiential knowledge ofourexperts men-\\ntioned before. Among alltheASD participants, eight hadinstances with VFTD ofthe6th\\nSONE higher than athreshold (Shown inthe3rdcolumn ofTable 5).The results ofclassifica-\\ntion based onthese twofeatures aredepicted inFig3.Asmentioned intheproposed method\\nsection, theparticipants with atleast oneinstance classified into thiscluster would beconsid-\\nered asaparticipant with ASD.\\nResults\\nInthispart, theperformance ofourproposed SSIclassifier against acommon WSI classifier is\\nevaluated onourtestsetofASD and TDparticipants. Each participant hasmultiple instances\\nwhich arecleaned using thecriteria explained inthedata collection and preprocessing section.\\nThe participants who hadatleast oneaccepted instance were used inthetraining and testing\\nphases, which areshown inTables 1and 2.\\nThe output oftheSSIapproach wastwoclassifiers, each ofthem works bysetting athresh-\\noldbased onafeature. The number ofinstances ofASD participants inthetraining set,cor-\\nrectly detected bythefirst and thesecond classifiers, areshown inthesecond and third\\ncolumns ofTable 5,respectively. Ontheother hand, thebest-resulting classifier fortheWSI\\napproach wasRadial Basis Function-Support Vector Machine (RBF-SVM) .\\nThe classification results onthetestsetfordifferent classifiers areshown inTable 6.The\\nportion ofeach participant’s instances, correctly classified byeach classifier, iswritten asa\\npercentage under thename oftheclassifier. The decision made bytheWSI and SSIclassifi-\\nersforeach participant isshown byASD orTD. Toclassify each subject using theWSI clas-\\nsifier, theMajority Pooling (MP) and theBest-chance threshold Pooling (BP) approaches\\nwere used. BPisathreshold-based pooling with thethreshold giving thebest accuracy on\\nthetestsetformale participants. Fortheboys, MPhasspecificity, sensitivity, and precision\\nequal to100%, 35.71%, and 67.85%, respectively. Ontheother hand, BPleads tospecificity,\\nsensitivity, and precision equal to85.71%, 71.42%, and 78.57%, respectively. The threshold\\nTable 5.The number ofinstanc esofeach participant inthetraining setthat areclassified asASD using each\\ntrained SSIclassifier.\\nID First SSIclassifier Second SSIclassifier\\nASD1 8 3\\nASD2 1 2\\nASD3 3 1\\nASD4 10 9\\nASD5 0 0\\nASD6 1 3\\nASD7 1 0\\nASD8 1 2\\nASD9 0 1\\nASD10 2 4\\nhttps://d oi.org/10.1371/j ournal.pon e.0241690.t00 5\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 12/21forBPwas setto20% that means if20% ofinstances ofaparticipant were classified asASD\\ninstance, theparticipant was classified ashaving ASD. The results ofthepercentage of\\ninstances correctly classified bythetwo classifiers intheSSIapproach areshown asC1(the\\nfirst SSIclassifier) and C2(the second SSIclassifier) inTable 6.The aggregated result ofthe\\ndecisions byC1and C2makes thefinal decision oftheSSIclassifier which isshown inthe\\ndecision column, under theSSIclassification section. The achieved specificity, sensitivity,\\nand precision using theproposed method fortheboys are100%, 85.71%, and 92.85%,\\nrespectively.\\nTofurther show theapplicability oftheproposed approach togirls, weapplied theboys’\\ntrained classifiers onthetestsetofthegirls. The results areshown inthelastrow ofTable 6\\nwhich show that theMPapproach hasspecificity, sensitivity, and precision equal to100%,\\n71.42%, and 85.71%, respectively. Furthermore, theBPapproach gives specificity, sensitivity,\\nand precision allequal to85.71%, respectively. The results oftheproposed SSIclassifier is\\n100% specificity, 71.42% sensitivity, and 85.71% precision.\\nAtwo-dimensional scatter plot ofthetwofeatures, used inC1and C2classifiers, areshown\\ninFig4.Ascanbeseen inthisfigure, theinstances ofaparticipant with ASD arescattered in\\nthearea containing instances ofboth TDand ASD participants. Nevertheless, there are\\ninstances forthisparticipant uniquely distinguishable using theselected twofeatures.\\nWecompared theresults ofourproposed method with that oftheonly method available\\nintheliterature which was trained using only cryfeatures  based onourdata. The\\nresults (Table 7)show thesuperiority ofourmethod, compared tothepreviously proposed\\nmethod.\\nInvestigating thetrained classifier onparticipants under 18months\\nThe SSIclassifier which was trained using thetraining setinTable 1was also tested onthe\\ndata ofchildren younger than 18months. From 57participants under 18months, two boys\\n(Child1 and Child2 inTable 3)were classified asASD bythementioned trained classifier.\\nThese participants were referred toourexperts fordiagnosis. These two were suspected of\\nhaving neurodevelopmental problems. Allother boys were classified asTD. However,\\namong them, Child3 was diagnosed with ASD attheageof2.Also, Child4 showed\\nFig3.Two classifiers trained onthetwo exclusive clusters found during theSSIclassifier training phase. (a)The Variance ofFrame-w iseTemporal Derivati ve\\n(VFTD) ofthe7thMFCC coefficient separates 27instances of8ASD subjects from allTDinstances ofthetraining set.(b)VFTD ofthe6thSONE coefficient\\nseparates 17instances of7ASD participants from allTDinstances ofthetraining set.\\nhttps://d oi.org/10.1371/j ournal.pon e.0241690.g0 03\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 13/21symptoms ofhaving ADHD and sensory processing disorder attheageof3.Three other\\nchildren had symptoms which suggested that they arenotTDchildren. Two ofthegirls\\nwho were 18months oldwere classified asASD, using thetrained classifier. The other girls\\nwere classified asTD. The results oftesting thetrained SSIclassifier onthis data setare\\nsummarized inTable 8.\\nThe original and cleaned voices and their extracted features (the data set)inthisresearch\\nand theimplementation codes oftheproposed method aredeposited inthefollowing\\nrepositories:\\nCodeOcean\\n10.24433/CO.0622 770.v1\\nHarvard Dataverse (Contains only ararfileofsounds):\\n10.7910/DVN/LSTBQ WTable 6.The results ofclassifier sontheinstances ofeach participa ntinthetest set.\\nTDchildren Children with ASD\\nID Portion ofinstances classified asTDasapercentag eand the\\ndecisionID Portion ofinstances classified asASD asapercentage and the\\ndecision\\nWSI classificat ion SSIclassificat ion WSI classificat ion SSIclassificat ion\\nSVM Dec. C1 C2 Dec. SVM Dec. C1 C2 Dec.\\nMP BP MP BPBoysTD11 100 TD TD 100 100 TD ASD11 50 ASD ASD 17 50 ASD\\nTD12 100 TD TD 100 100 TD ASD12 33 TD ASD 11 28 ASD\\nTD13 100 TD TD 100 100 TD ASD13 33 TD ASD 33 0 ASD\\nTD14 100 TD TD 100 100 TD ASD14 20 TD ASD 20 20 ASD\\nTD15 100 TD TD 100 100 TD ASD15 0 TD TD 0 40 ASD\\nTD16 100 TD TD 100 100 TD ASD16 50 ASD ASD 100 0 ASD\\nTD17 100 TD TD 100 100 TD ASD17 0 TD TD 0 100 ASD\\nTD18 83 TD TD 100 100 TD ASD18 50 ASD ASD 50 50 ASD\\nTD19 100 TD TD 100 100 TD ASD19 0 TD TD 0 0 TD\\nTD20 80 TD ASD 100 100 TD ASD20 42 TD ASD 42 16 ASD\\nTD21 100 TD TD 100 100 TD ASD21 100 ASD ASD 0 0 TD\\nTD22 100 TD TD 100 100 TD ASD22 0 TD TD 0 50 ASD\\nTD23 75 TD ASD 100 100 TD ASD23 33 TD ASD 33 17 ASD\\nTD24 92 TD TD 100 100 TD ASD24 86 ASD ASD 86 86 ASD\\nAcc. % 100 85.71 100 35.71 71.42 85.71GirlsTD25 100 TD TD 100 100 TD ASD25 42 TD ASD 17 0 ASD\\nTD26 100 TD TD 100 100 TD ASD26 60 ASD ASD 60 20 ASD\\nTD27 100 TD TD 100 100 TD ASD27 50 ASD ASD 0 0 TD\\nTD28 100 TD TD 100 100 TD ASD28 100 ASD ASD 0 50 ASD\\nTD29 100 TD TD 100 100 TD ASD29 62 ASD ASD 50 50 ASD\\nTD30 67 TD ASD 100 100 TD ASD30 100 ASD ASD 50 50 ASD\\nTD31 100 TD TD 100 100 TD ASD31 0 TD TD 0 0 TD\\nAcc. % 100 85.71 100 71.42 85.71 71.42\\nEach classifier result onaparticipant ’sinstances isreported asapercentage.\\nDec., Decision; MP, Majority Pooling; BC,Best-chance threshold Pooling; C1,Classifier1; C2,Classifier2 ;Acc., Accuracy.\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t006\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 14/21Discussion andconclusion\\nInthispaper, wepresented anovel cry-based screening method todistinguish between chil-\\ndren with autism and typically developing children. Intheproposed method, groups ofchil-\\ndren with autism who have specific features intheir crysounds canbedetermined. This\\nmethod isbased onanew classification approach called SubSet Instance (SSI) classifier. An\\nappealing property oftheproposed SSIclassifier, inthecase ofvoice-based autism screening,\\nisitshigh specificity such that anormal child canbedetected with noerror. Weapplied the\\nproposed method onagroup ofparticipants consisting of24boys with ASD between 20and\\nFig4.Instances ofseveral ASD and TDparticipa ntsscattered inthespace oftwo features given bytheproposed SSImethod. The\\ninstances ofachosen ASD participant areillustrated ingreen toshow that aparticipant may have instances inthearea common with TD\\ninstances besides those twoareas separated bytheselected thresholds asASD. The mention edASD participant (with green instances) is\\ntagged asASD, duetohaving atleast oneinstance with thegreater value than atleast oneofthethresho ldsonthetwofeatures.\\nhttps:// doi.org/10.1371 /journal.pone. 0241690.g004\\nTable 7.Compariso noftheresults onthetest setusing thetwo methods; SSIapproac hand abaseline approach.\\nSensitivity Specificity PrecisionBoysSSI 85.71% 100% 92.85%\\nBaseline 50.58% 81% 65%GirlsSSI 71.42% 100% 85.71%\\nBaseline 21% 86.48% 53%\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t007\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 15/2153months ofageand 24TDboys between 18and 51months ofage. The twofeatures, found\\ninthisstudy, were used totrain aclassifier on10boys with ASD and 10TDboys. Then, the\\nclassifier wasused todistinguish 14boys with ASD from 14TDboys, reaching 92.8% accuracy.\\nDue tothefactthat girls arelesslikely tohave autism and consequently, itisharder tocollect\\nenough data from girls than boys, thenumber ofgirls with ASD wasnotsufficient totrain a\\nseparate classifier forthisgender. Itshould benoted that wetested thetrained system on7\\ngirls with ASD and 7TDgirls. Itwasseen that thetrained classifier canscreen girls with 7%\\nlower accuracy than boys ofthetestset.Inother words, itseems that gender differences should\\nbeconsidered inthetraining ofthesystem. Intesting thedata from participants under 18\\nmonths, oneTDgirlwasclassified asASD which wasnotthecase foranyTDchildren ofthe\\nmale counterparts. This result also confirms theaforementioned point about thegender effect.\\nHowever, infuture work, wewould trytocollect more data ongirls tobeable totrain asystem\\ntoaccurately screen girls. Furthermore, wewould also trytotrain asingle classifier forboys\\nand girls todetermine whether itcanbeused forboth ofthem.\\nItshould bementioned that ourtraining and testdata were completely separate, tomake\\nthetrained model more general. The features found inthisstudy areapplicable intheage\\nrange ofourparticipants from 18to53months. This isincontrast toother approaches that\\neither used adataset ofchildren with aspecific age[33, 35]orused ageinformation forclassifi-\\ncation . Due totheageinvariant features found inthisstudy, itcanbeclaimed that there\\naremarkers inthevoices ofchildren with ASD that aresustained atleast inarange ofages.\\nThe twodiscriminative features, found inthisstudy, were acoefficient ofMFCC andaSONE\\ncoefficient. MFCC andSONE arerelated tothepower spectrum ofaspeech signal. SONE mea-\\nsures loudness inspecific Bark bands . Ontheother hand, MFCC, which istheinverse DFT\\noflog-spectrum intheMel scale, isrelated tothetimbre ofthevoice . Therefore, MFCC and\\nSONE canbeinterpreted toberelated tothetimbre andloudness ofatone. Furthermore, based\\nonthefeedback from ourexperts, there isunpredictability inthecrying sound ofchildren with\\nautism which isnotthecase forTDchildren. Consequently, weused thevariance oftemporal\\ndifference asafeature suitable forscreening children with autism. This isduetothefactthat ifa\\nsignal isconstant orchanges linearly over time, thevariance oftemporal difference iszero.\\nTherefore, thevariance oftemporal difference canbeseen astheamount ofambiguity orunpre-\\ndictability ofasound. Ontheother hand, theheightened variability inthetwofeatures, found in\\nthisstudy, forchildren with ASD issignificant duetothereports from other studies [22,61]\\nwhich shows increased biological signals variability inchildren with ASD andinfants athigh\\nriskforautism incomparison with TDchildren. These features arestatistical features ofthecry\\ninstances that hold constant, atleast, across anagerange studied inthisresearch.\\nTothebest ofourknowledge,  and  were theonly studies onscreening children\\nwith autism using voice features onchildren younger than 2years ofage. Our proposed\\nmethod hashigher precision than these two, i.e.6%more than  and 17% more than ,\\nusing only cryfeatures. The useofcryfeatures assuitable biomarkers forautism screening\\nmatches theclaims in.Table 8.Classificati onoftheparticipa ntsunder 18months using our trained SSIclassifier.\\nBoys Girls\\nASD TD OthersaASD TD Othersa\\nClassified asASD 0 0 2 0 1 0\\nClassified asTD 1 22 4 0 27 0\\naOther developm ental ormental disorders\\nhttps://do i.org/10.1371/j ournal.pone .0241690.t008\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 16/21Inthepresent study only children with ASD and TDchildren were tested. Other develop-\\nmental disorders orhealth issues were nottested toseehow children with such disorders\\nwould beclassified using theproposed method which candecrease thespecificity of100%.\\nHowever, thisapproach isproposed tobeused asascreening tool and thefinal diagnosis\\nshould bedone under experts’ supervision. So,thisapproach canbeapplied asageneral\\nscreener ofautism spectrum disorder.\\nThe trained classifier wasalso tested on57participants between 10to18months ofage. The\\nclassifier screened twoboys from therest, i.e.Child1 andChild2 (Table 3).Child1 showed evi-\\ndences ofgenetic disease andwasdiagnosed with developmental delay andChild2 received\\nUNDD classification byourexperts. This suggests that a)thesystem canbeused forchildren\\nunder 2years ofage, andb)itmay beable todistinguish other neurodevelopmental disorders.\\nOntheother hand, there were 5boys, i.e.Child3 toChild7 (Table 3),who hadnoevidence of\\nmental ordevelopmental disorders atthetime oftheir recording. Atthesame time, ourapproach\\ndidnotdistinguish them aschildren with ASD either. However, when they were older than 3\\nyears, they showed symptoms ofneurodevelopmental disorders. Out ofthese children, wecould\\nmanage tocollect new recordings from Child3 andChild4 that were classified aschildren with\\nASD using ourapproach. Unfortunately, Child5, Child6, andChild7 didnotcooperate and\\ncould notbeevaluated byanexpert tovalidate theresults ofourexpert-selected questionnaire.\\nFurthermore, theparents refused tocooperate send ustheir children’s recent crysounds.\\nThe result ofstudying these 57children under theageof18months may suggest that: a)\\nthere could besymptoms inthecrying sounds ofchildren with neurodevelopmental disorders\\nunder 18months (Child1 and Child2), b)theapproach may notbeable toscreen aparticipant\\nwith neurodevelopmental disorders under theageof18months duetothepossibility that: 1)\\ntheparticipant wasamong those children with neurodevelopmental disorders who donot\\nhave ourproposed specific features intheir crying sounds, 2)theparticipant’s recorded cry\\nsamples didnotinclude ourspecific features, and/or 3)neurodevelopmental disorders and\\ntheir features hadnotbeen developed inthechild atthetime ofinitial recording. The reason\\nbehind notclassifying Child3 and Child4, aschildren with ASD under theageof18,could be\\nb.2orb.3.Toclearly determine anyreason behind thisphenomena, afurther investigation is\\nneeded.\\nWebelieve that thisapproach canbeused toperform early autism screening under 18\\nmonths ofage. Thus, inthefuture, weneed tocollect data and testtheapproach onmore data\\nofchildren under 18months tovalidate these results with more confidence.\\nWehave tofurther check theproposed approach and theextracted features onother neuro-\\ndevelopmental disorders, such asADHD, toevaluate thecapability oftheapproach todistin-\\nguish thechildren with these disorders from TDchildren.\\nFurthermore, without comparing thecrysounds ofchildren with ASD tothose without\\nASD butanother disorder, wedonotreally know ifthese findings arespecific toautism orto\\ngeneral atypical brain developments. Thus, weshould collect crysounds ofchildren with other\\nneurodevelopmental disorders and compare voices ofchildren with ASD tovoices ofchildren\\nwith other neurodevelopmental disorders toseeifthese features would beable toseparate\\nthem ornot.\\nIthasbeen demonstrated that crying consists ofintricate motor activities . Ontheother\\nhand, ithasbeen shown that children with ASD have problems inthemotor domain and in\\ncoordination oftheir motor capabilities with other modalities . Consequently, itispossible\\nthat theextracted features inthecrying sounds ofchildren with ASD come from thisdefi-\\nciency/problem inthemotor domain which requires further investigations.\\nFinally, automating thepreprocessing part isatechnical issue that should beaddressed ifit\\nisdeemed necessary that thecry-based screening befully automated. This isimportant since\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 17/21such ascreening system canbedeployed insystems such asAmazon Alexa  toautomati-\\ncally screen problematic crysounds.\\nAcknowledgmen ts\\nWewould liketothank theCenter forTreatment ofAutism Disorder (CTAD) and itsmem-\\nbers forsupporting thisstudy. Wewould also liketothank allthefamilies who helped this\\nresearch bytaking thetime tocollect thecrysounds oftheir children. The authors would also\\nliketoexpress their gratitude toProf. H.Sameti from Sharif University ofTechnology forhis\\nvaluable and constructive feedbacks onthedata collection and voice processing.\\nAuthor Contributions\\nConceptualization: Aida Khozaei, Hadi Moradi, Reshad Hosseini, Hamidreza Pouretemad,\\nBahareh Eskandari.\\nData curation: Aida Khozaei.\\nFormal analysis: Aida Khozaei.\\nFunding acquisition: Hadi Moradi.\\nInvestigation: Hadi Moradi.\\nMethodology: Aida Khozaei.\\nProject administration: Hadi Moradi.\\nSoftware: Aida Khozaei.\\nSupervision: Hadi Moradi.\\nValidation: Aida Khozaei.\\nVisualization: Aida Khozaei.\\nWriting –original draft: Aida Khozaei, Hadi Moradi, Reshad Hosseini.\\nWriting –review &editing: Aida Khozaei, Hadi Moradi, Reshad Hosseini, Hamidreza Poure-\\ntemad, Bahareh Eskandari.\\nReferences\\n1.American Psychiatric Association .Diagnostic andstatistical manual ofmental disorders (DSM-5®).\\nAmerican Psychiatric Pub; 2013.\\n2.Chen JL,Sung C,PiS.Vocational rehabilitatio nservice pattern sandoutcomes forindividuals with\\nautism ofdifferent ages. JAutism DevDisord. 2015; 45(9):3015 –29. https://doi.or g/10.100 7/s10803-\\n015-2465- yPMID: 25982310\\n3.Fakhoury M.Autistic spectrum disorders: Areview ofclinical features, theories anddiagnosis .IntJDev\\nNeurosci. 2015; 43:70–7. https://doi.o rg/10.1016/j.ij devneu.2 015.04.003 PMID: 25862937\\n4.Constantino JN,Charma nT.Diagnosis ofautism spectrum disorder: reconciling thesyndrome, its\\ndiverse origins, andvariation inexpression .Lancet Neurol. 2016; 15(3):279– 91.https://do i.org/10.\\n1016/S1474- 4422(15)0 0151-9 PMID: 264977 71\\n5.Caldero niS,Billeci L,Narzisi A,Brambilla P,Retico A,Muratori F.Rehabilitat iveintervent ionsandbrain\\nplasticity inautism spectrum disorde rs:focus onMRI-bas edstudies. Front Neurosci. 2016; 10:139.\\nhttps://doi.or g/10.338 9/fnins.201 6.00139 PMID: 270657 95\\n6.Brentani H,Paula CSd, Bordini D,Rolim D,Sato F,Portolese J,etal.Autism spectrum disorders :an\\noverview ondiagnos isandtreatment. Braz JPsychiatry. 2013; 35:S62 –S72. https://doi.or g/10.159 0/\\n1516-4446- 2013-S104 PMID: 241421 29\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 18/217.Mandell DS,Novak MM, Zubritsky CD.Factors associat edwithageofdiagnosis among children with\\nautism spectrum disorde rs.Pediatrics. 2005; 116(6):148 0–6. https://doi. org/10.1542/p eds.2005 -0185\\nPMID: 163221 74\\n8.Thabtah F,Peebles D.Anewmachine learning model based oninduction ofrules forautism detection.\\nHealth Inform J.2019. https://doi. org/10.1177/1 46045821 8824711 PMID: 30693818\\n9.Volkmar F,Cook EH,Pomeroy J,Realmuto G,Tanguay P.Practice parame tersfortheassessme nt\\nandtreatment ofchildren, adolescen ts,andadults withautism andother pervasive develop mental dis-\\norders. JAmAcad Child Adolesc Psychiatry. 1999; 38(12, Supplem ent):32S–54 S.https://d oi.org/10.\\n1016/S0890- 8567(99)8 0003-3\\n10. Campbell M,Schopler E,Cueva JE,Hallin A.Treatmen tofautistic disorder. JAmAcad Child Adolesc\\nPsychiatry. 1996; 35(2):134– 43.https://doi.or g/10.109 7/00004583- 1996020 00-00005 PMID: 8720622\\n11. Zachor DA,Itzchak EB.Treatment approach, autism severity andinterven tionoutcomes inyoung chil-\\ndren. ResAutism Spectr Disord. 2010; 4(3):425–3 2.\\n12. Boyd BA,Hume K,McBee MT,Alessand riM,Gutierrez A,Johnson L,etal.Comparative Efficacy of\\nLEAP, TEACCH andNon-Model -Specific Special Education Program sforPreschoolers withAutism\\nSpectrum Disorders. JAutism DevDisord. 2014; 44(2):366– 80.https://doi.or g/10.1007/ s10803-013-\\n1877-9 PMID: 238126 61\\n13. Jacobson JW,Mulick JA,Green G.Cost–be nefitestima tesforearly intensive behavio ralintervention for\\nyoung children withautism —general model andsingle state case. Behav Interv. 1998; 13(4):20 1–26.\\n14. Jacobson JW,Mulick JA.System andCost Research Issues inTreatmen tsforPeople withAutistic Dis-\\norders. JAutism DevDisord. 2000; 30(6):585– 93.https://doi.or g/10.102 3/a:100569141 1255 PMID:\\n11261469\\n15. Thabtah F,Peebles D.Early Autism Screening: AComprehen siveReview. IntJEnviro nResPublic\\nHealth. 2019; 16(18):350 2.https:// doi.org/10.33 90/ijerph16 183502 PMID: 31546906\\n16. Rutter M,LeCouteur A,Lord C.ADI-R: Autism Diagnostic Interview-R evised. LosAngeles ,CA:West-\\nernPsychologi calServices; 2003.\\n17. Lord C.,RisiS.,Lambrech tL.,Cook E.H.Jr.,Leventha lB.L.,Lavore Di,etal.Theautism diagnostic\\nobservatio nschedule- generic: astandard measure ofsocial andcommu nication deficits associated\\nwiththespectrum ofautism. JAutism DevDisord. 2000; 30(3), 205–223. PMID: 110554 57\\n18. Levy S,Duda M,Haber N,Wall DP.Sparsifyi ngmachine learning models identify stable subsets ofpre-\\ndictive features forbehavio raldetection ofautism. MolAutism. 2017; 8(1):65. https://doi.or g/10.1186/\\ns13229-017 -0180-6 PMID: 29270283\\n19. Ku¨pper C,Stroth S,Wolff N,Hauck F,Kliewer N,Schad-Ha nsjosten T,etal.Identifying predictive fea-\\ntures ofautism spectrum disorders inaclinical sample ofadolescen tsandadults using machine learn-\\ning.SciRep. 2020; 10(1):4805 .https://doi.or g/10.1038/ s41598-020- 61607-w PMID: 32188882\\n20. Abbas H,Garberson F,Liu-Mayo S,Glover E,Wall DP.Multi-mod ularAIApproach toStreaml ine\\nAutism Diagnosis inYoung Children .Scientific Reports. 2020; 10(1):5014 .https://do i.org/10.1038 /\\ns41598-020 -61213-w PMID: 3219340 6\\n21. Emerson RW, Adams C,Nishino T,Hazlett HC,Wolff JJ,Zwaigenba umL,etal.Functional neuroim ag-\\ningofhigh-risk 6-month- oldinfants predicts adiagnos isofautism at24months ofage. SciTransl Med.\\n2017; 9(393):eaa g2882. https://doi.or g/10.1126 /scitranslmed.a ag2882 PMID: 28592562\\n22. Denisova K,Zhao G.Inflexible neurobiolo gical signatures precede atypical developme ntininfants at\\nhighriskforautism. SciRep. 2017; 7(1):11285 .https://doi.or g/10.103 8/s41598 -017-0902 8-0PMID:\\n28900155\\n23. Bosl WJ,Tager-Flus berg H,Nelson CA.EEG analytics forearly detection ofautism spectrum disorder:\\nadata-driven approac h.SciRep. 2018; 8(1):6828. https://d oi.org/10.103 8/s41598 -018-2431 8-xPMID:\\n29717196\\n24. Momeni N,Bergquist J,Brudin L,Behnia F,Sivberg B,Joghatae iM,etal.Anovel blood-based bio-\\nmarker fordetection ofautism spectrum disorders. Transl Psychiatry. 2012; 2(3):e91. https://doi.or g/10.\\n1038/tp.2 012.19 PMID: 228328 56\\n25. Glatt SJ,Tsuang MT,Winn M,Chandler SD,Collins M,Lopez L,etal.Blood-bas edgene expression\\nsignatures ofinfants andtoddlers withautism. JAmAcad Child Adolesc Psychiatry .2012; 51(9):934-\\n944.e2. https:/ /doi.org/10.10 16/j.jaa c.2012.07.0 07PMID: 22917206\\n26. Croen LA,Brauns chweig D,Haapanen L,Yoshida CK,Fireman B,Grether JK,etal.Maternal mid-preg-\\nnancy autoantibodie stofetalbrain protein: theearly markers forautism study. BiolPsychiatry .2008; 64\\n(7):583–8. https://doi.o rg/10.1016/j.b iopsych. 2008.05.006 PMID: 185716 28\\n27. Greene DJ,Black KJ,Schlagg arBL.Considera tions forMRIstudy design andimplementa tioninpediat-\\nricandclinical populations. DevCogn Neurosci. 2016; 18:101–112. https:// doi.org/10.10 16/j.dcn.20 15.\\n12.005 PMID: 26754461\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 19/2128. Webb SJ,Bernier R,Henderson HA,Johnson MH,Jones EJ,Lerner MD,etal.Guideline sandbest\\npractices forelectroph ysiological data collection, analysis andreporting inautism. JAutism DevDisord.\\n2015; 45(2):425– 243. https://doi.or g/10.1007/ s10803-013- 1916-6 PMID: 2397514 5\\n29. Engelhard tLE,RoeMA,Juranek J,DeMa sterD,Harden KP,Tucker-D robEM,etal.Children ’shead\\nmotion during fMRI tasks isheritable andstable over time. DevCogn Neurosci. 2017; 25:58–6 8.https://\\ndoi.org/10.10 16/j.dcn.20 17.01.011 PMID: 282230 34\\n30. Denisova K.Ageattenuate snoise andincreases symmetry ofhead movem entsduring sleep resting-\\nstate fMRI inhealthy neonates, infants, andtoddlers .Infant Behav Dev. 2019; 57:1013 17.https://doi.\\norg/10.1016/ j.infbeh.2019.0 3.008 PMID: 31102945\\n31. Brisson J,Martel K,Serres J,Sirois S,Adrien JL.Acoustic analysis oforalproducti onsofinfants later\\ndiagnos edwithautism andtheir mother. Infant Ment Health J.2014; 35(3):285– 95.https://doi.or g/10.\\n1002/imh j.21442 PMID: 257984 82\\n32. Nakai Y,Takigu chiT,Matsui G,Yamaoka N,Takada S.Detectin gabnormal word utterances in\\nchildren withautism spectrum disorders: machine-l earning-base dvoice analysis versus speech\\ntherapis ts.Percept MotSkills. 2017; 124(5):96 1–73. https://doi .org/10.1177/0 0315125 17716855\\nPMID: 2864992 3\\n33. Santos JF,Brosh N,FalkTH,Zwaigenba umL,Bryson SE,Roberts W,etal.Very early detection of\\nautism spectrum disorde rsbased onacoustic analysis ofpre-verbal vocalization sof18-mon tholdtod-\\ndlers. In:Proceedings oftheInternational Conferen ceonAcoustics ,Speech andSignal Processin g;\\n2013; Vancou ver,BC,Canada: IEEE. 2013. Doi:10.1109/ICA SSP.2013.6 639134\\n34. Oller D,Niyogi P,Gray S,Richards J,Gilkerson J,XuD,etal.Automate dvocal analysis ofnaturalis tic\\nrecordings from children withautism, language delay, andtypical developme nt.Proc NatlAcad Sci.\\n2010; 107(30):13 354–9. https://doi.or g/10.1073 /pnas.10038 82107 PMID: 20643944\\n35. Pokorny FB,Schuller BW,Marschik PB,Brueckner R,Nystro ¨mP,Cummins N,etal.Earlier Identifica-\\ntionofChildren withAutism Spectru mDisorder: AnAutomatic Vocalisati on-Based Approach. In:Pro-\\nceeding softheINTERS PEECH 2017; 2017; Stockholm ,Sweden: ISCA. 2017. Doi:10.2143 7/\\nInterspee ch.2017-1007\\n36. Little MA,Varoquaux G,Saeb S,Lonini L,Jayaraman A,Mohr DC,etal.Using andundersta nding\\ncross-valida tionstrategie s.Perspe ctives onSaeb etal.Gigascie nce.2017; 6(5):1–6. https://doi.or g/10.\\n1093/gigas cience/gix02 0PMID: 283279 89\\n37. Eyben F,Scherer KR,Schulle rBW,Sundberg J,Andre ´E,Busso C,etal.TheGeneva minimalis tic\\nacoustic parame terset(GeMAPS )forvoice research andaffective comput ing.IEEE Trans Affect Com-\\nput.2015; 7(2):190–2 02.\\n38. Esposito G,Hiroi N,Scattoni ML.Cry,Baby, Cry:Expressio nofDistres sAsaBiomarke randModulato r\\ninAutism Spectrum Disorder. IntJNeuropsyc hopharmac ol.2017; 20(6):498– 503. https:// doi.org/10.\\n1093/ijnp/py x014 PMID: 28204487\\n39. Sheinkopf SJ,Iverson JM,Rinaldi ML,Lester BM.Atypical CryAcoustics in6-Month-Ol dInfants atRisk\\nforAutism Spectrum Disorder. Autism Res. 2012; 5(5):331–9 .https://doi.or g/10.100 2/aur.1244 PMID:\\n22890558\\n40. Orlandi S,Manfredi C,Bocchi L,Scattoni ML,editors. Automatic newborn cryanalysis: anon-invas ive\\ntooltohelpautism early diagno sis.In:Proceedings oftheAnnual International Conferen ceoftheIEEE\\nEngineeri nginMedicine andBiology Society; 2012; SanDiego, CA,USA: IEEE. 2012. Doi:10.1109 /\\nEMBC.2012 .634658 3\\n41. Motlagh SHRE, Moradi H,Pouretemad H,editors. Using general sound descriptor sforearly autism\\ndetection: 2013 9thAsian Control Conferen ce(ASCC) Control; 2013; Istanbul, Turkey: IEEE. 2013.\\nDoi:10.1109/ASC C.2013.660 6386\\n42. Barrow WJ,Jaworski M,Accardo PJ.Persisten ttoewalking inautism. JChild Neurol. 2011; 26(5):619–\\n21.https://doi. org/10.1177/0 8830738 10385344 PMID: 21285033\\n43. Paul R,Norbury CF.Langua gedisorders from infancy throug hadolescence :Elsevier; 2012.\\n44. Jalilevand N,Ebrahim ipour M.Pronoun acquisition inFarsi-spe aking children from 12to36months. J\\nChild Lang Acquis Dev. 2013; 1(1):1–9.\\n45. Goldstein S,Ozonoff S.Assessme ntofautism spectrum disorder: Guilford Publica tions; 2018.\\n46. Lund NJ,Duchan JF.Assessing childre n’slanguage innaturalis ticcontexts: Prentice Hall; 1993.\\n47. Gilliam JE.Gilliam autism rating scale: GARS 2:Pro-ed; 2006.\\n48. Berk L.Developm entthrough thelifespan: Pearson Educatio nIndia; 2010.\\n49. Three ZT.Diagnostic classific ation ofmental health anddevelop mental disorde rsofinfancy andearly\\nchildhood: Revised edition (DC: 0-3R). Washingto n,DC: Zero ToThree Press; 2005.\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 20/2150. Paliwal KK,Lyons JG,Wo´jcicki KK,editors. Preference for20–40 mswindow duration inspeech analy-\\nsis.In:Proceedings ofthe4thInterna tional Conference onSignal Processin gandCommunic ation Sys-\\ntems; 2010: Gold Coast, QLD, Australia: IEEE. Doi:10.1109 /ICSPCS.201 0.57097 70\\n51. Molau S,PitzM,Schluter R,NeyH.Comput ingMel-fre quency cepstral coefficient sonthepower spec-\\ntrum. In:Proceedings oftheInternational Conferen ceonAcoustic s,Speech ,andSignal Processin g\\nProceedings (CatNo01C H37221); 2001; SaltLake City, UT,USA: IEEE. Doi:10.1109 /ICASSP. 2001.\\n940770\\n52. Esposito G,Venuti P.Developme ntalchanges inthefundamen talfrequency (f0)ofinfants’ cries: a\\nstudy ofchildren withAutism Spectrum Disorder. Early Child DevCare. 2010; 180(8):109 3–102.\\n53. Marchi E,Schuller B,Baron-Cohen S,Golan O,Bo¨lteS,Arora P,etal.Typicality andemotion inthe\\nvoice ofchildren withautism spectrum condition :Evidence across three langua ges.In:Proceedings of\\ntheINTERS PEECH 2015; 2015; Dresden, Germany :ISCA. 2015. p.115–119. Available from: https://\\nwww.isca -speech.o rg.\\n54. Belalca ´zar-Bolañ osE.A., Orozco-Arr oyave J.R., Vargas-Bo nillaJ.F., Haderle inT.,No¨thE.Glottal Flow\\nPatterns Analyses forParkinson’s Disease Detection: Acoustic andNonlinear Approaches .In:Sojka\\nP.,Hora´kA.,Kopeček I.,Pala K.,editors. Text, Speech, andDialogue :Proceedings ofthe19th Interna -\\ntional Conferen ceonText, Speech ,andDialogue ;2016 Sep12–16; Brno, Czech Republic .Cham:\\nSpringer; 2016. Doi:10.1007/978- 3-319-455 10-5_46\\n55. Rabiner LR,Schafer RW. Introduction todigital speech processing .Found andtrends insignal process.\\n2007; 1(1):1–194 .\\n56. Peeters G.Alarge setofaudio features forsound descriptio n(similarity andclassif ication) intheCUI-\\nDADO project. CUIDAD OISTProjRep. 2004; 54(0):1–25 .\\n57. Bone D,LeeC-C, Black MP,Williams ME,LeeS,Levitt P,etal.Thepsychologist asaninterlocut orin\\nautism spectrum diorder assessmen t:Insights from astudy ofspontane ousprosody. JSpeech Lang\\nHear Res. 2014; 57(4):1162 –77. https://doi. org/10.1044/2 014_JS LHR-S-13 -0062 PMID: 24686340\\n58. Ha¨nsler E,Schmidt G.Speech andaudio processing inadverse environm ents: Springer Science &\\nBusiness Media; 2008. https://doi.or g/10.105 5/s-2008-106 5331 PMID: 18473287\\n59. Theodoridis S,Koutroumbas K.Pattern recognition: Elsevier ;2003.\\n60. LiTL,Chan AB.Genre classification andtheinvariance ofMFCC feature stokeyandtempo. In:Lee\\nKT.,TsaiWH., LiaoHY.M., Chen T.,Hsieh JW., Tseng CC., editors. Advances inMultimedia Modeling:\\nProceedings ofthe17th Interna tional MultiMedia Modeling Conferen ce;2011 Jan5–7; Taipei, Taiwan.\\nBerlin, Heidelberg: Springer; 2011. Doi:10.1007 /978-3-642 -17832-0_ 30\\n61. Takahashi T,Yoshimur aY,Hiraishi H,Hasegawa C,Munesue T,Higashida H,etal.Enhanc edbrain\\nsignal variabi lityinchildren withautism spectrum disorder during early childhoo d.Hum Brain Mapp.\\n2016; 37(3):1038 –50. https://doi.or g/10.1002/ hbm.23089 PMID: 26859309\\n62. Lester BM,Boukydis CZ.Infant crying: Theoret icalandresearch perspectiv es:Springer; 1985.\\n63. MacDon aldM,Lord C,Ulrich D.Therelations hipofmotor skills andadaptive behavio rskills inyoung\\nchildren withautism spectrum disorders .ResAutism Spectr Disord. 2013; 7(11):1383 –90. https:// doi.\\norg/10.1016/ j.rasd.2013.07 .020 PMID: 25774214\\n64. HoyMB.Alexa, Siri,Cortana, andMore: AnIntroducti ontoVoice Assistant s.Med RefServ Q.2018; 37\\n(1):81–8. https://doi.or g/10.108 0/02763869. 2018.1404391 PMID: 293279 88\\nPLOS ONEEarly screening ofautism using cryfeatures\\nPLOS ONE |https://doi.or g/10.137 1/journal.po ne.02416 90December 10,2020 21/21', metadata={'source': 'papers/Asd_Cry_patterns_clean.txt'})],\n",
       " [Document(page_content='RESEARCH ARTICLE\\nDigital Behavioral Phenotyping Detects Atypical Pattern of Facial\\nExpression in Toddlers with Autism\\nKimberly L. H. Carpenter , Jordan Hahemi, Kathleen Campbell, Steven J. Lippmann, Jeffrey P. Baker,\\nHelen L. Egger, Steven Espinosa, Saritha Vermeer, Guillermo Sapiro, and Geraldine Dawson\\nCommonly used screening tools for autism spectrum disorder (ASD) generally rely on subjective caregiver questionnaires.\\nWhile behavioral observation is more objective, it is also expensive, time-consuming, and requires signi ﬁcant expertise to\\nperform. As such, there remains a critical need to develop feasible, scalable, and reliable tools that can characterize ASD\\nrisk behaviors. This study assessed the utility of a tablet-based behavioral assessment for eliciting and detecting one type\\nof risk behavior, namely, patterns of facial expression, in 104 toddlers (ASD N= 22) and evaluated whether such patterns\\ndifferentiated toddlers with and without ASD. The assessment consisted of the child sitting on his/her caregiver ’s lap and\\nwatching brief movies shown on a smart tablet while the embedded camera recorded the child ’s facial expressions. Com-\\nputer vision analysis (CVA) automatically detected and tracked facial landmarks, which were used to estimate head posi-tion and facial expressions (Positive, Neutral, All Other). Using CVA, speci ﬁc points throughout the movies were\\nidenti ﬁed that reliably differentiate between children with and without ASD based on their patterns of facial movement\\nand expressions (area under the curves for individual movies ranging from 0.62 to 0.73). During these instances, children\\nwith ASD more frequently displayed Neutral expressions compared to children without ASD, who had more All Other\\nexpressions. The frequency of All Other expressions was driven by non-ASD children more often displaying raised eye-brows and an open mouth, characteristic of engagement/interest. Preliminary results suggest computational coding of\\nfacial movements and expressions via a tablet-based assessment can detect differences in affective expression, one of the\\nearly, core features of ASD. Autism Res 2020, 00: 1 –12.© 2020 International Society for Autism Research and Wiley\\nPeriodicals LLC\\nLay Summary: This study tested the use of a tablet in the behavioral assessment of young children with autism. Children\\nwatched a series of developmentally appropriate movies and their facial expressions were recorded using the camera\\nembedded in the tablet. Results suggest that computational assessments of facial expressions may be useful in early detec-\\ntion of symptoms of autism.\\nKeywords: autism; risk behaviors; facial expressions; computer vision; early detection\\nIntroduction\\nAutism spectrum disorder (ASD) can be reliably diagnosed\\nas early as 24 months old and the risk signs can be\\ndetected as early as 6 –12 months old [Dawson &\\nBernier, 2013; Luyster et al., 2009]. Despite this, the aver-\\nage age of diagnosis in the United States remains around\\n4 years of age [Christensen et al., 2016]. While there is\\nmixed evidence for the stability of autism traits over early\\nchildhood [Bieleninik et al., 2017; Waizbard-Bartov\\net al., 2020], the delay in diagnosis can still impact timely\\nintervention during a critical window of development. Inresponse to this, in 2007 the American Academy of Pedi-\\natrics published guidelines supporting the need for all\\nchildren to be screened for ASD between 18- and24-months of age as part of their well-child visits [Myers,Johnson, & Council on Children With Disabilities, 2007].Current screening typically relies on caregiver report,such as the Modi ﬁed Checklist of ASD in Toddlers —\\nRevised with Follow-up (M-CHAT-R/F) (Robins\\net al., 2014). Evidence suggests that a two-tiered screen-\\ning approach, including direct observational assessmentof the child, improves the positive predictive value ofM-CHAT screening by 48% [Khowaja, Robins, &\\nFrom the Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke University School of Medicine, Dur-\\nham, North Carolina, USA (K.L.H.C., J.H., K.C., H.L.E., S.E., S.V., G.D.); Department of Electrical and Computer Engineering, Duke University, Dur ham,\\nNorth Carolina, USA (J.H., S.E.); Department of Pediatrics, University of Utah, Salt Lake City, Utah, USA (K.C.); Department of Population Health Sc i-\\nences, Duke University School of Medicine, Durham, North Carolina, USA (S.J.L.); Department of Pediatrics, Duke University School of Medicine, Dur -\\nham, North Carolina, USA (J.P.B.); NYU Langone Child Study Center, New York University, New York, New York, USA (H.L.E.); Departments of\\nBiomedical Engineering Computer Science, and Mathematics, Duke University, Durham, North Carolina, USA (G.S.); Duke Institute for Brain Sciences ,\\nDuke University, Durham, North Carolina, USA (G.D.)\\nReceived April 7, 2020; accepted for publication August 24, 2020Address for correspondence and reprints: Kimberly L. H. Carpenter, Duke Center for Autism and Brain Development, Department of Psychiatry and\\nBehavioral Sciences, Duke University School of Medicine, 2608 Erwin Rd #300, Durham, NC 27705. E-mail: kimberly.carpenter@duke.edu\\nPublished online 00 Month 2020 in Wiley Online Library (wileyonlinelibrary.com)DOI: 10.1002/aur.2391© 2020 International Society for Autism Research and Wiley Periodicals LLC\\nINSAR Autism Research 000: 1 –12, 2020 1\\nAdamson, 2017] and may reduce ethnic/racial disparities\\nin general screening [Guthrie et al., 2019]. Current toolsfor observational assessment of ASD signs in infants and\\ntoddlers, such as the Autism Observation Scale for Infants\\n(AOSI) and Autism Diagnostic Observation Schedule(ADOS), take substantial time and training to administer,resulting in a shortage of quali ﬁed diagnosticians to per-\\nform these observational assessments. As such, there\\nremains a critical need to develop feasible, scalable, andreliable tools that can characterize ASD risk behaviors\\nand identify those children who are most in need of\\nfollow-up by an ASD specialist. In an effort to address thiscritical need, we have embarked on a program of research\\nusing computer vision analysis (CVA) to develop tools for\\ndigitally phenotyping early emerging risk behaviors forASD [Dawson & Sapiro, 2019]. If successful, such digital\\nscreening tools have the opportunity to help existing\\npractitioners reach more children and assist in triagingboundary cases for review by specialists.\\nO n eo ft h ee a r l ye m e r g i n gs i g n so fA S Di sa\\ntendency to more often display a neutral facial expres-sion. This pattern is evident in the quality of facial\\nexpressions and in sharing emotional expressions\\nwith others [Adrien et al., 1993; Baranek, 1999;S. Clifford & Dissanayake, 2009; S. Clifford,\\nYoung, & Williamson, 2007; S. M. Clifford &\\nDissanayake, 2008; Maestro et al., 2002; Osterling, Daw-son, & Munson, 2002; Werner, Dawson, Osterling, &\\nDinno, 2000]. A restricted range of emotional expres-\\nsion and its integration with eye gaze (e.g., during socialreferencing) have been found to differentiate children\\nwith ASD from typically developing children, as well\\nas those who have other developmental delays, asearly as 12 months of age [Adrien et al., 1991;\\nS. Clifford et al., 2007; Filliter et al., 2015; Gangi,\\nIbanez, & Messinger, 2014; Nich ols, Ibanez, Foss-Feig, &\\nStone, 2014]. While core features of ASD vary by age,\\ncognitive ability, and language, one of the most stable\\nsymptoms from early childhood through adolescencesis increased frequency of neutral expression [Bal, Kim,\\nFok, & Lord, 2019]. As such, differences in facial affect\\nmay show utility in assessing early risk for ASD.\\nA recent meta-analysis of facial expression production\\nin autism found that individuals with ASD display facial\\nexpressions less often than non-ASD participants andthat, when they did display facial expressions, the expres-\\nsions occurred for shorter durations and were of different\\nquality than non-ASD individuals (Trevisan, Hoskyn, &Birmingham, 2018). Decreases in the frequency of both\\nemotional facial expressions and the sharing of those\\nexpressions with others has been demonstrated acrossnaturalistic interactions [Bieberich & Morgan, 2004;\\nCzapinski & Bryson, 2003; Dawson, Hill, Spencer,\\nGalpert, & Watson, 1990; Mcgee, Feldman, &Chernin, 1991; Snow, Hertzig, & Shapiro, 1987; Tantam,Holmes, & Cordess, 1993], in lab-based assessments such\\nas during the ADOS [Owada et al., 2018] or the AOSI[Filliter et al., 2015] and in response to emotion-eliciting\\nvideos [Trevisan, Bowering, & Birmingham, 2016]. Fur-\\nthermore, higher frequency of neutral expressions corre-lates with social impairment in children with ASD\\n[Owada et al., 2018] and differentiates them from chil-\\ndren with other delays [Bieberich & Morgan, 2004;Yirmiya, Kasari, Sigman, & Mundy, 1989]. As such, fre-\\nquency and duration of facial affect is a promising early\\nrisk marker for young children with autism.\\nPrevious research on atypical facial expressions in chil-\\ndren with ASD has relied on hand coding of facial expres-\\nsions, which is time intensive and often requiressigniﬁcant training [Bieberich & Morgan, 2004; S. Clifford\\net al., 2007; Dawson et al., 1990; Gangi et al., 2014;\\nMcgee et al., 1991; Nichols et al., 2014; Snowet al., 1987]. This approach is not scalable for use in gen-\\neral ASD risk screening or as a behavioral biomarker or\\noutcome assessment for use in large clinical trials. Assuch, the ﬁeld has moved toward automating the coding\\nof facial expressions. In one of the earliest studies of this\\napproach, Guha and colleagues demonstrated that chil-dren with ASD have atypical facial expressions when\\nmimicking others. However, their technology required\\nthe children to wear markers on their face for data cap-ture [Guha, Yang, Grossman, & Narayanan, 2018; Guha\\net al., 2015], which is both invasive and not scalable.\\nMore recently, several groups have applied non-invasiveCVA technology to measuring affect in older children\\nand adults with ASD within the laboratory setting\\n[Capriola-Hall et al., 2019; Owada et al., 2018; Samadet al., 2018]. This represents an important move toward\\nscalability as CVA approaches do not rely on the presence\\nof physical markers on the face to extract emotion infor-mation. Rather, CVA relies on videos of the individual in\\nwhich features around speci ﬁc regions on a face (e.g., the\\nmouth and eyes) are extracted. Notably, these featuresmirror those used by the manually rated facial affect cod-\\ning system (FACS) [Ekman, 1997]. Both our earlier work\\n[Hashemi et al., 2018] and that of others [Capriola-Hallet al., 2019] have shown good concordance between\\nhuman coding and CVA rating of facial emotions. Fur-\\nthermore, previous research in adults has demonstratedthat CVA can detect neutral facial expressions more reli-\\nably than human coders [Lewinski, 2015].\\nBuilding on previous work applying CVA in laboratory\\nsettings, we have developed a portable tablet-based tech-\\nnology that uses the embedded camera and automatic\\nCVA to code ASD risk behaviors in <10 min across a rangeof non-laboratory settings (e.g., pediatric clinics, at home,\\netc.). We developed a series of movies designed to capture\\nchildren ’s attention, elicit emotion in response to novel\\nand interesting events, and assess the toddler ’s ability to\\nsustain attention and share it with others. By embedding\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 2these movies in a fully automated system on a cost-\\neffective tablet whereby the elicited behaviors, in thiscase the frequency of different patterns of facial affect,\\nare automatically encoded with CVA, we aim to create a\\ntool that is objective, ef ﬁcient, and accessible. The cur-\\nrent analysis focuses on preliminary results supporting\\nthe utility of this tablet-based assessment for the detec-\\ntion of facial movement and affect in young children andthe use of facial affect to differentiate children with and\\nwithout ASD. Though facial affect is the focus of the cur-\\nrent analysis, the ultimate goal is to combine informationacross autism risk features collected through the current\\ndigital screening tool [e.g., delayed response to name as\\ndescribed in Campbell et al., 2019], to develop a risk scorebased on multiple behaviors [Dawson & Sapiro, 2019].\\nThis information could then be combined with addi-\\ntional measures of risk to enhance screening for ASD.\\nMethods\\nParticipants\\nParticipants were 104 children 16 –31 months of age\\n(Table 1). Children were recruited at their pediatric pri-\\nmary care visit by a research assistant embedded within\\nthe clinic or via referral from their physician, as well asthrough community advertisement ( N= 4 in the non-\\nASD group and N= 15 in the ASD group). For children\\nrecruited within the pediatric clinics, recruitmentoccurred at the 18- or 24-month well-child visit at the\\nsame time as they received standard screening for ASD\\nwith the M-CHAT-R/F. A total of 76% of the participantsrecruited in the clinic by a research assistant chose to par-\\nticipate. Of the participants who chose not to participate,\\n11% indicated that they were not interested in the study,whereas the remainder declined due to not having\\nenough time, having another child to take care of, want-ing to discuss with their partner, or their child was\\nalready too distressed after the physician visit. All chil-\\ndren who enrolled in the study found the procedureengaging enough that they were able to provide adequate\\ndata for analysis. Because the administration is very brief\\nand non-demanding, data loss was not a signi ﬁcant\\nproblem.\\nExclusionary criteria included known vision or hearing\\ndeﬁcits, lack of exposure to English at home, or insuf ﬁ-\\ncient English language skills for caregiver ’s informed con-\\nsent. Twenty-two children were diagnosed with ASD. The\\nnon-ASD comparison group ( N= 82) was comprised of\\n74 typically developing children and 8 children with a\\nnon-ASD delay, which was de ﬁned by a diagnosis of lan-\\nguage delay or developmental delay of clinical signi ﬁ-\\ncance suf ﬁcient to qualify for speech or developmental\\ntherapy as recorded in the electronic medical record. All\\ncaregivers/legal guardians gave written informed consent,and the study protocol was approved by the Duke Uni-\\nversity Health System IRB.\\nChildren recruited from the pediatric primary care\\nsettings received screening with a digital version of the\\nM-CHAT-R/F as part of a quality improvement study\\nongoing in the clinic [Campbell et al., 2017]. Participantsrecruited from the community received ASD screening\\nwith the digital M-CHAT-R/F prior to the tablet assess-\\nment. As part of their participation in the study, childrenwho either failed the M-CHAT-R/F or for whom there was\\ncaregiver or physician concern about possible ASD under-\\nwent diagnostic testing using the ADOS-Toddler (ADOS-T) [Luyster et al., 2009] conducted by a licensed psycholo-\\ngist or research-reliable examiner supervised by a licensed\\nTable 1. Sample Demographics\\nTypically developing ( N= 74; 71%) Non-ASD delay ( N= 8; 8%) ASD ( N= 22; 21%)\\nAge\\nMonths [mean (SD)] 21.7 (3.8) 23.9 (3.7) 26.2 (4.1)\\nSex\\nFemale 31 (42) 3 (38) 5 (23)\\nMale 43 (58) 5 (62) 17 (77)\\nEthnicity/race\\nAfrican American 10 (14) 1 (13) 3 (14)\\nCaucasian 46 (62) 2 (25) 10 (45)Hispanic 1 (1) 0 (0) 1 (4)\\nOther/unknown 17 (23) 5 (62) 8 (37)\\nInsurance\\na\\nMedicaid 11 (15) 1 (14) 6 (67)\\nNon-Medicaid 60 (85) 6 (86) 3 (33)\\nMCHAT resultb\\nPositive 1 (1) 0 (0) 18 (82)Negative 73 (99) 8 (100) 4 (18)\\naInsurance status was unknown for 17 (16%) of participants in this study.\\nbChildren for whom the MCHAT was negative but received and ASD diagnosis were referred for assessment due to concerns by either the parent or the\\nchild ’s physician.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 3psychologist. The mean ADOS-T score was 18.00\\n(SD = 4.67). A subset of the ASD children ( N= 13) also\\nreceived the Mullen Scales of Early Learning\\n(Mullen, 1995). The mean IQ based on the Early Learning\\nComposite Score for this subgroup was 63.58(SD = 25.95). None of the children in the non-ASD com-\\nparison group was administered the ADOS or Mullen.\\nChildren ’s demographic information was extracted\\nfrom the child ’s medical record or self-reported by the\\ncaregiver at their study visit. Children in the ASD group\\nwere, on average, 4 months younger than the compari-son group ( t = 4.64, P< 0.0001). Furthermore, as\\nwould be expected, there was a higher proportion of\\nmales in the ASD group than in the comparison group,though the difference was not statistically signi ﬁcant ( χ\\n2\\n[1, 104] = 2.60, P= 0.11). There were no differences in the\\nproportion of racial/ethnic minority children betweenthe two groups ( χ\\n2[1, 104] = 1.20, P= 0.27). When\\nlooking only at the children for which Medicaid status\\nwas known, there was no difference in the proportion ofchildren on Medicaid in the ASD and the non-ASD group\\n(χ\\n2[1, 87] = 1.82, P= 0.18).\\nStimuli and Procedure\\nA series of developmentally appropriate brief movies\\ndesigned to elicit affect and engage the child ’s attention\\nwere shown on a tablet while the child sat on a care-\\ngiver’s lap. The tablet was placed on a stand approxi-\\nmately 3 ft away from the child to prevent the child fromtouching the screen as depicted in previous publications\\n[Campbell et al., 2019; Dawson et al., 2018; Hashemi\\net al., 2015; Hashemi et al., 2018]. Movies consisted ofcascading bubbles (2 ×30 sec), a mechanical bunny\\n(66 sec), animal puppets interacting with each other\\n(68 sec), and a split screen showing a woman singingnursery rhymes on one side and dynamic, noise-making\\ntoys on the other side (60 sec; Fig. 1). These movies\\nincluded stimuli that have been used in previous studiesof ASD symptomatology [Murias et al., 2018], as well as\\ndeveloped speci ﬁcally for the current tablet-based tech-\\nnology to elicit autism symptoms, based on Dawsonet al. , Jones, Dawson, Kelly, Estes, and\\nWebb , Jones et al. , and Luyster et al. .At three points during the movies, the examiner located\\nbehind the child called the child ’s name.\\nPrior to the administration of the app, caregivers were\\nclearly instructed not to direct their child ’s attention or\\nin any way try to in ﬂuence the child ’s behavior during\\nthe assessment. Furthermore, if the caregiver began to tryto direct their child ’s attention, the examiner in the room\\nimmediately asked the caregiver to refrain from doing\\nso. If the caregiver persisted, this was noted on our valid-ity form and the administration would have been consid-\\nered invalid. Researchers stopped the task for one\\ncomparison participant due to crying. Researchersrestarted the task for three participants with ASD due to\\ndifﬁculty remaining in view of the tablet ’s camera for\\nmore than half of the ﬁrst video stimulus. If other family\\nmembers were present during the well-child visit, they\\nwere asked to stand behind the caregiver and child so as\\nto not distract the child during the assessment. Addition-ally, for children assessed during a well-child visit,\\nresearch assistants were instructed to collect data prior to\\nany planned shots or blood draws.\\nComputer Vision Analysis\\nThe frontal camera in the tablet recorded video through-\\nout the experiment at 1280 ×720 resolution and\\n30 frames per second. The CVA algorithm [Hashemi\\net al., 2018] ﬁrst automatically detected and tracked\\n49 facial landmarks on the child ’s face [De la Torre\\net al., 2015]. Head positions relative to the camera were\\nestimated by computing the optimal rotation parametersbetween the detected landmarks and a 3D canonical face\\nmodel [Fischler & Bolles, 1981]. A “not visible ”tag was\\nassigned to frames where the face was not detected or theface exhibited drastic yaw (>45\\n/C14from center). We\\nacknowledge that the current method used only indicates\\nwhether the child is oriented toward the stimulus anddoes not track eye movements. For each “visible ”frame,\\nthe probability of expressing three standard categories of\\nfacial expressions, Positive, Neutral (i.e., no active facial\\nFigure 1. Example of movie stimuli. Developmentally appropriate movies consisted of cascading bubbles, a mechanical bunny, animal\\npuppets interacting with each other, and a split screen showing a woman singing nursery rhymes on one side and dynamic, noise-makingtoys on the other side.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 4action unit), or Other (all other expressions), was\\nassigned [Hashemi et al., 2018]. The model for automaticfacial expression is an extension of the pose-invariant\\nand cross-modal dictionary learning approach originally\\ndescribed in Hashemi et al. . During training, thedictionary representation is setup to map facial informa-\\ntion between 2D and 3D modalities and is then able to\\ninfer discriminative facial information for facial expres-sions recognition even when only 2D facial images are\\navailable at deployment. For training, data from Bing-\\nhamton University 3D Facial Expression database [Yin,Wei, Sun, Wang, & Rosato, 2006] were used, along with\\nsynthesized faces images with varying poses [see Hashemi\\net al., 2015 for synthesis details]. Extracted image featuresand distances between a subset of facial landmarks were\\nused as facial features to learn the robust dictionary.\\nLastly, using the inferred discriminative 3D and frontal2D facial features, a multiclass support vector machine\\n[Chang & Lin, 2011] was trained to classify the different\\nfacial expressions.\\nIn recent years, there has been progress on\\nautomatic facial expression analysis of both children\\nand toddlers [Dys & Malti, 2016; Gadea, Aliño,Espert, & Salvador, 2015; Haines et al., 2019;\\nLoBue & Thrasher, 2014; Messinger, Mahoor, Chow,\\n& Cohn, 2009]. In addition to this, we have previouslyvalidated our CVA algorithm against expert human rater\\ncoding of facial affect in a subsample of 99 video record-\\nings across 33 participants (ASD = 15, non-ASD = 18).This represents 20% of the non-ASD sample and a mat-\\nched group from the ASD sample. The selection of partici-\\npants for this previously published validation study wasbased on age distribution to ensure representation across\\nthe range of ages for both non-ASD and ASD groups. This\\nprevious work showed strong concordance between CVA-and human-rated coding of facial emotion in this data\\nset, with high precision, recall, and F1 scores of 0.89,\\n0.90, and 0.89, respectively [Hashemi et al., 2018].\\nStatistical Approach\\nFor each video frame, the CVA algorithm produces a\\nprobability value for each expression (Positive, Neutral,\\nOther). We calculated the mean of the probability values\\nfor each of the three expression types within non-overlapping 90-frame (3 sec) intervals, excluding frames\\nwhen the face was not visible. A 3-sec window was\\nselected as it provided us with a continuous distributionof the emotion probabilities, while still being within the\\n0.5–4-sec window of a macroexpression (Ekman, 2003).\\nAdditionally, for each of the name call events weremoved the time window starting at cue for the name\\ncall prompt through the point where 75% of the audible\\nname calls actually occurred, plus 150 frames (5 sec). Thiswindow was selected based on our previous study[Campbell et al., 2019] showing that orienting tended to\\noccur within a few seconds after a name call. We calcu-lated the proportion of frames the child was not attend-\\ning to the movie stimulus, based on the “visible ”and\\n“not visible ”tags described above, within each 90-frame\\ninterval, excluding name call periods. Thus, for each\\nchild, we generated four variables (mean probabilities of\\nPositive, Neutral, or Other; and proportion of frames notattending) for each 90-frame interval within each of the\\nﬁve movies.\\nTo evaluate differences between ASD and non-ASD\\nchildren at regular intervals throughout the assessment,\\nweﬁt a series of bivariate logistic regressions to obtain\\nthe odds ratios for the associations between the meanexpression probability or attending proportion during a\\ngiven interval, parameterized as increments of 10%\\npoints, and ASD diagnosis. We then ﬁt a series of multi-\\nvariable logistic regression models, separately for each\\nmovie and variable, which included parameters for each\\nof the 3-sec intervals within the movie to predict ASDdiagnosis. Given the large number of intervals relative to\\nthe small sample size, we used a Least Absolute Shrinkage\\nand Selection Operator (LASSO) penalized regressionapproach [Tibshirani, 1996] to select a parsimonious set\\nof parameters representing the intervals within each\\nmovie and expression type that were most predictive ofASD diagnosis.\\nFor each of the ﬁve movies, we then combined the\\nLASSO-selected interval parameters into a full logisticmodel. When more than one expression parameter was\\nselected for a given interval, we selected the one with the\\nstronger odds ratio estimate. Analyses were conductedwith and without age as a covariate. Since the small study\\nsize precluded having separate training and validation\\nsets, we used leave-one-out cross-validation to assessmodel performance. Receiver-operating characteristics\\n(ROC) curves were plotted and the c-statistic for the area\\nunder the ROC curve was calculated for each movie.\\nResults\\nFigure 2 depicts the odds ratio analysis using the“Rhymes and Toys ”movie as one illustrative example. As\\nshown by the variability in the odds ratio estimates, some\\nparts of the movies elicited strongly differential responsesin certain patterns of expression (blue window, Fig. 2),\\nwhile in other sections, there were not substantial differ-\\nences between the two groups (green window, Fig. 2).Overlaid on the plot are the odds ratios and con ﬁdence\\nbands for the interval parameters selected by the\\nexpression-speci ﬁc LASSO models. These selected param-\\neters were then used in the movie-level logistic models\\nfor which we calculated classi ﬁcation metrics.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 5Figure 3 compares the ROC curves for the ﬁveﬁnal\\nmovie-level logistic models after leave-one-out cross-\\nvalidation. ROC curves analyses were performed for eachvideo individually. The model for the “Rhymes ”movie\\nyielded the strongest predictive ability, with an area\\nunder the curve (AUC) of 0.73 (95% con ﬁdence interval\\n[CI] 0.59 –0.86), followed by the “Puppets ”(AUC = 0.67;\\n95% CI 0.53 –0.80) and the “Bunny ”(AUC = 0.66; 95%CI 0.51 –0.82) videos. Finally, the two “Bubbles ”movies\\nthat bookend the stimulus sets were the least predictive,\\nwith AUCs of 0.62 (95% CI 0.49 –0.74) and 0.64 (95% CI\\n0.51–0.76), respectively. Because there was a signi ﬁcant\\ndifference in age between the ASD and non-ASD com-\\nparison groups, we ran a second set of ROC analyses\\nwhere age was included as a covariate, shown in Table 2.Results remained signi ﬁcant after including the age\\ncovariate.\\nGiven the preponderance for the Other emotional cate-\\ngory in our non-ASD comparison group, we explored:\\n(a) what speci ﬁc facial movements are driving this cate-\\ngory of Other expressions and (b) how it differs from theNeutral expression category. We focused on analyzing\\nmovements of facial landmarks and head pose angles and\\nhow they differ between the facial expression categories.Our CVA algorithm aligns the facial landmarks of the\\nchild to a canonical face model through an af ﬁne trans-\\nformation, which normalizes the landmark locationsacross all video frames to a common space. This normali-\\nzation process is commonly used across CVA tasks related\\nto facial analysis as it allows one to analyze/comparelandmark locations across different frames or partici-\\npants. With this alignment step, we were able to quantify\\nthe distances between the eye corners and the corners ofthe eyebrows, the vertical distance between the inner lip\\npoints, and the vertical distance between the outer lip\\npoints (Fig. 4, right). To interpret features that differenti-ated the Neutral from the Other facial expression cate-\\ngory, we assessed differences between these facial\\nlandmark distances of a given child when they were pre-dominately expressing Neutral versus Other facial expres-\\nsions. We also included yaw and pitch head pose angles\\nsince they may play a role in the alignment process.\\nFigure 2. Time series of odds ratios for the association between the mean expression probability or proportion attending and ASD\\ndiagnosis. Using the “Rhymes ”movie as one illustrative example, lines depict the odds of meeting criteria for ASD (OR > 1) or being in\\nthe non-ASD comparison group (OR < 1) for each of the outcomes of interest for each 3 sec time bin across the movie. Points with errorbars are intervals that were selected by the LASSO regression models and included in the ﬁnal logistic model. The blue window depicts a\\nsegment of the movie where there were differential emotional responses between the ASD and non-ASD children. The green window\\ndepicts a segment of the movie in which there was no difference in emotional responses between the groups.\\nFigure 3. Receiver-operating characteristics (ROC) curves. ROC\\ncurves were calculated for predictive ability of expression-speci ﬁc\\nLASSO selected interval parameters for facial expressions andattention to stimulus for each movie independently.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 6We focused in on three stimuli in which participants\\nexhibited high probabilities of Other expressions,\\nnamely, the ﬁrst Bubbles, Puppets, and Rhymes and Toys\\nvideos. Out of the 104 participants, all exhibited frames\\nwhere both Neutral and Other facial expressions were\\ndominant (probability of expression over 60%). A Wilsonsigned-ranks test, reported as (median difference,\\nPvalue), indicated that within individual participants for\\neach diagnostic group, the median differences betweenthe distances of Other versus Neutral facial expressions\\nwere signi ﬁcantly higher for inner right eyebrows (non-\\nASD: diff = 2.5, P< 8.1e-10; ASD: diff = 4.0, P< 1.1e-4),\\ninner left eyebrows (non-ASD: diff = 2.4, P< 1.5e-9; ASD:\\ndiff = 3.6, P< 1.4e-4), outer right eyebrows (non-ASD:\\ndiff = 1.2, P< 1.0e-5; ASD: diff = 2.3, P< 1.6e-4), outer left\\neyebrows (non-ASD: diff = 0.9, P< 3.4e-6; ASD: diff = 1.4,\\nP< 1.4e-3), and mouth height (non-ASD: diff = 1.5,\\nP< 1.0e-3), as well as for pitch head pose angles (non-\\nASD: diff = 2.8, P< 8.7e-10; ASD: diff = 4.4, P< 1.2e-3);\\nbut not for eye heights, lips parting, nor yaw head poseangles.\\nDiscussion\\nThe present study evaluated an application administeredon a tablet that was comprised of carefully designed\\nmovies that elicited affective expressions combined with\\nCVA of recorded behavioral responses to identify patternsof facial movement and emotional expression that differ-\\nentiate toddlers with ASD from those without ASD. We\\ndemonstrated that the movies elicited a range of affectivefacial expressions in both groups. Furthermore, using\\nCVA we found children with ASD were more likely to dis-\\nplay a neutral expression than children without ASDwhen watching this series of videos, and the patterns of\\nfacial expressions elicited during speci ﬁc parts of the\\nmovies differed between the two groups. We believe thisﬁnding has strong face validity that rests on both\\nresearch and clinical observations of a restricted range of\\nfacial expression in children with autism. Furthermore,this replicates a previous ﬁnding of our group reporting\\nincreased frequency of neutral expression in\\nyoung children who screened positive on the M-CHAT[Egger et al., 2018]. Together, these preliminary resultssupport the use of engaging brief movies shown on a\\ncost-effective tablet, combined with automated CVAbehavioral coding, as an objective and feasible tool for\\nmeasuring an early emerging symptom of ASD, namely,\\nincreased frequency of neutral facial expressions.\\nWhile the predictive power of emotional expression in\\nsome of the videos varied, all but one represent a medium\\neffect, equivalent to Cohen ’sd= 0.5 or greater [Rice &\\nHarris, 2005]. Overall, the best predictor from our battery\\nof videos is the “Rhymes ”video, which had an AUC with\\na large effect size (equivalent to d> 0.8). While this may\\nsuggest that presenting the “Rhymes ”video alone is suf ﬁ-\\ncient for differentiating between the ASD and non-ASD\\ngroups, we caution readers from coming to this conclu-sion for two reasons: First, it is possible that, had we had\\na larger sample, the other videos would have had a larger\\neffect. Second, we anticipate that there will be variabilityin the ASD group with regard to which features a single\\nchild will express and different videos may be better\\nsuited to elicit different features in any given individual.As such, we believe that it is important to understand\\nhow each independent feature, in this case facial affect,\\nperforms across the different videos so that we can beingto build better predictive models from combinations of\\nfeatures [e.g., facial affect and postural sway as described\\nin Dawson et al., 2018].\\nTo further understand the difference of facial expres-\\nsion in the non-ASD group as compared to our ASD sam-\\nple, we explored the facial landmarks differentiatingbetween the Other facial expression category that domi-\\nnated the non-ASD control group versus the Neutral\\nfacial expression, which was more common in the ASDgroup. Through this analysis, we identi ﬁed the features of\\nraised eyebrows and open mouth to play a role in dis-\\ncriminating between the Other vs. Neutral categories.This facial pattern is consistent with an engaged/inter-\\nested look displayed when a child is actively watching, as\\ndescribed in young children by Sullivan and Lewis .It is interesting to note a raised pitch angle was also statis-\\ntically signi ﬁcant. Since the median difference of this\\nangle between the two facial expression is small (3.2\\n/C14),\\nthis may be a natural movement of raising one ’s\\neyebrows.\\nOur results need to be considered in light of several\\nlimitations. First, the CVA models of facial expressions\\nused in the current study were trained on adult faces\\n[Hashemi et al., 2018]. Despite this, our previous ﬁndings\\nwith young children demonstrate good concordance\\nbetween human and CVA coding on the designation of\\nfacial expressions [Hashemi et al., 2018]. Furthermore,the Other facial expression category includes all non-\\npositive or negative expressions. As such, even though\\nwe were able to determine the predominant feature driv-ing those expressions was the raised eyebrows, which is\\nin line with our observations from watching the movies,Table 2. Comparison of ASD Versus Non-ASD: Area Under the\\nCurve (AUC) Analyses\\nAUC without covariates AUC with age in the model\\nBubbles 1 0.62 0.75\\nBunny 0.66 0.81\\nPuppets 0.67 0.78\\nRhymes 0.73 0.83Bubbles 2 0.64 0.79\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 7it is possible that there are a combination of facial expres-\\nsions in the non-ASD group driving this designation.\\nFuture studies will need to train on the engaged/inter-ested facial expression speci ﬁcally and test the robustness\\nof this ﬁnding. Additionally, though we have previously\\ndemonstrated good reliability between our CVA algo-rithms and human coding of emotions [Hashemi\\net al., 2015], future validation of our CVA analysis of\\nemotional facial expressions in larger datasets is currentlyunderway. Second, using the LASSO statistical approach\\nmeans our model may not select all features that have dif-\\nferentiating information. However, we selected thisapproach because it minimizes over ﬁtting the model.\\nThird, our sample size was relatively small and we do not\\nhave separate training and testing samples. To accountfor this, we applied cross-validation on the ROC curves\\neven though this decreased the performance metrics of\\nthe model. This suggests that our ROC results are poten-tially conservative. Finally, our comparison group con-\\ntains both children with typical development and\\nchildren with non-ASD developmental delays, a factorthat can be viewed as both a weakness and a strength.\\nPrevious research has demonstrated that increased fre-\\nquency of neutral expressions does differentiate childrenwith ASD from those with other developmental delays\\n[Bieberich & Morgan, 2004; Yirmiya et al., 1989]. How-\\never, due to the small sample size of children with non-ASD developmental delays, we were unable to directly\\ntest this in our data. Furthermore, because only a subset\\nof the sample received an assessment of cognitive ability,it is possible that there were additional children in the\\nnon-ASD comparison group that also had a developmen-\\ntal delay that was undetected. Ongoing research in a pro-spective, longitudinal study with larger samples is\\nunderway to further parse the ability of our CVA tools to\\ndifferentiate between children with ASD, children with anon-ASD developmental delay and/or attention de ﬁcit\\nhyperactivity disorder, and typically developing children.\\nWhile a difference in facial expression is one core fea-\\nture of ASD, the heterogeneity in ASD means we do not\\nexpect all children with ASD to display this sign of ASD.\\nAs such, our next step is to combine the current resultswith other measures of autism risk assessed through the\\ncurrent digital screening tool, including response to\\nname [Campbell et al., 2019], postural sway [Dawsonet al., 2018], and differential vocalizations [Tenenbaum\\net al., 2020], among other features, to develop a risk score\\nbased on multiple behaviors [Dawson & Sapiro, 2019].Since no one child is expected to display every risk behav-\\nior, a goal is to determine thresholds based on the total\\nnumber of behaviors, regardless of which combination ofbehaviors, to asses for risk. This is similar to what is done\\nin commonly used screening and diagnostic tools, such\\nas the M-CHAT [Robins et al., 2014], Autism Diagnostic\\nFigure 4. Analysis of Other Facial Expression. The 4 panels on the left depict heat maps of aligned landmarks across ASD and non-ASD\\nparticipants when they were exhibiting Neutral and other facial expressions (the color bar indicates proportion of frames where land-marks were displayed in a given image location). The single panel on the right is an example of the landmark distances explored.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 8Interview [Lord, Rutter, & Le Couteur, 1994], and ADOS\\n[Gotham, Risi, Pickles, & Lord, 2007; Lord et al., 2000].\\nIn summary, we evaluated an integrated, objective tool\\nfor the elicitation and measurement of facial movements\\nand expressions in toddlers with and without ASD. Thecurrent study adds to a body of research supporting digi-\\ntal behavioral phenotyping as a viable method for\\nassessing autism risk behaviors. Our goal is to furtherdevelop and validate this tool so that it can eventually be\\nused within the context of current standard of care to\\nenhance autism screening in pediatric populations.\\nAcknowledgments\\nFunding for this work was provided by NIH R01-MH120093 (Sapiro, Dawson PIs), NIH RO1-MH121329\\n(Dawson, Sapiro PIs), NICHD P50HD093074 (Dawson,\\nKollins, PIs), Simons Foundation (Sapiro, Dawson, PIs),Duke Department of Psychiatry and Behavioral Sciences\\nPRIDe award (Dawson, PI), Duke Education and Human\\nDevelopment Initiative, Duke-Coulter Translational Part-nership Grant Program, National Science Foundation, a\\nStylli Translational Neuroscience Award, and the Depart-\\nment of Defense. Some of the stimuli used for the movieswere created by Geraldine Dawson, Michael Murias, and\\nSara Webb at the University of Washington. This work\\nwould not have been possible without the help of Eliza-beth Glenn, Elizabeth Adler, and Samuel Marsan. We also\\ngratefully acknowledge the participation of the children\\nand families in this study. Finally, we could not havecompleted this study without the assistance and collabo-\\nration of Duke pediatric primary care providers.\\nConﬂict of interests\\nGuillermo Sapiro has received basic research gifts from\\nAmazon, Google, Cisco, and Microsoft and is a consul-tant for Apple and Volvo. Geraldine Dawson is on the Sci-\\nentiﬁc Advisory Boards of Janssen Research and\\nDevelopment, Akili, Inc., LabCorp, Inc., Tris Pharma, andRoche Pharmaceutical Company, a consultant for Apple,\\nInc, Gerson Lehrman Group, Guidepoint, Inc., Teva Phar-\\nmaceuticals, and Axial Ventures, has received grantfunding from Janssen Research and Development, and is\\nCEO of DASIO, LLC (with Guillermo Sapiro). Dawson\\nreceives royalties from Guilford Press, Springer, andOxford University Press. Dawson, Sapiro, Carpenter,\\nHashemi, Campbell, Espinosa, Baker, and Egger helped\\ndevelop aspects of the technology that is being used inthe study. The technology has been licensed and Daw-\\nson, Sapiro, Carpenter, Hashemi, Espinosa, Baker, Egger,\\nand Duke University have bene ﬁtedﬁnancially.References\\nAdrien, J. L., Faure, M., Perrot, A., Hameury, L., Garreau, B.,\\nBarthelemy, C., & Sauvage, D. (1991). Autism and familyhome movies: Preliminary ﬁndings. Journal of Autism and\\nDevelopmental Disorders, 21(1), 43 –49.\\nAdrien, J. L., Lenoir, P., Martineau, J., Perrot, A., Hameury, L.,\\nLarmande, C., & Sauvage, D. (1993). Blind ratings of earlysymptoms of autism based upon family home movies. Jour-\\nnal of the American Academy of Child and Adolescent Psy-\\nchiatry, 32(3), 617 –626. https://doi.org/10.1097/00004583-\\n199305000-00019\\nBal, V. H., Kim, S. H., Fok, M., & Lord, C. (2019). Autism spec-\\ntrum disorder symptoms from ages 2 to 19 years: Implicationsfor diagnosing adolescents and young adults. AutismResearch, 12(1), 89 –99. https://doi.org/10.1002/aur.2004\\nBaranek, G. T. (1999). Autism during infancy: a retrospective\\nvideo analysis of sensory-motor and social behaviors at 9 –12\\nmonths of age. Journal of Autism and Developmental Disor-ders, 29(3), 213 –224.\\nBieberich, A. A., & Morgan, S. B. (2004). Self-regulation and\\naffective expression during play in children with autism orDown syndrome: A short-term longitudinal study. Journal ofAutism and Developmental Disorders, 34(4), 439 –448.\\nhttps://doi.org/10.1023/b:jadd.0000037420.16169.28\\nBieleninik, Ł., Posserud, M.-B., Geretsegger, M., Thompson, G.,\\nElefant, C., & Gold, C. (2017). Tracing the temporal stabilityof autism spectrum diagnosis and severity as measured by theAutism Diagnostic Observation Schedule: A systematic reviewand meta-analysis. PLoS One, 12(9), e0183160 –e0183160.\\nhttps://doi.org/10.1371/journal.pone.0183160\\nCampbell, K., Carpenter, K. L., Hashemi, J., Espinosa, S.,\\nMarsan, S., Borg, J. S., …Dawson, G. (2019). Computer vision\\nanalysis captures atypical attention in toddlers with autism.Autism, 23(3), 619 –628. https://doi.org/10.1177/\\n1362361318766247\\nCampbell, K., Carpenter, K. L. H., Espinosa, S., Hashemi, J.,\\nQiu, Q., Tepper, M., …Dawson, G. (2017). Use of a Digital\\nModi ﬁed Checklist for Autism in Toddlers —Revised with\\nfollow-up to improve quality of screening for autism. TheJournal of Pediatrics, 183, 133 –139.e1. https://doi.org/10.\\n1016/j.jpeds.2017.01.021\\nCapriola-Hall, N. N., Wieckowski, A. T., Swain, D., Tech, V.,\\nAly, S., Youssef, A., …White, S. W. (2019). Group differences\\nin facial emotion expression in autism: Evidence for the util-ity of machine classi ﬁcation. Behavior Therapy, 50(4),\\n828–838. https://doi.org/10.1016/j.beth.2018.12.004\\nChang, C.-C., & Lin, C.-J. (2011). LIBSVM: A library for support\\nvector machines. ACM Transactions on Intelligent Systemsand Technology (TIST), 2(3), 1 –27.\\nChristensen, D. L., Baio, J., Braun, K. V., Bilder, D., Charles, J.,\\nConstantino, J. N., …Yeargin-Allsopp, M. (2016). Prevalence\\nand characteristics of autism spectrum disorder among chil-dren aged 8 years —Autism and Developmental Disabilities\\nMonitoring Network, 11 Sites, United States, 2012. MMWRSurveillance Summaries, 65(SS-3), 1 –23.\\nClifford, S., & Dissanayake, C. (2009). Dyadic and triadic behav-\\niours in infancy as precursors to later social responsiveness inyoung children with autistic disorder. Journal of Autism and\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 9Developmental Disorders, 39(10), 1369 –1380. https://doi.\\norg/10.1007/s10803-009-0748-x\\nClifford, S., Young, R., & Williamson, P. (2007). Assessing the\\nearly characteristics of autistic disorder using video analysis.Journal of Autism and Developmental Disorders, 37(2),301–313. https://doi.org/10.1007/s10803-006-0160-8\\nClifford, S. M., & Dissanayake, C. (2008). The early development\\nof joint attention in infants with autistic disorder using homevideo observations and parental interview. Journal of Autismand Developmental Disorders, 38(5), 791 –805. https://doi.\\norg/10.1007/s10803-007-0444-7\\nCzapinski, P., & Bryson, S. (2003). Reduced facial muscle move-\\nments in autism: Evidence for dysfunction in the neuromus-\\ncular pathway? Brain and Cognition, 51(2), 177 –179.\\nDawson, G., & Bernier, R. (2013). A quarter century of progress\\non the early detection and treatment of autism spectrumdisorder. Development and Psychopathology, 25(4 Pt 2),1455–1472. https://doi.org/10.1017/S0954579413000710\\nDawson, G., Campbell, K., Hashemi, J., Lippmann, S. J.,\\nSmith, V., Carpenter, K., …Sapiro, G. (2018). Atypical pos-\\ntural control can be detected via computer vision analysis intoddlers with autism spectrum disorder. Scienti ﬁc Reports, 8\\n(1), 17008. https://doi.org/10.1038/s41598-018-35215-8\\nDawson, G., Hill, D., Spencer, A., Galpert, L., & Watson, L.\\n(1990). Affective exchanges between young autistic-childrenand their mothers. Journal of Abnormal Child Psychology, 18(3), 335 –345. https://doi.org/10.1007/Bf00916569\\nDawson, G., & Sapiro, G. (2019). Potential for digital behavioral\\nmeasurement tools to transform the detection and diagnosisof autism spectrum disorder. JAMA Pediatrics, 173(4),305–306. https://doi.org/10.1001/jamapediatrics.2018.5269\\nDawson, G., Toth, K., Abbott, R., Osterling, J., Munson, J.,\\nEstes, A., & Liaw, J. (2004). Early social attention impairmentsin autism: social orienting, joint attention, and attention todistress. Developmental Psychology, 40(2), 271 –283. https://\\ndoi.org/10.1037/0012-1649.40.2.271\\nDe la Torre, F., Chu, W. S., Xiong, X., Vicente, F., Ding, X., &\\nCohn, J. (2015). IntraFace. IEEE International Conference on\\nAutomatic Face Gesture Recognition Workshops . (pp. 1 –8).\\nLjubljana, Slovenia: IEEE. https://doi.org/10.1109/fg.2015.7163082\\nDys, S. P., & Malti, T. (2016). It ’s a two-way street: Automatic\\nand controlled processes in children ’s emotional responses\\nto moral transgressions. Journal of Experimental Child Psy-chology, 152, 31 –40. https://doi.org/10.1016/j.jecp.2016.\\n06.011\\nEgger, H. L., Dawson, G., Hashemi, J., Carpenter, K. L. H.,\\nEspinosa, S., Campbell, K., …Sapiro, G. (2018). Automatic\\nemotion and attention analysis of young children at home: AResearchKit autism feasibility study. NPJ Digital Medicine, 1(1), 20. https://doi.org/10.1038/s41746-018-0024-6\\nEkman, P. (2003). Emotions revealed (2nd ed.). New York, NY:\\nTimes Books.\\nEkman, R. (1997). What the face reveals: Basic and applied stud-\\nies of spontaneous expression using the Facial Action CodingSystem (FACS). New York, NY: Oxford University Press.\\nFilliter, J. H., Longard, J., Lawrence, M. A., Zwaigenbaum, L.,\\nBrian, J., Garon, N., …Bryson, S. E. (2015). Positive affect in\\ninfant siblings of children diagnosed with autism spectrumdisorder. Journal of Abnormal Child Psychology, 43(3),\\n567–575. https://doi.org/10.1007/s10802-014-9921-6\\nFischler, M. A., & Bolles, R. C. (1981). Random sample consen-\\nsus: A paradigm for model ﬁtting with applications to image\\nanalysis and automated cartography. Communications of theAssociation for Computing Machinery, 24(6), 381 –395.\\nhttps://doi.org/10.1145/358669.358692\\nGadea, M., Aliño, M., Espert, R., & Salvador, A. (2015). Deceit\\nand facial expression in children: The enabling role of the“poker face ”child and the dependent personality of the\\ndetector. Frontiers in Psychology, 6, 1089 –1089. https://doi.\\norg/10.3389/fpsyg.2015.01089\\nGangi, D. N., Ibanez, L. V., & Messinger, D. S. (2014). Joint atten-\\ntion initiation with and without positive affect: Risk group\\ndifferences and associations with ASD symptoms. Journal ofAutism and Developmental Disorders, 44(6), 1414 –1424.\\nhttps://doi.org/10.1007/s10803-013-2002-9\\nGotham, K., Risi, S., Pickles, A., & Lord, C. (2007). The Autism\\nDiagnostic Observation Schedule: Revised algorithms forimproved diagnostic validity. Journal of Autism and Develop-mental Disorders, 37(4), 613 –627. https://doi.org/10.1007/\\ns10803-006-0280-1\\nGuha, T., Yang, Z., Grossman, R. B., & Narayanan, S. S. (2018).\\nA computational study of expressive facial dynamics inchildren with autism. IEEE Transactions on AffectiveComputing, 9(1), 14 –20. https://doi.org/10.1109/taffc.2016.\\n2578316\\nGuha, T., Yang, Z., Ramakrishna, A., Grossman, R. B., Darren, H.,\\nLee, S., & Narayanan, S. S. (2015). On quantifying facialexpression-related atypicality of children with autism spec-trum disorder. Proceedings of IEEE International Conferenceon Acoustics, Speech, and Signal Processing, 2015, 803 –807.\\nhttps://doi.org/10.1109/icassp.2015.7178080\\nGuthrie, W., Wallis, K., Bennett, A., Brooks, E., Dudley, J.,\\nGerdes, M., …Miller, J. S. (2019). Accuracy of autism screen-\\ning in a large pediatric network. Pediatrics, 144(4),e20183963. https://doi.org/10.1542/peds.2018-3963\\nHaines, N., Bell, Z., Crowell, S., Hahn, H., Kamara, D.,\\nMcDonough-Caplan, H., …Beauchaine, T. P. (2019). Using\\nautomated computer vision and machine learning to codefacial expressions of affect and arousal: Implications for emo-tion dysregulation research. Development and Psychopathol-ogy, 31(3), 871 –886. https://doi.org/10.1017/S09545794\\n19000312\\nHashemi, J., Campbell, K., Carpenter, K., Harris, A., Qiu, Q.,\\nTepper, M., …Calderbank, R. (2015). A scalable app for mea-\\nsuring autism risk behaviors in young children: A technical validity\\nand feasibility study . Paper presented at the Proceedings of the\\n5th EAI International Conference on Wireless Mobile Com-munication and Healthcare, Dublin, Ireland.\\nHashemi, J., Dawson, G., Carpenter, K. L. H., Campbell, K.,\\nQiu, Q., Espinosa, S., …Sapiro, G. (2018). Computer vision\\nanalysis for quanti ﬁcation of autism risk behaviors. IEEE\\nTransactions on Affective Computing, 1 –1. https://doi.org/\\n10.1109/taffc.2018.2868196\\nJones, E. J., Dawson, G., Kelly, J., Estes, A., & Webb, S. J. (2017).\\nParent-delivered early intervention in infants at risk for ASD:Effects on electrophysiological and habituation measures ofsocial attention. Autism Research, 10(5), 961 –972.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 10Jones, E. J., Venema, K., Earl, R., Lowy, R., Barnes, K., Estes, A., …\\nWebb, S. (2016). Reduced engagement with social stimuli in6-month-old infants with later autism spectrum disorder: Alongitudinal prospective study of infants at high familial risk.Journal of Neurodevelopmental Disorders, 8(1), 7.\\nKhowaja, M., Robins, D. L., & Adamson, L. B. (2017). Utilizing\\ntwo-tiered screening for early detection of autism spectrumdisorder. Autism, 22, 881 –890. https://doi.org/10.1177/\\n1362361317712649\\nLewinski, P. (2015). Automated facial coding software outper-\\nforms people in recognizing neutral faces as neutral fromstandardized datasets. Frontiers in Psychology, 6, 1386.https://doi.org/10.3389/fpsyg.2015.01386\\nLoBue, V., & Thrasher, C. (2014). The Child Affective Facial\\nExpression (CAFE) set: Validity and reliability from untrainedadults. Frontiers in Psychology, 5, 1532. https://doi.org/10.3389/fpsyg.2014.01532\\nLord, C., Risi, S., Lambrecht, L., Cook, E. H., Jr., Leventhal, B. L.,\\nDiLavore, P. C., …Rutter, M. (2000). The autism diagnostic\\nobservation schedule-generic: A standard measure of socialand communication de ﬁcits associated with the spectrum of\\nautism. Journal of Autism and Developmental Disorders, 30(3), 205 –223.\\nLord, C., Rutter, M., & Le Couteur, A. (1994). Autism Diagnostic\\nInterview-Revised: A revised version of a diagnostic interviewfor caregivers of individuals with possible pervasive develop-mental disorders. Journal of Autism and Developmental Dis-orders, 24(5), 659 –685.\\nLuyster, R., Gotham, K., Guthrie, W., Cof ﬁng, M., Petrak, R.,\\nPierce, K., …Lord, C. (2009). The Autism Diagnostic Observa-\\ntion Schedule-toddler module: A new module of astandardized diagnostic measure for autism spectrum disor-ders. Journal of Autism and Developmental Disorders, 39(9),1305–1320. https://doi.org/10.1007/s10803-009-0746-z\\nMaestro, S., Muratori, F., Cavallaro, M. C., Pei, F., Stern, D.,\\nGolse, B., & Palacio-Espasa, F. (2002). Attentional skills duringtheﬁrst 6 months of age in autism spectrum disorder. Journal\\nof the American Academy of Child and Adolescent Psychia-try, 41(10), 1239 –1245. https://doi.org/10.1097/00004583-\\n200210000-00014\\nMcgee, G. G., Feldman, R. S., & Chernin, L. (1991). A compari-\\nson of emotional facial display by children with autism andtypical preschoolers. Journal of Early Intervention, 15(3),237–245. https://doi.org/10.1177/105381519101500303\\nMessinger, D. S., Mahoor, M. H., Chow, S. M., & Cohn, J. F.\\n(2009). Automated measurement of facial expression in\\ninfant-mother interaction: A pilot study. Infancy, 14(3),\\n285–305. https://doi.org/10.1080/15250000902839963\\nMullen, E. M. (1995). Mullen scales of early learning. Circle\\nPines, MN: American Guidance Service Inc.\\nMurias, M., Major, S., Compton, S., Buttinger, J., Sun, J. M.,\\nKurtzberg, J., & Dawson, G. (2018). Electrophysiological bio-markers predict clinical improvement in an open-label trialassessing ef ﬁcacy of autologous umbilical cord blood for treat-\\nment of autism. Stem Cells Translational Medicine, 7(11),783–791. https://doi.org/10.1002/sctm.18-0090\\nMyers, S. M., Johnson, C. P., & Council on Children With Dis-\\nabilities. (2007). Management of children with autismspectrum disorders. Pediatrics, 120(5), 1162 –1182. https://\\ndoi.org/10.1542/peds.2007-2362\\nNichols, C. M., Ibanez, L. V., Foss-Feig, J. H., & Stone, W. L.\\n(2014). Social smiling and its components in high-risk infantsiblings without later ASD symptomatology. Journal ofAutism and Developmental Disorders, 44(4), 894 –902.\\nhttps://doi.org/10.1007/s10803-013-1944-2\\nOsterling, J. A., Dawson, G., & Munson, J. A. (2002). Early recog-\\nnition of 1-year-old infants with autism spectrum disorderversus mental retardation. Development and Psychopathol-ogy, 14(2), 239 –251.\\nOwada, K., Kojima, M., Yassin, W., Kuroda, M., Kawakubo, Y.,\\nKuwabara, H., …Yamasue, H. (2018). Computer-analyzed\\nfacial expression as a surrogate marker for autism spectrum\\nsocial core symptoms. PLoS One, 13(1), e0190442. https://doi.org/10.1371/journal.pone.0190442\\nRice, M. E., & Harris, G. T. (2005). Comparing effect sizes in\\nfollow-up studies: ROC area, Cohen ’sd, and r. Law and\\nHuman Behavior, 29(5), 615 –620. https://doi.org/10.1007/\\ns10979-005-6832-7\\nRobins, D. L., Casagrande, K., Barton, M., Chen, C. M., Dumont-\\nMathieu, T., & Fein, D. (2014). Validation of the modi ﬁed\\nchecklist for autism in toddlers, revised with follow-up (M-CHAT-R/F). Pediatrics, 133(1), 37 –\\n45. https://doi.org/10.\\n1542/peds.2013-1813\\nSamad, M. D., Diawara, N., Bobzien, J. L., Harrington, J. W.,\\nWitherow, M. A., & Iftekharuddin, K. M. (2018). Afeasibility study of autism behavioral markers in spontaneousfacial, visual, and hand movement response data. IEEETransactions on Neural Systems and Rehabilitation Engineer-ing, 26(2), 353 –361. https://doi.org/10.1109/tnsre.2017.\\n2768482\\nSnow, M. E., Hertzig, M. E., & Shapiro, T. (1987). Expression\\nof emotion in young autistic children. Journal of theAmerican Academy of Child and Adolescent Psychiatry, 26(6), 836 –838. https://doi.org/10.1097/00004583-198726060-\\n00006\\nSullivan, M. W., & Lewis, M. (2003). Emotional expressions of\\nyoung infants and children —A practitioner ’s primer. Infants\\nand Young Children, 16(2), 120 –142. https://doi.org/10.\\n1097/00001163-200304000-00005\\nTantam, D., Holmes, D., & Cordess, C. (1993). Nonverbal expres-\\nsion in autism of Asperger type. Journal of Autism and Devel-opmental Disorders, 23(1), 111 –133.\\nTenenbaum, E. J., Carpenter, K. L. H., Sabatos-DeVito, M.,\\nHashemi, J., Vermeer, S., Sapiro, G., & Dawson, G. (2020). A\\nsix-minute measure of vocalizations in toddlers with autism\\nspectrum disorder. Autism Research, 13(8), 1373 –1382.\\nhttps://doi.org/10.1002/aur.2293\\nTibshirani, R. (1996). Regression shrinkage and selection via the\\nlasso. Journal of the Royal Statistical Society: Series B(Methodological), 58(1), 267 –288. https://doi.org/10.1111/j.\\n2517-6161.1996.tb02080.x\\nTrevisan, D. A., Bowering, M., & Birmingham, E. (2016).\\nAlexithymia, but not autism spectrum disorder, may berelated to the production of emotional facial expressions.Molecular Autism, 7, 46. https://doi.org/10.1186/s13229-016-0108-6\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 11Trevisan, D. A., Hoskyn, M., & Birmingham, E. (2018). Facial\\nexpression production in autism: A meta-analysis. AutismResearch, 11(12), 1586 –1601. https://doi.org/10.1002/aur.2037\\nWaizbard-Bartov, E., Ferrer, E., Young, G. S., Heath, B., Rogers, S.,\\nWu Nordahl, C., …Amaral, D. G. (2020). Trajectories of\\nautism symptom severity change during early childhood.Journal of Autism and Developmental Disorders. https://doi.org/10.1007/s10803-020-04526-z\\nWerner, E., Dawson, G., Osterling, J., & Dinno, N. (2000). Brief\\nreport: Recognition of autism spectrum disorder before oneyear of age: A retrospective study based on home videotapes.Journal of Autism and Developmental Disorders, 30(2),\\n157–162.\\nYin, L., Wei, X., Sun, Y., Wang, J., & Rosato, M. J. (2006). A3 D\\nfacial expression database for facial behavior research . Paper pres-\\nented at the 7th International Conference on automatic faceand gesture recognition (FGR06), University of Southampton,Southampton, UK.\\nYirmiya, N., Kasari, C., Sigman, M., & Mundy, P. (1989). Facial\\nexpressions of affect in autistic, mentally retarded and normalchildren. Journal of Child Psychology and Psychiatry, 30(5),725–735.\\nINSAR Carpenter et al./Digital behavioral phenotyping in ASD 12', metadata={'source': 'papers/carpenter2020 (1)_clean.txt'})],\n",
       " [Document(page_content='1\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8www.nature.com/scientificreportsAtypical postural control can be \\ndetected via computer vision \\nanalysis in toddlers with autism \\nspectrum disorder\\nGeraldine Dawson   1, Kathleen Campbell2, Jordan Hashemi1,3, Steven J. Lippman n  4, \\nValerie Smith4, Kimberly Carpente r1, Helen Egger5, Steven Espinosa3, Saritha Vermeer1, \\nJeffrey\\xa0 Baker6 & Guillermo Sapiro3,7\\nEvidence\\xa0 suggests\\xa0 that\\xa0differences\\xa0 in\\xa0motor\\xa0function\\xa0 are\\xa0an\\xa0early\\xa0feature\\xa0of\\xa0autism\\xa0spectrum\\xa0 disorder\\xa0\\n(ASD).\\xa0One\\xa0aspect\\xa0of\\xa0motor\\xa0ability\\xa0that\\xa0develops\\xa0 during\\xa0childhood\\xa0 is\\xa0postural\\xa0 control,\\xa0reflected\\xa0 in\\xa0the\\xa0\\nability to maintain a steady head and body position without excessive sway. Observational studies have \\ndocumented\\xa0 differences\\xa0 in\\xa0postural\\xa0 control\\xa0in\\xa0older\\xa0children\\xa0with\\xa0ASD.\\xa0The\\xa0present\\xa0study\\xa0used\\xa0computer\\xa0\\nvision\\xa0analysis\\xa0to\\xa0assess\\xa0midline\\xa0head\\xa0postural\\xa0 control,\\xa0as\\xa0reflected\\xa0 in\\xa0the\\xa0rate\\xa0of\\xa0spontaneous\\xa0 head\\xa0\\nmovements\\xa0 during\\xa0states\\xa0of\\xa0active\\xa0attention,\\xa0 in\\xa0104\\xa0toddlers\\xa0 between\\xa0 16–31\\xa0months\\xa0of\\xa0age\\xa0(Mean\\xa0 =\\xa022\\xa0\\nmonths),\\xa0 22\\xa0of\\xa0whom\\xa0were\\xa0diagnosed\\xa0 with\\xa0ASD.\\xa0Time-series\\xa0 data\\xa0revealed\\xa0 robust\\xa0group\\xa0differences\\xa0\\nin\\xa0the\\xa0rate\\xa0of\\xa0head\\xa0movements\\xa0 while\\xa0the\\xa0toddlers\\xa0 watched\\xa0 movies\\xa0depicting\\xa0 social\\xa0and\\xa0nonsocial\\xa0\\nstimuli.\\xa0Toddlers\\xa0 with\\xa0ASD\\xa0exhibited\\xa0 a\\xa0significantly\\xa0 higher\\xa0rate\\xa0of\\xa0head\\xa0movement\\xa0 as\\xa0compared\\xa0 to\\xa0\\nnon-ASD\\xa0 toddlers,\\xa0 suggesting\\xa0 difficulties\\xa0 in\\xa0maintaining\\xa0 midline\\xa0position\\xa0of\\xa0the\\xa0head\\xa0while\\xa0engaging\\xa0\\nattentional\\xa0 systems.\\xa0 The\\xa0use\\xa0of\\xa0digital\\xa0phenotyping\\xa0 approaches,\\xa0 such\\xa0as\\xa0computer\\xa0 vision\\xa0analysis,\\xa0\\nto\\xa0quantify\\xa0 variation\\xa0 in\\xa0early\\xa0motor\\xa0behaviors\\xa0 will\\xa0allow\\xa0for\\xa0more\\xa0precise,\\xa0objective,\\xa0 and\\xa0quantitative\\xa0\\ncharacterization\\xa0 of\\xa0early\\xa0motor\\xa0signatures\\xa0 and\\xa0potentially\\xa0 provide\\xa0new\\xa0automated\\xa0 methods\\xa0 for\\xa0early\\xa0\\nautism\\xa0risk\\xa0identification.\\nAlthough the core symptoms of autism spectrum disorder (ASD) are defined by atypical patterns of social inter -\\naction and the presence of stereotyped and repetitive behaviors and interests, evidence suggests that differences \\nin motor function are also an important early feature of autism. Motor delays could contribute to early hall-mark autism symptoms, including difficulties in orienting to name involving the eyes and head turns, coordinat-ing head and limb movements involved in gaze following and other joint attention behavior, such as pointing. Teitelbaum et al.\\n1 found that atypical movements (e.g. shape of mouth, patterns of lying, righting, sitting) were \\npresent by 4–6 months of age in infants later diagnosed with ASD. Another study of videotapes taken of infants 12–21 weeks of age detected lower levels of positional symmetry among infants later diagnosed with ASD\\n2 sug-\\ngesting atypical development of cerebellar pathways that control balance and symmetry. Six-month-old infants who later were diagnosed with ASD tend to exhibit head lag when pulled to sit, reflecting early differences in motor development\\n3. A study of home videos taken between birth and six months of age found that some infants \\nwho were later diagnosed with ASD showed postural stiffness, slumped posture, and/or head lag4. Other motor \\nsymptoms observed in infants later diagnosed with ASD include fluctuating muscle tone5 and oral-motor abnor -\\nmalities, such as insufficient opening of the mouth in anticipation of the approaching spoon during feeding6. \\nLongitudinal research with very low birth weight infants revealed that infants who are later diagnosed with ASD had poorer ability in maintaining midline position of the head at 9–20 weeks of age\\n7. The authors used visual \\n1Duke Center for Autism and Brain Development, Department of Psychiatry and Behavioral Sciences, Duke \\nUniversity, Durham, North Carolina, USA. 2University of Utah, Salt Lake City, Utah, USA. 3Department of Electrical \\nand Computer Engineering, Duke University, Durham, North Carolina, USA. 4Department of Population Health \\nSciences, Duke University, Durham, North Carolina, USA. 5NYU Langone Child Study Center, New York University, \\nNew York, New York, USA. 6Department of Pediatrics, Duke University, Durham, NC, USA. 7Departments of \\nBiomedical Engineering, Computer Science, and Mathematics, Duke University, Durham, NC, USA. Correspondence \\nand requests for materials should be addressed to G.D. (email: geraldine.dawson@duke.edu )Received: 26 July 2018\\nAccepted: 31 October 2018\\nPublished online: 19 November 2018OPENCorrected: Author Correctionwww.nature.com/scientificreports/2\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8inspection to classify head position in each video frame to yield a measure of midline head position and number \\nof changes in position.\\nThe development of postural control is an index of neuromuscular reactions to the motion of body mass in \\norder to retain stability. Previous studies have documented the developmental progression of the ability to main-tain an upright posture that is accompanied by decreases in postural sway\\n8. Several studies with older children \\nwith ASD have documented deficiencies in postural control, reflected in the presence of postural sway, which is accentuated when children with ASD are viewing arousing stimuli, including complex multi-sensory and social stimuli\\n9–11. Less is known about the presence of postural sway in young children with ASD.\\nStudies of motor and other behaviors in young children have typically relied on subjective and labor-intensive \\nhuman coding to rate and measure behavior. The recent use of digital phenotyping approaches, such as computer vision analysis (CV A) of videotaped recordings of behavior, has allowed for automated, precise and quantitative measurement of subtle, dynamic differences in motor behavior. We reported previously on a result using CV A to \\nmore precisely measure toddlers’ orienting response to a name call, noting that, compared to toddlers without \\nASD, toddlers with ASD oriented less frequently; when they did orient, their head turn was a full second slower, on average\\n12. Such differences in motor speed would likely not be detected with the naked eye during a typical clinical \\nevaluation. Anzulewicz et al .13 used smart tablet computers with touch-sensitive screens and embedded inertial \\nmovement sensors to record movement kinematics and gesture forces in 3–6-year-old children with and without ASD. Children with ASD used greater force and faster and larger gesture kinematics. Machine learning analysis of the children’s motor patterns classified the children with ASD with a high level of accuracy. In another study using automated methods, differences in head movement dynamics were found between 2.5–6.5-year-old children \\nwith and without ASD while they watched movies of social and nonsocial stimuli. Children with ASD showed \\nmore frequent head turning, especially while watching social stimuli\\n14. The authors suggested that the children \\nwith ASD might be using head movement to modulate their arousal while watching social stimuli. Wu et al.15  \\nused electromagnetic sensors to analyze continuous movements at millisecond time scale in older children with ASD versus typical development. They applied a triangular smoothing algorithm to the 3D positional raw move-ment data that preserved the local speed fluctuations. They found that individuals with ASD exhibited signifi-cantly more “sensorimotor noise” when compared to individuals with typical development.\\nThe present study used CV A to characterize head movements that didn’t involve spontaneous or volitional \\norienting or turning away from the stimuli. Rather, we were interested in subtler midline head movements that are more likely related to postural stability. The study compared the behaviors of toddlers with ASD versus those without ASD while the children watched a series of dynamic movies involving different types of stimuli, includ-ing stimuli of both a social and nonsocial nature. While the children watched the movies, their head movements were automatically detected and tracked using landmarks on the participant’s face. The goal of this analysis was to quantify the rate of spontaneous head movements and to determine whether there were differences in this motor feature between young children with and without ASD.\\nMethods\\nParticipants. Participants were 104 children between 16–31 months of age (Mean = 22 months). \\nExclusionary criteria included known vision or hearing deficits, lack of exposure to English at home and/or car -\\negivers who did not speak and read English sufficiently for informed consent. Twenty-two of the children had autism spectrum disorder. The non-ASD comparison group was comprised of 96 typically developing children and 8 children with language delay or developmental delay of clinical significance sufficient to qualify for speech \\nor developmental therapy. Participants in the comparison group had a mean age of 21.91 months (SD = 3.78) and \\nthose in the ASD group had a mean age of 26.19 months (SD = 4.07). Ethnic/racial composition of the ASD and \\ncomparison groups, respectively, was 59% and 45% white, 13% and 14% African American, 6% and 5% Asian, \\nand 22% and 36% multi-racial/other. Percent males was 77% in the ASD group and 59% in the comparison group.\\nParticipants were recruited from primary care pediatric clinics by a research assistant, referral from their phy-\\nsician, and by community advertisement. All caregivers/legal guardians of participants gave written, informed consent, and the study protocol was approved by the Duke University Health System Institutional Review Board. Methods were carried out in accordance with institutional, state, and federal guidelines and regulation.\\nDiagnostic Assessments. Diagnostic evaluations to confirm ASD were based on the Autism Diagnostic \\nObservation Scale-Toddler (ADOS-T), which were conducted by a licensed psychologist or trained research-reliable examiner overseen by a licensed psychologist\\n16. The mean ADOS-T score was 18.81 (SD = 4.20). \\nThe mean IQ based on the Mullen Scales of Early Learning Composite Score for the ASD group was 63.58 (SD = 25.95). Developmental and/or language delay was determined based on the Mullen Scales (> 1 SD below \\nthe mean in overall learning composite or receptive/expressive language).\\nStimuli. A series of stimuli, comprised of brief movies, were shown on a smart tablet while the child sat on \\na caregiver’s lap. The tablet was placed on a stand approximately 3 feet away from the child to prevent the child from touching the screen. The stimuli consisted of a series of brief developmentally-appropriate movies designed to elicit positive affect and engage the child’s attention. The movies consisted of cascading bubbles, a mechanical bunny, animal puppets interacting with each other, and a split screen showing on one side a woman singing \\nnursery rhymes and on the other side dynamic, noise-making toys. The lengths of the movies were 30 seconds \\n(Bubbles), 60 seconds (Rhyme), and ∼ 70 seconds (Bunny and Puppets). Each movie was shown once except for \\nBubbles which was shown at the beginning and end of the series. The entire series of movies lasted 5 minutes. \\nExamples of the stimuli and experimental setup are presented in Fig.\\xa0 1 and described in two previous publi-\\ncations\\n17. Examples of clips from the movies are provided in the Supplementary Material. During three of the \\nmovies, the examiner, standing behind the child, called the child’s name. A failure to orient to name is an early www.nature.com/scientificreports/3\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8symptom of autism, and results of our analysis of the orienting results have previously been published12. However, \\nall segments when children looked away from the movie, including to orient to name, as well as all 5 second seg-\\nments post the name-call stimulus, were automatically removed from the present analyses. Specifically, in order to \\nremove any influence on head movement due to the child orienting when his or her name was called, we removed the time window starting at cue for the name call prompt (a subtle icon used to prompt the examiner to call the \\nname) through the point where 75% of the audible name calls actually occurred, plus 150 frames (5 seconds). \\nSince previous studies have shown that orienting tends to occur within a few seconds after a name call, this elim-\\ninated segments influenced by the name call.\\nParents were asked to attempt to keep the child seated in their lap, but to allow the child to get off their lap if \\nthe child became too distressed to stay seated. Researchers stopped the task for 1 child due to crying. Researchers restarted the task for three participants due to noncompliance.\\nComputer Vision Analysis. The frontal camera in the tablet recorded video of the child’s face throughout \\nthe experiment at 1280 × 720 spatial resolution and 30 frames per second. The fully automatic CV A algorithm \\ndetects and tracks 49 facial landmarks on the child’s face (see Fig.\\xa0 1)18 and estimates head pose angles relative to \\nthe camera by computing the optimal rotation parameters between the detected landmarks and a 3D canonical face model\\n19. For each video frame the algorithm outputted 2D positional coordinates of the facial landmarks and \\n3 head pose angles: yaw (left-right), pitch (up-down), and roll (tilting left-right). The yaw head pose angle was used to determine the frames when the child was engaged with the movie stimuli, where frames exhibiting a yaw \\npose with a magnitude less than 20° were considered as the child being engaged.\\nFollowing the work of\\n17, to quantify head movement when the child is engaged (less than 20° yaw), per-frame \\npixel-wise displacements of 3 central facial landmarks were computed and normalized with respect to the child’s \\neye width, thus head movement was measured as a (normalized) proportion of the child’s eye width per frame. The pixel-wise displacements of the central facial landmarks are dependent on the child’s distance to the camera in the tablet. Although the tablet was placed approximately 3 feet away from the child at the start of the experi-\\nment, the child is free to move throughout the experiment, thus affecting the magnitude of landmark displace-\\nments (when the child is near to the camera the pixel displacements are larger than if the child did the same movement but farther away from the camera). Normalizing the displacements with respect to the eye-width diminishes this distance to camera dependency. More formally, the head movement between frame n and n-1 is \\ndefined as the average Euclidean displacements of the central nose, left inner eye, and right inner eye landmarks (see Fig.\\xa0 1) normalized by a ±  second windowed-average, centered around frame n , of the Euclidean distances \\nbetween the inner left and right eye landmarks,\\n−\\n−+d\\nw,nn\\nnn1,\\n15,1 5\\nFigure 1. iPad movie task and facial landmark detection: (A) Two examples of facial landmark points detected \\nby CV A and estimated head pose (indicated by the three arrows). The landmarks colored in red are the inner left, inner right, and central nose landmarks that are used for head movement computation. The left example depicts landmarks and head pose of a participant engaged in the movie stimuli; while in the right example, the participant is looking away. Both states are automatically detected. (B) Example frames from movie stimuli. \\nEach row displays a frame from corresponding movie stimuli show in the columns (going from left-to-right): \\nBubbles (30 seconds, two repetitions), Bunny (66 seconds), Rhymes (60 seconds), and Puppet show (68 seconds).www.nature.com/scientificreports/4\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8where −dnn1, is the average landmark displacement of the three central landmarks between frame n  and n-1 , \\nand −+wnn15, 15 is the average Euclidean distance between the left and right eye landmarks when the child is \\nengaged between a half-second (15 frames) before and after frame n.\\nResults evaluating the validity of the CV A methods, which rely on landmark identification and tracking on the \\nface, have been previously published. One study demonstrated high reliability between the automatic methods \\nand the expert human rater of head movements, with agreement between the computer and expert clinical rater \\noccurring 92.5% of the time with interrater reliability based on Cohen’s kappa  = 0.7520. A second study compared \\nthe automatic classification based on landmarks to human coders for head movement, demonstrated inter-rater \\nreliability based on intraclass correlation coefficient (ICC) = 0.8917. Other papers report high reliability between \\nCV A and human coding for head turning in response to name (ICC = 0.84)12 and positive affective expression \\n(happy; ICC = 0.90 and 0.89 for ASD and non-ASD toddlers)21.\\nThe original dataset consisted of frame-by-frame measurements of head movement, with observations for \\neach 1/30th of a second. Groups interested in direct use of the data can do so via collaboration with the authors \\ndue to privacy and consent considerations as well as backend designs, and the data will be stored in a separate partition at Duke\\xa0University. In order to prepare the data for statistical analysis, we first aggregated the move-ment measurements by calculating the head movement rate, defined as the moving sum of the cumulative frame-by-frame movement measurements for each 10 frame period (representing 1/3\\nrd of a second). If any indi-\\nvidual frames within a 10-frame set were set to missing, such as when the facial landmarks were not visible or during the name-call period, the moving sum was also set to missing. Outliers were addressed by Winsorizing to the 95\\nth percentile prior to aggregation.\\nAll statistical analyses were performed separately for each of the movie stimuli. To visualize the time series, we \\ncalculated and plotted the median head movement rate as well as the 1st and 3rd quartiles at each 1/3 second time \\ninterval for both ASD and non-ASD children.\\nUnadjusted and adjusted rate ratios for the association between ASD diagnosis and the rate of head move-\\nment in each 1/3 second time interval were estimated using a generalized linear mixed log-gamma regression \\nmodel. Adjusted estimates controlled for ethnicity/race (white; other), age (in months), and sex (male; female). To account for potential within-subject correlations due to repeated measurement, we included a random intercept for each participant.\\nResults\\nThe time series data depicting the rate of head movement, defined as the distance traveled per 1/3 second (10 \\nvideoframes), for the ASD and non-ASD groups are shown in Fig.\\xa0 2.\\nBased on a generalized linear mixed regression model with a log link and gamma distribution (adjusting for \\nethnicity/race, age, and sex), significant associations between diagnostic group (ASD versus non-ASD) and rate of head movement were found during all movies except for the Bubbles 2, the last movie. For Bubbles 2, the shorter \\nduration might have affected power to detect a result as there was nevertheless a trend toward a group difference \\nin the same direction as all other movies. Results of the analysis are shown in Table\\xa0 1.\\nRobust group differences in the rate of head movement were evident during 4 out of 5 of the movies. For \\nexample, the rate of head movement among participants with ASD was 2.22 times that of non-ASD participants during the Bunny movie, after adjusting for age, ethnicity/race, and sex (95% Confidence Interval 1.60, 3.07). The rate ratio was higher for all movies that had animated and more complex stimuli (Bunny, Puppets, Rhymes and Toys), as compared to the less complex Bubbles videos.\\nAlthough the LD/DD group was too small to conduct independent analyses of that group, as a sensitivity anal-\\nysis, the 8 patients with LD/DD were from the main regression model and re-estimated the associations, as shown \\nin Table\\xa0 2. Overall, the results are consistent with those reported in the main analysis; in fact, the associations are \\nslightly stronger when the LD/DD group is removed from the non-ASD group.\\nDiscussion\\nThe present study adds to a large and growing body of literature indicating that differences in early motor devel-\\nopment are an important feature of ASD. We found highly significant differences in postural control, reflected in differences in the rate of spontaneous movement of the head between toddlers with ASD versus those without \\nASD. Using an automated, objective approach, we analyzed data comprised of video-frame-level measurements \\nof head movements with observation for each 1/30\\nth of a second and created 10-frame moving sums to capture \\nmovement. Time-series data revealed group differences in the rate of head movement across all movies repre-senting a wide range of stimuli, such as bubbles, a hopping bunny, and a woman singing a nursery rhyme paired with dynamic toys. An increase in the rate of head movement observed in young children with ASD during states of engaged attention might indicate underlying differences in the ability to maintain midline postural con-\\ntrol and/or atypical engagement of attentional systems in young toddlers with ASD. These movements were not \\ndefined by spontaneous looking away from the stimulus, as was reported by Martin et al .\\n14. Rather, they were \\ncharacterized by a failure to keep the head in a still midline position while viewing the movie. This is distinct from the feature studied by Martin et al ., which was characterized by greater yaw angular displacement and \\ngreater yaw and roll angular velocity, which was primarily present during the presentation of social stimuli and might reflect sensory modulation. The movements we describe in this paper may be similar to those described \\nin previous studies of postural sway in older children with ASD, as well as school aged children with attention \\ndeficit hyperactivity disorder (ADHD) by Heiser et al .\\n22. Heiser et al . used infrared motion analysis to record \\nhead movements during a continuous performance task and found that boys with ADHD moved their head 2.3 times as far as typically-developing boys performing the same task. In a study of siblings of children with ASD, Reiersen and colleagues\\n23 found that siblings who have impaired motor coordination, features of attention deficit \\nhyperactivity disorder (ADHD), or both are much more likely to have ASD than are other siblings. They suggest www.nature.com/scientificreports/5\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8that identification of nonspecific traits that can amplify risk for ASD, such as attention and motor differences, \\ncould allow for earlier identification and targeted therapy that modify these traits and potentially reduce later risk for ASD.\\nDelays and differences in sensorimotor development have been noted across the lifespan in individuals with \\nASD from early infancy through adulthood\\n24. For example, Lim et al . showed that postural sway and attention \\ndemands of postural control were larger in adults with ASD than in typically developed adults25. Morris et al . \\nfound that adults with ASD did not use visual information to control standing posture, in contrast to adults without ASD\\n26. Brain imaging studies suggest that atypical motor function in autism may be related to increased \\nFigure 2. Time series of head movement rate, measured as the distance traveled per 1/3 seconds (10 video \\nframes), by ASD diagnosis. Solid lines are the median values at each time point. Bands represent the first and third quartiles at each time point. Blank sections represent name calls, which were removed from this analysis.\\nMovieUnadjusted Adjusted\\nRate Ratio (95% \\nConfidence Interval) for ASD vs non-ASD P-valueRate Ratio (95% Confidence Interval) for ASD vs non-ASD P-value\\nVideo Bubbles 1 1.46 (1.09, 1.97) 0.011 1.53 (1.10, 2.12) 0.012\\nVideo Bunny 2.13 (1.60, 2.85) <0.0001 2.22 (1.60, 3.07) <0.0001\\nVideo Puppets 2.08 (1.50, 2.88) <0.0001 2.30 (1.60, 3.31) <0.0001\\nVideo Rhymes and Toys 2.37 (1.77, 3.16) <0.0001 2.45 (1.78, 3.39) <0.0001\\nVideo Bubbles 2 1.52 (1.08, 2.14) 0.018 1.43 (0.97, 2.10) 0.070\\nTable 1. Unadjusted and adjusted rate ratios for the associations between diagnostic group and rate of head \\nmovement.www.nature.com/scientificreports/6\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8sensitivity to proprioceptive error and a decreased sensitivity to visual error, aspects of motor learning dependent \\non the cerebellum27. Atypical presentation of motor functions of the cerebellum has been noted in children with \\nASD as young as 14 months of age. Esposito et al . identified significant differences in gait pattern, reflected in \\npostural asymmetry, in toddlers with ASD as compared to those without ASD28.\\nThe sample of toddlers with ASD was recruited from primary pediatric care where children suspected of \\nhaving autism were then evaluated using gold-standard diagnostic methods. Although this method of recruit-ment increases the likelihood of obtaining a more representative population-based sample, it also results in a comparison group of toddlers without ASD that is much larger than the ASD sample. Because the sample of ASD \\ntoddlers in this study was relatively small, it will be important to replicate these findings with a larger group of \\nchildren. A larger sample would also provide the statistical power to examine whether differences in postural control exist based on individual characteristics of children with ASD, such as age, sex, and co-morbid intellectual disability\\xa0and/or ADHD.\\nPrevious analyses of motor differences associated with ASD often have required labor-intensive coding of \\npatterns of behavior that are recognizable by the naked eye. Moreover, such studies typically use a “top down” approach in which specific behaviors of interest are defined and then rated by more than one person (for relia-bility assessments). The use of digital phenotyping offers multiple advantages over previous methods that rely on human coding, namely, the ability to automatically and objectively measure dynamic features of behavior on a \\nspatiotemporal scale that is not easily perceptible to the naked eye. Because digital approaches are scalable, they \\nalso allow for collection of larger data sets that can be analyzed using machine learning. We anticipate that the use of digital phenotyping will reveal a number of objective biomarkers, such as the head movements described in this report, which can be used as early risk indices and targets for intervention. By combining multiple fea-tures that reflect different aspects of sensorimotor function, including patterns of facial expressiomn, orienting, midline head movements, reaching behavior, and others, it might be possible to create a reliable, objective and automated risk profile for ASD and other neurodevelopmental disorders.\\nReferences\\n 1. Teitelbaum, P ., Teitelbaum, O., Nye, J., Fryman, J. & Maurer, R. G. Movement analysis in infancy may be useful for early diagnosis of \\nautism. Proc Natl Acad Sci USA 95, 13982–13987 (1998).\\n 2. Esposito, G., Venuti, P ., Maestro, S. & Muratori, F. An exploration of symmetry in early autism spectrum disorders: analysis of lying. Brain & development  31, 131–138, https://doi.org/10.1016/j.braindev.2008.04.005 (2009).\\n 3. Flanagan, J. E., Landa, R., Bhat, A. & Bauman, M. Head lag in infants at risk for autism: a preliminary study. The American journal of \\noccupational therapy: official publication of the American Occupational Therapy Association 66, 577–585, https://doi.org/10.5014/\\najot.2012.004192 (2012).\\n 4. Zappella, M. et al . What do home videos tell us about early motor and socio-communicative behaviours in children with autistic \\nfeatures during the second year of life–An exploratory study. Early human development 91, 569–575, https://doi.org/10.1016/j.\\nearlhumdev.2015.07.006 (2015).\\n 5. Dawson, G., Osterling, J., Meltzoff, A. N. & Kuhl, P . Case Study of the Development of an Infant with Autism from Birth to Two Y ears of Age. Journal of applied developmental psychology  21, 299–313, https://doi.org/10.1016/s0193-3973(99)00042-8 (2000).\\n 6. Brisson, J., Warreyn, P ., Serres, J., Foussier, S. & Adrien-Louis, J. Motor anticipation failure in infants with autism: a retrospective analysis \\nof feeding situations. Autism: the international journal of research and practice 16, 420–429, https://doi.org/10.1177/1362361311423385 \\n(2012).\\n 7. Gima, H. et al . Early motor signs of autism spectrum disorder in spontaneous position and movement of the head. Experimental \\nbrain research  236, 1139–1148, https://doi.org/10.1007/s00221-018-5202-x (2018).\\n 8. Hytonen, M., Pyykko, I., Aalto, H. & Starck, J. Postural control and age. Acta oto-laryngologica 113, 119–122 (1993).\\n 9. Ghanouni, P ., Memari, A. H., Gharibzadeh, S., Eghlidi, J. & Moshayedi, P . Effect of Social Stimuli on Postural Responses in Individuals with Autism Spectrum Disorder. J Autism Dev Disord  47, 1305–1313, https://doi.org/10.1007/s10803-017-3032-5  \\n(2017).\\n 10. Minshew, N. J., Sung, K., Jones, B. L. & Furman, J. M. Underdevelopment of the postural control system in autism. Neurology 63, \\n2056–2061 (2004).\\n 11. Gouleme, N. et al . Postural Control and Emotion in Children with Autism Spectrum Disorders. Translational neuroscience 8, \\n158–166, https://doi.org/10.1515/tnsci-2017-0022 (2017).\\n 12. Campbell, K. et al. Computer vision analysis captures atypical attention in toddlers with autism. Autism , 1362361318766247, https://\\ndoi.org/10.1177/1362361318766247 (2018).\\n 13. Anzulewicz, A., Sobota, K. & Delafield-Butt, J. T. Toward the Autism MotorSignature: Gesture patterns during smart tablet gameplay identify children with autism. Scientific reports  6, 31107, https://doi.org/10.1038/srep31107 (2016).\\n 14. Martin, K. B. et al. Objective measurement of head movement differences in children with and without autism spectrum disorder. \\nMolecular autism  9, 14, https://doi.org/10.1186/s13229-018-0198-4 (2018).\\n 15. Wu, D., Jose, J. V ., Nurnberger, J. I. & Torres, E. B. A Biomarker Characterizing Neurodevelopment with applications inAutism. \\nScientific reports  8, 614, https://doi.org/10.1038/s41598-017-18902-w (2018).MovieAdjusted Rate Ratio (95% \\nConfidence Interval) for ASD vs TD P-value\\nVideo Bubbles 1 1.58 (1.11, 2.24) 0.0109\\nVideo Bunny 2.34 (1.67, 3.30) <0.0001\\nVideo Puppets 2.38 (1.62, 3.50) <0.0001\\nVideo Rhymes and Toys2.54 (1.81, 3.57) <0.0001\\nVideo Bubbles 2 1.50 (1.00, 2.26) 0.0496\\nTable 2. Adjusted rate ratios for the associations between diagnostic group and rate of head movement after \\nremoving LD/DD participants.www.nature.com/scientificreports/7\\nSCiENtifiC  REPORtS  | (2018) 8:17008 | DOI:10.1038/s41598-018-35215-8 16. Gotham, K., Risi, S., Pickles, A. & Lord, C. The Autism Diagnostic Observation Schedule: revised algorithms for improved \\ndiagnostic validity. J Autism Dev Disord  37, 613–627, https://doi.org/10.1007/s10803-006-0280-1 (2007).\\n 17. Hashemi, J. et al. In Proceedings of the EAI International Conference on Wireless Mobile Communication and Healthcare. MobiHealth \\n(2015).\\n 18. De La Torre, F. IntraFace. Proceedings  of the IEEE International Conference on Automatic Face and Gesture Recognition Workshops  \\n(2015).\\n 19. Dementhon, D. D. L.D. Model-based object pose in 25 lines of code. International Journal of Computer Vision  15, 123–141 (1995).\\n 20. Hashemi, J. et al. Computer vision tools for low-cost and noninvasive measurement of autism-related behaviors in infants. Autism \\nRes Treat.  2014, 935686, https://doi.org/10.1155/2014/935686 (2014).\\n 21. Hashemi, J. et al Computer vision analysis for quantification of autism risk behaviors. IEEE Transactions on Affective Computing, 1–1 \\n(2018).\\n 22. Heiser, P . et al. Objective measurement of hyperactivity, impulsivity, and inattention in children with hyperkinetic disorders before \\nand after treatment with methylphenidate. European child & adolescent psychiatry  13, 100–104, https://doi.org/10.1007/s00787-004-\\n0365-3 (2004).\\n 23. Reiersen, A. M., Constantino, J. N. & Todd, R. D. Co-occurrence of motor problems and autistic symptoms in attention-deficit/hyperactivity disorder. J Am Acad Child Adolesc Psychiatry 47, 662–672, https://doi.org/10.1097/CHI.0b013e31816bff88 (2008).\\n 24. Cook, J. L., Blakemore, S. J. & Press, C. Atypical basic movement kinematics in autism spectrum conditions. Brain: a journal of \\nneurology 136, 2816–2824, https://doi.org/10.1093/brain/awt208 (2013).\\n 25. Lim, Y . H. et al . Effect of Visual Information on Postural Control in Adults with Autism Spectrum Disorder. J Autism Dev Disord , \\nhttps://doi.org/10.1007/s10803-018-3634-6 (2018).\\n 26. Morris, S. L. et al . Differences in the use of vision and proprioception for postural control in autism spectrum disorder. Neuroscience  \\n307, 273–280, https://doi.org/10.1016/j.neuroscience.2015.08.040 (2015).\\n 27. Marko, M. K. et al . Behavioural and neural basis of anomalous motor learning in children with autism. Brain  138, 784–797, https://\\ndoi.org/10.1093/brain/awu394 (2015).\\n 28. Esposito, G., Venuti, P ., Apicella, F. & Muratori, F. Analysis of unsupported gait in toddlers with autism. Brain & development  33, \\n367–373, https://doi.org/10.1016/j.braindev.2010.07.006 (2011).\\nAcknowledgements\\nFunding for this work was provided by NICHD 1P50HD093074, Duke Department of Psychiatry and Behavioral \\nSciences PRIDe award, Duke Education and Human Development Initiative, Duke-Coulter Translational Partnership Grant Program, National Science Foundation, and the Department of Defense. Some of the stimuli used for the movies were created by Geraldine Dawson, Michael Murias, and Sara Webb at the University of Washington. We gratefully acknowledge the editorial assistance of Elizabeth Sturdivant and the participation of \\nthe children and families in this study.\\nAuthor Contributions\\nG.D. and G.S. were responsible for conceptualizing and drafting the manuscript. All other authors reviewed and \\ncontributed to the manuscript. G.S. and J.H. were responsible for carrying-out the computer vision analyses. S.L. and V .S. were responsible for the statistical analyses. G.D., K.C., K.C. and S.V . were responsible for collection of the data and diagnostic confirmations. All authors were responsible for the design of the study and/or data analyses. K.C. and H.E. performed this work while employed at Duke University,\\nAdditional\\xa0 Information\\nSupplementary information accompanies this paper at https://doi.org/10.1038/s41598-018-35215-8.\\nCompeting Interests : Geraldine Dawson is on the Scientific Advisory Boards of Janssen Research and \\nDevelopment, Akili, Inc., LabCorps, and Roche Pharmaceutical Company, has received grant funding from \\nJanssen Research and Development, L.L.C. and PerkinElmer, speaker fees from ViaCord, and receives royalties from Guilford Press and Oxford University Press. Geraldine Dawson and Guillermo Sapiro are affiliated with \\nDASIO, LLC.\\nPublisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and \\ninstitutional affiliations.\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \\nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-ative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \\nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/. © The Author(s) 2018', metadata={'source': 'papers/Dawson_clean.txt'})],\n",
       " [Document(page_content='sensors\\nLetter\\nDeep-Learning-Based Detection of Infants with\\nAutism Spectrum Disorder Using Auto-Encoder\\nFeature Representation\\nJung Hyuk Lee1, Geon Woo Lee1, Guiyoung Bong2, Hee Jeong Yoo2,3and Hong Kook Kim1,*\\n1School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology,\\nGwangju 61005, Korea; ljh0412@gist.ac.kr (J.H.L.); geonwoo0801@gist.ac.kr (G.W.L.)\\n2Department of Psychiatry, Seoul National University Bundang Hospital, Seongnam-si,\\nGyeonggi-do 13620, Korea; 20409@snubh.org (G.B.); hjyoo@snu.ac.kr (H.J.Y.)\\n3Department of Psychiatry, College of Medicine, Seoul National University, Seoul 03980, Korea\\n*Correspondence: hongkook@gist.ac.kr\\nReceived: 29 October 2020; Accepted: 24 November 2020; Published: 26 November 2020\\n/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001\\n/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046\\nAbstract: Autism spectrum disorder (ASD) is a developmental disorder with a life-span disability.\\nWhile diagnostic instruments have been developed and qualiﬁed based on the accuracy of the\\ndiscrimination of children with ASD from typical development (TD) children, the stability of such\\nprocedures can be disrupted by limitations pertaining to time expenses and the subjectivity of\\nclinicians. Consequently, automated diagnostic methods have been developed for acquiring objective\\nmeasures of autism, and in various ﬁelds of research, vocal characteristics have not only been reported\\nas distinctive characteristics by clinicians, but have also shown promising performance in several\\nstudies utilizing deep learning models based on the automated discrimination of children with\\nASD from children with TD. However, di ﬃculties still exist in terms of the characteristics of the\\ndata, the complexity of the analysis, and the lack of arranged data caused by the low accessibility\\nfor diagnosis and the need to secure anonymity. In order to address these issues, we introduce\\na pre-trained feature extraction auto-encoder model and a joint optimization scheme, which can\\nachieve robustness for widely distributed and unreﬁned data using a deep-learning-based method\\nfor the detection of autism that utilizes various models. By adopting this auto-encoder-based\\nfeature extraction and joint optimization in the extended version of the Geneva minimalistic acoustic\\nparameter set (eGeMAPS) speech feature data set, we acquire improved performance in the detection\\nof ASD in infants compared to the raw data set.\\nKeywords: auto-encoder; bidirectional long short-term memory (BLSTM); joint optimization; acoustic\\nfeature extraction; autism spectrum disorder\\n1. Introduction\\nAutism spectrum disorder (ASD) is a developmental disorder with a high probability of causing\\ndiﬃculties in social interactions with other people [ 1]. According to the Diagnostic and Statistical\\nManual of Mental Disorders, Fifth Edition (DSM-5), ASD involves several characteristics such as being\\nconﬁned to speciﬁc interests or behaviors, delayed linguistic development, and poor functionality in\\nterms of communicating or functioning in social situations [ 2]. As there is wide variation in terms of\\nthe types and severities of ASD based on its characteristics, the disorder is referred to as a spectrum [ 1].\\nNot only does ASD have the characteristics of a developmental disorder with a life-span disability, but\\nits prevalence is also increasing—from 1 in 150 children in 2000 to 1 in 54 children in 2016 [ 3]. As diverse\\nevidence has been obtained from previous research showing that the chance of improvement in the\\nSensors 2020 ,20, 6762; doi:10.3390 /s20236762 www.mdpi.com /journal /sensorsSensors 2020 ,20, 6762 2 of 11\\nsocial abilities of people with ASD increases when an earlier clinical intervention is performed [ 4], the\\nearly detection of ASD characteristics has become a key point of current ASD research.\\nVarious instruments for discriminating ASD have been developed, and the commonly accepted\\ngold standard schemes are behavioral assessments, which are time-consuming procedures and require\\nmultidisciplinary teams (MDTs). However, most behavioral assessments su ﬀer in terms of the stability\\nof their ASD diagnosis as a result of the issues of accessibility or subjectivity and interpretive bias\\nbetween professions [ 5]. Therefore, several attempts to develop objective and precise diagnostic\\nmethods have been made in multiple ﬁelds, such as genetic determination [ 6], principle analysis of\\nbrain images , and physiological approaches .\\nOne prominent area of behavioral observations is that of infants’ vocal characteristics. Children\\nwith ASD are known to have abnormalities in their prosody resulting from deﬁcits in their ability to\\nrecognize the inherent mental conditions of others [ 9], and their atypical vocalizations are known to be\\nmonotonous or exaggerated, which can be revealed using various acoustic characteristics, followed\\nby engineering approaches for the discrimination of ASD or typical development (TD) in children\\nbased on the vocal and acoustic features. For example, in [ 10], the researchers estimated deﬁcits in the\\nvocalization of children with ASD at an average age of 18 months, such as “ﬂat” intonation, atypical\\npitch, or control of volume based on the variability of pitch and the long-term average spectrum\\n(LTAS) using fast Fourier transform, where signiﬁcant di ﬀerences were observed in the spectral\\ncomponents at low-band frequencies, as well as spectral peaks and larger pitch ranges and standard\\ndeviations. The development of linguistic abilities is also considered to be a distinguishable feature of\\ndelayed development in children with ASD. Earlier vocal patterns at age 6–18 months were proven\\nto be di ﬀerentiable in a study [ 11] that aimed to conﬁrm the hypothetical vocal patterns and social\\nquality of vocal behavior in order to di ﬀerentiate between ASD and TD cohorts in groups of children\\naged 0–6, 6–12, and 12–18 months in terms of categorized speech patterns consisting of vocalization,\\nlong reduplicated babbling, two-syllable babbling, and ﬁrst words. Evidence of abnormalities in\\nchildren with ASD were shown, in these cases, as a signiﬁcant decrease in vocalization and ﬁrst word\\nrate, while the di ﬀerence in babbling ability between children with ASD and TD was negligible.\\nGiven the development and improvement of machine learning algorithms, as the achievement\\nin the performance of state-of-the-art classiﬁcation and discrimination tasks [ 12], recent attempts to\\ndevelop automated classiﬁcation methods based on machine learning techniques have been based\\non the distinctiveness of vocal characteristics, and have been shown to be promising alternatives to\\nthe conventional methods in many publications [ 13]. For examples of machine learning classiﬁcation,\\nthe researchers of [ 14] employed various acoustic–prosodic features, including fundamental frequency,\\nformant frequencies, harmonics, and root mean square signal energy. In their research, support vector\\nmachines (SVMs) and probabilistic neural networks (PNNs) were adopted as classiﬁers, which showed\\neﬀectual accuracy in discriminating children with ASD from children with TD. Meanwhile, the authors\\nof  employed more recent deep learning techniques, such as convolutional neural networks (CNNs)\\nand recurrent neural networks (RNNs) with spectral features from short-time Fourier transform (STFT)\\nand constant Q transform (CQT), to classify children diagnosed using the autism diagnostic observation\\nschedule (ADOS), also showing promising results in multiple outcomes from SVMs, RNNs, and a\\ncombination of CNN and RNN classiﬁers.\\nA generalized acoustic feature set, an extended version of the Geneva minimalistic acoustic\\nparameter set (eGeMAPS) [ 16], and the bidirectional long short-term memory (BLSTM) model were\\nadopted to di ﬀerentiate between children with ASD and children with TD in [ 17], showing that 75% of\\nthe subjects’ utterances were correctly classiﬁed with the simple application of a deep learning model\\nand feature sets. While the quality of previous research based on various acoustic features has proven\\nthe eﬀectiveness of acoustic features and classiﬁcation algorithms for the detection of abnormalities in\\nchildren’s voices in ASD group compared to those of TD group, the complexity and relationship being\\ninherent between the features will remain uncertain until a large amount of data can be accumulated.\\nFurthermore, a limitation still remains in terms of the problems regarding data collection, since there areSensors 2020 ,20, 6762 3 of 11\\ndiﬃculties pertaining to the need to secure the anonymity of infant subjects, as well as the unintended\\nignorance of parents at earlier stages of their infant’s development. The data of infants are, accordingly,\\ndispersed by gender, age, and number of vocalizations, or consist of comparably small volumes of\\naudio engineering data in general. These problems were typically overlooked by previous research\\nwith controlled and small amounts of data.\\nIn order to provide suggestions for a method to overcome the abovementioned restrictions, we focus\\non examining the feasibility of neural networks as a feature extractor, employing an auto-encoder (AE),\\nwhich can modify acoustic features into lowered and separable feature dimensions [ 18]. We construct a\\nsimple six-layered stacked AE that contains an input layer, three fully connected (FC) layers, an output\\nlayer, and one auxiliary output layer, which has categorical targets for ASD and TD for the optimization\\nof the latent feature space of the AE. We train the AE and deep learning models and compare the\\nresults for each model based on SVMs and vanilla BLSTM, while adopting the same model parameters\\nfrom the method suggested in .\\nThe remainder of this paper is organized as follows. Section 2 describes the speciﬁcations of the\\nparticipants’ data, data processing, feature extraction, statistical analysis, and experimental setup.\\nSection 3 presents the performance evaluations for each algorithm of the SVMs and vanilla BLSTM.\\nLastly, Section 4 concludes the paper.\\n2. Proposed Method\\n2.1. Data Collection and Acoustic Feature Extraction\\nThis study was based on the audio data from video recordings of ASD diagnoses, which were\\ncollected from 2016 to 2018 at Seoul National University Bundang Hospital (SNUBH). We received\\napproval from the Institutional Review Board (IRB) at SNUBH to use fully anonymized data for\\nretrospective analysis (IRB no: B-1909 /567-110) from existing research (IRB no: B-1607 /353-005).\\nWe collected the audio data of 39 infants who were assessed using seven multiple instruments,\\nconsisting of (1) ADOS, second edition (ADOS-2), (2) the autism diagnostic interview, revised (ADI-R),\\n(3) the behavior development screening for toddlers interview (BeDevel-I), (4) the behavior development\\nscreening for toddlers play (BeDevel-P), (5) the Korean version of the childhood autism rating scale\\n(K-CARS) reﬁned from CARS-2, (6) the social communication questionnaire (SCQ), and (7) the social\\nresponsiveness scale (SRS) [ 19–22]. The ﬁnal diagnosis was based on the best clinical estimate diagnosis\\naccording to the DSM-5 ASD criteria by a licensed child psychiatrist using all of the available participant\\ninformation. The participants’ ages ranged between 6 and 24 months, where the average age was\\n19.20 months with a standard deviation (SD) of 2.52 months. Note here that the age means the age\\nat the time when each infant visited the hospital to undergo an initial diagnosis examination. There\\nwere four males and six females diagnosed with ASD, whose average age was 14.72 months with a\\nSD of 2.45. The remaining participants consisted of TD children (19 males and 10 females). Table 1\\ndisplays the collected data distribution, while Table 2 shows detailed information of collected data\\nfrom the infants.\\nTable 1. Distribution of age and gender (male /female).\\nAges (Month)No. of Subjects\\nDiagnosed as ASDNo. of Subjects\\nDiagnosed as TDNo. of Infant Subjects\\n6–12 months 0 5 M /1 F 5 M /1 F\\n12–18 months 1 M /3 F 14 M /9 F 15 M /12 F\\n18–24 months 3 M /3 F 0 3 M /3 F\\nAge (average±SD) 19.20 ±2.52 14.72 ±2.45 15.92 ±3.17Sensors 2020 ,20, 6762 4 of 11\\nTable 2. Detailed information on the age, gender, and initial and deﬁnite diagnosis dates of each infant\\nin Table 1.\\nInfant IDAge (Months) on\\nInitial Diagnosis\\nDateGenderInitial Diagnosis\\nDate\\n(Year /Month /Day)Deﬁnite Final\\nDiagnosis Date\\n(Year /Month /Day)ASD /TD\\n1 18 Male 2018 /07/28 2018 /08/28 TD\\n2 18 Male 2017 /07/27 2017 /08/27 TD\\n3 10 Male 2018 /08/10 2018 /09/10 TD\\n4 13 Male 2017 /06/10 2017 /07/10 TD\\n5 22 Female 2018 /01/31 2018 /02/28 ASD\\n6 16 Male 2018 /03/17 2018 /04/17 TD\\n7 17 Female 2018 /06/30 2018 /07/30 TD\\n8 14 Female 2018 /01/06 2018 /02/06 TD\\n9 18 Male 2018 /07/17 2018 /08/17 TD\\n10 14 Male 2017 /11/04 2017 /12/04 TD\\n11 17 Female 2017 /06/29 2017 /07/29 ASD\\n12 12 Female 2018 /01/20 2018 /02/20 TD\\n13 9 Male 2017 /02/18 2017 /03/18 TD\\n14 18 Female 2017 /03/04 2017 /04/04 ASD\\n15 18 Male 2018 /05/19 2018 /06/19 TD\\n16 24 Female 2018 /08/08 2018 /09/08 ASD\\n17 19 Male 2018 /02/24 2018 /03/24 ASD\\n18 19 Male 2017 /04/18 2017 /05/18 ASD\\n19 18 Female 2017 /03/04 2017 /04/04 TD\\n20 12 Male 2016 /12/31 2017 /01/31 TD\\n21 16 Female 2018 /03/16 2018 /04/16 TD\\n22 20 Male 2017 /10/14 2017 /11/14 ASD\\n23 15 Male 2018 /05/09 2018 /06/09 ASD\\n24 17 Female 2017 /02/04 2017 /03/04 TD\\n25 16 Male 2018 /03/17 2018 /04/17 TD\\n26 12 Male 2018 /03/29 2018 /04/29 TD\\n27 17 Female 2017 /01/25 2017 /02/25 TD\\n28 17 Male 2018 /02/08 2018 /03/08 ASD\\n29 14 Male 2018 /01/13 2018 /02/13 TD\\n30 16 Male 2016 /11/30 2016 /12/30 TD\\n31 12 Male 2017 /03/22 2017 /04/22 TD\\n32 15 Male 2017 /03/11 2017 /04/11 TD\\n33 16 Male 2017 /12/05 2018 /01/05 TD\\n34 13 Female 2017 /12/13 2018 /01/13 TD\\n35 15 Female 2017 /03/25 2018 /04/25 TD\\n36 13 Male 2018 /08/25 2018 /09/25 TD\\n37 21 Male 2017 /06/24 2017 /07/24 ASD\\n38 14 Male 2017 /02/22 2017 /03/22 TD\\n39 14 Male 2018 /01/27 2018 /02/27 TD\\nAs each infant’s audio data were recorded during the clinical procedure to elicit behaviors from\\ninfants, with the attendance of one doctor or clinician and one or both parents with the child in the\\nclinical area, the audio components consisted of various speeches from the child, the clinician, and the\\nparent(s), as well as noises from toys or dragging chairs. Note here that the recordings were done in one\\nof two typical clinical rooms in SNUBH, where the room dimensions were 365 cm×400 cm×270 cm\\nand 350 cm×350 cm×270 cm, and the hospital noise level was around 40 dB. In order to analyze\\nthe vocal characteristics of the infants, each audio clip was processed and split into audio segments\\ncontaining the infant’s voice, not disturbed by music or clattering noises from toys or overlapped\\nby the voices of the clinician or parent(s). Each segment was classiﬁed into one of ﬁve categories,\\nlabeled from 0 to 4, for measuring the data distribution. Each label was intended to show di ﬀerentiable\\ncharacteristics relative to the children’s linguistic development: (1) 0 for one syllable, which is a short,Sensors 2020 ,20, 6762 5 of 11\\nmomentary single vocalization such as “ah” or “ba”; (2) 1 for two syllables, commonly denoted as\\ncanonical babbling, as a reduplication of clear babbling of two identical or variant syllables such as\\n“baba” or “baga”; (3) 2 for babbling, not containing syllables; (4) 3 for ﬁrst word, such as “mother” or\\n“father”; and (5) 4 for atypical voice, including screaming or crying. The distribution of each type of\\nvocalization in seconds is shown in Table 3. The number of vocalizations per category is presented\\nalong with a rational value considering the di ﬀerence between the ASD and TD groups. While the\\ndata were unbalanced and very small, the distribution of ASD and TD vocalizations show the same\\ntendency as reported in [ 10], where the ASD group showed a signiﬁcantly lower ratio of ﬁrst words\\nand an increased ratio of atypical vocalizations, revealing developmental delay in linguistic ability.\\nTable 3. Amount (ratio) of each type of vocalization in seconds.\\nVocal Label ASD TD\\n0 80.134 (0.104) 267.897 (0.250)\\n1 314.405 (0.409) 443.498 (0.414)\\n2 33.241 (0.043) 34.766 (0.032)\\n3 8.311 (0.011) 57.286 (0.054)\\n4 333.400 (0.433) 266.794 (0.249)\\nTotal 769.491 1070.241\\nFor acquiring qualiﬁed and e ﬀective feature sets for the vocal data, eGeMAPS was employed\\nfor voice feature extraction. GeMAPS is a popular feature set providing minimalistic speech features\\ngenerally utilized for automatic voice analysis rather than as a large brute force parameter set. As an\\nextended version, eGeMAPS contains 88 acoustic features that were fully utilized in this experiment.\\nEach recorded set of audio data stored as a 48 kHz stereo ﬁle was down-sampled and down-mixed into a\\n16 kHz mono-audio ﬁle, taking into consideration its usability and resolution in mel-frequency cepstral\\ncoeﬃcients (MFCCs). To extract the speech features for ASD classiﬁcation, each infant’s utterances\\nwere segmented into 25 ms frames with a 10 ms overlap between frames. Then, 88 di ﬀerent features of\\nthe eGeMAPS were extracted for each frame with open source speech and music interpretation using\\nthe large-space extraction (OpenSMILE) toolkit [ 23], and these features were normalized by mean and\\nstandard deviation. The normalization scaling was acquired and ﬁxed by normalizing the factors of\\nthe training data set. The features were grouped for each ﬁve frames considering the time-relevant\\ncharacteristics of the speech data.\\n2.2. Pre-Trained AE for Acoustic Features\\nTo further process and reﬁne the acoustic data, a feature-extracting AE was introduced. An AE is a\\nhierarchical structure that is trained as a regression model for reproducing the input parameters. The AE\\ntakes inputs and converts them into latent representations, and then reconstructs the input parameters\\nfrom the latent values [ 24]. If we consider an input of AE, x∈Rd, then the latent representation z∈Rd′\\nand the reconstruction of the input y∈Rdare obtained by applying a nonlinear activation function\\nfto the weight sum of zusing a weighting matrix W∈Rd×d′and a bias vector b∈Rd′, such as\\nz=f(\\nWTx+b)\\n(1)\\ny=f(\\nWTz+b′)\\n(2)\\nwhere Tis a matrix transpose operator. When the latent dimension d′<d, the output from the latent\\nlayer is considered to be a compressed, meaningful value extracted from the input, which is also noted\\nas a bottleneck feature .\\nThe normalized eGeMAPS features were applied to train the feature-extracting AE, applying\\nthe same data as the input and the target. The AE model contained a latent layer with a lowered,\\ncompacted feature dimension compared to the input layer to achieve the useful bottleneck feature.Sensors 2020 ,20, 6762 6 of 11\\nThe model was symmetrically structured, centering around the latent layer, and the model could be\\ndivided into two components: the encoder, consisting of layers from the input to the latent layers,\\nand a decoder, consisting of layers from the bottleneck to the output layers.\\nThe AE structure is depicted in Figure 1. Our AE model consisted of FC layers, with the dimensions\\nof 88, 70, 54, 70, and 88 nodes for the input, hidden, latent, hidden, and output layers, respectively.\\nThe hidden dimension was selected experimentally and the bottleneck feature dimension was used for\\ncomparison with previous research [ 17], where 54 features were selected considering the statistical\\ndissimilarity of the distributions between the ASD and TD features based on the Mann–Whitney U\\ntest [ 26]. We additionally introduced an auxiliary output as the binary categorical target for ASD\\nand TD, which is known as the semi-supervised method, to train the AE model e ﬀectively [ 27]. The\\nauxiliary output is depicted as Aux in Figure 1. The reconstructed features and auxiliary classiﬁcation\\ncan be written as\\nzi=f(Wi−1,izi−1+bi−1,i) (3)\\nwhere z1=f(W0,1x+b0,1), and\\nyrec=W3,4z3+b3,4 (4)\\nyaux=∂(W2,az2+b2,a) (5)\\nwhere yrecrefers to the reconstructed eGeMAPS features, yauxis the auxiliary classiﬁcation result, fis\\nthe activation function, and ∂is the softmax activation.\\nSensors 2020, 20, x FOR PEER REVIEW  6 of 12 \\n where T is a matrix transpose operator. When the latent dimension 𝑑′<𝑑, the output from the latent \\nlayer is considered to be a compressed, meaningful value extracted from the input, which is also \\nnoted as a bottleneck feature .  \\nThe normalized eGeMAPS feature s were applied to train the feature -extracting AE, applying the \\nsame data as the input and the target. The AE model contained a latent layer with a lowered, \\ncompacted feature dimension compared to the input layer to achieve the useful bottleneck feature. \\nThe model was symmetrically structured, centering around the latent layer, and the model could be \\ndivided into two components: the encoder, consisting of layers from the input to the latent layers, \\nand a decoder, consisting of layers from the bottleneck to the output layers.  \\nThe AE structure is depicted in Figure 1. Our AE model consisted of FC layers, with the \\ndimensions of 88, 70, 54, 70, and 88 nodes for the input, hidden, latent, hidden, and output layers, \\nrespectively. The hidden dimension was selected experimentally and the bottleneck feature \\ndimension was used for comparison with previous research , where 54 features were selected \\nconsidering the statistical dissimilarity of the distributions between the ASD and TD features based \\non the Mann –Whitne y U test . We additionally introduced an auxiliary output as the binary \\ncategorical target for ASD and TD, which is known as the semi -supervised method, to train the AE \\nmodel effectively . The auxiliary output is depicted as Aux in Figure 1. The reconstructed features \\nand auxiliary classification can be written as  \\n𝒛𝑖=𝑓(𝑾𝑖−1,𝑖𝒛𝑖−1+𝒃𝑖−1,𝑖) (3) \\nwhere 𝒛1=𝑓(𝑾0,1𝒙+𝒃0,1), and  \\n𝒚𝑟𝑒𝑐=𝑾3,4𝒛3+𝒃3,4 (4) \\n  𝒚𝑎𝑢𝑥=𝜕(𝑾2,𝑎𝒛2+𝒃2,𝑎) (5) \\nwhere 𝒚𝑟𝑒𝑐 refers to the reconstructed eGeMAPS features, 𝒚𝑎𝑢𝑥 is the auxiliary classification result, \\n𝑓 is the activation function, and 𝜕 is the softmax activation.  \\n \\nFigure 1. Structure of a semi -supervised auto -encoder (AE) model. eGeMAPS , extended version of \\nthe Geneva minimalistic acoustic parameter set; ASD, autism spectrum disorder; TD, typical \\ndevelopment.  \\nThe losses of the reconstruction error for main AE target are measured using the mean absolute \\nerror, while the auxiliary ASD/TD t arget loss is the binary cross -entropy, and they are added and \\nsimultaneously optimized with rational hyper -parameters. The overall loss equation is  \\nFigure 1. Structure of a semi-supervised auto-encoder (AE) model. eGeMAPS, extended version of the\\nGeneva minimalistic acoustic parameter set; ASD, autism spectrum disorder; TD, typical development.\\nThe losses of the reconstruction error for main AE target are measured using the mean absolute\\nerror, while the auxiliary ASD /TD target loss is the binary cross-entropy, and they are added and\\nsimultaneously optimized with rational hyper-parameters. The overall loss equation is\\nLrecon =1\\nNN∑\\ni=1⏐⏐⏐yirec−yigt⏐⏐⏐ (6)\\nLaux=−y1gtlog(y1aux)−(1−t)y1gtlog(y1aux) (7)\\nLtotal=Lrecon +αLaux (8)\\nwhere Lrecon,Laux, and Ltotaldenote the reconstruction error, auxiliary loss using a binary cross-entropy\\nloss function, and total loss, respectively.Sensors 2020 ,20, 6762 7 of 11\\nFor our stacked AE model, a rational value of α=0.3was selected experimentally, considering\\nthe proportion of each loss. In order to train the AE e ﬀectively, both L2 normalization for weight\\nnormalization and batch normalization were adopted [ 28,29]. After the training was completed,\\nwe fetched the encoder of the AE as the feature extraction part for the joint optimization model in the\\ntraining procedures of the deep learning model.\\n2.3. Establishing and Training the Deep Learning Model for ASD Detection\\nAs the eGeMAPS data were set and the AE was trained through semi-supervised learning,\\nthe machine learning models, such as SVMs, BLSTM, and joint optimized BLSTM were constructed.\\nEach model had its own input parameter dimensions and the same output targets as ASD and TD\\nclassiﬁcation labels. The eGeMAPS feature data were paired with the diagnostic results for the\\nsupervised learning of the neural network models. For the binary decision, ASD was labeled as\\na positive data point, with a label of (0, 1), while TD was labeled as a negative data point (1, 0).\\nWe composed four kinds of models with the paired data: SVMs with linear kernel, the vanilla BLSTM\\nwith 88 eGeMAPS features, the vanilla BLSTM with 54 eGeMAPS features, and the jointly optimized\\nBLSTM layer with the AE. The joint optimization model is depicted in Figure 2. As the data set was\\nprepared as the input with ﬁve sequential frames, i.e., the grouped eGeMAPS features in Figure 2,\\nthe SVMs received a single frame parameter of 440 dimension which was ﬂattened from the original\\nﬁve input frames. For the deep learning models, batch normalization, rectangular linear unit (ReLU)\\nactivation, and dropout were applied for each layer, except for the output layer [ 30,31], and the\\nadaptive momentum (ADAM) optimizer [ 32] was used to train the network. The training procedure\\nwas controlled by early stopping for minimizing the validation error with 100 epoch patience, while\\nsaving the best models for improvement of the validation loss by each epoch. Because the amount\\nof speech data was relatively small for a deep learning model compared to the disparate ﬁeld of\\naudio engineering, we grouped the data into ﬁve segments, while the test utterances were separated\\nformerly, which were selected randomly for 10% of the total data, were evenly distributed across each\\nvocalization type, and underwent ﬁve-fold cross-validation for training; then, the best-performing\\nmodel was chosen. Our model was trained with the TensorFlow framework [ 33]. For comparison,\\nan SVM model with linear kernel was trained with the same data split as the proposed deep learning\\nmodel, and as well as the vanilla BLSTM suggested in , which has single BLSTM with eight cells.\\nSensors 2020, 20, x FOR PEER REVIEW  8 of 12 \\n  \\nFigure 2. Structure of a joint optimization model of an auto -encoder (AE) and bidirectional long short -\\nterm memory (BLSTM).  \\n3. Performance Evaluation  \\nThe performance of each method was evaluated through five -fold cross validation, where 95 \\naverage ASD utterances and 130 average TD utterances were proportionally distributed over five \\ncases of vocalizations for the gener alized estimation of unconcentrated utterance data. The averaged \\nperformances of the five validation splits of each model are described in Table 4. The labeled names \\nof the BLSTM were used as the features for training the BLSTM model, where eGeMAPS -88 deno tes \\n88 features of eGeMAPS, eGeMAPS -54 denotes 54 features selected by the Mann –Whitney U test, and \\nAE-encoded denotes the joint optimized model. In the classification stage, one utterance was \\nprocessed in the frame -wise method and the softmax output was c onverted to class indices 0 and 1, \\nand if the average of class indices of the frames was over 0.5, then the utterance was considered an \\nASD child’s utterance. The performances were scored with conventional measures, as well as \\nunweighted average recall (UA R) and weighted average recall (WAR), chosen in the INTERSPEECH \\n2009 Emotion challenge, which considered imbalanced classes . In the experiment, the SVM \\nmodel showed very low precision, which was extremely biased toward the TD class. The BLSTM \\nclassifi er with 88 features of eGeMAPS and the AE model showed considerable quality in terms of \\nclassifying ASD and TD children, while the AE model showed only marginal improvement in \\ncorrectly classifying children with ASD compared to eGeMAPS -88. The 54 selected features showed \\ndegraded quality compared to eGeMAPS -88, obtaining more biased results toward children with TD.  \\nTable 4. Classification results from the support vector machine (SVM), BLSTM with 88 or 54 \\neGeMAPS features, 54 selected eGeMAPS features, and B LSTM with AE -encoded features.  \\nModels  SVM  BLSTM \\n(eGeMAPS -54) BLSTM \\n(eGeMAPS -88) BLSTM  (AE-\\nEncoded)  \\nPredicted To ASD  TD ASD  TD ASD  TD ASD  TD \\nASD  62 18 170 103 196 99 215 98 \\nTD 413 632 305 547 279 551 260 552 \\nAccuracy  0.6178  0.6373  0.6640  0.6818  \\nPrecision  0.1305  0.3579  0.4126  0.4526  \\nRecall  0.7750  0.6227  0.6644  0.6869  \\nF1 score  0.2234  0.4545  0.5091  0.5457  \\nUAR  0.5514  0.5997  0.6302  0.6509  \\nUAR, unweighted average recall.  \\n  \\nFigure 2. Structure of a joint optimization model of an auto-encoder (AE) and bidirectional long\\nshort-term memory (BLSTM).\\n3. Performance Evaluation\\nThe performance of each method was evaluated through ﬁve-fold cross validation, where\\n95 average ASD utterances and 130 average TD utterances were proportionally distributed over ﬁveSensors 2020 ,20, 6762 8 of 11\\ncases of vocalizations for the generalized estimation of unconcentrated utterance data. The averaged\\nperformances of the ﬁve validation splits of each model are described in Table 4. The labeled names\\nof the BLSTM were used as the features for training the BLSTM model, where eGeMAPS-88 denotes\\n88 features of eGeMAPS, eGeMAPS-54 denotes 54 features selected by the Mann–Whitney Utest,\\nand AE-encoded denotes the joint optimized model. In the classiﬁcation stage, one utterance was\\nprocessed in the frame-wise method and the softmax output was converted to class indices 0 and 1, and\\nif the average of class indices of the frames was over 0.5, then the utterance was considered an ASD\\nchild’s utterance. The performances were scored with conventional measures, as well as unweighted\\naverage recall (UAR) and weighted average recall (WAR), chosen in the INTERSPEECH 2009 Emotion\\nchallenge, which considered imbalanced classes [ 34]. In the experiment, the SVM model showed very\\nlow precision, which was extremely biased toward the TD class. The BLSTM classiﬁer with 88 features\\nof eGeMAPS and the AE model showed considerable quality in terms of classifying ASD and TD\\nchildren, while the AE model showed only marginal improvement in correctly classifying children\\nwith ASD compared to eGeMAPS-88. The 54 selected features showed degraded quality compared to\\neGeMAPS-88, obtaining more biased results toward children with TD.\\nTable 4. Classiﬁcation results from the support vector machine (SVM), BLSTM with 88 or 54 eGeMAPS\\nfeatures, 54 selected eGeMAPS features, and BLSTM with AE-encoded features.\\nModels SVMBLSTM\\n(eGeMAPS-54)BLSTM\\n(eGeMAPS-88)BLSTM\\n(AE-Encoded)\\nPredicted To ASD TD ASD TD ASD TD ASD TD\\nASD 62 18 170 103 196 99 215 98\\nTD 413 632 305 547 279 551 260 552\\nAccuracy 0.6178 0.6373 0.6640 0.6818\\nPrecision 0.1305 0.3579 0.4126 0.4526\\nRecall 0.7750 0.6227 0.6644 0.6869\\nF1 score 0.2234 0.4545 0.5091 0.5457\\nUAR 0.5514 0.5997 0.6302 0.6509\\nUAR, unweighted average recall.\\n4. Discussion\\nThe vanilla BLSTM model presented in [ 17] conducted discrimination on well-classiﬁed subjects\\nwith 10-month-old children and sorted 54 features from eGeMAPS that had a distinctive distribution\\nbetween ASD and TD selected by the Mann–Whitney Utest using the three-fold cross-validation\\nmethod. However, because the di ﬀerence in the data distribution failed to achieve the same eGeMAPS\\nfeature selection between the test and classiﬁcation results with the speciﬁed feature set presented\\nherein, the application of an identical model structure and the adoption of the same feature domain\\nwill allow both approaches to be indirectly comparable.\\nThese results can be interpreted by the data distributions, and we performed t-stochastic neighbor\\nembedding (t-SNE) analysis [ 35] on the training data set, which can nonlinearly squeeze the data\\ndimension based on a machine learning algorithm. Figure 3 shows each data distribution as a\\ntwo-dimensional scatter plot. In the ﬁgure, the eGeMAPS features from eGeMAPS-88 and eGeMAPS-54\\nshowed almost identical distribution, except for the amount of ASD outliers, which implies that the\\nASD and TD features in the eGeMAPS features show similar distributions in this experiment. As shown\\nin , eGeMAPS includes temporal features that are relevant to vocalizations and utterances; thus, these\\nfeatures might cause confusion regarding the discrimination between ASD and TD. The AE-encoded\\nfeatures, however, showed a redistributed feature map with a more characteristic distribution compared\\nto the eGeMAPS features. This is because the AE-encoded features were compressed into a bottleneck\\nfeature, which was derived by weighting the matrix, paying attention to the signiﬁcant parametersSensors 2020 ,20, 6762 9 of 11\\nwhile reducing the inﬂuence from the ambiguous parameters. While the joint optimization model\\nachieved only marginally improved results compared to eGeMAPS-88, the distribution of the feature\\nmap would be more noticeable in improved feature extraction models, as well as more di ﬀerentiable in\\ncomplex models, although BLSTM with eight cells was employed for a comparison with conventional\\nresearch in this experiment.\\nSensors 2020, 20, x FOR PEER REVIEW  9 of 12 \\n 4. Discussion  \\nThe vanilla BLSTM model presented in  conducted discrimination on well -classified subjects \\nwith 10 -month -old children and sorted 54 features from eGeMAPS that had a distinctive distribution \\nbetween ASD and TD selected by the Mann –Whitney U test using t he three -fold cross -validation \\nmethod. However, because the difference in the data distribution failed to achieve the same \\neGeMAPS feature selection between the test and classification results with the specified feature set \\npresented herein, the applicatio n of an identical model structure and the adoption of the same feature \\ndomain will allow both approaches to be indirectly comparable.  \\nThese results can be interpreted by the data distributions, and we performed t -stochastic \\nneighbor embedding (t -SNE) analy sis  on the training data set, which can nonlinearly squeeze the \\ndata dimension based on a machine learning algorithm. Figure 3 shows each data distribution as a \\ntwo-dimensional scatter plot. In the figure, the eGeMAPS features from eGeMAPS -88 and eGeM APS -\\n54 showed almost identical distribution, except for the amount of ASD outliers, which implies that \\nthe ASD and TD features in the eGeMAPS features show similar distributions in this experiment. As \\nshown in , eGeMAPS includes temporal features that are relevant to vocalizations and utterances; \\nthus, these features might cause confusion regarding the discrimination between ASD and TD. The \\nAE-encoded features, however, showed a redistributed feature map with a more characteristic \\ndistribution compared to the eGeMAPS features. This is because the AE -encoded features were \\ncompressed into a bottleneck feature, which was derived by weighting the matrix, paying attention \\nto the significant parameters while reducing the influence from the ambiguous parameters . While the \\njoint optimization model achieved only marginally improved results compared to eGeMAPS -88, the \\ndistribution of the feature map would be more noticeable in improved feature extraction models, as \\nwell as more differentiable in complex models, alt hough BLSTM with eight cells was employed for a \\ncomparison with conventional research in this experiment.  \\nWhile the overall performance scores were comparably low for general classification problems \\non account of the subjectivity and complexity of problems, and the limitation in terms of the shortage \\nof data, the results of the jointly optimized model imply the possibility of deep -learning -based feature \\nextraction for the improvement of automated ASD/TD diagnosis under restricted circumstances.  \\n   \\n(a) (b) (c) \\nFigure 3. Two -dimensional scatter plot for ( a) eGeMAPS -88, ( b) eGeMAPS -54, and ( c) the AE \\nprocessed by t -stochastic neighbor embedding (t -SNE).  \\n5. Conclusion s \\nIn this paper, we conducted experiments for discovering the possibility of auto -encoder -based \\nfeature extraction and a joint optimization  method for the automated detection of atypicality in voices \\nof children with ASD during early developmental stages. Un der the condition of an insufficient and \\ndispersed data set, the clas sification results were relatively poor in comparison to the general \\nclassification tasks based on deep learning. Although our investigation used a limited number of \\nsubjects and an unbal anced data set, the suggested auto -encoder -based feature extraction and joint \\noptimization method revealed the possibility of feature dimension and a slight improvement in \\nmodel -based diagnosis under such uncertain circumstances.  \\nFigure 3. Two-dimensional scatter plot for ( a) eGeMAPS-88, ( b) eGeMAPS-54, and ( c) the AE processed\\nby t-stochastic neighbor embedding (t-SNE).\\nWhile the overall performance scores were comparably low for general classiﬁcation problems on\\naccount of the subjectivity and complexity of problems, and the limitation in terms of the shortage of\\ndata, the results of the jointly optimized model imply the possibility of deep-learning-based feature\\nextraction for the improvement of automated ASD /TD diagnosis under restricted circumstances.\\n5. Conclusions\\nIn this paper, we conducted experiments for discovering the possibility of auto-encoder-based\\nfeature extraction and a joint optimization method for the automated detection of atypicality in voices\\nof children with ASD during early developmental stages. Under the condition of an insu ﬃcient\\nand dispersed data set, the classiﬁcation results were relatively poor in comparison to the general\\nclassiﬁcation tasks based on deep learning. Although our investigation used a limited number of\\nsubjects and an unbalanced data set, the suggested auto-encoder-based feature extraction and joint\\noptimization method revealed the possibility of feature dimension and a slight improvement in\\nmodel-based diagnosis under such uncertain circumstances.\\nIn future work, we will focus on increasing the reliability of the proposed method by addition\\nof a number of infants’ speech data, reﬁnement of the acoustic features, an auto-encoder for feature\\nextraction, and better, deeper, and up-to-date model structures. This research can also be extended to\\nchildren with the age of 3 or 4 who can speak several sentences. In this case, we will investigate the\\nlinguistic features, as well as acoustic features, such as we have done in this paper. In addition to ASD\\ndetection, this research can be applied to the detection of infants with development delays.\\nAuthor Contributions: All authors discussed the contents of the manuscript. H.K.K. contributed to the research\\nidea and the framework of this study; G.B. and H.J.Y. provided the database and helped with the discussion; J.H.L.\\nperformed the experiments; G.W.L. contributed to the data collection and pre-processing. All authors have read\\nand agreed to the published version of the manuscript.\\nFunding: This work was supported by the Institute of Information & communications Technology Planning &\\nevaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00330, Development of AI Technology\\nfor Early Screening of Infant /Child Autism Spectrum Disorders based on Cognition of the Psychological Behavior\\nand Response).\\nConﬂicts of Interest: The authors declare no conﬂict of interest.Sensors 2020 ,20, 6762 10 of 11\\nReferences\\n1. National Institute of Mental Health. Autism Spectrum Disorder. Available online: https: //www.nimh.nih.\\ngov/health /topics /autism-spectrum-disorders-asd /index.shtml (accessed on 26 October 2020).\\n2. American Psychiatric Association. Diagnostic and Statistical Manual of Mental Disorders: DSM-5 ; American\\nPsychiatric Publishing: Washington, DC, USA, 2013.\\n3. Centers for Disease Control and Prevention (CDC). Data & Statistics on Autism Spectrum Disorder. Available\\nonline: https: //www.cdc.gov /ncbddd /autism /data.html (accessed on 26 October 2020).\\n4. Fenske, E.C.; Zalenski, S.; Krantz, P .J.; McClannahan, L.E. Age at intervention and treatment outcome for\\nautistic children in a comprehensive intervention program. Anal. Interv. Devel. Disabil. 1985 ,5, 49–58.\\n[CrossRef]\\n5. Falkmer, T.; Anderson, K.; Falkmer, M.; Horlin, C. Diagnostic procedures in autism spectrum disorders:\\nA systematic literature review. Eur. Child Adolesc. Psychiatry 2013 ,22, 329–340. [CrossRef] [PubMed]\\n6. Bailey, A.; Le Couteur, A.; Gottesman, I.; Bolton, P .; Simono ﬀ, E.; Yuzda, E.; Rutter, M. Autism as a strongly\\ngenetic disorder: Evidence from a British twin study. Physiol. Med. 1995 ,25, 63–77. [CrossRef] [PubMed]\\n7. Duﬀy, F.H.; Als, H. A stable pattern of EEG spectral coherence distinguishes children with autism from\\nneuro-typical controls—A large case control study. BMC Med. 2012 ,10, 64. [CrossRef] [PubMed]\\n8. Chaspari, T.; Lee, C.-C.; Narayanan, S.S. Interplay between verbal response latency and physiology of\\nchildren with autism during ECA interactions. In Proceedings of the Annual Conference of the International\\nSpeech Communication Association (Interspeech), Portland, OR, USA, 9–13 September 2012; pp. 1319–1322.\\n9. Baron-Cohen, S. Social and pragmatic deﬁcits in autism: Cognitive or a ﬀective? J. Autism Dev. Disord. 1988 ,\\n18, 379–402. [CrossRef] [PubMed]\\n10. Bonneh, Y.S.; Levanon, Y.; Dean-Pardo, O.; Lossos, L.; Adini, Y. Abnormal speech spectrum and increased\\npitch variability in young autistic children. Front. Hum. Neurosci. 2011 ,4, 237. [CrossRef] [PubMed]\\n11. Chericoni, N.; de Brito Wanderley, D.; Costanzo, V .; Diniz-Gonçalves, A.; Gille, M.L.; Parlato, E.; Cohen, D.;\\nApicella, F.; Calderoni, S.; Muratori, F. Pre-linguistic vocal trajectories at 6–18 months of age as early markers\\nof autism. Front. Psychol. 2016 ,7, 1595. [CrossRef] [PubMed]\\n12. Alom, M.Z.; Taha, T.M.; Yakopcic, C.; Westberg, S.; Sidike, P .; Nasrin, M.S.; Hasan, M.; van Essen, B.C.;\\nAwwal, A.A.S.; Asari, V .K. A state-of-the-art survey on deep learning theory and architectures. Electronics\\n2019 ,8, 292. [CrossRef]\\n13. Song, D.-Y.; Kim, S.Y.; Bong, G.; Kim, J.M.; Yoo, H.J. The use of artiﬁcial intelligence in screening and\\ndiagnosis of autism spectrum disorder: A literature review. J. Korean Acad. Child. Adolesc. Psychiatry 2019 ,30,\\n145–152. [CrossRef] [PubMed]\\n14. Santos, J.F.; Brosh, N.; Falk, T.H.; Zwaigenbaum, L.; Bryson, S.E.; Roberts, W.; Smith, I.M.; Szatmari, P .;\\nBrian, J.A. Very early detection of autism spectrum disorders based on acoustic analysis of pre-verbal\\nvocalizations of 18-month old toddlers. In Proceedings of the IEEE International Conference on Acoustics,\\nSpeech and Signal Processing (ICASSP), Vancouver, BC, Canada, 26–31 May 2013; pp. 7567–7571.\\n15. Li, M.; Tang, D.; Zeng, J.; Zhou, T.; Zhu, H.; Chen, B.; Zou, X. An automated assessment framework for\\natypical prosody and stereotyped idiosyncratic phrases related to autism spectrum disorder. Comput. Speech\\nLang. 2019 ,56, 80–94. [CrossRef]\\n16. Eyben, F.; Scherer, K.R.; Schuller, B.W.; Sundberg, J.; Andr é, E.; Busso, C.; Devillers, L.Y.; Epps, J.; Laukka, P .;\\nNarayanan, S.S.; et al. The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and\\naﬀective computing. IEEE Trans. A ﬀect. Comput. 2016 ,7, 190–202. [CrossRef]\\n17. Pokorny, F.B.; Schuller, B.W.; Marschik, P .B.; Brueckner, R.; Nyström, P .; Cummins, N.; Bölte, S.;\\nEinspieler, C.; Falck-Ytter, T. Earlier identiﬁcation of children with autism spectrum disorder: An automatic\\nvocalisation-based approach. In Proceedings of the Annual Conference of the International Speech\\nCommunication Association (Interspeech), Stockholm, Sweden, 20–24 August 2017; pp. 309–313.\\n18. Xing, C.; Ma, L.; Yang, X. Stacked denoise autoencoder based feature extraction and classiﬁcation for\\nhyperspectral images. J. Sens. 2016 ,2016 , 3632943. [CrossRef]\\n19. Bong, G.; Kim, J.; Hong, Y.; Yoon, N.; Sunwoo, H.; Jang, J.; Oh, M.; Lee, K.; Jung, S.; Yoo, H. The feasibility and\\nvalidity of autism spectrum disorder screening instrument: Behavior development screening for toddlers\\n(BeDevel)—A pilot study. Autism Res. 2019 ,12, 1112–1128. [CrossRef] [PubMed]Sensors 2020 ,20, 6762 11 of 11\\n20. Center for Autism Research. Social Communication Questionnaire (SCQ). Available online: https: //www.\\ncarautismroadmap.org /social-communication-questionnaire-scq /?print =pdf (accessed on 26 October 2020).\\n21. Center for Autism Research. Childhood Autism Rating Scale, 2nd Edition (CARS2). Available online: https:\\n//www.carautismroadmap.org /childhood-autism-rating-scale /?print =pdf (accessed on 26 October 2020).\\n22. Center for Autism Research. Social Responsiveness Scale, 2nd Edition (SRS-2). Available online: https:\\n//www.carautismroadmap.org /social-responsiveness-scale /?print =pdf (accessed on 26 October 2020).\\n23. Eyben, F.; Wöllmer, M.; Schuller, B. OpenSMILE—The Munich versatile and fast open-source audio feature\\nextractor. In Proceedings of the 18th ACM International Conference on Multimedia, Firenze, Italy, 25–29\\nOctober 2010; pp. 1459–1462.\\n24. Masci, J.; Meier, U.; Cire¸ san, D.; Schmidhuber, J. Stacked Convolutional Auto-Encoders for Hierarchical\\nFeature Extraction. In Artiﬁcial Neural Networks and Machine-ICANN 2011 ; Honkela, T., Duch, W., Girolami, M.,\\nKaski, S., Eds.; Springer: Berlin /Heidelberg, Germany, 2011; pp. 52–59.\\n25. Sainath, T.; Kingsbury, B.; Ramabhadran, B. Auto-encoder bottleneck features using deep belief networks. In\\nProceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\\nKyoto, Japan, 25–30 March 2012.\\n26. Nachar, N. The Mann-Whitney U: A test for assessing whether two independent samples come from the\\nsame distribution. Tutor. Quant. Methods Psychol. 2008 ,4, 13–20. [CrossRef]\\n27. Le, L.; Patterson, A.; White, M. Supervised autoencoders: Improving generalization performance with\\nunsupervised regularizers. In Advances in Neural Information Processing Systems ; Bengio, S., Wallach, H.,\\nLarochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R., Eds.; Curran Associates, Inc.: New York, NY,\\nUSA, 2018; pp. 107–117.\\n28. van Laarhoven, T. L2 regularization versus batch and weight normalization. arXiv 2017 , arXiv:1706.05350.\\n29. Ioﬀe, S.; Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate\\nshift. In Proceedings of the International Conference on Machine Learning, Lille, France, 6–11 July 2015;\\npp. 448–456.\\n30. Nair, V .; Hinton, G.E. Rectiﬁed linear units improve restricted Boltzmann machines. In Proceedings of the\\n27th International Conference on Machine Learning, Haifa, Israel, 21–24 June 2010; pp. 807–814.\\n31. Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent\\nneural networks from overﬁtting. J. Mach. Learn. Res. 2014 ,15, 929–1958.\\n32. Kingma, D.P .; Ba, J.L. ADAM: A method for stochastic optimization. In Proceedings of the 3rd International\\nConference on Learning Representations, San Diego, CA, USA, 7–9 May 2015; pp. 1–15.\\n33. Abadi, M.; Barham, P .; Chen, J.; Chen, Z.; Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; Irving, G.; Isard, M.;\\net al. TensorFlow: A system for large-scale machine learning. In Proceedings of the 12th USENIX Symposium\\non Operating Systems Design and Implementation, Savannah, GA, USA, 2–4 November 2016; pp. 265–283.\\n34. Schuller, B.; Steidl, S.; Batliner, A. The Interspeech 2009 emotion challenge. In Proceedings of the Annual\\nConference of the International Speech Communication Association (Interspeech), Brighton, UK, 6–10\\nSeptember 2009; pp. 312–315.\\n35. van der Maaten, L.; Hinton, G. Visualizing data using t-SNE. J. Mach. Learn. Res. 2008 ,9, 2579–2605.\\nPublisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional\\naﬃliations.\\n©2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access\\narticle distributed under the terms and conditions of the Creative Commons Attribution\\n(CC BY) license (http: //creativecommons.org /licenses /by/4.0/).', metadata={'source': 'papers/LEE_clean.txt'})],\n",
       " [Document(page_content='Vocal patterns in infants with Autism Spectrum Disorder:\\nCanonical babbling status and vocalization frequency\\nElena Patten, Ph.D.1, Katie Belardi, M.S.2, Grace T. Baranek, Ph.D.2, Linda R. Watson, Ed.D.\\n2, Jeffrey D. Labban, Ph.D.1, and D. Kimbrough Oller, Ph.D.3\\n1Univ. of North Carolina, Greensboro\\n2Univ. of North Carolina, Chapel Hill\\n3Univ. of Memphis, and Konrad Lorenz Institute for Evolution and Cognition Research,\\nKlosterneuburg, Austria\\nAbstract\\nCanonical babbling is a critical milestone for speech development and is usually well in place by\\n10 months. The possibility that infants with ASD show late onset of canonical babbling has so far\\neluded evaluation. Rate of vocalization or “volubility” has also been suggested as possibly\\naberrant in infants with ASD. We conducted a retrospective video study examining vocalizations\\nof 37 infants at 9–12 and 15–18 months. Twenty-three of the 37 infants were later diagnosed with\\nASD and indeed produced low rates of canonical babbling and low volubility by comparison with\\nthe 14 typically developing infants. The study thus supports suggestions that very early vocal\\npatterns may prove to be a useful component of early screening and diagnosis of ASD.\\nKeywords\\ncanonical babbling; volubility; vocal patterns; early detection\\nASD and early vocal development\\nEarly intervention is critical for positive outcomes for children with Autism Spectrum\\nDisorder (ASD). Early identification of atypical behaviors that manifest during infancy\\ncould significantly impact age of diagnosis and subsequent initiation of intervention.\\nCurrently, the minimum age at which the majority of children with ASD can be reliably\\ndiagnosed with relative stability is two years (e.g., Chawarska et al., 2009; Lord, 1995), but\\naccording to recent data from the Centers for Disease Control, many children are not\\nCorrespondence concerning this article should be addressed to: Elena Patten. University of North Carolina at Greensboro, 300\\nFerguson Building, P. O. Box 26170, Greensboro, NC 27402-6170. e_patten@uncg.edu.\\nElena Patten, UNC Greensboro, 300 Ferguson Building, Greensboro, NC 27412-6170\\nKatie Belardi, UNC Chapel Hill, Bondurant Hall, CB#7190, Chapel Hill, NC 27599-7190\\nGrace Baranek, UNC Chapel Hill, Bondurant Hall, CB #7122, Chapel Hill, NC 27599-7190\\nLinda Watson, UNC Chapel Hill, Bondurant Hall, CB#7190, Chapel Hill, NC 27599-7190\\nJeffrey Labban, UNC Greensboro, 231 HHP Building, Greensboro, NC 27412\\nD. Kimbrough Oller, The University of Memphis, 807 Jefferson Avenue, Memphis, TN 38105\\nElena Patten and Jeffrey Labban are at UNC Greensboro in North Carolina, USA; Katie Belardi, Grace Baranek and Linda Watson are\\nat UNC Chapel Hill in North Carolina, USA; D. Kimbrough Oller is at the University of Memphis in Tennessee, USA.\\nNIH Public Access\\nAuthor Manuscript\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nPublished in final edited form as:\\nJ Autism Dev Disord . 2014 October ; 44(10): 2413–2428. doi:10.1007/s10803-014-2047-4.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptdiagnosed until preschool or kindergarten age (2012). Research targeting early detection has\\nprimarily focused on behaviors exhibited during toddlerhood (12–36 months) and preschool\\nyears (36–60 months) (e.g., Matson, Fodstad, & Dempsey, 2009; Volkmar & Chawarska,\\n2008) after diagnosis has been made. Use of retrospective video analyses and studies of\\ninfant siblings of children diagnosed with ASD has allowed examination of possible\\nindicators of ASD in the first year of life (e.g., Baranek, 1999; Osterling, Dawson, &\\nMunson, 2002; Sheinkopf, Iverson, Rinaldi, & Lester, 2012; Zwaigenbaum et al., 2005).\\nStill, the most widely used autism screening tool for young children, the Modified Checklist\\nfor Autism Toddlers (MCHAT: Robins, Fein, Barton, & Green, 2001) is recommended for\\nages 16–30 months.\\nWe sought to identify potential communication markers of ASD that might be observed\\nwithin the first year of life in a retrospective evaluation of data from infants recorded at\\nhome and later diagnosed with ASD. We focused on presumed precursors to language for\\ntwo reasons: First, communication impairment is a core deficit in ASD, and second,\\nevaluation of very early vocal behaviors in typically developing infants has already\\nestablished markers that are critical to normal vocal communicative development. One\\nrobust pre-speech vocal milestone is the onset of canonical babbling. A canonical syllable\\n(e.g., [ba]) is comprised of a consonant-like sound and a vowel-like sound, with a rapid\\ntransition between them (Oller, 1980, 2000). A second potentially important vocal measure\\nthat we considered is volubility, the rate of infant vocalization independent of vocal type\\n(Nathani, Lynch, & Oller, 1999; Obenchain, Menn, & Yoshinaga-Itano, 1998).\\nCanonical babbling as a key milestone\\nIn typical development, infants from birth produce vegetative vocalizations (e.g., coughs,\\nburps, etc.) and cry, as well as vowel-like sounds that become more elaborate with time,\\nincorporating supraglottal articulations until canonical syllables emerge, usually by early in\\nthe second half-year of life. Robust onset of canonical babbling has been well documented\\nin typically developing infants by not later than 10 months (Koopmans-van Beinum & van\\nder Stelt, 1986; Oller, 1980; Stark, 1980). The impression of robustness has been reinforced\\nby the fact that no delay in onset of canonical babbling has been discerned in infants\\nanticipated to be at-risk for communication deficits due to premature birth or low\\nsocioeconomic status (Eilers et al., 1993; Oller, Eilers, Basinger, Steffens, & Urbano, 1995).\\nEven infants with Down syndrome usually show normal ages of onset, although a group\\nlevel delay of a month or more is detectable (Lynch et al., 1995). Furthermore, infants\\ntracheostomized at birth to provide an artificial airway that prevents or substantially inhibits\\nvocalization for many months tend to produce age-appropriate canonical syllables within a\\nshort period after decannulation (Bleile, Stark, & McGowan, 1993; Locke & Pearson, 1990;\\nRoss, 1983; Simon, Fowler, & Handler, 1983).\\nOnly profound hearing impairment and Williams syndrome have been shown to produce\\nconsistent substantial delays in the onset of canonical babbling (Kent, Osberger, Netsell, &\\nHustedde, 1987; Koopmans-van Beinum, Clement, & van den Dikkenberg-Pot, 1998;\\nMasataka, 2001; Oller & Eilers, 1988; Stoel-Gammon & Otomo, 1986). Further supporting\\nthe idea that restricted hearing prevents experiences critical to onset of canonical babbling,Patten et al. Page 2\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptage of onset in severely or profoundly hearing impaired infants has been reported to be\\npositively correlated with age of amplification (Eilers & Oller, 1994).\\nIn infants without known disorders, onset of canonical babbling after ten months has been\\nshown to be a significant predictor of language delay or other developmental disabilities\\n(Oller, Eilers, Neal, & Schwartz, 1999; Stark, Ansel, & Bond, 1988; Stoel-Gammon, 1989).\\nBut late onset of canonical babbling is a rare occurrence in infants without easily diagnosed\\nphysical or mental limitations. The seeming resistance to derailment of this developmental\\nmilestone suggests that canonical babbling is of such importance in human development that\\nit has been evolved to emerge within a relatively tightly constrained time period in spite of\\nsubstantial variations in home environments and perinatal events. The importance of\\ncanonical babbling in predicting later language functioning is assumed to be due to the fact\\nthat words are overwhelmingly composed of canonical syllables, and thus lexical learning\\ndepends on control of canonical syllables.\\nTo date, only two studies of which we are aware have targeted canonical babbling in ASD\\nand neither specifically examined the onset  of canonical babbling, But reasons for optimism\\nthat delays in onset of canonical babbling could constitute an early ASD marker can be\\nfound in research showing that various aspects of vocalization appear to be disrupted in\\nyoung children with ASD (Paul, Augustyn, Klin, & Volkmar, 2005; Peppe, McCann,\\nGibbon, O’Hara, & Rutherford, 2007; Sheinkopf, Mundy, Oller, & Steffens, 2000; Warren,\\nGilkerson, Richards, & Oller, 2010; Wetherby et al., 2004). Research using automated\\nanalysis of all-day recordings based on the automated LENA (Language ENvironment\\nAnalysis) system of classification has shown clear indications that young children with ASD\\n(16–48 months) display low rates of canonical syllable production compared with typically\\ndeveloping infants, even after matching of subgroups for expressive language (Oller et al.,\\n2010). Even more to the point, one recent study has assessed the usage of canonical syllables\\n(though not the onset  of canonical babbling) in infants at high-risk  for ASD because they\\nwere siblings of children with ASD; seven of 24 participants in the study received a\\nprovisional diagnosis of ASD at 24 months (Paul, Fuerst, Ramsay, Chawarska, & Klin,\\n2011). As a group, the at-risk infants (all 24) produced significantly lower mean canonical\\nbabbling ratios (canonical syllables divided by all “speech-like” vocalizations, i.e., those\\ndeemed “transcribable” by the researchers) compared to low-risk infants at nine-months of\\nage, but there were no significant differences at 12 months. “Non-speech” vocalizations\\n(those deemed “not transcribable” e.g., yells, squeals, growls) were not included in the\\nevaluation of canonical babbling. Other vocal measures—especially number of consonant-\\nlike elements and number of speech-like and proportion of non-speech-like vocalizations—\\nalso appeared to be potentially useful indicators of emergent ASD.\\nVolubility in ASD\\nVolubility, or rate of vocalization, measured in terms of frequency of syllable or utterance\\nproduction, may be limited in ASD, a possibility that is supported by automated analysis of\\ndata showing low volubility in ASD from all-day recordings on children from 16 to 48\\nmonths of age based on the LENA system (Warren et al., 2010). Volubility in infants with\\nsevere or profound hearing loss and in infants with Down syndrome has not been found toPatten et al. Page 3\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptbe depressed compared with typically developing infants; however, infants from lower\\nsocio-economic status (SES) have been shown consistently to produce fewer utterances per\\nminute than their middle or high SES peers (Eilers et al., 1993; Oller et al., 1995). Research\\nsuggests that children in low SES experience less communication from caregivers (Hart &\\nRisley, 1995; Snow, 1995). The lower volubility of these infants may be a product of\\ndecreased social-communication from adults, potentially resulting in lower levels of social\\nmotivation in the infants.\\nVariability in moment-to-moment parental interactivity clearly does affect infant volubility\\nby the middle of the first year of life, as indicated by research on parent-infant interaction in\\nthe “still-face” paradigm. The work suggests a strong tendency in the particular case of\\nparent still-face for infants to increase vocalization rate. Specifically, volubility during a\\nbaseline period of one to three minutes of face-to-face vocal interaction is substantially\\nlower than during a following still-face period of one to three minutes where the parent\\nwithholds any facial or vocal reaction while continuing to look directly at the infant. This\\npattern is seen in infants after 5 months, but not at 3 months, where volubility does not\\nchange at the shift from face-to-face interaction to still-face (Delgado, Messinger, & Yale,\\n2002; Goldstein, Schwade, & Bornstein, 2009; Yale, Messinger, Cobo-Lewis, Oller, &\\nEilers, 1999). The results from the still-face paradigm are interpreted to mean that infants\\nseek to re-engage the withdrawn parent during the still-face period, having learned by the\\nmiddle of the first year that their vocalizations can have impact (Tronick, 1982). This effect\\nraises the question of whether infants with emergent ASD similarly increase their volubility\\nto re-engage their caregivers after a period of withdrawn caregiver attention, or whether they\\ndecrease volubility, possibly due to diminished motivation to engage socially with others.\\nFrequency of vocalizations directed at others has been reported to be significantly lower in\\ninfants later diagnosed with ASD compared to typically developing infants at 12 months but\\nnot at 6 months (Ozonoff, Iosif, Baguio, Cook, Hill, et al., 2010). It is also notable that\\nfrequency of vocalization based on parent report is predictive of language abilities in\\ntoddlers with ASD (Weismer, Lord, & Esler, 2010). Paul et al. (2011) assessed frequency of\\nvocalization in infants at high-risk and low-risk for developing ASD and found no difference\\nbetween groups. However, the study did not actually test for volubility the way volubility is\\ndefined here and in much prior research. Frequency of vocalization was tallied in a special\\nway in the Paul et al. study, by counting all speech-like (phonetically transcribable) and\\nnonspeech-like (not phonetically transcribable) vocalizations that occurred within the first\\n50 speech-like vocalizations of each recorded sample. But not all participants produced 50\\nspeech-like utterances, and in the ones who did, the length of recording required to reach the\\n50 speech-like utterance criterion was variable. Thus, rate of vocalizations per unit of time\\nwas not examined in this study; consequently, given the common usage of the term\\nvolubility, it is not possible to determine whether there was a difference in volubility\\nbetween the groups. In addition, participants in this study were at high-risk for ASD—some\\nwere later diagnosed with ASD while some were not. This mixture may have attenuated\\ngroup differences. It should also be noted that Weismer et al. included only child\\nvocalizations directed at others while Paul et al. included vocalizations. Although ASD has\\nroots in social impairments, vocalizations directed at others as well as independent vocal\\nplay might well be abnormal in ASD.Patten et al. Page 4\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptA new study of early vocal development in ASD\\nOne reason the development of pre-speech vocal behaviors in ASD has not been well\\ndocumented may be that ASD is not reliably diagnosed until long after canonical syllables\\nare expected to emerge, thus making prospective analyses challenging. Retrospective\\ninterviews with parents whose children have been diagnosed with ASD regarding age at\\nwhich canonical syllables emerged may be hindered by poor parent recall, given that parents\\nare generally asked to remember the nature of child babbling that occurred one or more\\nyears prior to the time of the interview; also, parents’ awareness of the diagnosis may bias\\ntheir recall of the onset of canonical babbling. The effort by Paul et al. (2011) cited above\\nrepresents a key advancement in methodology because they assessed infants known to be at-\\nrisk in a prospective fashion. Our approach seizes an additional opportunity afforded by the\\nfortuitous existence of home video data from the first year of life that can be analyzed after\\ndiagnosis of ASD for comparison with similar video data from infants who did not receive\\nthe diagnosis.\\nAs indicated in studies cited above, emergence of canonical syllables is a critical milestone\\nin the development of spoken language, and delayed onset has been shown to be predictive\\nof significant communication impairment. Canonical babbling and volubility have not been\\nwell characterized in infants with ASD. To arrive at a better understanding of these two\\nvariables as potential indicators of ASD risk in infants, we investigated vocalizations of\\ninfants later diagnosed with ASD and typically developing (TD) infants at two age ranges,\\n9–12 months and 15–18 months, using retrospective video analysis methods. Previous\\nresearch has suggested that nearly all TD infants reach the canonical babbling stage by 9–12\\nmonths (Eilers & Oller, 1994), and on the assumption that a delay might be present in the\\nchildren later diagnosed with ASD, we predicted such delay would be observed in this age\\nrange. We took the opportunity also to evaluate the available data at 15–18 months because\\nany infant with a failure to show canonical babbling at that age would be greatly delayed in\\ncanonical babbling onset and would be considered at very high risk for a variety of\\ndisorders.\\nThe coding scheme for this study is based on a widely applied method for laboratory-based\\nevaluation of canonical babbling (Oller, 2000). In accord with this method, infants are\\nassumed to be in the canonical stage if they show a canonical babbling ratio (canonical\\nsyllables divided by all syllables) of at least .15, a value based on coding by trained listeners\\nof a recording. A value of .15 or greater from such laboratory coding has been empirically\\ndetermined in prior research as corresponding to parent judgments that infants are in the\\ncanonical stage (Lewedag, 1995). It has been reasoned that parent judgments constitute the\\nmost appropriate standard for establishing this criterion value (Oller, 2000). This reasoning\\nis based on three points: 1) Parents respond to interview questions by providing very\\nconsistent and accurate information about canonical babbling in their infants (Papoušek,\\n1994; Oller, Eilers, & Basinger, 2001); 2) this parental capability is predictable, given that\\nrecognizing canonical babbling represents nothing more than being able to recognize\\nsyllables as being well-formed enough that they could form parts of words in real speech\\n(and of course normal adults can easily recognize vocalizations of humans as speech or non-\\nspeech); and 3) parents appear to intuitively understand that the onset of canonical babblingPatten et al. Page 5\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptis an emergent foundation for speech, as evidenced by the fact that they initiate intuitive\\nlexical teaching as soon as they begin to recognize canonical babbling in their infants\\n(Papoušek, 1994). Consistent parent recognition of the onset of canonical babbling runs in\\nparallel with recognition of other developmental milestones (e.g., sitting unsupported,\\ncrawling, walking). In our study we could not use parents as informants about the age of\\nonset of canonical babbling since that onset had occurred a very long time before our first\\ncontact with them. Consequently, the canonical babbling ratio, determined from recordings\\ncoded in our laboratory, provided the best available measure upon which to base inference\\nabout whether infants had reached the canonical stage.\\nIn the present study the following hypotheses were tested:\\n1.Infants later diagnosed with ASD will be less likely than TD infants to be in the\\ncanonical stage at each age (9–12 and 15–18 months), as determined by whether\\ntheir canonical babbling ratios exceed the .15 criterion.\\n2.Infants later diagnosed with ASD will demonstrate significantly lower canonical\\nbabbling ratios (independent of the canonical stage criterion) compared to TD\\ninfants.\\n3.Infants later diagnosed with ASD will demonstrate significantly fewer total\\nvocalizations (lower volubility) at both age ranges compared to TD infants.\\n4.A combined analysis using both volubility and canonical babbling status will\\nsignificantly predict group membership.\\nMethod\\nParticipants\\nA total of 37 participants were included in the present study, 23 individuals later diagnosed\\nwith ASD and 14 individuals in the TD group (Table 1). There was one set of fraternal twins\\nin the ASD group. Participants were drawn from a larger study conducted at the University\\nof North Carolina-Chapel Hill based on availability of video recordings; participants must\\nhave had two five-minute edited video segments at 9–12 months and at least one edited\\nvideo segment at 15–18 months. As part of the larger study, participants were recruited from\\nthe Midwest and Southeast over a 15-year time period. Recruitment criteria included: (1)\\nchild age between two and seven years at the time of recruitment; (2) available home\\nvideotapes of the child between birth and two years of age that parents were willing to share;\\nand (3) enough video footage for at least one 5-minute codable segment (see video editing\\nsection below) of the child at either 9–12 or 15–18 months of age.\\nAll participants included in the ASD group received a clinical diagnosis of ASD from a\\nlicensed psychologist and/or physician at a point after the recordings were made. Thus, our\\ndesign is a retrospective analysis similar to others that have used home movies of children\\nlater diagnosed with ASD (Baranek, 1999; Werner, Dawson, Osterling, & Dinno, 2000). A\\ntrained research staff member validated diagnoses for each participant using criteria from\\nthe Diagnostic and Statistical Manual IV (American Psychiatric Association, 2000) and\\nfrom one or more ASD screening and diagnostic tools, including: the Childhood AutismPatten et al. Page 6\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptRating Scale (CARS; Schopler, Reichler, & Renner, 1992), the Autism Diagnostic\\nObservation Schedule (ADOS; Lord, et al., 1999), and/or the Autism Diagnostic Interview-\\nRevised (ADI-R; Rutter, LeCouteur, & Lord, 2003). All participants had CARS scores and\\neach participant in the ASD group had ADI/ADI-R scores and 13 of the 23 ASD participants\\nhad ADOS scores.\\nTypically developing group membership was based in part on scores within normal limits\\n(i.e., not more than one standard deviation below the mean) on the Mullen Scales of Early\\nLearning (Mullen, 1995) and/or the Vineland Adaptive Behavior Scales (VABS; Sparrow,\\nBalla, & Cicchetti, 1984). An additional exclusionary criterion for any participants in the TD\\ngroup was any history of learning or developmental difficulties per parent report. Individuals\\nwith significant physical, visual or hearing impairments or known genetic conditions (e.g.,\\nFragile X or Rett’s Syndrome) associated with ASD were excluded. As indicated in Table 1,\\nmean age (in months) was very similar across groups, gender was balanced, and the two\\ngroups were also similar with regard to SES based on maternal education. Our families were\\nmostly middle SES with access to videotaping equipment.\\nThe University of North Carolina-Chapel Hill Institutional Review Board approved the\\nstudy, and all families signed informed consents. For more information regarding\\nrecruitment and inclusion criteria see Baranek (1999).\\nVideo Editing Procedures\\nFamilies provided home videos of their child from birth to two years as available. The\\nvideotapes included footage from a variety of contexts including family play situations,\\nvacations, outings, special events, and familiar routines (e.g., mealtimes), with individual\\nvariation in situational content of each family’s videotapes as would be expected in home\\nvideotapes. All videotapes were copied, transformed to digital formats, and originals were\\nreturned to participating families.\\nVideo editing guidelines first focused on the identification of video footage during which the\\nchild was consistently visible and for which the parents felt they could accurately identify\\nthe child’s age. The two age ranges were originally selected for another study on early\\nbehavior in ASD (Baranek, 1999). At the same time, the two age ranges are well-suited to\\nour current purposes. The 9–12 month age range is the earliest age range in which parents\\nhad sufficient videotape footage for it to be useful in our research and represents the time\\nperiod when a number of communicative behaviors emerge. Further, this is a time frame\\nduring which the vast majority of TD children would be expected to already be in the\\ncanonical babbling stage. The 15–18 month range provided follow-up on the same children\\nwith the expectation that monitored behaviors would be more consistent and would allow for\\nconfirmation or clarification of data from the earlier age. In TD children, canonical babbling\\nis usually well consolidated by the 15–18 month age range (Vihman, 1996; Oller, 2000).\\nIn editing tapes for the larger study, the aim was to compile two 5-minute video segments\\nfor each child in the 9–12 age range, and two 5-minute segments in the 15–18 month age\\nrange. On average, each 5-minute segment consisted of 5 scenes. Research assistants who\\nwere blind to the research questions and not informed of the diagnostic status of thePatten et al. Page 7\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptparticipants edited the videotapes and coded each scene for the following content variables:\\n(a) number of people present; (b) amount of physical restriction on child’s freedom to move,\\nrated as low, medium, or high; (c) the amount of social intrusion another person was using\\nto engage the child in interaction, rated as low, medium, or high; (d) and the types of events\\n(e.g., meal time, bath time, active play, special events) (Baranek, 1999). The assistants were\\ninstructed to quasi-randomly select a cross-section of scenes from the available footage in\\nthe designated age ranges, purposely including scenes from each one-month age interval for\\nwhich video footage was available within each age range, provided that the child was visible\\nin each selected scene. All participants included in the current study had two 5-minute\\ncompilations (i.e., 10 minutes total) for the 9–12 month age range, but at the 15–18 month\\nage range, there were three TD infants and one infant with ASD for whom only a single 5-\\nminute segment was assembled due to insufficient video footage. As a result, the mean\\nduration of samples at the 15–18 month age range was 9.5 minutes rather than 10.\\nAlthough vocalization from the infants was common in these scenes, the segments were not\\nspecifically selected to capture vocal behavior. Therefore, volubility estimated from the\\npresent study may be lower than in prior works where infants have been observed in settings\\ndesigned to maximize vocal interaction. Similarly, the video segment selection procedure\\nmay yield differences in canonical babbling from prior studies. In most studies, 20–30\\nminutes of vocal interaction have been recorded, whereas here we had less than half that\\namount of data per sample. Our procedure can be predicted to produce greater variability in\\ncanonical babbling ratios than in studies with longer sampling periods (Molemans, 2011;\\nMolemans et al., 2011). Additionally, the audio-video quality of these home movies was not\\nas good as would be expected in laboratory studies, another factor that could reduce\\nperceived canonical babbling and volubility.\\nTo ensure that the contexts in which children were recorded were comparable, specific\\ncontent parameters were identified and compared (Tables 2 and 3). No differences were\\nfound between the groups on any content parameter including: number of people present,\\nlevel of physical restriction (i.e., amount of physical confinement such as a highchair versus\\nfree play; rated as low, medium or high), amount of social intrusion (rated as low, medium\\nor high), and the total number of event types (e.g., meal time, active play). The number of\\ntimes each event type (e.g., bath time, playtime) was represented in the ASD group versus\\nthe TD group for each age was compared using chi-square analyses. Results for the omnibus\\nchi-square test failed to reach significance in the 9–12 month age group ( p > 0.05), but did\\nreach significance in the 15–18 months age group ( p = 0.046). Typically developing children\\nwere more likely to be engaged in passive activities at the 15–18 month age range ( p =\\n0.046; TD = 16.6%, ASD = 4.6%) according to follow-up analysis of the six event\\ncategories. See Tables 4 and 5 for the percentage in each category. For a comprehensive\\ndescription of the coding procedures that yielded the data on situational context see Watson,\\nCrais, Baranek, Dykstra, and Wilson (2012).\\nCoding Procedure and Observer Agreement\\nThe videotapes analyzed in this study were coded for infant production of all syllables in\\nspeech-like vocalizations by two certified speech-language pathologists who were notPatten et al. Page 8\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptinformed of the diagnostic group of the infants. The intent was, of course, for the coders to\\nbe blind to diagnostic category, and with the exception of one infant to be discussed below,\\nthe coders reported they saw no reason to suspect any infant of having ASD.\\nWe defined speech-like vocalizations (as in the primary literature on canonical babbling) to\\ninclude both canonical and precanonical infant vocalizations (regardless of whether they\\nwould be deemed “transcribable”). Training of the two coders was provided by the last\\nauthor, who originated the definition of “canonical syllable” used in this study, and who has\\nconducted and collaborated on numerous studies on onset of canonical babbling, rate of\\ncanonical babbling, and volubility in infants (Cobo-Lewis, Oller, Lynch, & Levine, 1996;\\nLynch et al., 1995; Oller & Eilers, 1982, 1988; Oller, Eilers, & Basinger, 2001; Oller et al.,\\n1995; Oller, Eilers, Neal, & Cobo-Lewis, 1998). The two observers were trained in\\nidentifying canonical syllables and in counting all syllables independent of their canonical\\nstatus. The video samples used during training were separate (although drawn from similar\\nmaterials based on the home recordings) and not included in the analyses for this\\ninvestigation.\\nSyllables were defined as rhythmic units of speech-like vocalization, excluding raspberries,\\neffort “grunt” sounds (i.e., a schwa-like sounds produced as an artifact of physical exertion),\\ningressive sounds, sneezes, hiccups, crying and laughing. Within an “utterance”, which was\\ndefined as a vocal breath group (Lynch, Oller, Steffens, & Buder, 1995), it was possible to\\nidentify syllables as corresponding to sonority peaks (high points of pitch and/or amplitude)\\nthat are intuitively recognized by mature listeners. These rhythmic events occur in time\\nframes typical of syllables in real speech (usually with durations of 200–400 ms). A\\ncanonical syllable is defined as including a vowel-like nucleus, at least one margin (or\\nconsonant-like sound) and a transition between margin and nucleus that is rapid and\\nuninterrupted. In general, transitions that are too fast to be tracked auditorily (too fast to be\\nheard “as transitions”) are instead heard as gestalt syllables. Auditory tracking of these\\ntransitions focuses on formant (acoustic energy) transitions that can be measured on\\nspectrograms as typically < 120 ms (Oller, 2000). Formants are audible bands of energy\\ncorresponding to resonant frequencies of the vocal tract that change as the tract changes\\nshape or size. Audible formant transitions occur, then, when the vocal tract moves during\\nopening from a consonantal closure into a vowel or vice versa.\\nExamples of canonical utterances (which must include at least one canonical syllable) are\\nsyllables that a listener might perceive as ba, taka, or gaga. Vocalizations produced while\\nmouthing objects (e.g., toys or fingers) or eating were excluded from our analyses on the\\ngrounds that we could not be sure what role movement of the hands may have played in the\\napparent syllabification.\\nVideos were randomized and randomly distributed across the two coders with regard to\\ndiagnostic group. The 37 participants’ videos were randomly split between the coders by\\nparticipant and included both age ranges. The coders independently watched the videos,\\ncounting both syllables and canonical syllables in real time. This procedure is utilized\\nregularly in the laboratories of the last author in accord with reasoning presented in recentPatten et al. Page 9\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptpapers, especially Ramsdell et al. (2012). This naturalistic listening approach mimics how a\\nmother would hear her child, listening to each utterance only once.\\nThe measure of canonical babbling ratio used here (number of canonical syllables divided\\nby number of all syllables) is the measure utilized in the bulk of research on onset of\\ncanonical babbling to date. However, some studies have used a different ratio (number of\\ncanonical syllables divided by number of utterances). The former procedure is generally\\npreferred nowadays because the resulting value can be interpreted as a proportion with\\nvalues varying from 0 to 1, whereas the latter procedure yields a ratio with no effective\\nupper limit (Oller, 2000).\\nIn a coder agreement test, both observers independently coded twenty samples consisting of\\ntwo five-minute segments of ten participants’ video footage. A research assistant unaware of\\nthe study goals selected these test samples, and they represented both diagnostic groups and\\nboth ages. Reliability was gauged in accord with the degree to which coders agreed upon\\ncanonical syllables, total syllables, and whether the child was in the canonical babbling stage\\n(i.e., had a canonical babbling ratio > 0.15, the standard criterion). Inter-rater agreement\\nranged from good to excellent for canonical syllables (ICC = .98, CI95 = .96 – .99) and for\\ntotal syllables (ICC = .87, CI95 = .61 – .95). Reliability for canonical babbling ratios was\\nalso good (ICC = .89, CI95 = .69 – .96), with agreement on the canonical stage criterion at\\n95% for the twenty samples. Additionally, the coders differed by an average of only 10% of\\nthe total range of canonical babbling ratios obtained, and the correlation across the ratios for\\nthe twenty samples for the two coders was .89. For volubility, the coders differed by an\\naverage of 13% of the total range for volubility values, and the correlation across the twenty\\nsample videos for the two coders was .91.\\nResults\\nAnalyses were performed to confirm that the groups were matched on demographic\\nvariables. These analyses did not reveal significant differences between groups on any\\nvariable (see Table 1).\\nInitial descriptive statistics for within- and between-group variables revealed two outliers in\\nthe ASD group. Both cases produced very high canonical babbling ratios in the 9–12 month\\nrange (.93 and .64) relative to the mean for both groups (ASD = .12 for the 23 cases, TD = .\\n17) (see Figure 1). Based on prior research, the canonical babbling ratios observed for these\\ntwo ASD cases were substantially higher than would be expected in TD infants in the 9–12\\nmonth age range—infants grouped as having English or Spanish at home, as high or low\\nSES, and as born at term or prematurely all showed mean canonical babbling ratios under .4\\nfrom 8 to 12 months of age (Oller, Eilers, Urbano, & Cobo-Lewis, 1997; Oller, Eilers,\\nSteffens, Lynch, & Urbano, 1994). Analysis of z-scores revealed that infant 22 was 3.96\\nstandard deviations above the mean for the present sample, and infant 23 was 2.73 standard\\ndeviations above the mean, further suggesting outlier status. On this basis we decided to\\neliminate these two cases in the primary analyses on canonical babbling; the remaining 35\\ncases (21 ASD, 14 TD) were analyzed to address our research questions regarding canonical\\nbabbling (see Figures 1 and 2 for canonical babbling ratios by participant at both ages, withPatten et al. Page 10\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptthe two outliers indicated). However, there were no significant outliers with regard to\\nvolubility, and thus we included data from all 37 cases for that analysis (see Figures 3 and 4\\nfor syllable volubility by participant at both ages).\\nHypothesis 1: Infants later diagnosed with ASD will be less likely than typically developing\\ninfants to be in the canonical stage at each age (9–12 and 15–18 months)\\nLog odds ratios (log OR) were calculated to compare the classifications of both ASD and\\ntypically developing children with regard to their canonical babbling. The criterion for\\ncanonical babbling stage was set at 15% or greater canonical syllables compared to all\\nsyllables; this is a common criterion in studies of canonical babbling, and is based on data\\nreviewed in Oller (2000). TD infants were significantly more likely to have reached the\\ncanonical babbling stage based on the criterion than were infants later diagnosed with ASD\\nat the 9–12 months age range (N = 35, log OR = 2.84, CI 95 = 1.02 to 4.66, p = 0.002), and\\nremained more likely at the 15–18 month age range (N = 35, log OR = 1.78, CI 95 = −0.04 to\\n3.61, p = 0.054). As an easily interpretable effect size measure, the simple odds ratios (as\\nopposed to the log odds ratio, which is statistically preferable for significance testing with\\nsmall N’s) can be considered; the simple ORs indicated TD infants were 17 times more\\nlikely (OR = 17.1) to be categorized as in the canonical stage than ASD infants at 9–12\\nmonths and 6 times more likely (OR = 5.96) at 15–18 months.\\nHypothesis 2: Infants later diagnosed with ASD will demonstrate significantly lower\\ncanonical babbling ratios (independent of the canonical stage criterion) compared to\\ntypically developing infants\\nCanonical babbling ratios of infants later diagnosed with ASD and TD infants were\\ncontrasted using a Mixed ANOVA. The between-subjects variable was diagnostic category\\n(ASD vs. TD) and the within-subjects variable was age range (9–12 months and 15–18\\nmonths). The mean canonical babbling ratios at 9–12 months were .06 (SD = .06) for the 21\\ninfants later diagnosed with ASD and .17 (SD = .13) for the 14 TD infants; at 15–18 months\\nthe values were .16 (SD = .22) and .28 (SD = .16) respectively (Figure 5). Analyses revealed\\na significant main effect for diagnostic category ( F (1,1) = 6.79, p = .01, ŋp2 = 0.17), with\\ninfants later diagnosed with ASD producing significantly lower canonical babbling ratios,\\nand a significant main effect for age ( F (1,1) = 7.86, p < .01, ŋp2 = 0.19), with higher\\ncanonical babbling ratios at the older age. The effect size between groups for 9–12 months\\nwas d = 1.09 (a large effect) and for 15–18 months was .62 (a moderate effect; Cohen,\\n1992). The age by diagnosis interaction was not significant ( p > 0.66).\\nHypothesis 3: Infants later diagnosed with ASD will demonstrate significantly fewer total\\nvocalizations (lower volubility) at both age ranges compared to typically developing\\ninfants\\nFor this analysis, all 37 infants were included because there were no significant outliers.\\nVolubility of infants later diagnosed with ASD and TD infants were contrasted using a\\nMixed ANOVA. The between-subjects variable was diagnostic category (ASD vs. TD), and\\nthe within-subjects variable was age range (9–12 months and 15–18 months). Infants later\\ndiagnosed with ASD produced a mean of 4.55 (SD = .59) syllables per minute while TDPatten et al. Page 11\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptinfants produced a mean of 5.86 (SD = .67) syllables per minute at 9–12 months. At 15–18\\nmonths, infants later diagnosed with ASD produced a mean of 3.24 (SD = .49) syllables per\\nminute while TD infants produced a mean of 4.63 (SD = .51) syllables per minute (see\\nFigure 6). Analyses revealed a significant main effect for diagnostic category ( F (1,1) =\\n4.85, p = .034, ŋp2 = 0.12), and for age ( F (1,1) = 4.96, p = .032, ŋp2 = 0.12). Thus, infants\\nlater diagnosed with ASD displayed significantly lower volubility than TD infants. The\\neffect size for group at 9–12 months was d = 2.07 (large) and at 15–18 months was 2.77\\n(large).\\nHypothesis 4: A combined analysis using both volubility and canonical babbling status\\nwill significantly predict group membership\\nLogistic regression analysis was conducted to test whether canonical babbling status\\n(whether each participant met the .15 criterion) and volubility at age ranges of 9–12 months\\nand 15–18 months could reliably predict later diagnosis status (group membership). This test\\nwas conducted with all 37 cases included, partly in order to match the number of cases for\\nthe two predictor variables and partly because the goal of the analysis was to determine the\\npotential practical utility of identification of these children without any information other\\nthan volubility and canonical babbling ratio. This test may thus be the one of primary\\nclinical interest, since it evaluates the circumstance that screening implies, where there\\nwould be no basis for knowing whether an infant might be an outlier on any variable.\\nWithout this evaluation there would be no direct indication in our results of the degree of\\ngroup discriminability.\\nStatistical significance was reached in a test of the full model against a constant-only model,\\nwhich indicated that, as a set, canonical babbling status and volubility reliably predicted\\nlater diagnosis ( χ2 = 9.82, p = 0.044, df = 4). A small-to-moderate relationship between\\nprediction and grouping was observed ( Nagelkerke’s R2 = 0.317), with an overall prediction\\nsuccess of 75% (64% for TD and 82% for ASD). However, further examination of the\\npredictors using the Wald criterion revealed that when all four predictor variables were\\nincluded in the model, none significantly contributed to prediction of group membership at\\nan individual level ( p > 0.05). The status of infants with regard to canonical babbling stage\\nat the 9–12 months age range provided the largest observed predictive contribution, Wald =\\n3.06, p = 0.08, EXP(B) = 0.198. The contribution to group discriminability by volubility at\\n9–12 and 15–18 months age ranges approached nil, EXP(B) = 0.992 and 0.985 respectively.\\nExamination of the correlations among the predictor variables showed that all but volubility\\nat 9–12 months were significantly correlated with all other predictors (Table 6), with\\nvolubility at 9–12 months significantly correlated with only canonical babbling at 9–12\\nmonths. This inter-relation among the predictor variables suggests that, to some degree, they\\naccount for some of the same variance in diagnosis. However, the observed EXP(B)  values\\n(odds ratios of the outcomes given the value of an individual predictor) more strongly\\nsuggest that canonical babbling at 9–12 months accounted for the bulk of the variability in\\ndiagnosis.\\nIt seems clear that significance of the individual predictors in the logistic regression may\\nhave been hampered by the high level of relation among them. Individually, the volubilityPatten et al. Page 12\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptvariables did not appear to have much influence given small Betas and high p-values. When\\npredictors were entered into the model in a hierarchical fashion, no matter how predictor\\nentry was ordered (9–12 month variables at step 1 and 15–18 month variables at step 2, or\\nCB variables at step 1 and volubility at step 2), only the 9–12 month CB variable was a\\nsignificant independent predictor. R2 changes and diagnostic ability for all of the regression\\nand regression step iterations suggested little was added to the R2 by adding variables in the\\nsecond step (even with two more predictor variables, only ~ 2–3% was added to R2), nor did\\nthese additions substantially alter the ability of the model to predict later diagnosis. The\\nmost efficient model appeared to be a logistic regression with 9–12 month CB as the only\\npredictor.\\nDiscussion\\nThe importance of early intervention for children with ASD has resulted in attempts to\\nquantify behaviors in infancy that may lead to early detection. Substantial effort has\\naddressed gestural and social development and their potential roles in detection within the\\nfirst year of life (e.g., Watson, Crais, Baranek, Dykstra, & Wilson, 2013). The present\\nresults offer parallel findings in the domain of vocal development by demonstrating\\nsignificant group differences in canonical babbling status, canonical babbling ratio, and total\\nsyllables produced (volubility) during the first year of life.\\nIn our study, infants later diagnosed with ASD were significantly less likely to be classified\\nas being in the canonical babbling stage, and demonstrated significantly reduced canonical\\nbabbling ratios compared to TD peers. Although significant group differences were apparent\\nin both age ranges (9–12 and 15–18 months), the effect sizes for canonical babbling were\\nlarger at 9–12 months. Paul et al. (2011) demonstrated similar results in infants at high-risk\\nfor developing ASD who produced significantly lower canonical babbling ratios compared\\nto low risk infants at 9 months, though at 12 months the differences were not statistically\\nsignificant. Combined with the finding from Oller et al. (2010) that children with ASD up to\\n48 months of age show low canonical syllable production, the data here suggest that low\\nproduction of canonical syllables may be a helpful marker for ASD from infancy into early\\nchildhood.\\nSince canonical babbling is well established in the vast majority of TD infants by 10 months\\n(Eilers and Oller, 1994), it might seem odd that several of the TD infants (5 at 9–12 months\\nand 3 at 15–18 months) in the present study provided samples that did not meet the .15\\ncanonical babbling ratio criterion for assignment to the canonical stage of vocal\\ndevelopment. However, it is important to consider the fact that even infants who are clearly\\nin the canonical stage based on parent report often fail to reach the criterion in a single\\nlaboratory sample of 20–30 minutes (Lewedag, 1995). In addition, unlike the samples in\\nprior research on canonical babbling, the samples here were not designed to elicit\\nvocalizations, and consequently they may have been less rich in quantity and variety of\\nvocalization than the samples that were used to develop the criterion. Further, our samples at\\n9–12 months were only 10 minutes in duration, and at 15–18 months an average of slightly\\nless than 10 minutes; it has been shown that variability in obtained canonical babbling ratios\\nincreases as the length of samples decreases (Molemans, 2011; Molemans, Van den Berg,Patten et al. Page 13\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptVan Severen, & Gillis, 2011). Finally, our samples were based on home recordings with\\nconsiderable noise and variable camera management that may have impeded our ability to\\nrecognize vocalizations in the samples. Consequently, we are not surprised that some of the\\nTD infants failed to reach the criterion used to determine canonical status based on\\nlaboratory samples.\\nGiven the strong links between the onset of canonical babbling and language development\\n(Oller, Eilers, Urbano, & Cobo-Lewis, 1997; Stoel-Gammon, 1989), delayed onset of\\ncanonical babbling in infants with ASD may reflect latent communication impairment. It\\nalso may be that delayed canonical babbling directly contributes to communication\\nsymptoms in ASD. Canonical babbling requires motor ability as well as motivation to\\nproduce syllables, and practice in babbling may lay critical foundations for speech.\\nProspective research on motor development in infants later diagnosed with ASD is sparse\\nand often limited to high-risk groups, but available research does indicate that early motor\\nimpairment may be present (e.g., Matson, Mahan, Fodstad, Hess, & Neal, 2010; Manjiviona\\n& Prior 1995; Page & Boucher, 1998; Teitelbaum, Teitelbaurm, Nye, Fryman, & Maurer,\\n1998). Thus delayed canonical babbling may reflect an immature or disordered motor\\nsystem with specific implications for speech.\\nIf language develops as a consequence of social reinforcement of speech-like sounds that\\neventually evolve into true words (consider behavioral models of language development as\\nin Hulit & Howard, 2002; Goldstein, King, & West, 2003; Goldstein & Schwade, 2008;\\nGoldstein & West, 1999), then social reinforcement may encourage the production of\\ncanonical babbling. Children with ASD may be less motivated by social reinforcement,\\nyielding less frequent vocal exploration and production of canonical syllables than in TD\\ninfants. To add to the problem, a delay in canonical babbling may result in reduction in\\ncaregiver social-communication directed toward the infant. On average, by six to seven\\nmonths and very rarely later than ten months, canonical babbling emerges in TD infants\\n(Eilers & Oller, 1994). In response to recognition of canonical babbling, caregivers alter\\ntheir communication pattern, sometimes attempting to direct the infant toward using\\ncanonical syllables meaningfully—for example, the parent who hears [baba] may reply,\\n“Yes, that’s a bubble” (Papoušek, 1994; Stoel-Gammon, 2011). Therefore, infants who are\\ndelayed in canonical babbling may also be delayed in their exposure to important linguistic\\ninput, and thus may be given less opportunity to learn words. A final point is that infants\\nwith ASD may simply have lower motivation to vocalize socially in the first place. This\\nlower motivation could provide a further basis for slow vocabulary learning.\\nOur results on volubility included two statistically reliable findings. First, children in both\\ngroups had lower volubility at the second age than at the first. We attribute no particular\\ntheoretical importance to this finding but we take note of the fact that the lower level of\\nvolubility at 15–18 months compared to 9–12 months did correspond to greater physical\\nmovement of the children at the older age. In both groups combined, level of physical\\nrestriction during the selected recording samples was significantly less at the older age (p < .\\n001). As reported earlier, level of physical restriction was not significantly different between\\ndiagnostic groups.Patten et al. Page 14\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptThe second volubility finding is that infants later diagnosed with ASD produced\\nsignificantly fewer vocalizations deemed to be relevant for the emergence of speech (both\\ncanonical and non-canonical sounds) at both age ranges (9–12 and 15–18 months) compared\\nto TD peers. Other research has demonstrated that infants with ASD direct fewer\\nvocalizations to others (Ozonoff et al., 2010); our study extends this finding to a more\\ngeneral measure of volubility in terms of total vocalizations (syllables) rather than only ones\\ndirected to others. Our finding is also congruent with results from automated analysis of all-\\nday recordings indicating low volubility in children with ASD at 16–48 months of age\\n(Warren et al. 2010). The results may seem to run counter to Paul et al. (2011) whose\\nsample of high-risk infants were reported to not produce significantly fewer vocalizations\\nthan low-risk infants. However, as described in the introduction above, the Paul et al. study\\ndid not report data in a way that can be directly compared with the volubility data reported\\nhere.\\nSome disability groups (e.g., hearing impaired infants and infants with cleft palate) have\\nbeen reported to exhibit volubility similar to that of TD infants (Clement, 2004; Chapman,\\nHardin-Jones, Schulte, & Halter, 2001; Van den Dikkenberg-Pot, Koopmans-van Beinum, &\\nClement, 1998; Nathani, Oller, & Neal, 2007; Davis, Morrison, von Hapsburg, & Warner-\\nCzyz, 2005); however, infants from low SES households have been reported to have\\nsignificantly decreased volubility in comparison to those from higher SES households\\n(Oller, Eilers, Basinger, Steffens, & Urbano, 1995). Children from low SES backgrounds are\\noften presumed to be at-risk for language deficits. Although it would be impossible to\\nidentify and quantify all of the mechanisms through which poverty may affect language\\ndevelopment, research has demonstrated that the amount of communication caregivers direct\\ntoward their children is decreased in low SES situations (Hart & Risley, 1995). This\\nimpoverished linguistic environment may result in decreased dyadic social and\\ncommunicative interactions and thus in a decrease in overall volubility of infants.\\nIt is important to note that the relatively well-matched SES between our two groups suggests\\nthat the differences in volubility were not attributable to differences in SES. In the case of\\nlow SES households, an impoverished linguistic environment due to lack of parent\\nresponsiveness might be expected to lead to decreased volubility of the infant and later\\nlanguage difficulty. For infants later diagnosed with ASD, reduced volubility may be\\naffected by multiple factors, not related to inherent parental responsiveness, but related\\ninstead to the social impairments of ASD. One issue is that these children may experience\\nless linguistic stimulation due to having disrupted sensory processing systems corresponding\\nto sensory hyporesponsiveness; children with ASD are less likely to respond, or require\\nsubstantially more stimulation to respond to environmental events (Baranek, 1999; Baranek\\net al., 2013); Miller, Reisman, McIntosh, & Simon, 2001; Rogers & Ozonoff, 2005). This\\ncharacteristic of ASD is also reflected in the tendency for infants as young as eight months\\nwho will later be diagnosed with ASD to be less likely than TD infants to respond to their\\nname being called (Werner, Dawson, Osterling, & Dinno, 2000). This lack of\\nresponsiveness may indicate that infants with ASD are less affected by vocal\\ncommunication from caregivers than TD infants. If so, the lack of responsiveness may\\nreflect an effectively impoverished linguistic environment because of attenuated reception of\\ncaregiver input by infants with ASD and subsequent communication impairments. Indeed,Patten et al. Page 15\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptsensory hyporesponsiveness has been shown to be associated with poorer language\\nfunctioning in children with ASD (Watson et al., 2011).\\nAn additional way that the environment for children with ASD may be impoverished could\\ninvolve a social feedback loop (Warlaumont et al., 2010) that is under investigation using\\nautomated analysis of vocalizations of parents and infants from all-day home recordings.\\nSince infants with ASD produce fewer canonical syllables than TD infants, and since\\nparents respond strongly with language stimulation to canonical syllables, an infant with\\nASD may actually hear less language from parents, because parents provide input that is tied\\nto the infant’s output. The infant’s low volubility may then be aggravated by lower input\\nlevels resulting from the infant’s own anomalous pattern of vocalization.\\nFinally, the logistic regression analysis with four independent variables (age 1 and age 2\\ncanonical babbling classification and age 1 and age 2 volubility) demonstrated that\\nclassification of diagnostic category (ASD vs. TD) could be predicted with 75% accuracy,\\neven when the two outliers were included. The model more accurately classified infants later\\ndiagnosed with ASD (Sensitivity = 82.6%) than TD infants (Specificity = 64.3%). The\\nstrongest predictor of group membership was canonical babbling classification at 9–12\\nmonths as it alone correctly classified 90% of infants later diagnosed with ASD and 63% of\\nTD infants. Thus, in the search for markers of ASD risk in infancy, canonical babbling\\nstatus at 9–12 months appears to be the single best candidate among the variables considered\\nin the current study. The utility of the measure as a group marker is age dependent, since a\\nlarger proportion of infants in the ASD group at 15–18 months had reached the canonical\\nstage than at 9–12 months.\\nTo help better understand the high canonical babbling ratios of the two outliers, the coders,\\nboth certified speech-language pathologists, viewed the videos from those infants again after\\ntheir outlier status was identified. We speculated that the outlier status of these two infants\\nmay be related to the phenomenon of motor stereotypy that is common in ASD, that is, that\\nthe two infants were engaged, at least in the 9–12 month samples, in a motor stereotypy\\nfocused precisely on canonical babbling. In re-examining the videos of the two outliers, the\\ncoders looked for qualitative evidence that might speak to the credibility of this speculation.\\nIn the second viewing of the recordings, the coders noticed that the first outlier infant\\nproduced the majority of the canonical syllables during a single scene while walking\\noutside. He repeatedly produced a [da] syllable during this brief episode, but did not direct\\nhis vocalizations to the caregiver. The sense that a prelinguistic vocal stereotypy may have\\nbeen operating was enhanced by the fact that the same syllable was repeated throughout.\\nThe stereotypy of canonical babbling in this infant was reported by the coders as\\nconstituting the only evidence either had noticed as specifically suggesting the possibility of\\nASD while they were coding, and thus, this was the single case where the intended blinding\\nof the coders to diagnostic group seems to have been foiled. The coders did not observe any\\nother stereotypic behaviors vocal or otherwise in these samples. The second infant engaged\\nin high canonical babble production while roughhousing with his father, but to our clinical\\neyes, that behavior did not seem particularly unusual. Further research on the possibility that\\nbabbling can be a focus of motor stereotypy in ASD seems in order. It may be worthy ofPatten et al. Page 16\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptnote that the two outliers’ CARS scores (25 and 31) fell within the range of the scores for\\nthe ASD group (23–50).\\nIn addition to the findings suggesting possible clinically useful markers for ASD, the present\\nresults provide a new scientific view on the robustness of canonical babbling. There has\\nbeen no prior empirical indication that canonical babbling onset is delayed in ASD, nor that\\nvolubility is low in infants later diagnosed with ASD. Our results thus suggest that the\\ndevelopment of vocalization in infancy is affected by whatever the fundamental disorders of\\nASD may be. Assuming ASD to be a social  disorder, it is not obvious that babbling would\\nnecessarily be disturbed in the disorder because the extent to which babbling is a social (as\\nopposed to an endogenously generated) phenomenon is itself an empirical question. Our\\nresults can then be thought to provide a new empirical perspective on the possible social\\nnature of babbling. The results also suggest that the vocal differentiation of the two groups is\\nrobust, given the relative clarity of the results indicating low canonical babbling and\\nvolubility in the infants in the ASD group, even though we had samples of low recording\\nquality and very limited duration. The results seem especially significant in the context of a\\nbroad body of research cited above on the robustness of canonical babbling as a foundation\\nfor language and on the robust resistance  of canonical babbling to delay as seen in prior\\nstudies cited in our paper—no delay has been found in cases of prematurity, low SES, or\\nmultilingual exposure.\\nFuture Directions and Limitations\\nThis study provides a proof of concept regarding the notion of atypical emergence of the\\ncanonical babbling stage in the developing infant who will later be diagnosed with ASD and\\nthe possibility that tracking canonical babbling in infancy may add to our repertoire of\\nmarkers for ASD prior to one year of age. Future research to address some of the limitations\\nof the current study and advance our understanding of the development of canonical\\nbabbling among infants with ASD is warranted by the findings of the current study. One\\nlimitation in the current study was the lack of a comparison group of infants with later\\ndiagnoses of non-ASD disabilities, which prevents us from definitively attributing the\\ndifferences found in this study to ASD rather than general impairments in cognition or\\ncommunication. Our working hypothesis to test in future studies will be that these\\ndifferences in canonical babbling onset and in volubility are specific to ASD.\\nAnother limitation was that our study used only short video segments from each time point,\\nwhich surely impacted our ability to precisely assess important aspects of vocalization,\\nbecause it has been shown that variability in obtained canonical babbling ratios increases as\\nthe length of samples decreases (Molemans, 2011; Molemans, Van den Berg, Van Severen,\\n& Gillis, 2011). The low canonical babbling ratios obtained for a few of the TD infants\\npresumably would not have occurred with larger sample sizes. In future studies we hope to\\nobtain longer samples, and if possible to more precisely identify canonical babbling onset\\nthrough longitudinal laboratory assessments paired with caregiver report of onset. But of\\ncourse to make this possible, prospective studies may be necessary, with several years of\\nfollow-up, presumably taking advantage of the opportunity presented by sibling studies.\\nSuch studies would also afford the opportunity to obtain much better recordings than arePatten et al. Page 17\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscriptavailable in retrospective studies such as the present one. Indeed, sibling studies can now\\ncapitalize on all-day recording, yielding the opportunity to assess vocal development in ASD\\nwith much greater ecological validity and representativeness.\\nOnset of canonical babbling usually occurs between 5 and 9 months in TD infants. It\\nappears from the present data that onset may occur within a much wider range in ASD.\\nQuantification of onset in ASD may yield prognostic value regarding core communication\\nsymptoms. For example, if canonical stage onset is delayed beyond a certain threshold, the\\ninfant may be at especially high-risk for remaining nonverbal. Discovery of such a delay\\ncould allow specific interventions to be tailored based on prognosis earlier in development.\\nFuture research should also focus on caregivers and their roles in canonical stage\\ndevelopment and its identification. Prior work suggests that with TD infants, parents are\\nextremely accurate in their reports of the onset of canonical babbling (Oller et al., 2001). If\\ncaregivers of infants with ASD are similarly capable of identifying onset of canonical\\nbabbling, it may be possible to use canonical babbling onset as part of a parent-report\\nscreening tool for early identification. In addition, alterations in communication directed to\\ninfants by caregivers as canonical babbling emerges may help to elicit and maintain social-\\ncommunicative interaction, and subsequently impact language development.\\nOur findings on volubility represent another potential avenue for understanding early social-\\ncommunication development processes in ASD. Perhaps the most intriguing aspect of this\\npossibility is suggested by the proposal that there may be a feedback loop involving low\\ncanonical syllable production in ASD followed by low parental rate of vocalization to\\ninfants, aggravating the low volubility and low rate of canonical syllables in ASD\\n(Warlaumont et al., 2010). We anticipate rapid growth of studies tracking this possibility,\\nespecially since there is a rapidly growing possibility of conducting some aspects of such\\nanalysis based on automated classification of vocalizations in all-day recordings as indicated\\nby the growth of LENA system studies.\\nClinical Implications\\nOur findings suggest that canonical babbling should be considered an important milestone in\\ninfancy that may be delayed in infants who are later diagnosed with ASD. If infants\\ndemonstrate delays in canonical babbling, a developmental assessment that includes\\nevaluation of early warning signs for ASD should be administered. Although volubility\\nappears less promising as a marker for ASD, it may be useful in combination with other\\nitems in the context of early identification screening tools. For infants demonstrating either\\nlow canonical babbling ratios or low volubility, interventions to draw infants’ attention to\\nsocial-communicative stimuli in that context of dyadic interactions may help stimulate\\ngrowth of vocal communication.\\nAcknowledgments\\nThis research was made possible through a grant from the National Institute for Child Health and Human\\nDevelopment (R01-HD42168) and a grant from Cure Autism Now Foundation (Sensory-Motor and Social-\\nCommunicative Symptoms of Autism in Infancy). We thank the families whose participation made this study\\npossible and the staff who collected and processed data for this project.Patten et al. Page 18\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptReferences\\nAcevedo MC. The role of acculturation in explaining ethnic differences in the prenatal health-risk\\nbehaviors, mental health, and parenting beliefs of Mexican American and European American at-\\nrisk women. Child Abuse & Neglect. 2000; 24:111–127. [PubMed: 10660014]\\nAmerican Psychiatric Association. Diagnostic and statistical manual of mental disorders. 4.\\nWashington, DC: Author; 2000. text rev\\nBaranek GT. Autism during infancy: A retrospective video analysis of sensory-motor and social\\nbehaviors at 9–12 months of age. Journal of Autism and Developmental Disorders. 1999; 29:213–\\n224. [PubMed: 10425584]\\nBaranek GT, David FJ, Poe MD, Stone WL, Watson LR. Sensory Experiences Questionnaire:\\nDiscriminating sensory features in young children with autism, developmental delays, and typical\\ndevelopment. Journal of Child Psychology and Psychiatry. 2006; 47(6):591–601. [PubMed:\\n16712636]\\nBaranek GT, Watson LR, Boyd BA, Poe MD, David FJ, McGuire L. Hyporesponsiveness to social and\\nnonsocial sensory stimuli in children with autism, children with developmental delays, and typically\\ndeveloping children. Development and Psychopathology. 2013; 25(2013):307–320. [PubMed:\\n23627946]\\nBleile KM, Stark RE, McGowan JS. Speech development in a child after decannulation: Further\\nevidence that babbling facilitates later speech development. Clinical Linguistics and Phonetics.\\n1993; 7:319–337.\\nCenter for Disease Control and Prevention. Prevalence of autism spectrum disorders –Autism and\\ndevelopmental disabilities monitoring network, 14 Sites, United States, 2008. Morbidity and\\nMortality Weekly Report Surveillance Summaries. 2012; 61:1–19. Retrieved from http://\\nwww.cdc.gov/mmwr/preview/mmwrhtml/ss6103a1.htm .\\nClarke E, Reichard U, Zuberbühler K. The anti-predator behaviour of wild white-handed gibbons\\n(Hylobates lar). Behavioral Ecology and Sociobiology. 2012; 66:85–96.10.1007/\\ns00265-011-1256-5\\nChapman K, Hardin-Jones M, Schulte J, Halter K. Vocal development of 9 month-old babies with cleft\\npalate. Journal of Speech, Language and Hearing Research. 2001; (44):1268–1283.\\nChawarska K, Klin A, Paul R, Macari Volkmar F. A prospective study of toddlers with ASD: Short-\\nterm diagnostic and cognitive outcomes. Journal of Autism and Developmental Disorders. 2009;\\n50(10):1235–1245.\\nClement, CJ. PhD Dissertation. Netherlands Graduate School of Linguistics; Amsterdam: 2004.\\nDevelopment of vocalizations in deaf and normally hearing infants.\\nCobo-Lewis AB, Oller DK, Lynch MP, Levine SL. Relations of motor and vocal milestones in\\ntypically developing infants and infants with Down syndrome. American Journal on Mental\\nRetardation. 1996; 100:456–467. [PubMed: 8852298]\\nCohen, J. Statistical Power Analysis for the Behavioral Sciences. Hillsdale,, NJ: Erlbaum Associates;\\n1988.\\nDavis BL, Morrison HM, von Hapsburg D, Warner AD. Early vocal patterns in infants with varied\\nhearing levels. Volta Review. 2005; 105(1):7–27.\\nDelgado CEF, Messinger DS, Yale ME. Infant responses to direction of parental gaze: A comparison\\nof two still-face conditions. Infant Behavior and Development. 2002; 25(3):311–318.\\nEilers RE, Oller DK, Levine S, Basinger D, Lynch MP, Urbano R. The role of prematurity and\\nsocioeconomic status in the onset of canonical babbling in infants. Infant Behavior and\\nDevelopment. 1993; 16:297–315.\\nEilers RE, Oller DK. Infant vocalizations and the early diagnosis of severe hearing impairment. The\\nJournal of Pediatrics. 1994; 124(2):199–203. [PubMed: 8301422]\\nGoldstein MH, Schwade JA, Bornstein MH. The value of vocalizing: Five-month-old infants associate\\ntheir own noncry vocalizations with responses from adults. Child Development. 2009; 80:636–\\n644. [PubMed: 19489893]Patten et al. Page 19\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptGoldstein MH, King AP, West MJ. Social interaction shapes babbling: Testing parallels between\\nbirdsong and speech. Proceedings of the National Academy of Sciences. 2003; 100(13):8030–\\n8035.\\nGoldstein MH, Schwade JA. Social feedback to infants’ babbling facilitates rapid phonological\\nlearning. Psychological Science. 2008; 19:515–522. [PubMed: 18466414]\\nGoldstein MH, West MJ. Consistent responses of human mothers to prelinguistic infants: The effect of\\nprelinguistic repertoire size. Journal of Comparative Psychology. 1999; 113(1):52–58. [PubMed:\\n10098268]\\nHart, B.; Risley, TR. Meaningful differences in the everyday experience of young American children.\\nBaltimore: Paul H. Brookes; 1995.\\nHulit, LM.; Howard, MR. Born to talk: An introduction to speech and language development. Boston:\\nAllyn and Bacon; 2002.\\nKent R, Osberger MJ, Netsell R, Hustedde CG. Phonetic development identical twins differing in\\nauditory function. Journal of Speech and Hearing Disorders. 1987; 52:64–75. [PubMed: 3807347]\\nKoopmans-van Beinum, FJ.; Clement, CJ.; van den Dikkenberg-Pot, I. Influence of lack of auditory\\nspeech perception on sound productions of deaf infants. Berne, Switzerland: International Society\\nfor the Study of Behavioral Development; 1998.\\nKoopmans-van Beinum, FJ.; van der Stelt, JM. Early stages in the development of speech movements.\\nIn: Lindblom, B.; Zetterstrom, R., editors. Precursors of early speech. New York: Stockton Press;\\n1986. p. 37-50.\\nLewedag, VL. Doctoral Dissertation. University of Miami; Coral Gables, FL: 1995. Patterns of onset\\nof canonical babbling among typically developing infants.\\nLocke JL, Pearson D. Linguistic significance of babbling: Evidence from a tracheostomized infant.\\nJournal of Child Language. 1990; 17:1–16. [PubMed: 2312634]\\nLord C. Follow-up of two-year-olds referred for possible autism. Journal of Child Psychology and\\nPsychiatry, and Allied Disciplines. 1995; 36(8):1365–1382.\\nLord, C.; Rutter, M.; DiLavore, P.; Risi, S. Autism Diagnostic Observation Schedule (ADOS). Los\\nAngeles, CA: Western Psychological Services; 1999.\\nLynch MP, Oller DK, Steffens ML, Levine SL, Basinger D, Umbel V. The onset of speech-like\\nvocalizations in infants with Down syndrome. American Journal of Mental Retardation. 1995;\\n100(1):68–86. [PubMed: 7546639]\\nLynch MP, Oller DK, Steffens ML, Buder EH. Phrasing in prelinguistic vocalizations. Developmental\\nPsychobiology. 1995; 28:3–23. [PubMed: 7895922]\\nManjiviona J, Prior M. Comparison of Asperger syndrome and high-functioning autistic children on a\\ntest of motor impairment. Journal of Autism and Developmental Disorders. 1995; 25:23–29.\\n[PubMed: 7608032]\\nMasataka N. Why early linguistic milestones are delayed in children with Williams syndrome: Late\\nonset of hand banging as a possible rate-limiting constraint on the emergence of canonical\\nbabbling. Developmental Science. 2001; 4:158–164.\\nMatson JL, Fodstad JC, Dempsey T. What symptoms predict the diagnosis of autism or PDD-NOS in\\ninfants and toddlers with developmental delays using the Baby and Infant Screen for Autism\\nTraits. Developmental Neurorehabilitation. 2009; 12(6):381–388. [PubMed: 20205546]\\nMatson JL, Mahan S, Hess JA, Fodstad JC, Neal D. Convergent validity of the Autism Spectrum\\nDisorder-Diagnostic for Children (ASD-DC) and Childhood Autism Rating Scales (CARS).\\nResearch in Autism Spectrum Disorders. 2010; 4(4):633–638.\\nMiller, LJ.; Reisman, JE.; McIntosh, DN.; Simon, J. An ecological model of sensory modulation:\\nPerformance in children with fragile X syndrome, autistic disorder, attention–deficit/hyperactivity\\ndisorder, and sensory modulation dysfunction. In: Smith-Roley, S.; Blanche, EI.; Schaaf, RC.,\\neditors. Understanding the Nature of Sensory Integration with Diverse Populations. San Antonio,\\nTX: Therapy Skill Builders; 2001. p. 57-88.\\nMolemans, I. PhD. University of Antwerp; Antwerp, Belgium: 2011. Sounds like babbling: A\\nLongitudinal investigation of aspects of the prelexical speech repertoire in young children\\nacquiriing Dutch: Normally hearing children and hearing impaired children with a cochlear\\nimplant.Patten et al. Page 20\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptMolemans I, Van den Berg R, Van Severen L, Gillis S. How to measure the onset of babbling reliably.\\nJournal of Child Language. 2011; 39:1–30. [PubMed: 21418730]\\nMullen, EM. Mullen Scales of Early Learning: AGS Edition. Circle Pines, MN: American Guidance\\nService; 1995.\\nNathani S, Oller DK, Neal AR. On the robustness of vocal development: An examination of infants\\nwith moderate-to-severe hearing loss and additional risk factors. Journal of Speech, Language, and\\nHearing Research. 2007; 50(6):1425–1444.\\nObenchain P, Menn L, Yoshinaga-Itano C. Can speech development at 36 months in children with\\nhearing loss be predicted from information available in the second year of life? Volta Review.\\n1998; 100:149–180.\\nOller, DK. The emergence of the sounds of speech in infancy. In: Yeni-Komshian, G.; Kavanagh, J.;\\nFerguson, C., editors. Child phonology, Vol 1: Production. New York: Academic Press; 1980. p.\\n93-112.\\nOller, DK. The Emergence of the Speech Capacity. Mahwah, NJ: Lawrence Erlbaum Associates;\\n2000.\\nOller DK, Eilers RE. Similarities of babbling in Spanish- and English-learning babies. Journal of Child\\nLanguage. 1982; 9:565–578. [PubMed: 7174757]\\nOller DK, Eilers RE. The role of audition in infant babbling. Child Development. 1988; 59:441–449.\\n[PubMed: 3359864]\\nOller DK, Eilers RE, Steffens ML, Lynch MP, Urbano R. Speech-like vocalizations in infancy: an\\nevaluation of potential risk factors. Journal of Child Language. 1994; 21:33–58. [PubMed:\\n8006094]\\nOller DK, Eilers RE, Urbano R, Cobo-Lewis AB. Development of precursors to speech in infants\\nexposed to two languages. Journal of Child Language. 1997; 27:407–425. [PubMed: 9308425]\\nOller DK, Eilers RE, Basinger D. Intuitive identification of infant vocal sounds by parents.\\nDevelopmental Science. 2001; 4:49–60.\\nOller DK, Eilers RE, Basinger D, Steffens ML, Urbano R. Extreme poverty and the development of\\nprecursors to the speech capacity. First Lang. 1995; 15:167–188.\\nOller DK, Eilers RE, Neal AR, Cobo-Lewis AB. Late onset canonical babbling: a possible early\\nmarker of abnormal development. American Journal on Mental Retardation. 1998; 103:249–265.\\n[PubMed: 9833656]\\nOller DK, Eilers RE, Neal AR, Schwartz HK. Precursors to speech in infancy: the prediction of speech\\nand language disorders. Journal of Communication Disorders. 1999; 32:223–246. [PubMed:\\n10466095]\\nOller DK, Niyogi P, Gray S, Richards JA, Gilkerson J, Xu D, Warren SF. Automated Vocal Analysis\\nof Naturalistic Recordings from Children with Autism, Language Delay and Typical\\nDevelopment. Proceedings of the National Academy of Sciences. 2010; 107:13354–13359.\\nOsterling JA, Dawson G, Munson JA. Early recognition of 1-year-old infants with autism spectrum\\ndisorder versus mental retardation. Development and Psychopathology. 2002; 14(2):239–251.\\n[PubMed: 12030690]\\nOzonoff S, Iosif A, Baguio F, Cook IC, Hill MM, Hutman T, Rogers SJ, Rozga A, Sangha S, Sigman\\nM, Steinfeld MB, Young GS. A prospective study of the emergence of early behavioral signs of\\nautism. Journal of the American Academy of Child & Adolescent Psychiatry. 2010; 49(3):256–\\n266. [PubMed: 20410715]\\nPage J, Boucher J. Motor impairments in children with autistic disorder. Child Language Teaching and\\nTherapy. 1998; 14(3):233.\\nPapoušek, M. Vom ersten Schrei zum ersten Wort: Anfänge der Sprachentwickelung in der\\nvorsprachlichen Kommunikation. Bern: Verlag Hans Huber; 1994.\\nPaul R, Augustyn A, Klin A, Volkmar FR. Perception and production of prosody by speakers with\\nASD spectrum disorders. Journal of Autsim and Developmental Disorders. 2005; 35:205–220.\\nPaul R, Fuerst Y, Ramsay G, Chawarska K, Klin A. Out of the mouths of babes: Vocal production in\\ninfant siblings of children with ASD. Journal of Child Psychology and Psychiatry. 2011; 52(5):\\n588–598. [PubMed: 21039489]Patten et al. Page 21\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPeppe S, McCann J, Gibbon F, O’Hara A, Rutherford M. Receptive and expressive prosodic ability in\\nchildren with high-functioning ASD. Journal of Speech, Language, and Hearing Research. 2007;\\n50:1015–1028.\\nRamsdell HL, Oller DK, Buder EH, Ethington CA, Chorna L. Identification of prelinguistic\\nphonological categories. Journal of Speech Language and Hearing Research. 2012; 55:1626–1629.\\nRobins DI, Fein D, Barton MI, Green JA. The modified checklist for autism in toddlers: An initial\\nstudy investigating the early detection of autism and pervasive developmental disorders. Journal of\\nAutism and Developmental Disorders. 2001; 31:131–144. [PubMed: 11450812]\\nRogers SJ, Ozonoff S. Annotation: What do we know about sensory dyfunction in autism? A critical\\nreview of the empirical evidence. Journal of Child Psychology and Psychiatry. 2005; 46(12):1255–\\n1268. [PubMed: 16313426]\\nRoss GS. Language functioning and speech development of six children receiving tracheostomy in\\ninfancy. Journal of Communication Disorders. 1983; 15:95–111. [PubMed: 7096617]\\nRutter, M.; Le Couteur, A.; Lord, C. Autism Diagnostic Interview-Revised. Los Angeles, CA: Western\\nPsychological Services; 2003.\\nSchopler, E.; Reichler, RJ.; Rochen Renner, B. The Childhood Autism Rating Scale. Lost Angeles,\\nCA: Western Psychological Services; 1992.\\nSheinkopf SJ, Mundy P, Oller DK, Steffens M. Vocal atypicalities of preverbal autistic children.\\nJournal of Autism and Developmental Disorders. 2000; 30:345–353. [PubMed: 11039860]\\nSimon BM, Fowler SM, Handler SD. Communication development in young children with long-term\\ntracheostomies: Preliminary report. International Journal of Otorhinolaryngology. 1983; 6:37–50.\\nSnow, CE. Issues in the study of input: fine-tuning universality, individual and devlopmental\\ndifferences and necessary causes. In: MacWhinney, B.; Fletcher, P., editors. NETwerken:\\nBijdragen van het vijfde NET symposium: Antwerp Papers in Linguistics. Vol. 74. Antwerp:\\nUniversity of Antwerp; 1995. p. 5-17.\\nSparrow, SS.; Balla, DA.; Cicchetti, DV. Vineland Adaptive Behavior Scales. Circle Pines, MN:\\nAmerican Guidance Service; 1984.\\nStark, RE. Stages of speech development in the first year of life. In: Yeni-Komshian, G.; Kavanagh, J.;\\nFerguson, C., editors. Child Phonology. Vol. 1. New York: Academic Press; 1980. p. 73-90.\\nStark, RE.; Ansel, BM.; Bond, J. Are prelinguistic abilities predictive of learning disability? A follow-\\nup study. In: Masland, RL.; Masland, M., editors. Preschool Prevention of Reading Failure.\\nParkton, MD: York Press; 1988.\\nStoel-Gammon C. Prespeech and early speech development of two late talkers. First Language. 1989;\\n9:207–224.\\nStoel-Gammon C, Otomo K. Babbling development of hearing impaired and normally hearing\\nsubjects. Journal of Speech and Hearing Disorders. 1986; 51:33–41. [PubMed: 3945058]\\nStoel-Gammon C. Relationships between lexical and phonological development in young children.\\nJournal of Child Language. 2011; 38(1):1–34. [PubMed: 20950495]\\nTeitelbaum P, Teitelbaum O, Nye J, Fryman J, Maurer RG. Movement analysis in infancy may be\\nuseful for early diagnosis of autism. Proceedings of the National Academy of Sciences of the\\nUnited States of America. 1998; 95(23):13982–13987. [PubMed: 9811912]\\nTronick, EZ. Social interchange in infancy. Baltimore: University Park Press; 1982.\\nVan den Dikkenberg-Pot I, Koopmans-van Beinum F, Clement C. Influence of lack of auditory speech\\nperception of sound productions of deaf infants. Proceedings of the Institute of Phonetic Sciences,\\nUniversity of Amsterdam. 1998; 22:47–60.\\nVihman, MM. Phonological Development: The Origins of Language in the Child. Cambridge, MA:\\nBlackwell Publishers; 1996.\\nVolkmar FR, Chawarska K. Autism in infants: An update. World Psychiatry: Official Journal of the\\nWorld Psychiatric Association. 2008; 7(1):19–21.\\nWatson LR, Patten E, Baranek GT, Boyd BA, Freuler A, Lorenzi J. Differential associations between\\nsensory response patterns and language, social, and communication measures in children with\\nautism or other developmental disabilities. Journal of Speech, Language, and Hearing Research.\\n2011; 54(6):1562–1576.Patten et al. Page 22\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptWatson LR, Crais ER, Baranek GT, Dykstra JR, Wilson KP. Communicative Gesture Use in Infants\\nwith and without Autism: A Retrospective Home Video Study. American Journal of Speech-\\nLanguage Pathology. 2013; 22:25–39. [PubMed: 22846878]\\nWarlaumont, AS.; Oller, DK.; Dale, R.; Richards, JA.; Gilkerson, J.; Xu, D. Vocal Interaction\\nDynamics of Children With and Without Autism. Paper presented at the Proceedings of the 32nd\\nAnnual Conference of the Cognitive Science Society; Austin, TX. 2010.\\nWarren SF, Gilkerson J, Richards JA, Oller DK. What Automated Vocal Analysis Reveals About the\\nLanguage Learning Environment of Young Children with Autism. Journal of Autism and\\nDevelopmental Disorders. 2010; 40:555–569. [PubMed: 19936907]\\nWeismer SE, Lord C, Esler A. Early language patterns of toddlers on the autism spectrum compared to\\ntoddlers with developmental delay. Journal of Autism and Developmental Disorders. 2010;\\n40(10):1259–1273. [PubMed: 20195735]\\nWerner E, Dawson G, Osterling J, Dinno N. Brief report: Recognition of autism spectrum disorder\\nbefore one year of age: A retrospective study based on home videotapes. Journal of Autism and\\nDevelopmental Disorders. 2000; 30:157–162. [PubMed: 10832780]\\nWetherby AM, Woods J, Allen L, Cleary J, Dickinson H, Lord C. Early indicators of ASD spectrum\\ndisorders in the second year of life. Journal of ASD and Developmental Disorders. 2004; 34:473–\\n493.\\nYale ME, Messinger DS, Cobo-Lewis AB, Oller DK, Eilers RE. An event-based analysis of the\\ncoordination of early infant vocalizations and facial actions. Developmental Psychology. 1999;\\n35(2):505–513. [PubMed: 10082021]\\nZwaigenbaum L, Bryson S, Rogers T, Roberts W, Brian J, Szatmari P. Behavioral manifestations of\\nautism in the first year of life. International Journal of Developmental Neuroscience. 2005; 23(2–\\n3):143–152. [PubMed: 15749241]Patten et al. Page 23\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptFigure 1.\\nCanonical babbling ratios by participant at 9–12 monthsPatten et al. Page 24\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptFigure 2.\\nCanonical babbling ratios by participant at 15–18 monthsPatten et al. Page 25\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptFigure 3.\\nSyllable volubility by participant at 9–12 months.Patten et al. Page 26\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptFigure 4.\\nSyllable volubility by participant at 15–18 months.Patten et al. Page 27\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptFigure 5.\\nCanonical babbling ratios by age and diagnosisPatten et al. Page 28\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptFigure 6.\\nVolubility by age and diagnosisPatten et al. Page 29\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.\\nNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptNIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 30\\nTable 1\\nParticipant Demographics\\nASD; n=23 TD; n=14\\nAge at 9–12months; mean (SD) 10.89 (1.39) 10.63 (.53)\\nAge at 15–18 months; mean (SD) 16.33 (.83) 16.28 (.70)\\nSex 19 males, 4 females 11 males, 3 females\\nRace 23 White, 1 Black 13 White, 1 Asian\\nMaternal education 1 5.485.82\\nChildhood Autism Rating Scale; mean (SD) 34.17 (1.52)16.15 (.39)3\\n1Maternal education: 1=6th grade or lower; 2=7th to 9th grade; 3=partial high school; 4=high school graduate/GED; 5=associate of arts/associate\\nof science or technical training or partial college training; 6=bachelor of arts/science; 7=master of arts/science or doctorate or other professional\\ndegree completed\\n2missing information for two participants\\n3missing information for four participants\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 31\\nTable 2\\nContent Variables for Videos, 9–12 months\\nASD; mean (SD) TD; mean (SD)\\nNumber of people present 3.22 (1.53) 3.28 (1.24)\\nAmount of physical restrictiona 1.58 (.35) 1.51 (.32)\\nAmount of social intrusiona 2.02 (.38) 2.04 (.32)\\nTotal number of different event types 5.32 (1.05) 5.07 (1.02)\\naRated by coders on a 1 to 3 scale\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 32\\nTable 3\\nContent Variables for Videos, 15–18 months\\nASD; mean (SD) TD; mean (SD)\\nNumber of people present 2.84 (1.20) 2.82 (1.24)\\nAmount of physical restrictiona 1.37 (.29) 1.28 (.33)\\nAmount of social intrusion a 2.06 (.40) 1.95 (.34)\\nTotal number of different event types 5.34 (1.17) 5.23 (1.11)\\naRated by coders on a 1 to 3 scale\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 33\\nTable 4\\nPercentage of each activity type, 9–12 month videos\\nASD; n=23 TD; n=14\\nMealtime 10% 11%\\nActive 53.9% 60.6%\\nBathtime 4.5% 5.5%\\nOther 2.5% 4.1%\\nspecial activity 20.3% 16.5%\\npassive activity 8.7% 3.4%\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 34\\nTable 5\\nPercentage of each activity type, 15–18 month videos\\nASD; n=23 TD; n=14\\nMealtime 7.5% 2.6%\\nActive 64% 72.8%\\nBathtime 2.5% 1.8%\\nOther 8.3% 11.4%\\nSpecial activity 12.9% 4.4%\\nPassive activity 4.6% 16.6%\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author ManuscriptPatten et al. Page 35\\nTable 6\\nIntercorrelations Between Canonical Babbling Ratios and Volubility\\n12 3 4\\n1. Canonical Babbling 9–12 mos __.352*.528**.354*\\n2. Volubility 9–12 mos __ 0.21 0.14\\n3. Canonical Babbling 15–18 mos __.510**\\n4. Volubility 15–18 mos __\\n*p < .05,\\n**p < .01\\nJ Autism Dev Disord . Author manuscript; available in PMC 2014 October 01.', metadata={'source': 'papers/Patten_Audio_clean.txt'})],\n",
       " [Document(page_content='ORIGINAL RESEARCH\\npublished: 05 June 2020\\ndoi: 10.3389/fped.2020.00290\\nFrontiers in Pediatrics | www.frontiersin.org 1 June 2020 | Volume 8 | Article 290Editedby:\\nSaraCalderoni,\\nFondazioneStellaMaris(IRCCS),Italy\\nReviewedby:\\nWhitneyI.Mattson,\\nNationwideChildren’sHospital,\\nUnitedStates\\nLori-AnnRosalindSacrey,\\nUniversityofAlberta,Canada\\n*Correspondence:\\nXiaoyanKe\\nkexiaoyan@njmu.edu.cn\\nSpecialtysection:\\nThisarticlewassubmittedto\\nChildandAdolescentPsychiatry,\\nasectionofthejournal\\nFrontiersinPediatrics\\nReceived: 05January2020\\nAccepted: 07May2020\\nPublished: 05June2020\\nCitation:\\nQiuN,TangC,ZhaiM,HuangW,\\nWengJ,LiC,XiaoX,FuJ,ZhangL,\\nXiaoT,FangHandKeX(2020)\\nApplicationoftheStill-FaceParadigm\\ninEarlyScreeningforHigh-Risk\\nAutismSpectrumDisorderinInfants\\nandToddlers.Front.Pediatr.8:290.\\ndoi:10.3389/fped.2020.00290Application of the Still-Face\\nParadigm in Early Screening for\\nHigh-Risk Autism Spectrum Disorder\\nin Infants and Toddlers\\nNanaQiu1,ChuangaoTang2,MengyaoZhai1,WanqingHuang3,JiaoWeng1,ChunyanLi1,\\nXiangXiao1,JunliFu1,LiliZhang4,TingXiao1,HuiFang1andXiaoyanKe1*\\n1NanjingBrainHospitalAfﬁliatedtoNanjingMedicalUnivers ity,Nanjing,China,2SchoolofBiologicalScience&Medical\\nEngineering,SoutheastUniversity,Nanjing,China,3CollegeofTelecommunications&InformationEngineering,N anjing\\nUniversityofPostsandTelecommunications,Nanjing,Chin a,4WuxiChildren’sHospital,Wuxi,China\\nBackground: Although autism spectrum disorder (ASD) can currently be di agnosed at\\nthe age of 2 years, age at ASD diagnosis is still 40 months or ev en later. In order to early\\nscreeningforASDwithmoreobjectivemethod,behavioralvi deoswereusedinanumber\\nof studies in recent years.\\nMethod: The still-face paradigm (SFP) was adopted to measure the fre quency and\\nduration of non-social smiling, protest behavior, eye cont act, social smiling, and active\\nsocial engagement in high-risk ASD group (HR) and typical de velopment group (TD)\\n(HR:n=45; TD:n=43). The HR group was follow-up until they were 2 years old\\nto conﬁrm ﬁnal diagnosis. Machine learning methods were use d to establish models for\\nearly screening of ASD.\\nResults: During the face-to-face interaction (FF) episode of the SFP , there were\\nstatistically signiﬁcant differences in the duration and f requency of eye contact, social\\nsmiling,andactivesocialengagementbetweenthetwogroup s.Duringthestill-face(SF)\\nepisode, there were statistically signiﬁcant differences in the duration and frequency of\\neye contact and active social engagement between the two gro ups. The 45 children in\\ntheHRgroupwerereclassiﬁedintotwogroupsafterfollow-u p:ﬁvechildrenintheN-ASD\\ngroup who were not meet the criterion of ASD and 40 children in the ASD group. The\\nresults showed that the accuracy of Support Vector Machine ( SVM) classiﬁcation was\\n83.35% for the SF episode.\\nConclusion: The use of the social behavior indicator of the SFP for a child with HR\\nbefore2yearsoldcaneffectivelypredicttheclinicaldiag nosis of thechildattheageof2\\nyears.ThescreeningmodelconstructedusingSVMbasedonth eSFepisodeoftheSFP\\nwasthebest.ThisalsoprovesthattheSFPhascertainvaluei nhigh-riskautismspectrum\\ndisorderscreening.Inaddition,becauseofitsconvenient ,itcanprovideaself-screening\\nmode for use at home.\\nTrial registration: Chinese Clinical Trial Registry, ChiCTR-OPC-17011995.\\nKeywords: high-risk autism spectrum disorder, the Still-F ace Paradigm, social behavior, machine learning, model\\nfor early screeningQiu et al. Early Screening for High-Risk ASD\\nINTRODUCTION\\nAutism spectrum disorder (ASD) is a serious\\nneurodevelopmental disorder that starts in early childhood\\nandischaracterizedbysocialcommunicationbarriers,res tricted\\ninterests, repetitive stereotyped behaviors, and abnormali ties in\\nperception ( 1). In recent years, epidemiological survey data on\\nthe incidence of ASD showed that the prevalence rate increase d\\nfrom 0.07 to 1.8% in China ( 2). A large number of studies on\\nASD have shown that early intervention helps improve patient\\nprognosis ( 3). However, age at ASD diagnosis is still 40 months\\nor even later ( 4). Therefore, early detection, early diagnosis and\\neﬀective intervention are essential to achieve a better progn osis.\\nUnderstanding the early childhood behaviors of ASD will help\\nfacilitate early intervention in infants and toddlers who a re\\nsuspectedofhavingASDandimprovetheirprognosis,whichhas\\nveryimportantsocialandeconomicimplications.\\nThe early social interaction between adults and children is\\nthe basis of more complex social cognition. For children with\\nASD, the lack of social abilities is especially prominent in th eir\\nearly life. According to the 2013 version of the Diagnostic and\\nStatistical Manual of Mental Disorders , ﬁfth edition ( DSM-5),\\nASD symptoms are usually present in young children at the\\nage of 1–2 years. Occasionally, initial symptoms often invol ve\\ndelayed language development, accompanied by a lack of social\\ninterest or unusual social interactions, quirky play modes a nd\\nunusual communication patterns ( 1). A study by Barbaro and\\nDissanayakeonfamilyvideosandparentalreportsrevealedea rly\\nwarning signs in social interactions of children with ASD ag ed\\n12 to 24 months old ( 5), which included lack of joint attention,\\nlack of eye contact, lack of social smiling, lack of social in terest\\nand sharing, no response to calling name, lack of gestures, an d\\ncommunication impairments ( 5–9). This study would focus on\\nthe diﬀerences of early social behavior between HR group and\\nTD group. Previous studies on early behavioral abnormalitie s in\\nASD were mostly in the form of retrospective interviews with\\nparents or scale-based evaluations, which are highly subjec tive\\nand unfavorable for widespread promotion. In recent years, an\\nincreasing number of studies have adopted objective methods\\ninvolvingvideoforusingbehavioralcoding( 10–12).\\nTronick et al. proposed the still-face paradigm (SFP) to test\\ninfants’ emotion regulation ability and social expectation s in\\nsocial interaction ( 13). Early maternal-infant interaction is the\\ncore basis of infants’ social emotion, emotion regulation, and\\nsocial and communication development ( 14). The maternal-\\ninfant relationship is the ﬁrst relationship developed in ea rly\\nchildhood. Flexible and frequent interaction is the basis f or\\nearlychildhoodemotionalorganization,attentionswitch ingand\\nthe emergence of social skills ( 15). In the early maternal-\\ninfant interaction, non-verbal communication is dominant . In\\naddition, in this process, young children learn the rules of\\nsocial participation and expressing forms of social expectatio ns,\\nwhich provide a social framework for future social interacti ons\\nand relationships ( 16). There were studies applied SFP in\\nemotional regulatory of ASD and they found most of children\\nwith ASD employed more simple regulatory behavior and\\nless complex strategies ( 17,18). Additional, Cassel et al. alsofound that there were diﬃculties for children with ASD to\\ndevelop socioemotional ability ( 10,19). In this study, the\\nSFP was used to measure social behavior of HR group and\\nTDgroup.\\nTherewerestudiesappliedmachinelearningmethodstobuild\\nmodel for early screening of ASD based on the characteristic\\nvalues of biological indicators, such as electro-encephalog ram\\n(20) and brain images ( 21), and they found the accuracy was\\nmore than 80%. And in order to improve the stability and\\nreliability of model for early screening of ASD, it is essenti al to\\ncombiningsocialbehavioralindicatorswithbiologicalin dicators\\nin the subsequent research. In this study, we would try to\\nbuild a model for early screening of ASD based on social\\nbehavioralindicators.\\nMETHODS\\nParticipants\\nForty-ﬁve infants and toddlers with high-risk autism spectrum\\ndisorder (HR) who sought treatment at the outpatient clinic of\\nChild Mental Health Research Center, Nanjing Brain Hospital\\nAﬃliated to Nanjing Medical University, from December 2017\\ntoDecember2018wereenrolledintheHRgroup,and43infants\\nand toddlers with typical development (TD) in the Nanjing area\\nwere recruited during the same period and enrolled into the\\ncontrolgroup(theTDgroup).\\nThe inclusion criteria for the HR group were as follows:\\n(1) children with positive results based on the Modiﬁed\\nChecklist for Autism in Toddlers (M-CHAT) ( 22); (2) pediatric\\npsychiatrist recognized that the children met the core crite ria\\nof ASD in DSM-5 but the months age was under 24 months;\\n(3) children aged 8 to 23 months old; (4) children whose\\nprimary caregiver was the mother; and (5) children whose\\nguardian(s) agreed to participate in this study. The exclusion\\ncriteria for the HR group were as follows: (1) children with\\ngenetic or metabolic diseases, such as Rett syndrome and\\nfragile X syndrome; (2) children with neurodevelopmental\\ndisorders other than ASD, such as language development\\ndisorders alone and intellectual disability; (3) children with\\na clear history of craniocerebral trauma; and (4) children\\nwith a history of nervous system diseases and serious\\nphysicalillnesses.\\nThe inclusion criteria for the TD group were as follows:\\n(1) children with TD whose sex matched that of the children\\nin the HR group; (2) children aged 8 to 23 months old; (3)\\nchildren whose primary caregiver was the mother; and (4)\\nchildren whose guardian(s) agreed to participate in this stud y.\\nThe exclusion criteria for the TD group were as follows: (1)\\nchildrenwhosuﬀeredfromvarioustypesofneurodevelopmental\\ndisorders and mental disorders; (2) children with a clear his tory\\nof craniocerebral trauma; and (3) children with a history of\\nnervoussystemdiseasesandseriousphysicalillness.\\nThisstudywasapprovedbytheMedicalEthicsCommitteeof\\nNanjing Brain Hospital Aﬃliated to Nanjing Medical University\\n(2017-KY089-01).Allsubjects’guardiansagreedtopartici patein\\nthisstudyandsignedinformedconsentforms.\\nFrontiers in Pediatrics | www.frontiersin.org 2 June 2020 | Volume 8 | Article 290Qiu et al. Early Screening for High-Risk ASD\\nMeasure and Procedure\\nGeneral Psychological Evaluation of the HR and TD\\nGroups\\nA self-guided general information questionnaire was used t o\\ncollect the general demographic data, past history, medicati on\\nhistoryandfamilyhistoryofthestudyparticipants.\\nThe Gesell Developmental Scale was used to assess the\\ndevelopmental levels of all subjects after they were enrolle d.\\nThe Gesell Developmental Scale was used to evaluate the\\ndevelopmental quotient (DQ) of children from 5 skill domains :\\nadaptive,grossmotor,ﬁnemotor,languageandpersonal-soci al.\\nThe severity of ASD symptoms in the HR group was\\nassessedusingtheCommunicationandSymbolicBehaviorSca les\\nDevelopmentalProﬁle(CSBS-DP),theChildhoodAutismRatin g\\nScale (CARS) and the Autism Behavior Checklist (ABC). The\\nCSBS-DP has 3 factor scores (social communication, languag e,\\nand symbolic behavior) and a total score. The lower the CSBS-\\nDP factor scores are, the more serious the ASD symptoms. The\\nCARS and the ABC only have a total score. The higher the total\\nscoresare,themoreseveretheASDsymptoms.\\nVideo of the Behaviors of the HR and TD Groups in\\nthe SFP\\nThe classic paradigm consists of three episodes: (1) the basel ine\\nepisode (face-to-face interaction, FF episode), during whic h the\\nmother and the child are required to have normal interaction s;\\n(2) the still-face (SF) episode, during which the mother’s fa ce is\\nrequiredtopresentaneutralexpressionwithoutanyresponset o\\nthe child’s action; and (3) the reunion episode, during which the\\nmother resumes normal interactions with the child. Current ly,\\ntheSFPcanarousechildren’sbehavioralchanges,e.g.,are duction\\nin eye gaze and positive facial emotion and an increase in\\nnegative emotion when transitioning from the baseline episo de\\nto the SF episode ( 13); this eﬀect has been recognized and\\ntermed the SF eﬀect. In relevant studies, researchers also fo und\\nthat the reason for the generation of the SF eﬀect in infants\\nand toddlers is the disappearance of social responses, such\\nas eye contact, when their mothers show a still face; the\\ndisappearanceofsocialsignalscausestheappearanceofnegati ve\\nemotions in infants and toddlers ( 23). The ﬁrst two episodes\\nare often used in research in a randomly presented order. Inthe past few decades, the basic settings of the SFP have been\\nused as a method to explore early childhood social behaviors\\n(24–26), which the reason why this study chosen the ﬁrst\\ntwoepisodes.\\nIn the present study, all participants and their mothers were\\nvideorecordedduringtheSFPinadesignatedobservationro om\\natthetimeofenrollment.Themotherwassittingoppositetothe\\nchild,interactedwiththechildfor2minunderﬁxedinstruc tions,\\nand then stopped the interaction and maintained a neutral\\nface for 1min. Figure1 shows the setup of the experimental\\nenvironmentandprocedure.\\nInfant and Caregiver Engagement Phases (ICEP) ( 27) and\\nNichols et al. ( 28) deﬁne the coding indicators as follows: (1)\\nprotest behavior - the infant shows facial expressions of ange r\\nand frowns; the infant is upset, crying, arching the body, tryi ng\\nto escape and expresses anger using gestures; (2) non-social\\nsmiling—the infant smiles not at the mother but toward other\\ndirections or at other objects; (3) eye contact—the infant l ooks\\ndirectly at the mother’s eyes or face instead of looking at th e\\ncameraortowardotherdirections;(4)socialsmiling—thein fant\\nlooks at the mother and takes the initiative to smile (initia ting\\na smile); after the mother smiles, the infant immediately\\nresponds with a smile (smiling at each other); and (5) active\\nsocial engagement—the infant displays happy facial expressions ,\\nincluding a clear smile, occasional cooing or active vocali zation,\\nlaughing, or babbling, and looks at the mother to initiate\\ninteractions(proactivelyinitiateactivesocialengagemen t),orthe\\ninfant has a positive response to the interaction initiated by the\\nmother(respondingtoactivesocialengagement).\\nThe Observer XT 12 behavioral observation and recording\\nanalysissystemwasusedforcoding.Videocodingwascomplete d\\nby 2 trained graduate students, and the duration and frequen cy\\nofthe5indicatorsduringtheFFandSFepisodeswerecalculate d.\\nThe duration was measured using seconds (s) as the unit, and\\nfrequencywasmeasuredusingthenumberoftimesastheunit. A\\ntotal of 18 videos ( ∼20%) were randomly selected to determine\\nintercoderconsistencyusingtheintraclasscorrelationco eﬃcient\\n(ICC). It was found that the 2 coders had high consistency: the\\nICCs for protest behavior, non-social smiling, eye contact, social\\nsmiling, and activesocial engagement were 0.76, 0.82,0.81 , 0.83,\\nand0.79,respectively.\\nFIGURE 1 | SFP for infants and toddlers with HR.\\nFrontiers in Pediatrics | www.frontiersin.org 3 June 2020 | Volume 8 | Article 290Qiu et al. Early Screening for High-Risk ASD\\nTABLE 1 | Comparison of the general conditions between the HR group an d the\\nTD group ( mean±SD).\\nHR group TD group t/χ2P-value\\n(n=45) ( n=43)\\nSex −0.06 0.08\\nMale 40 32\\nFemale 5 11\\nAge\\n(months)\\nDQ19.71±3.43 16.40 ±4.70 3.80 <0.01\\nAdaptive 80.29 ±17.62 92.98 ±7.89−4.34 <0.01\\nGross motor 92.02 ±17.60 92.77 ±8.46−0.25 0.80\\nFine motor 86.64 ±19.03 93.70 ±8.29−2.24 0.03\\nLanguage 60.84 ±21.27 86.51 ±8.353−7.39 <0.01\\nPersonal-social 78.80 ±17.19 92.28 ±7.18−4.76 <0.01\\nHR, high-risk autism spectrum disorder; TD, typical development; DQ, dev elopmental\\nquotientoftheGesellDevelopmentalScale.\\nDiagnostic Evaluation of Children in the HR Group at\\n2 Years of Age\\nAt 2 years of age, the children in the HR group were evaluated\\nusing the Autism Diagnosis Interview-R (ADI-R) and the\\nAutismDiagnosticObservationSchedule(ADOS).Twopediatr ic\\npsychiatrists then clinically diagnosed the children based on the\\nASD diagnostic criteria in the DSM-5 and the aforementioned\\nevaluationresults.AllparticipantswithconﬁrmedASD(theA SD\\ngroup) reached the cut-oﬀ scores for ASD diagnosis for both\\nevaluationscales.\\nAnalytic Approach\\nThesexdiﬀerencebetweentheHRandTDgroupswascompared\\nusing the χ2test. The diﬀerences between the HR group and\\nthe TD group in social behaviors were determined using the\\nindependent samples t-test. The correlations of social behaviors\\nin the HR group with age, DQ, and symptom severity were\\nanalyzed using Pearson’s rho. Finally, models for early ASD\\nscreening were constructed using machine learning methods\\nbased on ASD group and TD group, and the HR group\\n(contained 40 children with conﬁrmed ASD and 5 children\\nwith not met the criterions of ASD) would be used to veriﬁed\\nthe eﬀectiveness of models for early ASD screening. P<0.05\\nindicatedthatthediﬀerencewasstatisticallysigniﬁcant.\\nRESULTS\\nComparison of the General Conditions\\nBetween the HR Group and the TD Group\\nAge (months), adaptive DQ, language DQ, ﬁne motor DQ,\\nand personal-social DQ were signiﬁcantly diﬀerent ( P<0.05)\\nbetweentheHRgroupandtheTDgroup,whilesexDQandgross\\nmotorDQwerenotsigniﬁcantlydiﬀerent( P>0.05)betweenthe\\ntwogroups.See Table1andFigureS1 .Comparison of the Social Behaviors\\nBetween the HR and TD Groups During\\nDifferent SFP Episodes\\nDuring the FF episode of the SFP, there were statistically\\nsigniﬁcant diﬀerences in the duration and frequency of eye\\ncontact,socialsmiling,andactivesocialengagementbetw eenthe\\nHR group and the TD group ( t=-4.93,−6.17,−3.54,−2.90,\\n−9.56,−8.34; allP<0.05), while the diﬀerences in length and\\nfrequency of non-social smiling and protest behaviors betwe en\\ntheHRgroupandtheTDgroupwerenotstatisticallysigniﬁcan t\\n(t=1.89,1.69,1.62,1.55;all P>0.05).See Figures2A,B .\\nDuring the SF episode of the SFP, there were statistically\\nsigniﬁcant diﬀerences in the duration and frequency of eye\\ncontact and active social engagement between the HR group\\nand the TD group ( t= −4.94,−5.34,−4.49,−6.16; allP<\\n0.05), while the diﬀerences in length and frequency of non-\\nsocial smiling, protest behaviors and social smiling betwee n the\\nHR group and the TD group were not statistically signiﬁcant\\n(t=1.91, 1.33, 0.80, −0.01,−1.98,−1.71; allP>0.05). See\\nFigures2C,D .\\nAnalysis of the Correlation of Social\\nBehaviors With Each Factor for the HR and\\nTD Groups During the Different SFP\\nEpisodes\\nIn the TD group, the frequency of eye contact was signiﬁcantly\\npositively correlated with gross motor DQ for the SF episode\\n(P<0.05); the other indicators had no statistically signiﬁcant\\ncorrelationswithageandDQs( P>0.05;Table2).\\nDuring the FF episode, the duration of eye contact,\\nsocial smiling, and active social engagement for the HR\\ngroup were signiﬁcantly positively correlated with the\\nadaptive DQ, and the duration of eye contact and the gross\\nmotor DQ were also signiﬁcantly positively correlated.\\nDuring the SF episode, the duration of eye contact was\\nsigniﬁcantly positively correlated with the language DQ\\nand the ﬁne motor DQ, and the duration of active social\\nengagement and the language DQ were also signiﬁcantly\\npositively correlated (all P<0.05). The other indicators\\nhad no signiﬁcant correlation with age and DQs ( P>0.05;\\nTable3).\\nTheanalysisofthecorrelationbetweenclinicalASDsymptom\\nseverity and social behavior indicators suggested that the re was\\nno statistically signiﬁcant diﬀerence between social behav ior\\nindicators and clinical symptom severity during the FF episode\\n(P>0.05); during the SF episode, the duration of eye contact\\nwaspositivelycorrelatedwithsymbolicbehaviorfactorsco reand\\nthetotalCSBS-DPscore,thedurationofactivesocialengag ement\\nwas positively correlated with the total CSBS-DP score, and t he\\nfrequencyofeyecontactwaspositivelycorrelatedwiththesoc ial\\ncommunication factor score, the language factor score, and the\\ntotal CSBS-DP score (all P<0.05). The other indicators had\\nno signiﬁcant correlation with the symptom severity ( P>0.05;\\nTable3).\\nFrontiers in Pediatrics | www.frontiersin.org 4 June 2020 | Volume 8 | Article 290Qiu et al. Early Screening for High-Risk ASD\\nFIGURE 2 | Comparison of the differences in social behaviors between t he HR group and the TD group during the different SFP episodes .(A)Comparison of the\\ndifferences in the duration of social behaviors during the F F episode. (B)Comparison of the differences in the frequency of social beh aviors during the FF episode. (C)\\nComparison of the differences in the duration of social beha vior during the SF episode. (D)Comparison of the differences in the frequency of social beh aviors during\\nthe SF episode. HR, high-risk autism spectrum disorder; TD, typical development; SFP, still-face paradigm; ** P<0.01.\\nUsing Machine Learning to Construct\\nModels for Early ASD Screening\\nThrough the follow-up of the HR group and the re-diagnosis of\\nthe HR group at 2 years of age, we found that 5 (1 female and\\n4 males) out of the 45 infants and toddlers with HR no longer\\nmet the diagnostic criteria for ASD [the non-ASD (N-ASD)\\ngroup]; the other 40 children still met the diagnostic standa rd\\n(the ASD group). And then the models for early ASD screening\\nwereconstructedusingmachinelearningmethodsbasedonAS D\\ngroupandTDgroup.\\nWe used the duration and frequency of eye contact, active\\nsocial engagement, and social smiling during the FF episode a s\\nwellasthedurationandfrequencyofeyecontactandactives ocial\\nengagement during SF episode as the behavioral characterist ics\\nof the samples. The ASD group and the TD group were usedas the samples. In the classiﬁcation of ASD (40 samples) and\\nits comparison group of TD (43 samples), each subject within\\nthese83sampleswasusedfortestingthemodel thatwas trained\\non the rest 82 samples. Then, the prediction labels of each test\\nsample corresponding to 83 classiﬁcation models were collected\\nto calculate the overall accuracy on this dataset. There wer e\\n82 training samples, and 1 sample was selected for testing.\\nThe test set data were classiﬁed using the following machine\\nlearning methods: support vector machine (SVM), naïve Bayes\\nand random forest. And the Python platform was used for\\nanalyses. The Random Forest Classiﬁer, Gaussian NB and SVM\\nfunctions in Scikit-learn toolbox developed by Python were\\nemployedforbuildingclassiﬁcationmodels,respectively.\\nTheresultsshowedthattheaccuracyofBayesianclassiﬁcati on\\nwas80.54%fortheFFepisodeand82.35%fortheSFepisode,and\\nFrontiers in Pediatrics | www.frontiersin.org 5 June 2020 | Volume 8 | Article 290Qiu et al. Early Screening for High-Risk ASD\\nTABLE 2 | Analysis of the correlation of social behavior with age and D Q in the TD group during different SFP episodes ( r-value).\\nEpisodes and indicators Age\\n(months)Adaptive DQ Gross motor DQ Fine motor DQ Language DQ Personal-s ocial DQ\\nDuration during the FF episode (s)\\nEye contact −0.17 0.20 0.09 0.09 0.15 0.09\\nSocial smiling −0.04 −0.06 −0.05 −0.22 −0.04 −0.11\\nActive social engagement −0.03 −0.18 −0.12 −0.25 0.11 −0.12\\nFrequency during the FF episode (number of times)\\nEye contact −0.04 0.09 −0.10 0.11 0.03 0.01\\nSocial smiling 0.15 −0.07 −0.08 −0.21 0.03 −0.09\\nActive social engagement 0.05 −0.15 −0.04 0.02 0.02 −0.14\\nDuration during the SF episode (s)\\nEye contact 0.05 −0.09 −0.02 −0.02 0.04 −0.12\\nSocial smiling 0.04 −0.02 0.19 0.06 0.07 0.12\\nActive social engagement −0.13 −0.17 0.07 −0.15 0.13 −0.01\\nFrequency during the SF episode (number of times)\\nEye contact 0.01 −0.08 0.31* 0.01 0.04 −0.22\\nActive social engagement −0.12 −0.03 0.02 −0.04 0.14 −0.01\\nTD,typicaldevelopment;SFP,still-faceparadigm;DQ,developmentalquotie ntoftheGesellDevelopmentalScale;*P<0.05.\\nTABLE 3 | Analysis of the correlation between social behaviors and ag e (months), DQ, and clinical symptoms of the HR group during d ifferent SFP episodes ( r-value).\\nEpisodes and indicators Age\\n(months)DQ CSBS-DP CARS ABC\\nAdaptive Gross\\nmotorFine\\nmotorLanguage Personal-\\nsocialSocial\\ncommunication\\nfactorLanguage\\nfactorSymbolic\\nbehavior factorTotal\\nscore\\nDuration during the FF episode (s)\\nEye contact −0.08 0.43** 0.31* 0.18 0.25 0.29 0.09 −0.01 −0.01 0.03 0.07 −0.15\\nSocial smiling −0.11 0.36* 0.25 0.08 0.21 0.14 0.10 −0.01 −0.04 0.02 0.01 −0.12\\nActive social engagement −0.04 0.39* 0.26 0.18 0.16 0.20 0.27 −0.01 0.19 0.19 −0.04−0.08\\nFrequency during the FF episode (number of times)\\nEye contact −0.18 0.23 0.21 0.28 0.10 0.09 0.14 −0.04 0.05 0.05 −0.13−0.24\\nSocial smiling −0.09 0.09 0.11 0.14 0.01 0.17 0.01 −0.11 −0.02 −0.06−0.04−0.13\\nActive social engagement −0.05 0.17 0.03 0.22 −0.07 0.03 0.21 −0.07 0.21 −0.10 0.15 0.11\\nDuration during the SF episode (s)\\nEye contact −0.04 0.29 0.22 0.37* 0.30* 0.18 0.27 0.17 0.32* 0.30* −0.34−0.25\\nActive social engagement 0.02 0.27 0.23 0.26 0.38* 0.12 0.29 0.24 0.29 0.30* −0.11−0.16\\nFrequency during the SF episode (number of times)\\nEye contact −0.29 0.17 0.12 0.29 0.26 0.12 0.35* 0.33* 0.25 0.36* −0.22−0.18\\nActive social engagement −0.18 0.15 0.27 0.29 0.28 0.29 0.15 −0.25 0.15 −0.18 0.13 −0.17\\nHR, high-risk autism spectrum disorder; SFP, still-face paradigm; DQ , developmental quotient of the Gesell Developmental Scale; CSBS-DP, Comm unication and Symbolic Behavior\\nScalesDevelopmentalProﬁle;CARS,ChildhoodAutismRatingScal e;ABC,AutismBehaviorChecklist;*P<0.05,**P<0.01.\\nthat the accuracy of random forest classiﬁcation was 80.72% for\\nthe FF episode and 83.13% for the SF episode. And the accuracy\\nof SVM classiﬁcation was 81.18% for FF episode and 83.35%\\nfor the SF episode, which has higher accuracy. The confusion\\nmatrix was showed in Table4. And in order to ﬁnd the age\\ndiﬀerences between the kids who were not picked up by the\\nmachine learning, ASD group and TD group (total 83 kids) was\\ndividedinto4groupswithmonthage,respectively,8–11,12–15 ,\\n16–19,and20–23( FigureS2 ).\\nSubsequently, the eﬀectiveness of the SVM classiﬁcation\\nmodel was veriﬁed in 40 children with conﬁrmed ASD and5 N-ASD children in the HR group. Unfortunately, even\\nthough the average classiﬁcation accuracy of SVM was more\\nthan 80%, the 5 N-ASD was not classiﬁed correctly. The lack\\nof enough samples of N-ASD resulted in a large imbalance\\nbetween two groups. Such limited samples with only 10\\nindicators within each sample could be a possible reason for\\nlow recall rate of N-ASD samples. Therefore, it is essential t o\\nexpand the sample of N-ASD in future study to verify the\\neﬀectiveness of the SVM classiﬁcation model. What’s more, it\\nis necessary to build more eﬀective machine learning model in\\nfollowingstudy.\\nFrontiers in Pediatrics | www.frontiersin.org 6 June 2020 | Volume 8 | Article 290Qiu et al. Early Screening for High-Risk ASD\\nTABLE 4 | The confusion matrix of SVM.\\nASD TD\\nFF episode\\nASD 35 5\\nTD 10 33\\nSF episode\\nASD 34 6\\nTD 7 36\\nASD, autism spectrum disorder; TD, typical development; FF episode, face to face\\ninteraction;SFepisode,still-faceepisode.\\nDISCUSSION\\nFace-to-face interaction constitutes the beginning of ear ly\\nchildhood learning and deﬁning social interaction, and fac e-\\nto-face interactions between infants and toddlers and prima ry\\ncaregivers allows the former to learn (1) the meaning of self -\\nexpressionbehaviors;(2)thecharacteristicsofpeoplewithwh om\\nthey have a close relationship; and (3) emotional informatio n\\nand the perception of local culture, primary caretaker identit y\\nand self-identity ( 13). Emotion regulation is an important link\\nofearlychildhooddevelopmentmilestonesandiscloselyrel ated\\nto primary caregivers ( 29). Studies have shown that strong\\nemotion regulation abilities in children is associated wit h good\\ndevelopment and can predict social emotional outcomes at late r\\nstages(30–32)andthatweakemotionregulationabilitiesduring\\nearly childhood are associated with behavioral problems and\\ndevelopment problems at later stages ( 29,33,34). Especially\\nfrom 4 to 9 months old, infants quickly learn how to regulate\\nemotionsthroughface-to-faceinteractions;therefore,t hequality\\nofinfant-motherinteractionsiscrucialatthisstage( 24,35).\\nThrough comparative analysis of the diﬀerences in social\\nbehaviorsbetweeninfantsandtoddlerswithHRandinfantsa nd\\ntoddlerswithTDbeforetheageof2,comparedwithinfantsand\\ntoddlerswithTD,infantsandtoddlerswithHRexhibitedsho rter\\ndurations and lower frequencies of eye contact, social smil ing,\\nand active social engagement during the FF episode of the SFP.\\nThis ﬁnding is consistent with those of most studies ( 28,36–\\n38). In the SF episode, compared with infants and toddlers with\\nTD, infants and toddlers with HR showed shorter durations an d\\nlower frequencies of eye contact and active social engageme nt,\\nwhichmeansthatalthoughchildrenwithHRexhibitedbehavi ors\\nto attract the attention of the non-responsive mothers, thei r\\nability to initiate active social engagement was lower than that\\nof children with TD. For infants with HR, avoiding eye contac t\\nresults in a low-quality infant-mother interaction; there fore, the\\ndevelopment of emotion regulation abilities in these infant s and\\ntoddlersmaybedelayed,whichexplainstosomeextentthecau ses\\nof the delayed development of social smiling and active socia l\\nengagementinchildrenwithASD.\\nFrom the results of the correlation analysis, there was a\\ndiﬀerence between age and the developmental level and social\\nbehaviors of some infants and toddlers in the HR group and\\nthe TD group, but the diﬀerence was not representative, i.e., th e\\nage and developmental level of the infants and toddlers did no t\\ninﬂuence their social behaviors under general conditions. Forinfants and toddlers with HR, the analysis of the correlatio n\\nbetween clinical symptoms and social behaviors showed that\\nthere was no correlation between social behaviors and sympto m\\nseverity during the FF episode of the SFP. During the SF\\nepisode, the duration of eye contact by infants and toddlers\\nwith HR was positively correlated with the symbolic behavior\\nfactor score and the total CSBS-DP score; the duration of soc ial\\nsmiling was positively correlated with the social communica tion\\nfactor score and the total CSBS-DP score; and the duration of\\nactivesocialengagementwaspositivelycorrelatedwiththeso cial\\ncommunication factor score, the symbolic behavior factor s core\\nand the total CSBS-DP score; and the frequency of eye contact\\nwas positively correlated with the social communication fac tor\\nscore,thelanguagefactorscoreandthetotalCSBS-DPscore .The\\nresults indicated that the more ﬂexible and appropriate the eye\\ncontact and active social engagement of the infants and todd lers\\nwith HR, the less severe were the ASD symptoms, which is also\\nconsistentwiththeresultsofmoststudies( 39–41).Althoughthe\\nsocial behaviors of infants and toddlers with ASD develop ove r\\ntime, their development level is limited, the gap between inf ants\\nand toddlers with ASD and infants and toddlers with TD also\\nincreases over time, and infants and toddlers with ASD develo p\\nmoreclinicalsymptomsofASD.\\nThe SFP presents changes in children’s expressions, emotions\\nand behaviors in a more microscopic coding mode. On the\\nbasis of setting a normal interaction, the SFP provides a soci al\\nchallenge scenario by setting the SF episode, during which the\\nsocial signals of the mother are completely missing during th e\\nperiod. For typically developing children, their inability to adapt\\nto the loss of social signals stimulated their ability to ini tiate\\nsocialinteractions,expressemotions,regulateemotions,an dbear\\nstress. The analysis of the above results showed that the soci al\\nbehaviors in infants and toddlers with HR, especially their soc ial\\nbehaviors during the SF episode of the SFP, were associated wit h\\nthe core ASD symptoms. According to the extreme male brain\\ntheory of autism ( 42,43) the toddlers with ASD are more prone\\ntooversystematizationandthushavelowerempathicability than\\ndoTDtoddlers,makingthemmorepronetodeﬁcienciesinsocia l\\nand verbal communication. By examining the diﬀerences in\\nsocialbehaviorsbetweentheinfantsandtoddlersintheHRgr oup\\nand the TD group, we found that although there were many\\ndiﬀerencesintheabnormalsocialbehaviorsbetweeninfants and\\ntoddlers with ASD and infants and toddlers with TD during the\\nFF and SF episodes of the SFP, the social behaviors of infants\\nand toddlers with HR, such as eye contact and active social\\nengagement,duringtheSFepisode(afrustrationscenario),we re\\nsigniﬁcantly correlated with core communication impairmen ts,\\nsuch as the social communication factor score, the symbolic\\nbehaviorfactorscoreandthetotalCSBS-DPscore.Thatis,t heSF\\nepisode of the SFP can better induce the social communication\\nimpairments in infants and toddlers with ASD. Markram et al.\\n(44) proposed the intense world theory, suggesting that an\\nexcessively active brain would excessively amplify ordinary\\nsensory experiences, causing the toddlers with ASD to be in a\\nstate of fragmented sensory information and to be overloade d,\\nand because of such a strong reaction, the intense emotions\\nperceived by them from the surrounding environment causes\\nsocial withdrawal, resulting in a series of autism symptoms s uch\\nFrontiers in Pediatrics | www.frontiersin.org 7 June 2020 | Volume 8 | Article 290Qiu et al. Early Screening for High-Risk ASD\\nassocialcommunicationimpairmentsandstereotypedbehavio rs.\\nTherefore,facingsocialcommunicationchallengessuchas theSF\\nepisode, infants and toddlers with TD attempted to arouse thei r\\nmothers’ responses by pointing and social smiling. In contras t,\\nforinfantsandtoddlerswithHR,evenforthosewhohadhighe r\\nfunction,theymayhavehadgoodinteractionswiththeirmot hers\\nduring the FF episode, but when their mothers did not respond,\\nsocialpressurewasreduced.Theymadefewerattemptsorshort er\\nattemptstoinitiatesocialinteractions.\\nIn addition, some studies have shown that when responding\\nto emotional reactions, children with ASD have worse emotio n\\nregulation abilities and more unreasonable expression and a re\\nmore likely to show negative emotions ( 45). However, in this\\nstudy, the negative emotions (protest behavior and non-soci al\\nsmiling) in infants and toddlers with HR and TD were not\\ndiﬀerent, and there were no diﬀerences during the frustration\\nscenario,i.e.,whenthemothersusedstillfaces.Furtherv alidation\\nanddiscussionareneededinfuturestudies.\\nBased on the re-diagnosis and regrouping of the children\\nat 2 years of age, machine learning methods, including SVM,\\nnaïve Bayes and random forest, were used to construct models\\nfor early ASD screening. And we found the classiﬁcation mode l\\nestablishedusingtheSVMhadthebestperformance,especiall yit\\nwasfoundtohavebetterscreeningabilityandreliabilityf ortheSF\\nepisode.Unfortunately,whenthemodelwasselectedandapplied\\nto the diﬀerential diagnosis of the children in the HR group, th e\\n5 N-ASD was not classiﬁed correctly. And also, it is the goal t o\\nidentify N-ASD from ASD group in our future eﬀorts. Since the\\nagediﬀerencesbetweentheASDgroupandTDgroup,weadded\\ntheFigureS2 , in which we divided ASD group and TD group\\n(total 83 kids) into 4 groups with month age, respectively, 8–11 ,\\n12–15,16–19,and20–23,andwefoundtherewerenotregulari ty\\nbetweentheclassiﬁcationaccuracyvs.monthage.\\nSimilarly, there are limitations in this study. First, beca use\\ninfants and toddlers with ASD generally have delayed\\ndevelopment, the 2 groups were not matched by age to\\nmake the development level of the HR group the same as that\\nof the TD group. Second, the sample size was small. In view of\\nthese limitations, we will continue to expand the sample size\\nin future studies to further verify the ﬁndings under contro lledphysiological and psychological ages. We hope that the SFP will\\nbe widely applied for the early ASD screening and that a more\\nobjective, standardized and convenient way for self-scree ning at\\nhomewillbeachieved.\\nDATA AVAILABILITY STATEMENT\\nAll datasets generated for this study are included in the\\narticle/SupplementaryMaterial .\\nETHICS STATEMENT\\nThe studies involving human participants were reviewed and\\napproved by the Medical Ethics Committee of Nanjing Brain\\nHospital Aﬃliated to Nanjing Medical University. Written\\ninformed consent to participate in this study was provided\\nby the participants’ legal guardian/next of kin. Written\\ninformed consent was obtained from the individual(s), and\\nminor(s)’ legal guardian/next of kin, for the publication\\nof any potentially identiﬁable images or data included in\\nthisarticle.\\nAUTHOR CONTRIBUTIONS\\nNQ and XK designed experiments. NQ, CT, MZ, JW, CL, XX,\\nJF, LZ, TX, and HF carried out experiments. NQ, CT, and WH\\nanalyzedexperimentalresults.NQwrotethemanuscript.\\nFUNDING\\nKey Research (Social Development) Foundation of Jiangsu\\nProvince (BE2016616); the Fifth ‘‘333 High Level of Cultiva ting\\nTalentsProjects’’inJiangsuProvince.\\nSUPPLEMENTARY MATERIAL\\nThe Supplementary Material for this article can be found\\nonline at: https://www.frontiersin.org/articles/10.338 9/fped.\\n2020.00290/full#supplementary-material\\nREFERENCES\\n1. Marty MA, Segal DL. Diagnosis and Statistical Manual of Mental Disorders\\nDSM-5.AmericanPsychiatricAssociation,W.D.A.P.Association(20 13).\\n2. Kim YS, Leventhal BL, Koh Y-J, Fombonne E, Laska E, Lim E-C, e t al.\\nPrevalence of autism spectrum disorders in a total population sample. Am J\\nPsychiatry. (2011)168:904–12.doi:10.1176/appi.ajp.2011.10101532\\n3. Clark MLE, Vinen Z, Barbaro J, Dissanayake C. School age outco mes of\\nchildren diagnosed early and later with autism spectrum disorder. J Autism\\nDevDisord .(2017)48:92–102.doi:10.1007/s10803-017-3279-x\\n4. Daniels AM, Mandell DS. Explaining diﬀerences in age at autism s pectrum\\ndisorder diagnosis: a critical review. Autism Int J Res Pract. (2014) 18:583.\\ndoi:10.1177/1362361313480277\\n5. Barbaro J, Dissanayake C. ASD in infancy and toddlerhood: a re view of\\nthe evidence on early signs early identiﬁcation tools and early diag nosis.\\nJ Dev Behav Pediatr. (2009) 30:447–59. doi: 10.1097/DBP.0b013e3181\\nba0f9f6. Adrien JL, Lenoir P, Martineau J, Perrot A, Hameury L, Larmande C,\\net al. Blind ratings of early symptoms of autism based upon family\\nhome movies. J Am Acad ChildAdolesc Psychiatry. (1993) 32:617–27.\\ndoi:10.1097/00004583-199305000-00019\\n7. Wetherby AM, Woods J, Allen L, Cleary J, Dickinson H, Lord C. Early\\nindicators of autism spectrum disorders in the second year of lif e.J Autism\\nDevDisord. (2004)34:473–93.doi:10.1007/s10803-004-2544-y\\n8. Barbaro J, Dissanayake C. Early markers of autism spectrum\\ndisorders in infants and toddlers prospectively identiﬁed in the\\nSocial Attention and Communication Study. Autism. (2013) 17:64–86.\\ndoi:10.1177/1362361312442597\\n9. Zwaigenbaum L, Bauman ML, Stone WL, Yirmiya N, Estes A, Hansen RL,\\net al. Early identiﬁcation of autism spectrum disorder: recommendati ons\\nfor practice and research. Pediatrics. (2015) 48:92–102. 136(Suppl 1):S10–40.\\ndoi:10.1542/peds.2014-3667C\\n10. Cassel TD, Messinger DS, Ibanez LV, Haltigan JD, Acosta SI , Buchman AC.\\nEarly social and emotional communication in the infant siblings of children\\nFrontiers in Pediatrics | www.frontiersin.org 8 June 2020 | Volume 8 | Article 290Qiu et al. Early Screening for High-Risk ASD\\nwith autism spectrum disorders: an examination of the broad phenot ype.J\\nAutismDevDisord. (2007)37:122–32.doi:10.1007/s10803-006-0337-1\\n11. Heimann M, Laberg KE, Nordøen B. Imitative interaction increas es social\\ninterestandelicitedimitationinnon-verbalchildrenwithauti sm.InfantChild\\nDev.(2010)15:297–309.doi:10.1002/icd.463\\n12. Harker CM, Ibanez LV, Nguyen TP, Messinger DS, Stone WL. Th e eﬀect of\\nparenting style on social smiling in infants at high and low risk for A SD.J\\nAutismDevDisord. (2016)46:2399–407.doi:10.1007/s10803-016-2772-y\\n13. Tronick E, Als H, Adamson L. The infant’s response to entrapment\\nbetween contradictory messages in face-to-face interaction. J Am Acad Child\\nPsychiatry. (1978)17:1–13.doi:10.1016/S0002-7138(09)62273-1\\n14. MantisI,StackDM,NgL,SerbinLA,SchwartzmanAE.Mutualt ouchduring\\nmother–infant face-to-face still-face interactions: inﬂuenc es of interaction\\nperiod and infant birth status. Infant Behav Dev. (2014) 37:258–67.\\ndoi:10.1016/j.infbeh.2014.04.005\\n15. Carey WB. From neurons to neighborhoods: the science of early\\nchildhood development. Aust N Zeal J Psychiatry. (2002) 41:625–6.\\ndoi:10.1097/00004583-200205000-00022\\n16. Mercer J. Understanding Attachment: Parenting, Child Care, and Emoti onal\\nDevelopment Praeger .(2006)53:12–12.doi:10.1179/004049606X132096\\n17. YirmiyaN,GamlielI,PilowskyT,FeldmanR,Baron-CohenS,SigmanM .The\\ndevelopment of siblings of children with autism at 4 and 14 months: so cial\\nengagement,communication,andcognition. JChildPsycholPsychiatry. (2006)\\n47:511.doi:10.1111/j.1469-7610.2005.01528.x\\n18. Ostfeld-Etzion S, Golan O, Hirschler-Guttenberg Y, Zagoory- Sharon\\nO, Feldman R. Neuroendocrine and behavioral response to social\\nrupture and repair in preschoolers with autism spectrum disorders\\ninteracting with mother and father. Mol Autism. (2015) 6:1–13.\\ndoi:10.1186/s13229-015-0007-2\\n19. Merin N, Young GS, Ozonoﬀ S, Rogers SJ. Visual ﬁxation patte rns during\\nreciprocal social interaction distinguish a subgroup of 6-month- old infants\\nat-risk for autism from comparison infants. J Autism Dev Disord. (2007)\\n37:108–21.doi:10.1007/s10803-006-0342-4\\n20. Jamal W, Das S, Oprescu IA, Maharatna K, Apicella F, and Sicca F.\\nClassiﬁcationofautismspectrumdisorderusingsupervisedlearni ngofbrain\\nconnectivity measures extracted from synchrostates. J. Neural Eng. (2014)\\n11:046019.doi:10.1088/1741-2560/11/4/046019\\n21. Abraham A, Milham M, Martino AD, Craddock RC, Samaras D, Thirion\\nB, and Varoquaux G. (2016). Deriving reproducible biomarkers from multi -\\nsite resting-state data: An Autism-based example. Neuroimage . 147:736.\\ndoi:10.1016/j.neuroimage.2016.10.045\\n22. Robins DL, Fein D, Barton ML, Green JA. The modiﬁed checklist f or autism\\nin toddlers: an initial study investigating the early detectio n of autism and\\npervasive developmental disorders. J Autism Dev Disord. (2001) 31:131–44.\\ndoi:10.1023/A:1010738829569\\n23. Legerstee M, Markova G. Intentions make a diﬀerence: infant responses to\\nstill-faceandmodiﬁedstill-faceconditions. InfantBehavDev. (2007)30:232–\\n50.doi:10.1016/j.infbeh.2007.02.008\\n24. Yato Y, Kawai M, Negayama K, Sogon S, Tomiwa K, Yamamoto H. Infan t\\nresponses to maternal still-face at 4 and 9 months. Infant Behav Dev. (2008)\\n31:570–7.doi:10.1016/j.infbeh.2008.07.008\\n25. Montirosso R, Provenzi L, Tavian D, Morandi F, Bonanomi A, Mis saglia S,\\netal.Socialstressregulationin4-month-oldinfants:contributi onofmaternal\\nsocial engagement and infants’ 5-HTTLPR genotype. Early Hum Dev. (2015)\\n91:173–9.doi:10.1016/j.earlhumdev.2015.01.010\\n26. Provenzi L, Fumagalli M, Bernasconi F, Sirgiovanni I, Morandi F , Borgatti\\nR, et al. Very preterm and full-term infants’ response to socio-emotiona l\\nstress: the role of postnatal maternal bonding. Infancy.(2017) 22:695–712.\\ndoi:10.1111/infa.12175\\n27. Tronick EZ, Messinger DS, Weinberg MK, Lester BM, Lagasse L, Seifer R,\\net al. Cocaine exposure is associated with subtle compromises of infa nts’ and\\nmothers’ social-emotional behavior and dyadic features of thei r interaction\\nin the face-to-face still-face paradigm. Dev Psychol. (2005) 41:711–22.\\ndoi:10.1037/0012-1649.41.5.711\\n28. Nichols CM, Ibanez LV, Foss-Feig JH, Stone WL. Social smiling and its\\ncomponentsinhigh-riskinfantsiblingswithoutlaterASDsymptomat ology.J\\nAutismDevDisord. (2014)44:894–902.doi:10.1007/s10803-013-1944-2\\n29. CalkinsSD,DedmonSE.Physiologicalandbehavioralregulatio nintwo-year-\\nold children with aggressive/destructive behavior problems. J Abnorm Child\\nPsychol.(2000)28:103–18.doi:10.1023/A:100511291290630. Schultz D, Izard CE, Ackerman BP, Youngstrom EA. Emotion\\nknowledge in economically disadvantaged children: self-regulatory\\nantecedents and relations to social diﬃculties and withdrawal.\\nDev Psychopathol. (2001) 13:53–67. doi: 10.1017/S095457940100\\n1043\\n31. Denham SA, Blair KA, Demulder E, Levitas J, Sawyer K, Auerbac h-Major S,\\net al. Preschool emotional competence: pathway to social competence ?Child\\nDev.(2003)74:238.doi:10.1111/1467-8624.00533\\n32. Eisenberg N, Zhou Q, Losoya SH, Fabes RA, Shepard SA, Murphy BC,\\net al. The relations of parenting, eﬀortful control, and ego control\\nto children’s emotional expressivity. Child Dev. (2003) 74:875–95.\\ndoi:10.1111/1467-8624.00573\\n33. Calkins SD, Fox NA. Self-regulatory processes in early personality\\ndevelopment: a multilevel approach to the study of childhood social\\nwithdrawal and aggression. Dev Psychopathol. (2002) 14:477–98.\\ndoi:10.1017/S095457940200305X\\n34. Supplee LH, Skuban EM, Shaw DS, Prout J. Emotion regulation\\nstrategies and later externalizing behavior among European American\\nand African American children. Dev Psychopathol. (2009) 21:393.\\ndoi:10.1017/S0954579409000224\\n35. Haley DW, Stansbury K. Infant stress and parent responsiveness: regulation\\nof physiology and behavior during still-face and reunion. Child Dev. (2003)\\n74:1534–46.doi:10.1111/1467-8624.00621\\n36. Ozonoﬀ S, Iosif AM, Baguio F, Cook IC, Hill MM, Hutman T, et al.\\nA prospective study of the emergence of early behavioral signs of\\nautism.J Am Acad Child Adolesc Psychiatry. (2010) 49:256–66e252.\\ndoi:10.1016/j.jaac.2009.11.009\\n37. Lambert-BrownBL,McdonaldNM,MattsonWI,MartinKB,IbanezL V,Stone\\nWL,etal.Positiveemotionalengagementandautismrisk. DevPsychol. (2015)\\n51:848–55.doi:10.1037/a0039182\\n38. Chen X, Zou X, Chen K, Cen C, Cheng S. Analysis of early symptoms of\\nautismspectrumdisorderchildrenbasedonthree-minutevideos. ChinJAppl\\nClinPediatr. (2017)32:777–9.doi:10.3760/cma.j.issn.2095-428X.2017. 10.015\\n39. Cliﬀord SM, Dissanayake C. The early development of joint attent ion\\nin infants with autistic disorder using home video observatio ns\\nand parental interview. J Autism Dev Disord. (2008) 38:791–805.\\ndoi:10.1007/s10803-007-0444-7\\n40. Parlade MV, Iverson JM. The development of coordinated communica tion\\nin infants at heightened risk for autism spectrum disorder. J\\nAutism Dev Disord. (2015) 45:2218–34. doi: 10.1007/s10803-015-\\n2391-z\\n41. Campbell SB, Leezenbaum NB, Mahoney AS, Moore EL, Brownell CA.\\nPretend play and social engagement in toddlers at high and low genet ic\\nrisk for autism spectrum disorder. J Autism Dev Disord. (2016) 46:2305–16.\\ndoi:10.1007/s10803-016-2764-y\\n42. Baron-Cohen S. Autism: the empathizing-systemizing (E-S) th eory.\\nAnn N Y Acad Sci. (2009) 1156:68–80. doi: 10.1111/j.1749-6632.2009.\\n04467.x\\n43. Baron-Cohen S, Lombardo MV, Auyeung B, Ashwin E, Knickmeyer\\nR. Why are autism spectrum conditions more prevalent in\\nmales?PLoS Biol. (2011) 9:e1001081. doi: 10.1371/journal.pbio.10\\n01081\\n44. Markram H, Tania R, Kamila M. The intense world syndrome–an\\nalternative hypothesis for autism. Front Neurosci. (2007) 1:77–96.\\ndoi:10.3389/neuro.01.1.1.006.2007\\n45. Samson AC, Huber O, Gross JJ. Emotion regulation in Asperger’s\\nsyndrome and high-functioning autism. Emotion. (2012) 12:659–65.\\ndoi:10.1037/a0027975\\nConﬂict of Interest: The authors declare that the research was conducted in the\\nabsence of any commercial or ﬁnancial relationships that could be c onstrued as a\\npotentialconﬂictofinterest.\\nCopyright © 2020 Qiu, Tang, Zhai, Huang, Weng, Li, Xiao, Fu, Zhang, Xiao, Fang\\nand Ke. This is an open-access article distributed under the terms of the Creative\\nCommons Attribution License (CC BY). The use, distribution or reproduction in\\notherforumsispermitted,providedtheoriginalauthor(s)a ndthecopyrightowner(s)\\nare credited and that the original publication in this journ al is cited, in accordance\\nwith accepted academic practice. No use, distribution or re production is permitted\\nwhichdoesnotcomplywiththeseterms.\\nFrontiers in Pediatrics | www.frontiersin.org 9 June 2020 | Volume 8 | Article 290', metadata={'source': 'papers/Qiu_clean.txt'})],\n",
       " [Document(page_content='RESEA RCH ARTICL E\\nMobile detection ofautism through machine\\nlearning onhome video: Adevelopment and\\nprospective validation study\\nQandeel Tariq ID\\n1,2,Jena Daniels ID\\n1,2,Jessey Nicole Schwartz ID\\n1,2,Peter Washington ID\\n1,2,\\nHaik Kalantarian1,2,Dennis Paul Wall ID\\n1,2*\\n1Department ofPediatrics, Divisio nofSystems Medicine, Stanfor dUniversity, Californi a,United States of\\nAmerica, 2Department ofBiomed icalData Science, Stanford University ,California, United States ofAmerica\\n*dpwall@ stanford.ed u\\nAbstract\\nBackground\\nThestandard approaches todiagnosing autism spectrum disorder (ASD) evaluate between\\n20and100behaviors andtakeseveral hours tocomplete. Thishasinpartcontributed to\\nlongwaittimes foradiagnosis andsubsequent delays inaccess totherapy. Wehypothesize\\nthattheuseofmachine learning analysis onhome video canspeed thediagnosis without\\ncompromising accuracy. Wehave analyzed item-level records from 2standard diagnostic\\ninstruments toconstruct machine learning classifiers optimized forsparsity, interpretability,\\nandaccuracy. Inthepresent study, weprospectively testwhether thefeatures from these\\noptimized models canbeextracted byblinded nonexpert raters from 3-minute home videos\\nofchildren withandwithout ASD toarrive atarapid andaccurate machine learning autism\\nclassification.\\nMethods andfindings\\nWecreated amobile webportal forvideo raters toassess 30behavioral features (e.g., eye\\ncontact, social smile) thatareused by8independent machine learning models foridentify-\\ningASD, each with>94% accuracy incross-validation testing andsubsequent independent\\nvalidation from previous work. Wethen collected 116short home videos ofchildren with\\nautism (mean age=4years 10months, SD=2years 3months) and46videos oftypically\\ndeveloping children (mean age=2years 11months, SD=1year 2months). Three raters\\nblind tothediagnosis independently measured each ofthe30features from the8models,\\nwithamedian timetocompletion of4minutes. Although several models (consisting ofalter-\\nnating decision trees, support vector machine [SVM], logistic regression (LR), radial kernel,\\nandlinear SVM) performed well, asparse 5-feature LRclassifier (LR5) yielded thehighest\\naccuracy (area under thecurve [AUC]: 92% [95% CI88%–97%]) across allages tested. We\\nused aprospectively collected independent validation setof66videos (33ASD and33non-\\nASD) and3independent rater measuremen tstovalidate theoutcome, achieving lower but\\ncomparable accuracy (AUC: 89% [95% CI81%–95%]) .Finally, weapplied LRtothe162-\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 1/20a1111111111\\na1111111111\\na1111111111\\na1111111111\\na1111111111\\nOPEN ACCESS\\nCitation: Tariq Q,Daniels J,Schwartz JN,\\nWashingto nP,Kalantaria nH,WallDP(2018)\\nMobile detection ofautism through machine\\nlearning onhome video: Adevelopm entand\\nprospecti vevalidation study. PLoS Med15(11):\\ne1002705. https://d oi.org/10.1371/j ournal.\\npmed.1002 705\\nAcademic Editor: Suchi Saria, Johns Hopkins\\nUniversity ,UNITED STATES\\nReceived: June 8,2018\\nAccepted: October 25,2018\\nPublished: November 27,2018\\nCopyright: ©2018 Tariq etal.Thisisanopen\\naccess article distributed under theterms ofthe\\nCreative Commons Attribution License, which\\npermits unrestricte duse,distribu tion,and\\nreproduction inanymedium, provided theoriginal\\nauthor andsource arecredited.\\nData Availabilit yStatement: Thede-identified data\\nhave been made available atthefollowing github\\nrepository andinclude theprimary dataset andthe\\nvalidation dataset :https://github .com/qandeelt/\\nvideo_phen otyping_autis m_plos/tree/ master/\\ndatasets. Thecode hasbeen made available atthe\\nfollowing github repository andinstructions on\\nhowtoruneach classifier have been provided:\\nhttps://github .com/qandee lt/video_phe notyping_\\nautism_plosvideo-feature matrix toconstruct an8-feature model, which achieved 0.93 AUC (95% CI\\n0.90–0.97) ontheheld-out testsetand0.86 onthevalidation setof66videos. Validation on\\nchildren withanexisting diagnosis limited theability togeneralize theperformance toundi-\\nagnosed populations.\\nConclusions\\nThese results support thehypothesis thatfeature tagging ofhome videos formachine learn-\\ningclassification ofautism canyield accurate outcomes inshort timeframes, using mobile\\ndevices. Further work willbeneeded toconfirm thatthisapproach canaccelerate autism\\ndiagnosis atscale.\\nAuthor summary\\nWhy wasthisstudy done?\\n•Autism hasrisen inincidence byapproximately 700% since 1996 and now impacts at\\nleast 1in59children intheUnited States.\\n•The current standard fordiagnosis requires adirect clinician-to-child observation and\\ntakes hours toadminister.\\n•The sharp riseinincidence ofautism, coupled with theun-scalable nature ofthestan-\\ndard ofcare (SOC), hascreated strain onthehealthcare system, and theaverage ageof\\ndiagnosis remains around 4.5years, 2years past thetime when itcould bereliably\\ndiagnosed.\\n•Mobile measures that scale could help toalleviate thisstrain onthehealthcare system,\\nreduce waiting times foraccess totherapy and treatment, and reach underserved\\npopulations.\\nWhat didtheresearchers doandfind?\\n•Weapplied 8machine learning models to162two-minute home videos ofchildren with\\nand without autism diagnosis totesttheability toreliably detect autism onmobile\\nplatforms.\\n•Three nonexpert raters measured 30behavioral features needed formachine learning\\nclassification bythe8models inapproximately 4minutes.\\n•Leveraging video ratings, amachine learning model with only 5features achieved 86%\\nunweighted average recall (UAR) on162videos and UAR =80% onadifferent and\\nindependently evaluated setof66videos, with UAR =83% onchildren atorunder 4.\\n•The above machine learning process ofrendering amobile video diagnosis quickly cre-\\nated anovel collection oflabeled video features and anew video feature–based model\\nwith>90% accuracy.\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 2/20Funding: Thework wassupported inpartbyfunds\\ntoDPW from NIH(1R01EB02 5025-01 &\\n1R21HD0915 00-01), TheHartwell Foundation, Bill\\nandMelinda Gates Foundation, Coulter Foundation,\\nLucile Packard Foundation, andprogram grants\\nfrom Stanford University’s Human Centere d\\nArtificial Intellig ence Program, Precision Health and\\nIntegrate dDiagnostics Center (PHIND), Beckman\\nCenter, Bio-X Center, Predicti vesandDiagnostic s\\nAccelerator ,andtheChild Health Researc h\\nInstitute. Wealsoreceived philanthr opicsupport\\nfrom Bobby Dekesye randPeter Sullivan. No\\nfunding bodies hadanyroleinstudy design, data\\ncollection andanalysis ,decision topublish, or\\npreparation ofthemanuscript.\\nCompeting interests :Ihave readthejournal’s\\npolicy andtheauthors ofthismanuscript have the\\nfollowing competing interests: DPW isthescientific\\nfounder ofCognoa, acompany focused ondigital\\npediatric healthcare; theapproach andfindings\\npresented inthispaper areindepende ntfrom/not\\nrelated toCognoa. Allother authors have declared\\nnocompeting interests exist.\\nAbbreviati ons:ADI-R, Autism Diagnost ic\\nInterview- Revised; ADOS, Autism Diagnost ic\\nObservation Schedule; ADTree, alterna tingdecision\\ntree;ADTree7, 7-feature alterna tingdecision tree;\\nADTree8, 8-feature alterna tingdecision tree;ASD,\\nautism spectrum disorder; AUC, areaunder the\\ncurve; AUC-ROC, areaunder thereceiver operating\\ncharacteristi ccurve; BID,Balanced Independent\\nDataset; IRA,interrater agreement ;LR,logistic\\nregressio n;LR10, 10-feature logistic regressio n\\nclassifier ;LR5, 5-feature logistic regressi on\\nclassifier ;LR9, 9-feature logistic regressi on\\nclassifier ;LR-EN-VF, logistic regressio nwithan\\nelastic netpenalty; ROC, receiver operating\\ncharacteristi c;SOC, standard ofcare; SVM,\\nsupport vector machine ;SVM10, 10-feature\\nsupport vector machine ;SVM12, 12-feature\\nsupport vector machine ;SVM5, 5-feature support\\nvector machine ;UAR, unweighte daverage recall.What dothese findings mean?\\n•Short home videos canprovide sufficient information torunmachine learning classifi-\\nerstrained todetect children with autism from those with either typical oratypical\\ndevelopment. Features needed bymachine learning models designed todetect autism\\ncanbeidentified and measured inhome videos onmobile devices bynonexperts in\\ntimeframes close tothetotal video length and under 6minutes.\\n•The machine learning models provide aquantitative indication ofautism risk that pro-\\nvides more granularity than abinary outcome toflaginconclusive cases, potentially add-\\ningvalue foruseinclinical settings, e.g., fortriage.\\n•The process ofmobile video analysis forautism detection generates agrowing matrix of\\nvideo features that canbeused toconstruct new machine learning models that may\\nhave higher accuracy forautism detection inhome video.\\n•Clinical prospective testing ingeneral pediatric settings onpopulations notyetdiag-\\nnosed willbeneeded. However, these results support thepossibility that mobile video\\nanalysis with machine learning may enable rapid autism detection outside ofclinics to\\nreduce waiting periods foraccess tocare and reach underserved populations inregions\\nwith limited healthcare infrastructure.\\nIntroduction\\nNeuropsychiatric disorders arethesingle greatest cause ofdisability duetononcommunicable\\ndisease worldwide, accounting for14% oftheglobal burden ofdisease .Asignificant con-\\ntributor tothismetric isautism spectrum disorder (ASD, orautism), which hasrisen ininci-\\ndence byapproximately 700% since 1996 [2,3] and now impacts 1in59children intheUnited\\nStates [4,5]. ASD isarguably oneofthelargest pediatric health challenges, assupporting an\\nindividual with thecondition costs upto$2.4 million during his/her lifespan intheUSand\\nover $5billion annually inUShealthcare costs .\\nLike most mental health conditions, autism hasacomplex array ofsymptoms that are\\ndiagnosed through behavioral exams. The standard ofcare (SOC) foranautism diagnosis uses\\nbehavioral instruments such astheAutism Diagnostic Observation Schedule (ADOS) and\\ntheAutism Diagnostic Interview-Revised (ADI-R) .These standard exams aresimilar to\\nothers indevelopmental pediatrics  inthat they require adirect clinician-to-child observa-\\ntion and take hours toadminister [11–14]. The sharp riseinincidence ofautism, coupled with\\ntheunscalable nature oftheSOC, hascreated strain onthehealthcare system. Wait times fora\\ndiagnostic evaluation canreach orexceed 12months intheUS, and theaverage ageof\\ndiagnosis intheUSremains near 5years ofage[2,13], with underserved populations’ average\\nageatASD diagnosis ashigh as8years [16–18]. The high variability inavailability ofdiagnos-\\nticand therapeutic services iscommon tomost psychiatry and mental health conditions across\\ntheUS,with severe shortages ofmental health services in77% ofUScounties . Behavioral\\ninterventions forASD aremost impactful when administered byorbefore 5years ofage\\n[12,20–23]; however, thediagnostic bottleneck that families face severely limits theimpact of\\ntherapeutic interventions. Scalable measures arenecessary toalleviate these bottlenecks,\\nreduce waiting times foraccess totherapy, and reach underserved populations inneed.\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 3/20Asastep toward enabling fastand accurate access tocare forASD, wehave used supervised\\nmachine learning approaches toidentify minimal setsofbehaviors that align with clinical diag-\\nnoses ofASD [24–30]. Weassembled and analyzed item-level outcomes from theadministra-\\ntion oftheADOS and ADI-R totrain and testtheaccuracy ofarange ofclassifiers. Forthe\\nADOS, wefocused ouranalysis onordinal outcome data from modules 1,2,and 3,which\\nassess children with limited ornovocabulary, with phrased speech, and with fluent speech,\\nrespectively. Each ofthe3ADOS modules uses approximately 10activities foraclinical obser-\\nvation ofthechild atriskand 28–30 additional behavioral measurements used toscore the\\nchild following theobservation. Our machine learning analyses focused onarchived records of\\nthecategorical and ordinal data generated from thescoring component ofthese ADOS exami-\\nnations. Similarly, theADI-R involves 93multiple-choice questions asked byaclinician ofthe\\nchild’s primary care provider during anin-clinic interview; aswith theADOS, wefocused our\\nclassification task ontheordinal outcome data that resulted from thetest’s administration.\\nThese preliminary studies focused onbuilding models optimized foraccuracy, sparsity, and\\ninterpretability that differentiate autism from non-autism while managing class imbalance.\\nWechose models with small numbers offeatures, with performance atornomore than 1stan-\\ndard error away from best testperformance, and with interpretable outcomes—for example,\\nscores generated byaboosted decision tree orlogistic regression (LR) approach. Inall,these\\nstudies have used score data from 11,298 individuals with autism (mixed with low-, medium-,\\nand high-severity autism) and 1,356 controls (including some children forwhom autism may\\nhave been suspected butwasruled out) and have identified thefollowing 8classifiers: a7-fea-\\nture alternating decision tree (ADTree7) , an8-feature alternating decision tree (ADTree8)\\n, a12-feature support vector machine (SVM12) , a9-feature LRclassifier (LR9) , a\\n5-feature support vector machine (SVM5) , a5-feature LRclassifier (LR5) , a10-fea-\\nture LRclassifier (LR10) , and a10-feature support vector machine (SVM10) .\\nTwo ofthese 8classifiers have been independently tested in4separate analyses. Inapro-\\nspective head-to-head comparison between theclinical outcome and ADTree7 (measured\\nprior totheclinical evaluation and official diagnosis) on222children (NASD=69;Ncontrols =\\n153; median age=5.8years), theperformance, measured astheunweighted average recall\\n(UAR ; themean ofthesensitivity and specificity), was84.8% . Separately, Bone and\\ncolleagues  tested theADTree7 ona“Balanced Independent Dataset” (BID) consisting of\\nADI-R outcome data from 680participants (462 ASD, mean age=9.2years, SD=3.1years)\\nand 218non-ASD (mean age=9.4years, SD=2.9years) and found theperformance tobe\\nsimilarly high at80%. Duda and colleagues  tested theADTree8 with 2,333 individuals\\nwith autism (mean age=5.8years) and 283“non-autism” control individuals (mean age=6.4\\nyears) and found theperformance tobe90.2%. Bone and colleagues  also tested this\\nADTree8 model in1,033 participants from theBID—858 autism (mean age=5.2years,\\nSD=3.6years), 73autism spectrum (mean age=3.9years, SD=2.4years), and 102non-spec-\\ntrum (mean age=3.4years, SD=2.0years)—and found theperformance tobeslightly higher\\nat94%. These independent validation studies report classifier performance intherange ofthe\\npublished testaccuracy and lend additional support tothehypothesis that models using mini-\\nmal feature setsarereliable and accurate forautism detection.\\nOthers have runsimilar training and testing experiments toidentify top-ranked features\\nfrom standard instrument data, including Bone  and Bussu . These approaches have\\narrived atsimilar conclusions, namely that machine learning isaneffective way tobuild objec-\\ntive, quantitative models with fewfeatures todistinguish mild-, medium-, and high-severity\\nautism from children outside oftheautism spectrum, including those with other developmen-\\ntaldisorders. However, thetranslation ofsuch models into clinical practice requires additional\\nsteps that have notyetbeen adequately addressed. Although some ofourearlier work has\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 4/20shown that untrained video annotators canmeasure autism behaviors onhome videos with\\nhigh interrater reliability and accuracy , thequestion ofwhat steps must betaken tomove\\nfrom minimal behavioral models into clinical practice remains.\\nThe present study builds onthisprior work toaddress thisquestion and thehypothesis that\\nfeatures represented inourminimal viable classifiers canbelabeled quickly, accurately, and\\nreliably from short home videos byvideo raters with noofficial training inautism diagnosis or\\nchild development. Wedeployed crowdsourcing and real-time video analysis forfeature label-\\ningtorunand evaluate theaccuracy ofthe8machine learning models trained todetect autism\\nin2independent home video repositories. This procedure enabled ustotesttheability to\\nreduce topractice theprocess ofrapid mobile video analysis asaviable method foridentifying\\nautism symptoms and screening. Inaddition, asthemobile feature tagging ofvideos automati-\\ncally generates arich feature matrix, itpresents theopportunity totrain anew artificial intelli-\\ngence model that haspotentially higher generalizability tothetask ofautomatic detection of\\nautism inshort video clips. Wetestthisrelated hypothesis byconstructing anovel video fea-\\nture classifier and comparing itsresults toalternative models inaheld-out subset oftheorigi-\\nnalvideo feature matrix and inanindependent external validation set.The results from this\\nwork support thehypothesis that autism detection canbedone from mobile devices outside of\\nclinical settings with high efficiency and accuracy.\\nMethods\\nSource classifiers forreduce-to-practice testing\\nWeassembled 8published machine learning classifiers totestviability foruseintherapid\\nmobile detection ofautism inshort home videos. Forallofthe8models, thesource oftraining\\nand validation data wasmedical records generated through theadministration ofoneoftwo\\ngold-standard instruments inthediagnosis ofautism, theADOS ortheADI-R. The ADOS has\\nseveral modules containing approximately 30features that correspond todevelopmental level\\noftheindividual under assessment. Module 1isused onindividuals with limited ornovocabu-\\nlary. Module 2isused onindividuals who usephrase speech butwho arenotfluent. Module 3\\nisused onindividuals who arefluent speakers. The ADI-R isaparent-directed interview that\\nincludes>90 elements each asked oftheparent, with multiple choices foranswers. Each\\nmodel wastrained onitem-level outcomes from theadministration ofeither theADOS and\\nADI-R and optimized foraccuracy, sparsity offeatures, and interpretability.\\nForthepurpose ofbrevity without omission ofdetail, weopted tocreate anabbreviation\\nforeach model using abasic naming convention. This abbreviation took theform of“model_-\\ntype”-“number offeatures.” Forexample, weused ADTree8 torefer totheuseofanalternating\\ndecision tree (ADTree) with 8features developed from medical data from theadministration\\nofthediagnostic instrument ADOS Module 1,and LR5 torefer totheLRwith 5behavioral\\nfeatures developed from analysis ofADOS Module 2medical record data, and soon.\\nADTree7. We(Wall and colleagues ) applied machine learning toelectronic medical\\nrecord data recorded through theadministration oftheADI-R inthediagnostic assessment of\\nchildren atrisk forautism. Weused an80%:20% training and testing split and performed\\n10-fold cross-validation forasample of891children with autism and 75non-autism control\\nparticipants with anADTree model containing 7features. The ADTree uses boosting toman-\\nageclass imbalance [36,37]. Wealso performed up-sampling through 1,000 bootstrap permu-\\ntations tomanage class imbalance. The model wasvalidated inaclinical trial on222\\nparticipants  and inaBID consisting of680individuals (462 with autism) . The lowest\\nsensitivity and specificity exhibited were 89.9 and 79.7, respectively (UAR =84.8%).\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 5/20ADTree8. We used adataset ofscore sheets from ADOS Module 2for612children\\nwith ASD and 15non-autism control participants with a90%:10% training and testing split\\nand 10-fold cross-validation totrain and testanADTree model with 8ofthe29Module 2fea-\\ntures. The ADTree uses boosting and hasinherent robustness toclass imbalance [36,37]. We\\nalso performed up-sampling through 1,000 bootstrap permutations totestthesensitivity of\\nmodel performance toclass imbalance. This 8-feature ADTree model wasindependently\\ntested on446individuals with autism byWall and colleagues , on2,333 individuals with\\nautism and 238without autism byDuda and colleagues , and on1,033 individuals (858\\nautism, 73autism spectrum, 102non-spectrum) byBone and colleagues . The lowest sensi-\\ntivity and specificity reported were 97.1% and 83.3%, respectively (UAR =90.2%).\\nLR9. We performed training with ADOS Module 2records on362individuals with\\nautism and 282individuals without autism with backward feature selection and iterative\\nremoval ofthesingle lowest-ranked feature across 10folds each with a90%:10% class split.\\nClasses were weighted inversely proportional toclass sizetomanage imbalance. The model\\nwith thehighest sensitivity and specificity and lowest number offeatures, LRwith L1regulari-\\nzation and 9features, wasselected fortesting. Wetested themodel onindependent data from\\n1,089 individuals with autism and 66individuals with noautism diagnosis. The lowest sensitiv-\\nityand specificity identified were 98.8% and 89.4%, respectively (UAR =94.1%).\\nSVM12. We used score sheets from ADOS Module 3generated bytheevaluation of\\n510children with ASD and 93non-ASD control participants. These data were split into a90%\\ntraining and 10% testing set.Training and parameter tuning were performed with stepwise\\nbackward feature selection and iterative removal ofthesingle lowest-ranked feature across 10\\nfolds. Classes were weighted inversely proportional toclass sizetomanage imbalance. Several\\nmodels were fittoeach ofthefeature cross-validation folds. The model with thehighest sensi-\\ntivity and specificity and lowest number offeatures, aSupport Vector Machine (SVM) with a\\nradial basis function, wasthen applied tothetestsettomeasure generalization error. Wetested\\nthemodel on1,924 individuals with autism and 214individuals who didnotqualify foran\\nautism diagnosis. The lowest sensitivity and specificity identified onthetestsetwere 97.7%\\nand 97.2%, respectively (UAR =97.5%).\\nLR5 and SVM5. Inthisexperiment, we used medical records generated through the\\nadministration ofADOS Module 2for1,319 children with autism and 70non-autism control\\nparticipants. The dataset wassplit 80%:20% into train and testsets, with thesame proportion\\nforparticipants with and without ASD ineach set.Class imbalance wasmanaged bysetting\\nclass weights inversely proportional totheclass sizes. A10-fold cross-validation wasused to\\nselect features, and aseparate 10-fold cross-validation wasrunforhyperparameter tuning\\nprior totesting theperformance. AnSVM and anLRmodel with L1regularization showed the\\nhighest testperformance with 5features. The lowest sensitivity and specificity exhibited onthe\\ntestsetforSVM5 were 98% and 58%, respectively, (UAR =78%) and 93% and 67%, respec-\\ntively, (UAR =80%) forLR5.\\nLR10 and SVM10. Inthisexperiment, we used medical records generated through\\ntheadministration ofADOS Module 3for2,870 children with autism and 273non-autism\\ncontrol participants. The dataset wassplit 80%:20% into train and testsets, with thesame pro-\\nportion forparticipants with and without ASD ineach set.Class imbalance wasmanaged by\\nsetting class weights inversely proportional totheclass sizes. A10-fold cross-validation was\\nused toselect features, and aseparate 10-fold cross validation wasrunforhyperparameter tun-\\ningprior totesting theperformance. AnSVM and anLRmodel with L1regularization showed\\nthehighest testperformance with 10features. The lowest sensitivity and specificity exhibited\\nontheindependent testsetforSVM10 were 95% and 87%, respectively, (UAR =91%) and\\n90% and 89%, respectively, (UAR =89.5%) forLR10.\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 6/20Accounting foroverlap inthefeatures selected, these 8models measure 23unique features\\nintotal. The testaccuracy foreach model was>90%. Allmodels contain approximately 90%\\nfewer questions than theADI-R and 70%–84% fewer questions than thetotal features mea-\\nsured within theADOS. Anadditional 7features were chosen fortheir potential diagnostic\\nvalue and scored byvideo raters toassess their suitability forscoring home videos, creating a\\ntotal of30features forthemobile video rating process described below (Fig 1).\\nRecruitment and video collection\\nUnder anapproved Stanford University IRB protocol, wedeveloped amobile portal tofacili-\\ntatethecollection ofvideos ofchildren with ASD, from which participants electronically con-\\nsented toparticipate and upload their videos. Participants were recruited viacrowdsourcing\\nmethods [38–41] targeted atsocial media platforms and listservs forfamilies ofchildren with\\nautism. Interested participants were directed toasecure and encrypted video portal website to\\nconsent toparticipate. Werequired participants tobeatleast 18years ofageand theprimary\\ncare provider(s) forachild with autism between theages of12months and 17years. Partici-\\npants provided videos either through direct upload totheportal orviareference toavideo\\nalready uploaded toYouTube together with age, diagnosis, and other salient characteristics.\\nWeconsidered videos eligible ifthey (1)were between 1and 5minutes inlength, (2)showed\\ntheface and hands ofthechild, (3)showed clear opportunities forordirect social engagement,\\nand (4)involved opportunities fortheuseofanobject such asautensil, crayon, ortoy.\\nFig1.Feature- to-classifie rmapping. Video analysts scored each video with 30features. This matrix shows which feature\\ncorrespond stowhich classifier. Darker colored features indicate higher overlap, and lighter colors indicate lower overlap\\nacross themodels. The features arerank ordered according totheir frequency ofuseacross the8classifiers. Further details\\nabout theclassifiers areprovid edinTable 1.The bottom 7features were notpart ofthemachine learning process butwere\\nchosen because oftheir potential relations hipwith theautism phenotype and foruseinfurther evaluation ofthemodels’\\nfeature setswhen constructi ngavideo feature–sp ecific classifier. ADTree 7,7-feature alternating decision tree; ADTree8,\\n8-feature alternatin gdecision tree; LR5, 5-feature logistic regressio nclassifier; LR10, 10-feature logistic regressio nclassifier;\\nSVM5, 5-feature suppor tvector machine; SVM10, 10-feat uresupport vector machine; SVM12, 12-feature support vector\\nmachine.\\nhttps://doi. org/10.1371/j ournal.pmed. 1002705.g001\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 7/20Werelied onself-reported information provided bytheparents concerning thechild’s offi-\\ncialdiagnosis ofautism ornon-autism, theageofthechild when thevideo wassubmitted, and\\nadditional demographic information forvideos that were submitted directly totheweb portal.\\nForvideos that were provided viaYouTube URLs, weused YouTube metatags toconfirm the\\nageand diagnosis ofthechild inthevideo. Ifavideo didnotinclude ametatag fortheageof\\nthechild inthevideo, theagewasassigned following fullagreement among theestimates\\nmade by3clinical practitioners inpediatrics. Toevaluate theaccuracy oftheparents’ self-\\nreport and tosafeguard against reporting biases, wecommissioned apracticing pediatric spe-\\ncialist certified toadminister theADOS toreview arandom selection of20videos. Wealso\\ncommissioned adevelopmental pediatrician toreview anonoverlapping random selection of\\n10additional videos. These clinical experts classified each video as“ASD” or“non-ASD.”\\nFeature tagging ofvideos torun machine learning models\\nWeemployed atotal of9video raters who were either students (high school, undergraduate,\\norgraduate-level) orworking professionals. None hadtraining orcertification fordetection or\\ndiagnosis ofautism. Allwere given instructions onhow totagthe30questions and were asked\\ntoscore 10example videos before performing independent feature tagging ofnew videos.\\nTable 1.Eight machine learning classifier sused forvideo analysis and autism detection .The models were constructe dfrom ananalysis ofarchived medical records\\nfrom theuseofstandard instruments, including theADOS and theADI-R. All8models identified asmall, stable subset offeatures incross-validat ionexperime nts.The\\ntotal numbers ofaffected and unaffected control participants fortraining and testing areprovided together with measures ofaccuracy onthetestset.Four models were\\ntested onindependen tdatasets and have been mentioned inaseparate “Test” categor y.The remaining 4,indicated with “Train/test ,”used thegiven dataset with an\\n80%:20% train:test split tocalculate testaccuracy onthe20% held-out testset.The naming convention oftheclassifiers is“model type”-“numb eroffeatures”.\\nClassifier Medical record\\nsource#features NASD Nnon-ASD Mean age\\n(SD)%Male (N) Test\\nsensitivi tyTest\\nspecificityTest\\naccuracy\\nADTree8  ADOS\\nModule 18 Train: 612 Train:15 6.16\\n(4.16)76.8%\\n(N=2,009)100% 100% 100%\\nTest : 446 Test : 0\\nTest :\\n2,333Test :\\n238\\nTest : 931 Test :\\n102\\nADTree7  ADI-R 7 Train: 891 Train: 75 8.5(3.3) 65%\\n(N=628)100% 1.13% 99.9%\\nTest : 222 Test : 0\\nTest : 462 Test :\\n218\\nSVM with L1norm\\n(SVM5) ADOS\\nModule 25 Train/test :\\n1,319Train/t est:\\n706.92\\n(2.83)80%\\n(N=1,101)98% 58% 98%\\nLRwith L2norm (LR5)\\nADOS\\nModule 25 Train/test :\\n1,319Train/t est:\\n706.92\\n(2.83)80%\\n(N=1,101)93% 67% 95%\\nLRwith L1norm (LR9)\\nADOS\\nModule 29 Train: 362 Train: 282 11.75\\n(10)76.4%\\n(N=1,375)98.81% 89.39% 98.27%\\nTest: 1,089 Test: 66\\nRadial kernel SVM\\n(SVM12) ADOS\\nModule 312 Train: 510 Train: 93 16.25\\n(11.58)76.4%\\n(N=2,094)97.71% 97.2% 97.66%\\nTest: 1,924 Test: 214\\nLinear SVM (SVM10)  ADOS\\nModule 310 Train/test :\\n2,870Train/t est:\\n2739.08\\n(3.08)81%\\n(N=2,557)95% 87% 97%\\nLR(LR10)  ADOS\\nModule 310 Train/test :\\n2,870Train/t est:\\n2739.08\\n(3.08)81%\\n(N=2,557)90% 89% 94%\\nAbbreviation s:ADI-R, Autism Diagnostic Interview -Revised; ADOS, Autism Diagnostic Observation Schedule; ADTr ee7, 7-feature alternating decision tree; ADTree8,\\n8-feature alternating decision tree; LR,logistic regression ;LR5, 5-feature LRclassifier; LR10, 10-feature LRclassifier; SVM, support vector machine; SVM5, 5-feature\\nSVM; SVM10, 10-feature SVM; SVM12, 12-feature SVM.\\nhttps://do i.org/10.1371/j ournal.pm ed.1002705.t0 01\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 8/20After training, weprovided theraters with unique usernames and passwords toaccess the\\nsecure online portal towatch videos and answer 30questions foreach video needed bythefea-\\nture vectors torunthe8machine learning classifiers (Table 1).Features were presented tothe\\nvideo raters asmultiple-choice questions written atanapproximately seventh-grade reading\\nlevel. The raters, who remained blind todiagnosis throughout thestudy, were tasked tochoose\\noneofthetags foreach feature that best described thechild’s behavior inthevideo. Each\\nresponse toafeature wasthen mapped toascore between 0and 3,with higher scores indicat-\\ningmore severe autism features inthemeasured behavior, or8toindicate that thefeature\\ncould notbescored. The behavioral features and theoverlap across themodels areprovided in\\nFig1.\\nTotesttheviability offeature tagging videos forrapid machine learning detection and diag-\\nnosis ofautism, weempirically identified aminimum number ofvideo raters needed toscore\\nparent-provided home videos. Weselected arandom subset ofvideos from thefullsetofvid-\\neoscollected through ourcrowdsourced portal and rantheADTree8  model onfeature\\nvectors tagged byall9raters. Wechose torunonly ADTree8 forefficiency reasons and\\nbecause thismodel hasbeen previously validated in2independent studies [25,32]. Weused a\\nsample-with-replacement permutation procedure tomeasure accuracy asafunction ofmajor-\\nityrater agreement with thetrue diagnostic classification. Weincrementally increased the\\nnumber ofvideo raters pertrial by1rater, starting with 1and ending with 9,drawing with\\nreplacement 1,000 times pertrial. When considering only 2raters, werequired perfect class\\nagreement between theraters. With anodd number ofraters, werequired astrict majority\\nconsensus. When aneven number ofraters disagreed onclassification, weused anindepen-\\ndent and randomly chosen rater’s score tobreak thetie.\\nAfter determining theminimally viable number ofvideo raters, weused that minimum to\\ngenerate thefullsetof30-feature vectors onallvideos. Seven ofthemodels were written in\\nPython 3using thepackage scikit-learn, and onewaswritten inR.Weranthese 8models on\\nourfeature matrices after feature tagging onvideos. Wemeasured themodel accuracy through\\ncomparison oftheraters’ majority classification result with thetrue diagnosis. Weevaluated\\nmodel performance further byagecategories:�2years,>2to�4years,>4years to�6years,\\nand>6years. Foreach category, wecalculated accuracy, sensitivity, and specificity.\\nWecollected timed data from each rater foreach video, which began when avideo rater\\npressed “play” onthevideo and concluded when avideo rater finished scoring byclicking\\n“submit” onthevideo portal. Weused these time stamps tocalculate thetime spent annotating\\neach video. Weapproximated thetime taken toanswer thequestions byexcluding thelength\\nofthevideo from thetotal time spent toscore avideo.\\nBuilding avideo feature classifier\\nThe process ofvideo feature tagging provides anopportunity togenerate acrowdsourced col-\\nlection ofindependent feature measurements that arespecific tothevideo ofthechild aswell\\nasindependent rater impressions ofthat child’s behaviors. This inturn hastheability togener-\\nateavaluable feature matrix todevelop models that include video-specific features rather than\\nfeatures identified through analysis onarchived data generated through administration ofthe\\nSOC (asisthecase forallclassifiers contained inTable 1).Tothisend, and following thecom-\\npletion oftheannotation onallvideos bytheminimum number ofraters, weperformed\\nmachine learning onourvideo feature set.Weused LRwith anelastic netpenalty  (LR-\\nEN-VF) topredict theautism class from thenon-autism class. Werandomly split thedataset\\ninto training and testing, reserving 20% forthelatter while using cross-validation onthetrain-\\ningsettotune forhyperparameters. Weused cross-validation formodel hyperparameter\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 9/20tuning byperforming agrid search with different values ofalpha (varying penalty weights)\\nand L1ratio (the mixing parameter determining how much weight toapply toL1versus L2\\npenalties). Based ontheresulting area under thecurve (AUC) and accuracy from each combi-\\nnation, weselected thetop-performing pair ofhyperparameters. Using thispair, wetrained\\nthemodel using LRand balanced class weights toadjust weights inversely proportional to\\nclass frequencies intheinput data. After determining thetop-ranked features based onthe\\ntrained model and theresulting coefficients, wevalidated themodel onthereserved testset.\\nIndependent test setforvalidation ofvideo phenotyping processes\\nWeused ourvideo portal and crowdsourcing approaches togenerate anindependent collec-\\ntion ofvideos forevaluation and feature tagging by3different raters than those used inthepri-\\nmary analysis. These raters hadsimilar characteristics totheoriginal group (age, education, no\\nclinical certifications indevelopmental pediatrics) and were trained forvideo tagging through\\nthesame procedures.\\nEthics statement\\nThis study wasconducted under approval byStanford University’s IRB under protocol IRB-\\n31099. Informed and written consent wasobtained from allstudy participants who submitted\\nvideos tothestudy.\\nResults\\nAllclassifiers used fortesting thetime and accuracy ofmobile video rating hadaccuracies\\nabove 90% (Table 1).The union offeatures across these 8classifiers (Table 1)was23(Fig 1).\\nThese features plus anadditional 7chosen forclinical validity testing were loaded into a\\nmobile video rating portal toenable remote feature tagging bynonclinical video raters.\\nWecollected atotal of193videos (Table 2)with average video length of2minutes 13sec-\\nonds (SD =1minute 40seconds). Ofthe119ASD videos, 72were direct submissions made by\\nTable 2.Demographi cinformation onchildr eninthecollected home videos. WecollectedN=193(119 ASD, 74non-ASD) home videos foranalysis. Weexcluded 31\\nvideos because ofinadequate labeling orvideo quality. Weused arandoml ychosen 25autism and 25non-autism videos toempirical lydefine anoptimal number ofraters.\\nVideo feature tagging formachine learning wasthen done on162home videos.\\nVideos NNASDNnon-\\nASDMean age\\n(SD)�2years>2years and\\n�4years>4years and\\n�6years>6years Percent male,\\nASDPercent male,\\nnon-ASD\\nTotal 193 119 74 4years 4\\nmonths\\n(2years 1\\nmonth)24.4%\\n(n=47)33.7% (n=65) 25.9% (n=50) 15.5%\\n(n=31)39.38%\\n(n=76)23.32% (n=45)\\nExcluded 31 3 28 3years 8\\nmonths\\n(1year 11\\nmonths )32.3%\\n(n=10)29.0% (n=9) 25.8% (n=8) 9.6%\\n(n=4)3.23% (n=1) 58.06% (n=18)\\nTotal videos used foranalysis ofall\\n8classifie rs162 116 46 4years 4\\nmonths\\n(2years 2\\nmonths )22.8%\\n(n=37)34.5% (n=56) 25.9% (n=42) 16.7%\\n(n=27)67.2%\\n(n=78)56.5% (n=26)\\nSubset ofvideos used tofind\\nminimally viable number ofraters50 25 25 4years 6\\nmonths\\n(2years 4\\nmonths )28%\\n(n=14)34% (n=17) 18% (n=9) 20%\\n(n=10)48%\\n(n=12)44% (n=11)\\nAbbreviation :ASD, autism spectrum disorder.\\nhttps://do i.org/10.1371/j ournal.pm ed.1002705.t0 02\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 10/20theprimary caregiver ofthechild, and 47were links toanexisting video onYouTube. Ofthe\\n74non-ASD videos, 46non-ASD videos were links toexisting YouTube videos, and 28were\\ndirect submissions from theprimary caregiver. Weexcluded 31videos because ofinsufficient\\nevidence forthediagnosis (n=25)orinadequate video quality (n=6),leaving 162videos (116\\nwith ASD and 46non-ASD) which were loaded into ourmobile video rating portal forthepri-\\nmary analysis. Tovalidate self-reporting ofthepresence orabsence ofanASD diagnosis, 2\\nclinical staff trained and certified inautism diagnosis evaluated arandom selection of30videos\\n(15with ASD and 15non-ASD) from the162videos. Their classifications hadperfect corre-\\nspondence with thediagnoses provided through self-report bytheprimary caregiver.\\nWerandomly selected 50videos (25ASD and 25non-ASD) from thetotal 162collected\\nvideos and had9raters feature tagallinaneffort toevaluate thepotential foranoptimal num-\\nberofraters, with optimal being defined through abalance ofscalability and information con-\\ntent. The average video length ofthisrandom subset was1minute 54seconds (SD =46\\nseconds) fortheASD class and 2minutes 36seconds (SD =1minute 15seconds) forthenon-\\nASD class. Wethen rantheADTree8 (Table 1)model onthefeature vectors generated bythe\\n9raters. Wefound thedifference inaccuracy tobestatistically insignificant between 3raters—\\ntheminimum number tohave amajority consensus ontheclassification with noties—and 9\\nFig2.Accurac yacross different permutation sof9raters for50videos. Weperformed theanalysis todetermine theoptimal number (the minim umnumber toreach\\naconsensus onclassificati on)ofvideo raters needed tomaintain accuracy without lossofpower. Nine raters analyzed and genera tedfeature tags forasubset ofn=50\\nvideos (n=25ASD,n=25non-ASD) onwhich werantheADTree 8classifier (Table 1).The increase inaccuracy conferre dbytheuseof3versus 9raters wasnot\\nsignificant .Wetherefore settheoptimal rater number to3forsubsequ entanalyse s.ADTree8, 8-feature alternating decision tree; ASD, autism spectru mdisorder.\\nhttps://doi.o rg/10.1371/j ournal.pmed.1 002705.g002\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 11/20raters (Fig 2).Wetherefore elected tousearandom selection of3raters from the9tofeature\\ntagall162crowdsourced home videos.\\nModel performance\\nThree raters performed video screening and feature tagging togenerate vectors foreach ofthe\\n8machine learning models forcomparative evaluation ofperformance (Fig 3).Allclassifiers\\nhadsensitivity>94.5%. However, only 3ofthe8models exhibited specificity above 50%. The\\ntop-performing classifier wasLR5, which showed anaccuracy of88.9%, sensitivity of94.5%,\\nand specificity of77.4%. The next-best-performing models were SVM5 with 85.4% accuracy\\n(54.9% specificity) and LR10 with 84.8% accuracy (51% specificity).\\nLR5 exhibited high accuracy onallageranges with theexception ofchildren over 6years\\nold(although note that wehadlimited examples ofnon-ASD [n=1]class inthisrange). This\\nmodel performed best onchildren between theages of4and 6years, with sensitivity and speci-\\nficity both above 90% (Fig 4,Table 3).SVM5 and LR10 showed anincrease inperformance on\\nchildren ages 2–4years, both with 100% sensitivity and theformer with 66.7% and thelatter\\nwith 58.8% specificity. The 3raters agreed unanimously on116outof162videos (72%) when\\nusing thetop-performing classifier, LR5. The interrater agreement (IRA) forthismodel was\\nabove 75% inallageranges with theexception oftheyoungest agegroup ofchildren, those\\nFig3.Overall procedure forrapid and mobile classificat ionofASD versus non-ASD and performa nceofmodels from Table 1.Participa ntswere recruited to\\nparticipate viacrowdsourci ngmethods and provid edvideo bydirect upload orviaapreexisti ngYouTub elink. The minimum formajority rules of3video raters\\ntagged allfeatures, genera ting feature vectors toruneach ofthe8classifiers automatica lly.The sensitivity and specificit ybased onmajority outcome generated bythe\\n3raters on162(119 with autism) videos areprovided. Highligh tedinyellow isthebest performing model, LR5. ADTree7 ,7-feature alternatin gdecision tree;\\nADTree8, 8-feature alternating decision tree; ASD, autism spectru mdisorder; LR5, 5-feature logistic regression classifier; LR9, 9-feature logistic regression classifier;\\nLR10, 10-feature logistic regressio nclassifier; SVM5, 5-feature support vector machine; SVM10, 10-feature support vector machine; SVM12, 12-feature suppor t\\nvector machine.\\nhttps://doi.org/10 .1371/journal.p med.1002705 .g003\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 12/20under 2years, forwhich there wasagreater frequency ofdisagreement. The numbers ofnon-\\nASD representatives were small fortheolder ageranges evaluated (Table 3).\\nThe median time forthe3raters towatch and score avideo was4minutes (Table 4).\\nExcluding thetime spent watching thevideo, raters required amedian of2minutes 16seconds\\ntotagall30features intheanalyst portal. Wefound asignificant difference (p=0.0009)\\nbetween theaverage time spent toscore thevideos ofchildren with ASD and theaverage time\\nspent toscore thenon-ASD videos (6minutes 36seconds compared with 5minutes 8\\nseconds).\\nIndependent validation\\nTovalidate thefeasibility and accuracy ofrapid feature tagging and machine learning onshort\\nhome videos, welaunched asecond effort forcrowdsourcing videos ofchildren with and with-\\noutautism togenerate anindependent replication dataset. Wecollected 66videos, 33ofchil-\\ndren with autism and 33non-ASD. This setofvideos wascomparable totheinitial setof162\\nvideos interms ofgender, age, and video length. The average ageforchildren with ASD was4\\nyears 5months (SD =1year 9months), and theaverage agefornon-ASD children was3years\\n11months (SD =1year 7months). Forty-two percent (n=14)ofthechildren with ASD were\\nmale and 45% (n=15)ofthenon-ASD children were male. The average video length was3\\nminutes 24seconds, with anSDof45seconds. Forthisindependent replication, weused 3dif-\\nferent raters, each with noofficial training orexperience with developmental pediatrics. The\\nraters required amedian time of6minutes 48seconds forcomplete feature tagging. LR5 again\\nyielded thehighest accuracy, with asensitivity of87.8% and aspecificity of72.7%. Atotal of13\\nofthe66videos were misclassified, with 4false negatives.\\nFig4.Performance forLR5 byage. LR5 exhibited thehighest classifier performan ce(89% accuracy) outofthe8classifiers tested (Table 1).This model\\nperformed best onchildren between theages of2and 6years. (A)shows theperforman ceofLR5 across 4ageranges, and (B)provides theROC curve forLR5’s\\nperforman ceforchildren ages 2to6years. Table 3provides addition aldetails, includin gthenumber ofaffected and unaffected control participants within each\\nagerange. AUC, area under thecurve; LR5, 5-feature logistic regression classifier; ROC, receiver operating characte ristic.\\nhttps://do i.org/10.1371/j ournal.pm ed.1002705. g004\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 13/20Given thehigher average time forvideo evaluation, wehypothesized that thevideos con-\\ntained challenging displays ofautism symptoms. Therefore, weexamined theprobabilities\\ngenerated bytheLR5 model forthe13misclassified videos. Two ofthe4false negatives and 4\\nofthe9false positives hadborderline probabilities scores between 0.4and 0.6.Weelected to\\ndefine aprobability threshold between 0.4and 0.6toflagvideos asinconclusive cases. Twenty-\\nsixofthe66videos fellwithin thisinconclusive group when applying thisthreshold. When we\\nexcluded these 26from ouraccuracy analysis, thesensitivity and specificity increased to91.3%\\nand 88.2%, respectively.Table 3.Model performan cebyage. This table details theaccuracy ,sensitivity, specificity, precision, andrecall for8classifiers (Table 1)and for4ageranges found in\\nevaluation of162home videos with anaverage length of2minutes. Wealso provide theIRA, which indicates thefrequency with which themodel results from all3raters’\\nfeature tags agreed onclass. The top-perform ingclassifier wasLR5, which yielded anaccuracy of88.9%, sensitivity of94.5%, andspecificity of77.4%. Other notable classifi-\\nerswere SVM5 and LR10, which yielded 85.4% and 84.8% accurac y,respective ly.These 3best-perfo rming classifiers showed improved classification power within certain\\nageranges.\\nAge group Statistic ADTree 8 ADTree7 SVM5 LR5 LR9 SVM1 2 SVM10 LR10\\nOverall\\n(116 ASD, 46non-ASD, 64.4% male)Sensitivity 100% 94.5% 100% 94.5% 100% 100% 100% 100%\\nSpecificity 22.4% 37.3% 54.9% 77.4% 31.4% 0% 17.6% 51.0%\\nAccuracy 76.1% 76.3% 85.4% 88.9% 78.4% 71.6% 73.9% 84.8%\\nIRA 76.1% 67.5% 68.4% 71.7% 75.9% 71.6% 79.5% 70.9%\\nPrecision 74.3% 76.3% 82.3% 89.7% 76.0% 71.6% 72.4% 82.0%\\nUAR 61.2% 65.9% 77.5% 86.0% 65.7% 50% 58.8% 75.5%\\n�2years\\n(17ASD, 20non-ASD, 56.8% male)Sensitivity 100% 100% 100% 93.3% 100% 100% 100% 100%\\nSpecificity 14.2% 18.2% 38.1% 77.3% 22.7% 0% 14.3% 47.6%\\nAccuracy 50% 51.4% 62.9% 83.8% 54.1% 40.5% 50.0% 96.2%\\nIRA 53% 48.6% 45.7% 51.4% 48.6% 100% 66.7% 100%\\nPrecision 45.5% 45.5% 51.9% 73.7% 46.9% 45.9% 45.4% 57.7%\\nUAR 57.1% 59.1% 69.1% 85.3% 61.4% 50.0% 57.2% 73.8%\\n>2years and�4years\\n(39ASD, 17non-ASD, 66.1% male)Sensitivity 100% 97.3% 100% 91.8% 100.0% 100% 100% 100%\\nSpecificity 23.6% 50% 66.7% 73.7% 38.9% 0% 22.2% 58.8%\\nAccuracy 76.4% 50% 88.9% 85.7% 80.4% 66.7% 74.5% 86.8%\\nIRA 74.5% 81.8% 63.0% 75.0% 76.8% 100% 85.4% 77.4%\\nPrecision 74.5% 80.0% 85.7% 87.2% 77.6% 69.6% 72.5% 83.7%\\nUAR 61.8% 73.7% 83.4% 82.8% 69.5% 50.0% 61.1% 79.4%\\n>4years and�6years\\n(34ASD, 8non-ASD, 61.9% male)Sensitivity 100% 96.8% 100% 96.9% 100.0% 100% 100% 100%\\nSpecificity 40.0% 60% 72.7% 90.9% 40.0% 0% 18.2% 50.0%\\nAccuracy 85.4% 87.8% 92.9% 95.3% 85.7% 74.4% 79.1% 88.1%\\nIRA 85.4% 78.0% 78.6% 79.1% 85.7% 93.0% 76.7% 71.4%\\nPrecision 83.8% 88.2% 91.2% 96.9% 84.2% 80.9% 78.0% 86.5%\\nUAR 70.0% 78.4% 86.4% 93.9% 70.0% 50.0% 59.1% 75.0%\\n>6years (26ASD, 1non-ASD, 74.1% male) Sensitivity 100% 84.6% 100% 96.2% 100% 100% 100% 100%\\nSpecificity 0% 0% 0% 0% 0% 0% 0% 0%\\nAccuracy 96.2% 81.5% 96.2% 92.6% 96.2% 96.2% 96.2% 96.3%\\nIRA 96.2% 70.4% 96.2% 81.5% 96.2% 100% 100% 85.2%\\nPrecision 96.3% 95.7% 96.3% 96.2% 96.3% 96.3% 96.3% 96.3%\\nUAR 50.0% 42.3% 50.0% 48.1% 50.0% 50.0% 50.0% 50.0%\\nAbbreviation s:ADTree7, 7-feature alternatin gdecision tree; ADTree8, 8-feature alternat ingdecision tree; ASD, autism spectrum disorder; IRA, interrater agreemen t;\\nLR5, 5-feature logistic regression classifier; LR9, 9-feature logistic regression classifier; LR10, 10-feature logistic regression classifier; SVM5, 5-feature support vector\\nmachine; SVM10, 10-feature support vector machin e;SVM12, 12-feature support vector machine; UAR, unweighted average recall.\\nhttps://do i.org/10.1371/j ournal.pm ed.1002705.t0 03\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 14/20Training avideo feature–specific classifier\\nTobuild avideo feature–specific classifier, wetrained anLR-EN-VF model on528(3rat-\\ners×176videos) novel measures ofthe30video features used todistinguish theautism class\\nfrom theneurotypical cohort. Out ofthese 176videos (ASD =121, non-ASD =58), 162\\n(ASD =116, non-ASD =46)were from theanalysis set,and 14videos (ASD =5,non-\\nASD =12)were from thesetof66validation videos. Model hyperparameters (alpha and L1\\nratio) identified through 10-fold cross-validation were 0.01 and 0.6,respectively. Weused a\\nhigh L1ratio toenforce sparsity and todecrease model complexity and thenumber offeatures.\\nWehadsimilar proportions (0.60) fornon-ASD and ASD measures inthetraining setand\\nheld-out testset,which allowed ustocreate amodel that generalizes well without asignificant\\nchange insensitivity orspecificity onnovel data. The model hadanarea under thereceiver\\noperating characteristic curve (AUC-ROC) of93.3% and accuracy of87.7% ontheheld-out\\ntestset.Acomparison ofLR-EN-VF with LRL2penalty (nofeature reduction) revealed similar\\nresults (AUC-ROC: 93.8%, testaccuracy: 90.7%) (Fig 5).The top-8 features selected bythe\\nmodel consisted ofthefollowing, inorder ofhighest tolowest rank: speech patterns, commu-\\nnicative engagement, understands language, emotion expression, sensory seeking, responsive\\nsocial smile, stereotyped speech. One ofthese 8features—sensory seeking—was notpart ofthe\\nfullsetsofitems onthestandard instrument data used inthedevelopment and testing ofthe8\\nmodels depicted inTable 1.Wethen validated thisclassifier ontheremaining 52videos\\n(ASD =28,non-ASD =21)from thevalidation set,and theresults showed anaccuracy of\\n75.5% and anAUC-ROC of86.0%.\\nDiscussion\\nPrevious work [26–29] hasshown that machine learning models built onrecords from stan-\\ndard autism diagnoses canachieve high classification accuracy with asmall number offea-\\ntures. Although promising interms oftheir minimal feature requirements and ability to\\ngenerate anaccurate riskscore, their potential forimproving autism diagnosis inpractice hasTable 4.Time require dformobile tagging ofvideo features needed torun themachine learning models. Wehighlight theaverage length ofvideos (allparticipan ts,\\nonly participants with ASD, and only participants without ASD) aswell astheaverage time required towatch and score thevideos and theaverage time requir edfrom start\\ntoendofthescoring compone ntalone.\\nTotal time required forreview and feature\\ntaggingTotal time required forfeature tagging\\naloneVideo length\\nOverall Mean\\n(SD)6minutes 9seconds (5minutes 28seconds) 3minutes 36seconds (5minutes 52\\nseconds)2minutes 13seconds (1minute 40\\nseconds)\\nMedian 4minutes 0seconds 2minutes 16seconds 1minute 45seconds\\nRange 1minute 0seconds to\\n37minutes 0seconds0minutes 50seconds to\\n35minutes 42seconds0minutes 25seconds to8minutes 6\\nseconds\\nASD only Mean\\n(SD)6minutes 36seconds (5minutes 54seconds) 4minutes 22seconds (6minutes 20\\nseconds)2minutes 4seconds (1minute 40\\nseconds)\\nMedian 5minutes 0seconds 2minutes 40seconds 1minute 30seconds\\nRange 1minute 0seconds to\\n37minutes 0seconds0minutes 50seconds to\\n35minutes 42seconds0minutes 25seconds to8minutes 6\\nseconds\\nNon-ASD\\nonlyMean\\n(SD)5minutes 8seconds (4minutes 8seconds) 2minutes 18seconds (4minutes 22\\nseconds)2minutes 38seconds (1minute 34\\nseconds)\\nMedian 4minutes 0seconds 1minute 21seconds 2minutes 11seconds\\nRange 1minute 0seconds to\\n30minutes 0seconds0minutes 50seconds to\\n25minutes 42seconds0minutes 36seconds to6minutes 42\\nseconds\\nAbbreviation :ASD, autism spectrum disorder.\\nhttps://do i.org/10.1371/j ournal.pm ed.1002705.t0 04\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 15/20remained anopen question. The present study tested theability toreduce these models tothe\\npractice ofhome video evaluation bynonexperts using mobile platforms (e.g., tablets, smart-\\nphones). Independent tagging of30features by3raters blind todiagnosis enabled majority\\nrules machine learning classification of162two-minute (average) home videos inamedian of\\n4minutes at90% AUC onchildren ages 20months to6years. This performance wasmain-\\ntained at89% AUC (95% CI81%–95%) inaprospectively collected and independent external\\nsetof66videos each with 3independent rater measurement vectors. Taking advantage ofthe\\nprobability scores generated bythebest-performing model (L1-regularized LRmodel with 5\\nfeatures) toflaglow-confidence cases, wewere able toachieve a91% AUC, suggesting that the\\napproach could benefit from theuseofthescores onamore quantitative scale rather than just\\nasabinary classification outcome.\\nByusing amobile format that canbeaccessed online, weshowed that itispossible toget\\nmultiple independent feature vectors forclassification. This hasthepotential toelevate confi-\\ndence inclassification outcome atthetime ofdiagnosis (i.e., when 3ormore agree onclass)\\nwhile fostering thegrowth ofanovel matrix offeatures from short home videos. Inthesecond\\npart ofourstudy, wetested theability forthisvideo feature matrix toenable development ofa\\nnew model that cangeneralize tothetask ofvideo-based classification ofautism. Wefound\\nthat an8-feature LRmodel could achieve anAUC of0.93 ontheheld-out subset and 0.86 on\\ntheprospective independent validation set.One ofthefeatures used bythismodel, sensory\\nseeking, wasnotused bytheinstruments onwhich theoriginal models were trained, suggest-\\ningthepossibility that alternative features may provide added power forvideo classification.\\nThese results support thehypothesis that thedetection ofautism canbedone effectively at\\nscale through mobile video analysis and machine learning classification toproduce aquanti-\\nfied indicator ofautism risk quickly. Such aprocess could streamline autism diagnosis to\\nenable earlier detection and earlier access totherapy that hasthehighest impact during earlier\\nwindows ofsocial development. Further, thisapproach could help toreduce thegeographic\\nand financial burdens associated with access todiagnostic resources and provide more equal\\nFig5.ROC curve forLR-EN-VF showin gperforma nceontest data along with anROC forL2loss with nofeature\\nreduction .The former chose 8outof30video features. AUC, area under thecurve; LR-EN-VF, logistic regression\\nwith anelastic netpenalty; ROC, receiver operating characterist ic.\\nhttps://d oi.org/10.1371/j ournal.pm ed.1002705. g005\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 16/20opportunity tounderserved populations, including those indeveloping countries. Further test-\\ningand refinement should beconducted toidentify themost viable method(s) ofcrowdsourc-\\ningvideo acquisition and feature tagging. Inaddition, prospective trials inundiagnosed and in\\nlarger, more-balanced cohorts including examples ofchildren with non-autism developmental\\ndelays willbeneeded tobetter understand theapproach’s potential foruseinautism diagnosis.\\nSupporting information\\nS1Table. Results of8classifiers onindependent validation set.LR10, LR5, and ADTree7\\narethetop-3 best-performing classifiers onthevalidation set,which falls inlinewith the\\nresults observed onthetestdataset of162videos used earlier. LR5 stillperforms with thehigh-\\nestspecificity outofthe8models. ADTree7, 7-feature alternating decision tree; LR5, 5-feature\\nlogistic regression classifier; LR10, 10-feature logistic regression classifier.\\n(DOCX)\\nS1Text. Instructions forvideo raters.\\n(DOCX)\\nS1Checklist. The tripod checklist.\\n(DOCX)\\nAcknowledgmen ts\\nWewould liketothank Kaitlyn Dunlap, theparticipating families, and each ofourvideo raters\\nfortheir important contributions tothisstudy.\\nAuthor Contributions\\nConceptualization: Dennis Paul Wall.\\nData curation: Qandeel Tariq, Jena Daniels, Jessey Nicole Schwartz, Peter Washington, Haik\\nKalantarian, Dennis Paul Wall.\\nFormal analysis: Qandeel Tariq, Peter Washington, Haik Kalantarian, Dennis Paul Wall.\\nFunding acquisition: Dennis Paul Wall.\\nInvestigation: Qandeel Tariq, Jena Daniels, Jessey Nicole Schwartz, Dennis Paul Wall.\\nMethodology: Qandeel Tariq, Jena Daniels, Dennis Paul Wall.\\nProject administration: Jena Daniels, Jessey Nicole Schwartz, Dennis Paul Wall.\\nResources: Jena Daniels, Jessey Nicole Schwartz, Peter Washington, Haik Kalantarian, Dennis\\nPaul Wall.\\nSoftware: Qandeel Tariq, Dennis Paul Wall.\\nSupervision: Dennis Paul Wall.\\nValidation: Qandeel Tariq, Dennis Paul Wall.\\nVisualization: Qandeel Tariq, Jessey Nicole Schwartz, Dennis Paul Wall.\\nWriting –original draft: Qandeel Tariq, Jessey Nicole Schwartz, Dennis Paul Wall.\\nWriting –review &editing: Qandeel Tariq, Jena Daniels, Jessey Nicole Schwartz, Peter Wash-\\nington, Haik Kalantarian, Dennis Paul Wall.\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 17/20References\\n1.Prince M,Patel V,Saxena S,MajM,Maselk oJ,Phillips MR,etal.Global mental health 1-Nohealth\\nwithout mental health. Lancet. 2007; 370(9590) :859–77. https:/ /doi.org/10.10 16/S014 0-6736(07)\\n61238-0 PMID: 178040 63\\n2.Baio J,Wiggins L,Christensen DL,Maenner MJ,Daniels J,Warren Z,etal.Prevalence ofAutism Spec-\\ntrum Disorder Among Children Aged 8Years—Auti smandDevelopm ental Disabilities Monitori ngNet-\\nwork, 11Sites, United States, 2014. MMWR Surveillanc eSummarie s.2018; 67(6):1. https:// doi.org/10.\\n15585/mmw r.ss6706a 1PMID: 29701730. PMCID: PMC5 919599.\\n3.Hertz-Piccio ttoI,Delwiche L.TheRise inAutism andtheRole ofAgeatDiagnosis. Epidem iology.\\n2009; 20(1):84–9 0.https://doi.or g/10.1097/ED E.0b013e318 1902d1 5PMID: 19234401. PMCID:\\nPMC411360 0.\\n4.Christensen DL,Baio J,VanNaarden Braun K,Bilder D,Charles J,Constantino JN,etal.Preval ence\\nandCharacter istics ofAutism Spectru mDisorder Among Children Aged 8Years–Autism andDevelop-\\nmental Disabilities Monitoring Networ k,11Sites, United States, 2012. MMWR Survei llSumm. 2016; 65\\n(3):1–23. https://doi.or g/10.155 85/mmwr. ss6503a1 PMID: 27031587.\\n5.Christensen DL,Bilder DA,Zahoro dnyW,Pettygrove S,Durkin MS,Fitzgerald RT,etal.Prevalence\\nandcharacter istics ofautism spectrum disorde ramong 4-year-old children intheautism anddevelop -\\nmental disabilities monitori ngnetwork. Journal ofDevelop mental &Behavioral Pediatrics .2016; 37\\n(1):1–8. https://doi.or g/10.109 7/DBP.00 00000000000 235PMID: 26651088.\\n6.Buescher AV,Cidav Z,Knapp M,Mandell DS.Costs ofautism spectrum disorders intheUnited King-\\ndom andtheUnited States. JAMA Pediatr. 2014; 168(8):721 –8.https://doi.or g/10.1001 /jamapediatric s.\\n2014.210 PMID: 24911948.\\n7.McPartlan dJC,Reichow B,Volkmar FR.Sensitivity andspecific ityofpropose dDSM-5 diagno sticcrite-\\nriaforautism spectrum disorder. JAmAcad Child Adolesc Psych iatry. 2012; 51(4):368– 83.https://doi.\\norg/10.1016/ j.jaac.2012. 01.007 PMID: 22449643. PMCID: PMC3 424065.\\n8.Lord C,Rutter M,Goode S,Heemsb ergen J,Jordan H,Mawhood L,etal.Austism diagnostic observa-\\ntionschedule: Astandard izedobservatio nofcommunic ative andsocial behavio r.Journal ofautism and\\ndevelop mental disorders .1989; 19(2):185– 212. PMID: 2745388.\\n9.Lord C,Rutter M,LeCouteur A.Autism Diagnostic Interview-R evised: arevised version ofadiagnosti c\\ninterview forcaregivers ofindividuals withpossible pervasive developme ntaldisorders .Journal of\\nautism anddevelop mental disorde rs.1994; 24(5):659– 85.PMID: 7814313.\\n10. Association AP.Diagnostic andstatistic almanual ofmental disorders (DSM-5® ).Arlington, VA:Ameri-\\ncanPsychiat ricPub; 2013.\\n11. Bernier R,Mao A,YenJ.Psychopa thology, families, andculture: autism. Child Adolesc Psychiatr Clin\\nNAm.2010; 19(4):855– 67.https://doi.or g/10.1016/ j.chc.2010.0 7.005 PMID: 21056350.\\n12. Dawson G.Early behavioral intervention ,brain plasticity ,andthepreventi onofautism spectrum disor-\\nder.DevPsychopatho l.2008; 20(3):775– 803. https://doi.or g/10.101 7/S09545794 080003 70PMID:\\n18606031.\\n13. Mazurek MO,Handen BL,Wodka EL,Nowinski L,Butter E,Engelha rdtCR.Ageatfirstautism spec-\\ntrum disorde rdiagnosis: theroleofbirth cohort, demogr aphic factors, andclinical features. JDevBehav\\nPediatr. 2014; 35(9):561– 9.https://doi.or g/10.1097/ DBP.00000000 00000097 PMID: 25211371.\\n14. Wiggins LD,Baio J,Rice C.Examinati onofthetimebetween firstevaluation andfirstautism spectrum\\ndiagnos isinapopulation- based sample. Journal ofDevelopment alandBehavioral Pediatrics. 2006; 27\\n(2):S79–S 87.PMID: 166851 89.\\n15. Gordon-L ipkin E,Foster J,Peacock G.Whittling Down theWait Time: Exploring Models toMinimize the\\nDelay from Initial Concern toDiagno sisandTreatment ofAutism Spectru mDisord er.Pediatr ClinNorth\\nAm.2016; 63(5):851 –9.https://doi.or g/10.1016/j .pcl.2016. 06.007 PMID: 27565363. PMCID:PMC 5583718.\\n16. Howlin P,Moore A.Diagnosis inautism: Asurvey ofover 1200 patients intheUK.autism. 1997; 1\\n(2):135–62 .\\n17. Kogan MD,Stricklan dBB,Blumberg SJ,Singh GK,Perrin JM,vanDyck PC.ANational Profile ofthe\\nHealth Care Experiences andFamily Impact ofAutism Spectrum Disorder Among Children intheUnited\\nStates, 2005-2006. Pediatrics .2008; 122(6):E1 149–E58. https://d oi.org/10.154 2/peds.20 08-1057\\nPMID: 190472 16.\\n18. Siklos S,Kerns KA.Assessing thediagnos ticexperie nces ofasmall sample ofparents ofchildren with\\nautism spectrum disorde rs.ResDevDisabil. 2007; 28(1):9–22 .https://do i.org/10.1016 /j.ridd.2005. 09.\\n003PMID: 164422 61.\\n19. Thomas KC,EllisAR,Konrad TR,Holzer CE,Morrissey JP.County -level estimates ofmental health\\nprofession alshortage intheUnited States. Psychiatr Serv. 2009; 60(10):132 3–8. https://doi.or g/10.\\n1176/ps.2 009.60.10 .1323 PMID: 19797371.\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 18/2020. Dawson G,Jones EJH, Merkle K,Venema K,Lowy R,Faja S,etal.Early Behavioral Interventi onIs\\nAssociated With Normalized Brain Activity inYoung Children With Autism. Journal oftheAmerican\\nAcademy ofChild andAdolesce ntPsychiatry. 2012; 51(11):115 0–9. https://doi.or g/10.101 6/j.jaac.\\n2012.08. 018PMID: 23101741. PMCID: PMC360742 7.\\n21. Dawson G,Rogers S,Munson J,Smith M,Winter J,Greenson J,etal.Randomized, controll edtrialof\\nanintervent ionfortoddlers withautism: theEarly Start Denver Model. Pediatrics. 2010; 125(1):e17 –23.\\nhttps://doi.or g/10.154 2/peds.20 09-0958 PMID: 19948568. PMCID: PMC4951 085.\\n22. Landa RJ.Efficacy ofearly interven tions forinfants andyoung children with, andatriskfor,autism spec-\\ntrum disorde rs.Internationa lReview ofPsychiatry .2018; 30(1):25–3 9.https://doi. org/10.1080/\\n09540261.2 018.1432574 PMID: 295373 31.PMCID: PMC603470 0.\\n23. Phillips DA,Shonko ffJP.From neurons toneighborho ods:Thescience ofearly childhood develop ment.\\nWashingto n,D.C.: National Academie sPress; 2000. https://doi.or g/10.17226 /9824 PMID: 25077268.\\n24. Duda M,Daniels J,Wall DP.Clinical Evaluati onofaNovel andMobile Autism Risk Assessme nt.J\\nAutism DevDisord. 2016; 46(6):1953 –61. https:// doi.org/10.10 07/s1080 3-016-271 8-4PMID:\\n26873142. PMCID: PMC4860 199.\\n25. Duda M,Kosmick iJA,Wall DP.Testing theaccuracy ofanobservation-ba sedclassifier forrapid detec-\\ntionofautism risk.Transl Psychiatry .2014; 4(8):e424. https://doi. org/10.1038/tp .2014.65 PMID:\\n25116834.\\n26. Kosmicki JA,Sochat V,Duda M,Wall DP.Searching foraminimal setofbehavio rsforautism detection\\nthrough feature selection- based machine learning. Translational Psychiatry .2015; 5(2):e514. https://\\ndoi.org/10.10 38/tp.2015.7 PMID: 25710120. PMCID: PMC444575 6.\\n27. Levy S,Duda M,Haber N,Wall DP.Sparsifyi ngmachine learning models identify stable subsets ofpre-\\ndictive features forbehavio raldetection ofautism. MolAutism. 2017; 8(1):65. https://doi.or g/10.1186/\\ns13229-017 -0180-6 PMID: 29270283. PMCID: PMC573553 1.\\n28. Wall DP,Kosmicki J,DeLuca TF,Harstad E,Fusaro VA.Useofmachine learning toshorten observa-\\ntion-based screening anddiagnos isofautism. Translati onalPsychiatry. 2012; 2(4):e100. https://doi.\\norg/10.1038/ tp.2012.10 PMID: 22832900. PMCID: PMC333707 4.\\n29. Wall DP,Dally R,Luyster R,Jung JY,Deluca TF.Useofartificial intelligence toshorten thebehavio ral\\ndiagnos isofautism. PLoS One. 2012; 7(8):e4385 5.https://doi.or g/10.1371/jour nal.pon e.0043855\\nPMID: 229527 89.\\n30. Wall DP,Kosmisck iJ,Deluca TF,Harstad L,Fusaro VA.Useofmachine learning toshorten observa-\\ntion-based screening anddiagnos isofautism. Translati onalPsychiatry. 2012; 2(e100). https://doi.or g/\\n10.1038/ tp.2012.10 PMID: 22832900. PMCID: PMC333707 4.\\n31. Schuller B,Vlasenko B,Eyben F,Wollmer M,Stuhlsatz A,Wendemuth A,etal.Cross-Co rpus Acoustic\\nEmotion Recognition :Variances andStrategies. Ieee Transact ionsonAffective Computing. 2010; 1\\n(2):119–31 .https://doi. org/10.1109/T- Affc.2010.8\\n32. Bone D,Goodwin MS,Black MP,LeeCC,Audhkh asiK,Narayanan S.Applying machine learning to\\nfacilitate autism diagnostic s:pitfalls andpromises. JAutism DevDisord. 2015; 45(5):1121 –36. https://\\ndoi.org/10.10 07/s10803-014 -2268-6 PMID: 252946 49.PMCID: PMC439040 9.\\n33. Bone D,Bishop SL,Black MP,Goodwi nMS,Lord C,Narayanan SS.Useofmachine learning to\\nimprove autism screening anddiagno sticinstrumen ts:effectivenes s,efficiency ,andmulti-instrum ent\\nfusion. Journal ofChild Psychology andPsychiatry .2016; 57(8):927– 37.https://do i.org/10.1111 /jcpp.\\n12559 PMID: 270906 13.PMCID: PMC495855 1.\\n34. Bussu G,Jones EJH, Charman T,Johnson MH,Buitelaar JK,Team B.Prediction ofAutism at3Years\\nfrom Behavio uralandDevelopment alMeasures inHigh-Risk Infants: ALongitu dinal Cross- Domain\\nClassifier Analysis. Journal ofAutism andDevelopmental Disorders. 2018; 48(7):2418 –33. https://doi.\\norg/10.1007/ s10803-018- 3509-x PMID: 29453709. PMCID: PMC599600 7.\\n35. Fusaro VA,Daniels J,Duda M,DeLuca TF,D’Angelo O,Tambure lloJ,etal.ThePotential ofAccelera t-\\ningEarly Detection ofAutism throug hContent Analysis ofYouTube Videos. Plos One. 2014; 9(4):\\ne93533. https:// doi.org/10.13 71/journal.p one.009 3533 PMID: 2474023 6.PMCID: PMC398917 6.\\n36. Freund Y,Schapire RE,editors. Experiments withanewboosting algorithm. Icml; 1996 July3,1996;\\nBari, Italy. SanFrancisco, CA,USA: Morgan Kaufman Publish ersInc.;1996.\\n37. Freund Y,Mason L,editors. Thealternating decision treelearning algorithm. icml; 1999 June 27,1999;\\nBled, Slovenia. SanFrancisc o,CA,USA: Morgan Kaufman nPublishe rsInc.\\n38. Behrend TS,Sharek DJ,Meade AW,Wiebe EN.Theviability ofcrowdsourcin gforsurvey research .\\nBehav ResMethods .2011; 43(3):800– 13.https://doi. org/10.3758/s 13428-011- 0081-0 PMID:\\n21437749.\\n39. David MM, Babineau BA,Wall DP.Canweaccelerate autism discoverie sthrough crowds ourcing?\\nResearch inAutism Spectrum Disorders. 2016; 32:80–3 .\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 19/2040. Ogunsey eS,Parsons J,editors. What Makes aGood Crowd? Rethinking theRelationshi pbetween\\nRecruitmen tStrategies andData Quality inCrowds ourcing. Proceedings ofthe16th AISSIGSAN D\\nSymposium ;2017 May19-20, 2017; Cincinnati, OH.\\n41. Swan M.Crowds ourced health research studies: animportant emergin gcomplement toclinical trials in\\nthepublic health research ecosystem. JMed Interne tRes. 2012; 14(2):e46. https:// doi.org/10.21 96/\\njmir.1988 PMID: 22397809. PMCID: PMC337650 9.\\n42. ZouH,Hastie T.Regular ization andvariable selection viatheelastic net.Journal oftheRoyal Statistical\\nSociety: Series B(Statistica lMethodolo gy).2005; 67(2):301– 20.https:// doi.org/10.11 11/j.146 7-9868.\\n2005.00503 .x\\nMobile detection ofautism through machine learning onhome video\\nPLOS Medicine |https://doi.or g/10.1371/ journal.pmed.1 002705 November 27,2018 20/20', metadata={'source': 'papers/Tariq2018_clean.txt'})],\n",
       " [Document(page_content='', metadata={'source': 'papers/Tariq_2019_clean.txt'})],\n",
       " [Document(page_content='A Video-Based Measure to Identify Autism Risk in Infancy\\nGregory S. Young, PhDa, John N. Constantino, MDb, Simon Dvorak, BSc, Ashleigh Belding, \\nMPHa, Devon Gangi, PhDa, Alesha Hill, BAa, Monique Hill, MAa, Meghan Miller, PhDa, \\nChandni Parikh, PhDa, AJ Schwichtenberg, PhDd, Erika Solis, BSa, Sally Ozonoff, PhDa\\naDepartment of Psychiatry & Behavioral Sciences, MIND Institute, University of California-Davis\\nbDepartment of Psychiatry, Washington University-St. Louis School of Medicine\\ncInformation and Educational Technology, University of California-Davis\\ndDepartment of Human Development & Family Studies, Purdue University\\nAbstract\\nBackground: Signs of autism are present in the first two years of life, but the average age of \\ndiagnosis lags far behind. Instruments that improve detection of autism risk in infancy are needed. \\nThis study developed and tested the psychometric properties of a novel video-based approach to \\ndetecting ASD in infancy.\\nMethods: A prospective longitudinal study of children at elevated or lower risk for autism \\nspectrum disorder was conducted. Participants were 76 infants with an older sibling with ASD and \\n37 infants with no known family history of autism. The Video-referenced Infant Rating System for \\nAutism (VIRSA) is a web-based application that presents pairs of videos of parents and infants \\nplaying together and requires forced-choice judgments of which video is most similar to the child \\nbeing rated. Parents rated participants on the VIRSA at 6, 9, 12, and 18 months of age. We \\nexamined split-half and test-retest reliability; convergent and discriminant validity; and sensitivity, \\nspecificity, and negative and positive predictive value for concurrent and 36-month ASD \\ndiagnoses.\\nResults: The VIRSA demonstrated satisfactory reliability and convergent and discriminant \\nvalidity. VIRSA ratings were significantly lower for children ultimately diagnosed with ASD than \\nchildren with typical development by 12 months of age. VIRSA scores at 18 months identified all \\nchildren diagnosed with ASD at that age, as well as 78% of children diagnosed at 36 months.\\nConclusions: This study represents an initial step in the development of a novel video-based \\napproach to detection of ASD in infancy. The VIRSA’s psychometric properties were promising \\nwhen used by parents with an older affected child, but still must be tested in community samples \\nwith no family history of ASD. If results are replicated, then the VIRSA’s low-burden, web-based \\nformat has the potential to reduce disparities in communities with limited access to screening.\\nKeywords\\nAutism; Screening; Infancy; Social Development\\nCorrespondence:  Sally Ozonoff, MIND Institute, UC Davis Health, 2825 50th Street, Sacramento CA 95817; 916-703-0259; \\nsozonoff@ucdavis.edu. \\nHHS Public Access\\nAuthor manuscript\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nPublished in final edited form as:\\nJ Child Psychol Psychiatry . 2020 January ; 61(1): 88–94. doi:10.1111/jcpp.13105.\\nAuthor Manuscript Author Manuscript Author Manuscript Author ManuscriptIntroduction\\nThe developmental course of autism spectrum disorder (ASD) involves the onset of \\nsymptoms in the first three years of life. Differences between children who will later receive \\nan ASD diagnosis and those with typical development emerge before the second birthday \\n(Gammer et al., 2015 ; Landa & Garrett-Mayer, 2006 ; Ozonoff et al., 2010 ; Zwaigenbaum et \\nal., 2005 ), with some studies documenting signs in the first year of life ( Maestro et al., 2002 ; \\nMiller et al., 2017 ; Werner, Dawson, Osterling & Dinno, 2000 ), and parents first expressing \\nconcerns at an average age of 14 months ( Chawarska et al., 2007 ). Despite advances in \\nknowledge about the earliest presentations of ASD, the mean age of diagnosis has \\nstubbornly remained over 4 years ( Baio et al., 2018 ) and has not declined over the last two \\ndecades, squandering years of potential intervention when the brain is most plastic. It is \\ncritical that further attempts are made to decrease the age of ASD diagnosis so that it better \\naligns with the age of first symptom emergence.\\nOne of the identified barriers to more prompt recognition of ASD is measurement ( Al \\nQabandi, Gorter & Rosenbaum, 2011 ). Over the last decade, much effort has gone into the \\ndevelopment of instruments for earlier detection of ASD ( Zwaigenbaum et al., 2015 ). The \\nmost feasible method for large-scale screening is parent report and most existing measures \\nuse this methodology. However, recent studies have demonstrated low agreement between \\nparent report and more objective measures of ASD symptoms ( Ozonoff et al., 2011 ), as well \\nas lower reliability for screening instruments when used in rural, low income, less educated, \\nand racially diverse samples ( Khowaja, Hazzard & Robins, 2015 ; Scarpa et al., 2013 ). A \\npopulation screening study of 10,479 twelve-month-olds ( Pierce et al., 2011 ) using a parent-\\nreport measure ( Wetherby, Brosnan-Maddox, Peace & Newton, 2008 ) identified 32 infants \\nwith ASD. This represents significant under-identification, even after accounting for cases \\nwith later onset ( Barger, Campbell & McDonough, 2013 ), since current prevalence studies \\nestimate that 170 of 10,000 children have ASD ( Baio et al., 2018 ).\\nThe lower sensitivity of early screening measures may be due to the subtlety of initial ASD \\nsymptoms and the difficulty of accurately conveying them to parents through written \\ndescriptions. Major sources of error in parent questionnaires include comprehension and \\ninterpretation problems ( Koriat, Goldsmith & Pansky, 2000 ; Krosnick & Presser, 2010 ), \\nsuch as limited understanding of the queried constructs, inadequate knowledge of \\ndevelopmental milestones, and bias due to post-event information (e.g., eventual diagnosis). \\nThe current study moves beyond verbal descriptions by employing video examples to reduce \\nsubjective interpretations. The use of videos has been shown to dramatically increase clarity \\nin other fields, from music instruction to motor vehicle repair ( Arguel & Jamet, 2009 ). \\nRecently, video was incorporated in ASD screening by Marrus and colleagues (2015) , who \\nhad parents complete ratings after watching a video of a socially competent toddler, in order \\nto “reduce discrepant interpretations of items by providing informants with a common \\nnaturalistic standard for comparison” (p. 1340).\\nHere we describe the development of a new instrument, the Video-referenced Infant Rating \\nSystem for Autism (VIRSA). It extends previous approaches ( Marrus et al., 2015 ) by Young et al. Page 2\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscriptcreating a large library of video clips depicting a wide range of social-communication ability \\nand relying solely on video in the ratings, with no written descriptions of behavior. We \\nhypothesized that the semantic clarity afforded by video would improve early discrimination \\nof infants at highest risk for ASD.\\nMethods\\nInstrument Development\\nThe VIRSA was developed using video from participants in a longitudinal infant sibling \\nstudy and then validated on an independent sample of infants. Videos used in the VIRSA \\nwere drawn from an archive of over 300,000 minutes of digitized video recorded in a clinical \\nlaboratory setting. Video depicted infants and parents playing together with age-appropriate \\ntoys. Segments were selected from a task that used a standardized toy set and instructed \\nparents to play with their child as they would at home ( Schwichtenberg, Kellerman, Miller, \\nYoung & Ozonoff, 2019 ). Video recordings utilized a consistent camera angle facing the \\nchild, with the parent in profile. All families gave both informed consent and legal \\nauthorization to include their videos in the VIRSA.\\nSocial behaviors, including smiles, vocalizations, and eye contact, were coded by research \\nassistants unaware of participant risk group or outcome, using a previously validated coding \\nscheme that is sensitive to the changes that occur during the onset of ASD symptoms as \\nearly as 6 months ( Gangi et al., 2019 ; Ozonoff et al., 2010 ). In order to include a broad \\nrange of behaviors in the VIRSA, candidate videos were ranked by frequencies of the coded \\nbehaviors. Twenty-second segments were then excised from the original video files, \\nresulting in a collection of over 3,000 video segments from 100 past participants between 6 \\nand 18 months of age. Next, video segments were rated by 9 clinical research staff on a scale \\nfrom 1 (least socially competent) to 10 (most socially competent). Each clinician rated a \\nrandomly selected set of 39 videos twice to establish test-retest reliability (mean=0.89, range \\n0.78–0.98). Inter-rater reliability was examined on a larger randomly selected set of 260 \\nvideo clips rated by all raters using a two-way random ICC model. The average measures \\nICC for absolute agreement was 0.92, with a lower bound of 0.86, suggesting strong inter-\\nrater reliability of the 10-point scale. Video segments were excluded for poor lighting or \\naudio quality, obscured video angles, or use of the child’s name. This resulted in a pool of \\nvideo comprising 1,132 individual 20-second clips, which was then constrained to insure \\nadequate representation across the 10-point rating scale within each age (6, 9, 12, and 18 \\nmonths). To limit the software overhead for the VIRSA app, 268 videos were then randomly \\nselected to create the final VIRSA video library. The final pool of video included segments \\nfrom 11 children with ASD, 23 children with non-ASD developmental concerns (e.g. \\nspeech-language delays), and 29 children with typical development, based on 36-month \\noutcome (63 children total). Thirty-eight (60.32%) of the children depicted in VIRSA videos \\nwere male and 43 (68.25%) were Non-Hispanic Caucasian. Analysis of ratings of VIRSA \\nvideos, using a generalized linear model with random effects for subjects and age, revealed a \\nsignificant group effect ( X2=6.76, df=2, p<.05), with the ASD videos rated an average of \\n4.18 (95% CI = 2.81 to 5.55), the videos of children with non-ASD developmental concerns \\nrated an average of 6.14 (95% CI = 5.19 to 7.09), and the videos of typically developing Young et al. Page 3\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscriptparticipants rated an average of 6.35 (95% CI = 5.47 to 7.22). Simple comparisons indicated \\nthat the ratings of the ASD videos differed significantly from the videos of both the non-\\nASD developmental concerns ( t=2.31, p=.027) and typically developing cases ( t=2.62, p=.\\n013), who did not differ from one another ( t=0.31, p=0.76), as expected. Since the VIRSA \\nwas designed specifically to detect the social-communication behaviors relevant to ASD, but \\nnot broader developmental delays, this pattern of results provided further validation of the \\nfinal video pool.\\nThe library of video segments was incorporated into a web-based application that presented \\npairs of videos, depicting differing degrees of social competence, side by side, accompanied \\nby the prompt, “Which video is more like your child’s interaction with you on a typical \\nday?” On each trial, the video on the left played automatically, followed by the video on the \\nright, at which point the viewer selected the one most like the child. Presentation of video \\nfollowed an algorithm that always began with a pair of videos rated as 3 (less social) and 8 \\n(more social) on the 10-point scale. After each choice, the algorithm selected and displayed \\na second pair of videos with new scale values contingent upon the previously chosen video’s \\nranking. In each subsequent trial, the viewer’s video choice dictated the range of sociability \\nrepresented in the videos on the next trial, analogous to how optometrists help patients select \\neyeglass prescriptions. In this way, the algorithm presented videos of increasing similarity \\nover subsequent trials until the distance between videos reduced to 1 rating scale point on 2 \\nsubsequent trials, at which point the average rating of the last 2 trials was recorded as the \\nfinal score (see Figure S1 in online Appendix for examples).\\nThe VIRSA web application was designed with a brief introductory video that oriented \\nparents to the concepts and range of social behaviors depicted in the videos and provided \\nrating instructions. The VIRSA app also asked for confirmation of the child’s age in order to \\npresent videos from a matching age group. Since multiple video exemplars of each scale \\npoint (1 to 10) were available, videos were sampled from the pool without replacement.\\nThe UC Davis Institutional Review Board approved the study procedures. Parents signed an \\ninformed consent form prior to participation. They completed VIRSA ratings when their \\nchild was 6-, 9-, 12-, and 18-months-old and again two weeks later to examine test-retest \\nreliability. An automated email invited parents to the online VIRSA app, which could be \\naccessed by computer or mobile device (e.g., smartphones, tablets). VIRSA ratings were \\nalways done prior to in-person assessments, which were conducted at 6, 12, 18, 24, and 36 \\nmonths by examiners unaware of risk group or previous test results.\\nParticipants\\nThe VIRSA validation sample consisted of 110 infants (73 with an older sibling with ASD, \\n37 with no known family history of autism), none of whom supplied videos used in \\ninstrument development. Twenty-one children in the familial high-risk (HR) group received \\na diagnosis of ASD, whereas none of the low-risk (LR) children did. ASD diagnoses were \\nmade at any age that a child met DSM-5 criteria, based on all information available. One \\nchild was diagnosed with ASD at 12 months, 7 were diagnosed at 18 months, 7 at 24 \\nmonths, and 6 at 36 months. All children diagnosed before 36 months retained the ASD \\ndiagnosis at the final visit. The rest of the sample was classified as Non-ASD and then Young et al. Page 4\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscriptstratified by familial risk to yield the HR Non-ASD and LR Non-ASD comparison groups. \\nDescriptive statistics are shown in Table 1.\\nMeasures\\nMullen Scales of Early Learning  (MSEL; Mullen, 1995 ) is an assessment of cognitive, \\nmotor, and language development for children aged 1 to 68 months. MSEL scores were used \\nto describe the sample. Scores on the Fine Motor and Visual Reception subtests were used to \\nevaluate discriminant validity since these developmental domains are theoretically unrelated \\nto the social-communicative focus of the VIRSA.\\nAutism Diagnostic Observation Schedule, 2nd edition  (ADOS-2; Lord et al., 2012 ) is an \\nobservational measure that assesses ASD symptoms through semi-standardized interactions \\nbetween the clinician and child. It is comprised of five modules appropriate for various ages \\nand language levels; the Toddler module was administered at 18 and 24 months and either \\nmodule 1 or 2 was used at 36 months, depending upon the child’s verbal level. Analyses \\nutilized the overall total algorithm (Social Affect + Restricted Repetitive Behavior or \\nSARRB) score. Scores on the ADOS at 18 months were also used to examine convergent \\nvalidity with VIRSA ratings at 18 months.\\nAnalysis Plan\\nPsychometric properties of the VIRSA were examined in several ways. Split-half reliability \\nwas analyzed by comparing the first half of the ratings within a given session to the second \\nhalf. Test-retest reliability  was analyzed by comparing initial VIRSA ratings to those \\nobtained two weeks later. Parents were shown the same series of paired videos they had seen \\ntwo weeks earlier, instead of video pairs dictated by the VIRSA algorithm, permitting \\nexamination of the reliability of individual trial choices. Convergent validity  was examined \\nthrough correlations at 18 months with concurrent SARRB algorithm scores on the ADOS-2 \\nToddler module. Discriminant validity  was examined by correlating VIRSA scores with \\nconcurrent MSEL fine motor and visual reception age equivalents. Predictive validity  was \\nassessed by examining diagnostic outcome group differences on the VIRSA scores at each \\nage, using a mixed model with group, age, and their interaction included as fixed effects and \\nVIRSA score as the time-varying dependent variable. We also used ROC analysis to assess \\nthe VIRSA’s sensitivity, specificity, positive predictive value (PPV), and negative predictive \\nvalue (NPV)  in predicting ASD diagnosis. Area under the curve (AUC)  was computed as a \\nmeasure of the ability to distinguish between groups. Sample sizes initially projected were \\n90 high-risk and 45 low-risk infants. With an anticipated ASD outcome rate of \\napproximately 20% in the high-risk group, power was estimated to be .82 to detect an AUC \\nvalue of .70 on the ROC analyses. All analyses were conducted in R, version 3.5.0.\\nResults\\nVIRSA trials (selections between paired videos) took an average of 56.49 seconds \\n(range=38 to 133, SD=11.49). The average number of trials before a final score was reached \\nwas 7.67 (range=6 to 14, SD=1.26), with completion of the VIRSA taking an average of \\n7.21 minutes (range=3.82 to 15.57, SD=1.61). Split-half reliability was moderate, at r=.48, Young et al. Page 5\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author Manuscriptand test-retest reliability relatively strong, with 72% of the same video choices made two \\nweeks later, which was significantly greater than chance ( t=19.58, df=156, p<.0001). \\nConvergent validity correlation with concurrent ADOS-2 SARRB algorithm scores at 18 \\nmonths was r=−.36, which was significantly stronger than discriminant validity correlations \\nwith concurrent MSEL fine motor ( r=.05, Fisher’s z=2.50, p<.05) and visual reception ( r=.\\n10, Fisher’s z=2.11, p<.05) age equivalent scores.\\nFigure 1 shows the modeled parent VIRSA scores for each group between 6 and 18 months. \\nExamination of the model revealed a main effect for group ( χ2=10.32, df=2, p<.01). The \\nmain effect for age and the interaction between age and group were not significant. Planned \\ncontrasts revealed no significant differences in VIRSA scores between the ASD and \\ncomparison groups at 6 months (HR Non-ASD: t=0.71, p=0.48; LR Non-ASD: t=0.89, \\np=0.38) and 9 months (HR Non-ASD: t=1.75, p=.08; LR Non-ASD: t=1.46, p=0.14). At 12 \\nmonths, VIRSA scores were significantly lower in the ASD group than both the HR Non-\\nASD ( t=3.15, p=.002) and the LR Non-ASD groups ( t=2.11, p=.04). At 18 months, VIRSA \\nscores were significantly lower in the ASD group than in the HR Non-ASD group ( t=3.29, \\np=.002) and marginally lower than the LR Non-ASD group ( t=1.73, p=.08).\\nROC analyses were conducted on VIRSA scores at each age, predicting a binary 36-month \\noutcome (see Table 2). The threshold/cutoff at which ROC analyses best separated ASD and \\nNon-ASD cases was defined as the VIRSA score closest to the theoretical limit of maximum \\nspecificity and sensitivity. The VIRSA performed best at 18 months. Table 3 presents ROC \\nmodels that examined how well VIRSA scores at 18 months predicted concurrent 18-month \\ndiagnoses ( n=8 diagnosed with ASD at that age). Sensitivity was 100% (no false negatives), \\nbut specificity and positive predictive value were low.\\nDiscussion\\nWe hypothesized that employing video examples within a screening tool would enable \\ndetection of ASD in infancy. Starting at 12 months of age, VIRSA ratings were significantly \\nlower for the group eventually diagnosed with ASD than for the comparison groups. \\nSensitivity of 18-month VIRSA scores in predicting 36-month diagnosis was approximately \\n0.80. This compares quite favorably to a study reporting sensitivity of 18-month clinical \\ndiagnostic assessment in predicting 36-month diagnosis of only 0.37 ( Ozonoff et al., 2015 ). \\nIn fact, VIRSA scores had better sensitivity even at 6–12 months of age than that reported \\nfor clinical diagnosis at 18 months in Ozonoff et al. (2015) . We hypothesize that the use of \\nvideo allowed parents to “see” differences in their child that preceded the full onset of \\nsymptoms. The VIRSA’s sensitivity is especially impressive given the extended time course \\nof development of ASD symptoms. Multiple previous studies have demonstrated that \\nsymptoms slowly unfold over the first two years of life and many children who are \\nultimately diagnosed with ASD do not show overt signs before the first birthday ( Gammer et \\nal., 2015 ; Landa & Garrett-Mayer, 2006 ; Ozonoff et al., 2010 ; Zwaigenbaum et al., 2005 ). \\nChildren who are not identified by the VIRSA may not yet be showing signs of ASD for \\nparents to rate.Young et al. Page 6\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author ManuscriptWe also examined the VIRSA’s ability to index concurrent symptoms. ROC analyses of \\nVIRSA ratings at 18 months identified all eight children who had been diagnosed with ASD \\nby that age, with no false negatives. A recent meta-analysis of the accuracy of ASD \\nscreeners between 14 and 36 months of age ( Sanchez-Garcia et al., 2019 ) reported a pooled \\nsensitivity of 0.72 and specificity of 0.98. The sensitivity of the VIRSA at 18 months is thus \\ncomparable to or better than existing measures and suggests that it may be a useful adjunct \\nin identifying toddlers in need of referral for an ASD evaluation. Its specificity and positive \\npredictive value, however, were lower than recommended standards ( Cicchetti et al., 1995 ), \\nresulting in over-identification of risk. For this reason, the present results do not support use \\nof the VIRSA as a stand-alone ASD screener in infancy yet. Future studies could examine \\nwhether using the VIRSA as an initial step in a two-stage screening process improves \\naccuracy.\\nIn addition to the predictive validity of the VIRSA, we examined a number of other \\npsychometric properties. Test-retest reliability was strong, with parents selecting over 70% \\nof the same videos when they retook the VIRSA two weeks later. VIRSA scores at 18 \\nmonths were significantly correlated with ADOS-2 scores and correlations were \\nsignificantly higher than with measures of divergent abilities (e.g., fine motor and visual \\nreception skills).\\nThe majority of participants, and all the children who developed ASD, came from the high-\\nrisk group (e.g., had older siblings with ASD). It is imperative, prior to recommending the \\nVIRSA for clinical use, to examine its psychometric properties when used by parents who \\nare naïve to ASD. The positive predictive value of an instrument is dependent upon the base \\nrate of the condition in the population ( Clark & Harrington, 1999 ; Grimes & Schulz, 2002 ) \\nand thus the VIRSA’s predictive ability may be reduced in a community-based sample that \\nhas a lower prevalence of ASD than in high-risk families. Studies are currently underway in \\nour laboratory to determine whether the present results generalize to low-risk samples.\\nDespite these limitations, the VIRSA makes several contributions to the literature. First, it \\ndemonstrates that it is possible to develop a parent report instrument capable of identifying \\nASD risk in the first year of life. Second, it demonstrates that video can be used to clarify \\ndevelopmental phenomena and improve parent reporting of early development. Finally, an \\ninnovation of the VIRSA is its web-based, mobile-optimized application. Over 90% of \\nAmerican adults of childbearing age own a smartphone, with rates over 65% even in lower \\nincome, rural, and minority communities ( Pew Research Center, 2018 ). It is vital that \\nscreening procedures keep pace with such advances in technology and society’s increasingly \\ninternet-based preferences for information acquisition and communication. Thus, this study \\nprovides an initial step in the proof of principle of video- and web-based screening for ASD. \\nWith further development, the VIRSA, with its low-burden, quick, online ratings, has \\npotential to reduce disparities in communities with limited access to screening and provide \\nthe possibility of initiating intervention before the full symptom set of ASD has emerged.\\nSupplementary Material\\nRefer to Web version on PubMed Central for supplementary material.Young et al. Page 7\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author ManuscriptAcknowledgments\\nThis work was supported by NIH grants R01 MH099046 (Ozonoff) and U54 HD079125 (Abbeduto; MIND \\nInstitute Intellectual and Developmental Disabilities Research Center) and Autism Speaks grant 8370 (Ozonoff). \\nWe are deeply grateful to the parents who authorized the use of their child’s video in the development of the VIRSA \\nand the children and families who participated in the validation study.\\nDisclosures: Dr. Constantino receives royalties from Western Psychological Services for the commercial \\ndistribution of the Social Responsiveness Scale. Dr. Miller has received research grant funding from the National \\nInstitutes of Health and travel reimbursement and/or honoraria from the Society for Clinical Child and Adolescent \\nPsychology and the Help Group. Dr. Ozonoff has received research grant funding from the National Institutes of \\nHealth and Autism Speaks, travel reimbursement and honoraria for editorial activities from Autism Speaks, Autism \\nScience Foundation, and Wiley, and book royalties from Guilford Press and American Psychiatric Press, Inc. Dr. \\nSchwichtenberg has received grant funding from the National Institutes of Health and travel support from Autism \\nSpeaks and the Autism Science Foundation. All other authors report no financial disclosures or potential conflicts \\nof interest.\\nAbbreviations:\\nASD autism spectrum disorder\\nVIRSA Video-referenced Infant Rating System for Autism\\nReferences\\nAl-Qabandi M, Gorter JW, Rosenbaum P. (2011). Early autism detection: Are we ready for routine \\nscreening? Pediatrics, 128(1), e211–e217. [PubMed: 21669896] \\nArguel A, Jamet E. (2009). Using video and static pictures to improve learning of procedural contents. \\nComputers in Human Behavior, 25, 354–359.\\nBaio J, Wiggins L, Christensen DL, Maenner MJ, Daniels J, Warren Z, Kurzius-Spencer M, Zahorodny \\nW, Robinson-Rosenberg C, White T, Durkin MS, Imm P, Nikolaou L, Yeargin-Allsopp M, Lee LC, \\nHarrington R, Lopez M, Fitzgerald RT, Hewitt A, Pettygrove S, Constantino JN, Vehorn A, \\nShenouda, Hall-Lande J, Van Naarden-Braun K, Dowling NF. (2018). Prevalence of autism \\nspectrum disorder among children aged 8 years: autism and developmental disabilities monitoring \\nnetwork, 11 sites, United States 2014. MMWR Surveillance Summary, 67(6), 1–23.\\nBarger BD, Campbell JM, McDonough JD. (2013). Prevalence and onset of regression within autism \\nspectrum disorders: A meta-analytic review. Journal of Autism and Developmental Disorders, 43(4), \\n817–828. [PubMed: 22855372] \\nChawarska K, Paul R, Klin A, Hannigen S, Dichtel LE, V olkmar F. (2007). Parental recognition of \\ndevelopmental problems in toddlers with autism spectrum disorders. Journal of Autism and \\nDevelopmental Disorders, 37(1), 62–72. [PubMed: 17195921] \\nCicchetti DV , V olkmar F, Klin A, Showalter D. (1995). Diagnosing autism using ICD-10 criteria: A \\ncomparison of neural networks and standard multivariate procedures. Child Neuropsychology, 1, \\n26–37.\\nClark A, & Harrington R (1999). On diagnosing rare disorders rarely: Appropriate use of screening \\ninstruments. Journal of Child Psychology and Psychiatry, 40(2), 287–290. [PubMed: 10188711] \\nGammer I, Bedford R, Elsabbagh M, Garwood H, Pasco G, Tucker L, V olein A, Johnson MH, \\nCharman T, BASIS Team. (2015). Behavioral markers for autism in infancy: Scores on the Autism \\nObservational Scale for Infants in a prospective study of at-risk siblings. Infant Behavior and \\nDevelopment, 38, 107–115. [PubMed: 25656952] \\nGangi DN, Boterberg S, Schwichtenberg AJ, Solis E, Young GS, Iosif A, & Ozonoff S (2019). Use of \\nprospective longitudinal gaze measurements in defining regression Paper presented at the annual \\nmeeting of the International Society for Autism Research, Montreal, 5.\\nGrimes DA, & Schulz KF (2002). Uses and abuses of screening tests. Lancet, 359, 881–884. [PubMed: \\n11897304] Young et al. Page 8\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author ManuscriptKhowaja MK, Hazzard AP, Robins DL. (2015). Sociodemographic barriers to early detection of \\nautism: Screening and evaluation using the M-CHAT, M-CHAT-R, and follow-up. Journal of \\nAutism and Developmental Disorders, 45(6), 1797–1808. [PubMed: 25488122] \\nKoriat A, Goldsmith M, Pansky A. (2000). Toward a psychology of memory accuracy. Annual Review \\nof Psychology, 51, 481–537.\\nKrosnick JA, Presser S. (2010). Question and questionnaire design In Marsden PV , Wright JD (Eds.), \\nHandbook of Survey Research, 2nd edition (pp. 263–314). London: Emerald.\\nLanda R, Garrett-Mayer E. (2006). Development in infants with autism spectrum disorders: A \\nprospective study. Journal of Child Psychology and Psychiatry, 47(6), 629–638. [PubMed: \\n16712640] \\nLord C, Rutter M, DiLavore PC, Risi S, Gotham K, Bishop SL. (2012). Autism Diagnostic \\nObservation Schedule Manual, 2nd edition Torrance, CA: Western Psychological Services.\\nMaestro S, Muratori F, Cavallaro MC, Pei F, Stern D, Golse B, Palacio-Espasa F. (2002). Attentional \\nskills during the first six months of age in autism spectrum disorder. Journal of the American \\nAcademy of Child and Adolescent Psychiatry, 41(10), 1239–1245. [PubMed: 12364846] \\nMarrus N, Glowinski AL, Jacob T, Klin A, Jones W, Drain CE, Holzhauer KE, Hariprasad V , \\nFitzgerald RT, Mortensen EL, Sant SM, Cole L, Siegel SA, Zhang Y , Agrawal A, Heath AC, \\nConstantino JN. (2015). Rapid video-referenced ratings of reciprocal social behavior in toddlers: A \\ntwin study. Journal of Child Psychology and Psychiatry, 56(12), 1338–1346. [PubMed: 25677414] \\nMiller M, Iosif A, Hill M, Young GS, Schwichtenberg AJ, Ozonoff S. (2017). Response to name in \\ninfants developing autism spectrum disorder: A prospective study. Journal of Pediatrics, 183, 141–\\n146. [PubMed: 28162768] \\nMullen EM. (1995). Mullen Scales of Early Learning. Circle Pines, MN: AGS Publishing.\\nOzonoff S, Iosif AM, Baguio F, Cook IC, Hill MM, Hutman T, Rogers SJ, Rozga A, Sangha S, Sigman \\nM, Steinfeld MB, Young GS. (2010). A prospective study of the emergence of early behavioral \\nsigns of autism. Journal of the American Academy of Child and Adolescent Psychiatry, 49(3), \\n256–266. [PubMed: 20410715] \\nOzonoff S, Iosif A, Young GS, Hepburn S, Thompson M, Colombi C, Cook IC, Werner E, Goldring S, \\nBaguio F, Rogers S. (2011). Onset patterns in autism: Correspondence between home video and \\nparent report. Journal of the American Academy of Child and Adolescent Psychiatry, 50(8), 796–\\n806. [PubMed: 21784299] \\nOzonoff S, Young GS, Landa RJ, Brian J, Bryson S, Charman T. Chawarska K, Macari SL, Messinger \\nD, Stone WL, Zwaigenbaum L, Iosif AM. (2015). Diagnostic stability in young children at risk for \\nautism spectrum disorder: A baby siblings research consortium study. Journal of Child Psychology \\nand Psychiatry, 56(9), 988–998. [PubMed: 25921776] \\nPew Research Center. (2018). Demographics of mobile device ownership and adoption in the United \\nStates. http://www.pewinternet.org/fact-sheet/mobile/ . Accessed March 15, 2019.\\nPierce K, Carter C, Weinfeld M, Desmond J, Hazin R, Bjork R, Gallagher N. (2011). Detecting, \\nstudying, and treating autism early: The one-year well-baby check-up approach. Journal of \\nPediatrics, 159(3), 458–465. [PubMed: 21524759] \\nSanchez-Garcia AB, Galindo-Villardon P, Nieto-Librero AB, Martin-Rodero H, & Robins DL (2019). \\nToddler screening accuracy for autism spectrum disorder: A meta-analysis of diagnostic accuracy. \\nJournal of Autism and Developmental Disorders, 49(5), 1837–1852. [PubMed: 30617550] \\nScarpa A, Reyes NM, Patriquin MA, Lorenzi J, Hassenfeldt TA, Desai VJ Kerkering KW. (2013). The \\nmodified checklist for autism in toddlers: Reliability in a diverse rural American sample. Journal \\nof Autism and Developmental Disorders, 43(10), 2269–2279. [PubMed: 23386118] \\nSchwichtenberg AJ, Kellerman A, Miller M, Young GS, Ozonoff S. (2019). Mothers of children with \\nautism spectrum disorder: Play behaviors with infant siblings and social responsiveness. Autism \\n[epub ahead of print]. doi: 10.1177/1362361318782220.\\nWerner E, Dawson G, Osterling J, Dinno N. (2000). Recognition of autism spectrum disorder before \\none year of age: A retrospective study based on home videotapes. Journal of Autism and \\nDevelopmental Disorders, 30(2),157–162. [PubMed: 10832780] Young et al. Page 9\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author ManuscriptWetherby AM, Brosnan-Maddox S, Peace V , Newton L. (2008). Validation of the infant-toddler \\nchecklist as a broadbrand screener for autism spectrum disorders from 9 to 24 months of age. \\nAutism, 12(5), 487–511. [PubMed: 18805944] \\nZwaigenbaum L, Bryson S, Rogers T, Roberts W, Brian J, Szatmari P. (2005). Behavioral \\nmanifestations of autism in the first year of life. International Journal of Developmental \\nNeuroscience, 23(2–3), 143–152. [PubMed: 15749241] \\nZwaigenbaum L, Bauman ML, Fein D, Pierce K, Buie T, Davis PA, Newschaffer C, Robins DL, \\nWetherby A, Choueiri R, Kasari C, Stone WL, Yirmiya N, Estes A, Hansen RL, McPartland JC, \\nNatowicz MR, Carter A, Granpeesheh D, Mailloux Z, Smith-Roley S, Wagner S. (2015). Early \\nscreening of autism spectrum disorder: Recommendations for practice and research. Pediatrics, \\n136(Suppl 1), S41–S59. [PubMed: 26430169] Young et al. Page 10\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author ManuscriptKey Points\\n1. Signs of ASD are present in the first two years of life, but the average age of \\ndiagnosis lags far behind. Instruments that improve detection of autism risk in \\ninfancy are needed.\\n2. We hypothesized that employing video examples within a screening tool \\nwould improve detection of ASD in infancy.\\n3. A newly developed video-based screening tool had high sensitivity at 18 \\nmonths in concurrently identifying the toddlers diagnosed with ASD at that \\nage, as well as predicting ASD at 36 months.\\n4. Employing video examples within a screening tool may be helpful in \\nidentifying ASD in infancy. A brief, low-burden, web-based screening tool \\ncould help reduce disparities in communities with limited access to care.Young et al. Page 11\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author ManuscriptFigure 1: \\nVIRSA ratings by group from 6 to 18 months.Young et al. Page 12\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.\\nAuthor Manuscript Author Manuscript Author Manuscript Author ManuscriptAuthor Manuscript Author Manuscript Author Manuscript Author ManuscriptYoung et al. Page 13\\nTable 1:\\nSample Descriptives.\\nASD HR Non-ASD LR Non-ASD\\nSample size 21 52 37\\nSex (% male) 61.91%a40.39%a59.46%a\\nRace/Ethnicity\\n % Non-Hispanic Caucasian 36.84%a42.00%a62.16%a\\n % Non-White Race or Multiracial 21.05%a38.00%a27.03%a\\n % Hispanic 42.11%a20.00%a10.81%b\\nMaternal education\\n % Graduate degree 14.29%a48.08%b35.14%ab\\n % College degree 57.14%a40.39%a59.46%a\\n % High school or V ocational training 19.05%a9.62%a5.41%a\\nHousehold income\\n % $60k or less 19.05%a7.69%a16.22%a\\n % $61k to $100k 33.33%a25.00%a18.92%a\\n % $101k or higher 23.81%a46.15%a51.35%a\\nAge at outcome (months) 36.39 (0.72)a36.92 (1.82)a36.61 (0.84)a\\nADOS-2 SARRB* algorithm score at 36 months 14.95 (6.25)a3.08 (2.31)b2.24 (2.02)b\\nMSEL outcome fine motor age eq 26.35 (6.76)a33.86 (4.79)b37.11 (5.48)c\\nMSEL outcome visual reception age eq 29.45 (8.93)a41.58 (7.58)b42.89 (6.57)b\\nMSEL outcome expressive language age 26.25 (9.53)a38.14 (4.97)b39.54 (4.60)b\\nMSEL outcome receptive language age eq 25.60 (9.55)a35.66 (5.68)b37.51 (4.78)b\\nNote:\\nValues with different subscripts are significantly different at p<.05\\n*Social Affect + Restrictive Repetitive Behavior overall total\\nMSEL outcome = Mullen Scales of Early Learning at 36 months of age\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.Author Manuscript Author Manuscript Author Manuscript Author ManuscriptYoung et al. Page 14\\nTable 2:\\nROC analyses with 36-month ASD diagnostic classification.\\n6 months 9 months 12 months 18 months\\nTrue positives 7 10 8 14\\nFalse positives 11 37 31 31\\nTrue negatives 36 29 51 35\\nFalse negatives 6 6 8 4\\nAUC .62 .42 .59 .71\\nSpecificity (95% CI) .77 (.65 to .89) .44 (.32 to .56) .62 (.52 to .73) .53 (.41 to .65)\\nSensitivity (95% CI) .54 (.26 to .81) .63 (.39 to .86) .50 (.26 to .75) .78 (.59 to .97)\\nNegative Predictive Value (95% CI) .86 (.75 to .96) .83 (.70 to .95) .86 (.78 to .95) .90 (.80 to .99)\\nPositive Predictive Value (95% CI) .39 (.16 to .61) .21 (.10 to .33) .21 (.08 to .33) .31 (.18 to .45)\\nThreshold 5.25 8.25 6.75 8.25\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.Author Manuscript Author Manuscript Author Manuscript Author ManuscriptYoung et al. Page 15\\nTable 3:\\nROC analyses for 18-month VIRSA with concurrent 18-month diagnosis.\\nVIRSA (18 months)\\nTrue positives 8\\nFalse positives 34\\nTrue negatives 38\\nFalse negatives 0\\nAUC .78\\nSpecificity .53\\nSensitivity 1.00\\nNegative Predictive Value 1.00\\nPositive Predictive Value .19\\nThreshold 7.75\\nJ Child Psychol Psychiatry . Author manuscript; available in PMC 2021 January 01.', metadata={'source': 'papers/Young_Behavior_clean.txt'})],\n",
       " [Document(page_content='Research and development of\\nautism diagnosis information\\nsystem based on deep convolution\\nneural network and facial\\nexpression data\\nWang Zhao and Long Lu\\nSchool of Information Management, Wuhan University, Wuhan, China\\nAbstract\\nPurpose –Facial expression provides abundant information for social interaction, and the analysis and\\nutilization of facial expression data are playing a huge driving role in all areas of society. Facial expression data\\ncan reflect people ’s mental state. In health care, the analysis and processing of facial expression data can\\npromote the improvement of people ’s health. This paper introduces several important public facial expression\\ndatabases and describes the process of facial expression recognition. The standard facial expression database\\nFER2013 and CK þwere used as the main training samples. At the same time, the facial expression image data\\nof 16 Chinese children were collected as supplementary samples. With the help of VGG19 and Resnet18\\nalgorithm models of deep convolution neural network, this paper studies and develops an information system\\nfor the diagnosis of autism by facial expression data.\\nDesign/methodology/approach –The facial expression data of the training samples are based on the\\nstandard expression database FER2013 and CK þ. FER2013 and CK þdatabases are a common facial\\nexpression data set, which is suitable for the research of facial expression recognition. On the basis of FER2013and CK þfacial expression database, this paper uses the machine learning model support vector machine\\n(SVM) and deep convolution neural network model CNN, VGG19 and Resnet18 to complete the facial\\nexpression recognition.\\nFindings –In this study, ten normal children and ten autistic patients were recruited to test the accuracy of the\\ninformation system and the diagnostic effect of autism. After testing, the accuracy rate of facial expression\\nrecognition is 81.4 percent. This information system can easily identify autistic children. The feasibility ofrecognizing autism through facial expression is verified.\\nResearch limitations/implications –The CK þfacial expression database contains some adult facial\\nexpression images. In order to improve the accuracy of facial expression recognition for children, more facial\\nexpression data of children will be collected as training samples. Therefore, the recognition rate of the\\ninformation system will be further improved.\\nOriginality/value –This research uses facial expression data and the latest artificial intelligence technology,\\nwhich is advanced in technology. The diagnostic accuracy of autism is higher than that of traditional systems,so this study is innovative. Research topics come from the actual needs of doctors, and the contents and\\nmethods of research have been discussed with doctors many times. The system can diagnose autism as early as\\npossible, promote the early treatment and rehabilitation of patients, and then reduce the economic and mental\\nburden of patients. Therefore, this information system has good social benefits and application value.\\nKeywords Facial expression data, FER2013, CK þ, Deep convolution neural network, VGG19, Resnet18,\\nAutism, Diagnostic information system\\nPaper type Research paper\\n1. Introduction\\nFacial expression recognition is an important social cognitive skill. Emotions are expressed\\nby facial expressions. Therefore, recognition and understanding of facial expressions is theFacial\\nexpressions for\\nautism\\ndiagnosis\\nThis research has been possible thanks to the support of projects: National Natural Science Foundation\\nof China (No. 61772375) and Independent Research Project of School of Information ManagementWuhan University (No: 413100032).The current issue and full text archive of this journal is available on Emerald Insight at:\\nhttps://www.emerald.com/insight/0737-8831.htm\\nReceived 31 August 2019\\nRevised 16 December 2019\\nAccepted 23 January 2020\\nLibrary Hi Tech\\n© Emerald Publishing Limited\\n0737-8831\\nDOI 10.1108/LHT-08-2019-0176basis of communication and interpersonal relationships with others. Abnormal expression is\\na prominent manifestation of autism, and it is also one of the criteria for the diagnosis of\\nautism. Doctors can diagnose autism by responding to abnormal facial expressions in\\nchildren.\\nAutism, also known as autism or autism disorders, is a representative disease of\\ngeneralized developmental disorders. In recent years, the incidence of autism in children has\\nbecome higher and higher, experiencing a transition from rare diseases to epidemics. At\\npresent, research on autism is still in its infancy at home and abroad, and research methods\\nand tools are still developing.\\nThe main symptoms of autism include impaired social and interpersonal communication,\\nlanguage retardation, repetitive behavior and sensory dysfunction. It is difficult for autistic\\npatients to correctly recognize faces and explain facial emotions. They have different\\nemotional expressions from ordinary people, and they cannot correctly perceive and\\nunderstand some basic expressions such as anger ( Yan, 2008 ).\\nAt present, the diagnostic methods for autism spectrum disorders include: traditional\\nstandard DSM-IV-TR ( Segal, 2010 ) and ICD-10 ( Organization W H, 1992 ), various autism\\ndiagnostic assessment scales such as “Childhood Autism Rating Scale (CARS) ”,“the autism\\nchild behavior scale (ABC) ”and autism behavior rating scale and questionnaire interviews\\n(Wang and Lu, 2015 ). Most of these methods rely on doctors ’direct observation of the\\npatient ’s expression, speech and behavior based on their experience. Diagnostic results are\\neasily disturbed by external factors such as hospital level, physician ’s subjective level,\\npatient ’s education level, age and so on. There are relatively large subjective factors, resulting\\nin a certain degree of missed diagnosis and misdiagnosis. It takes about 1 –2 h for each autistic\\npatient to diagnose, so doctors have a lot of work to do. The best period of treatment for\\nautistic patients is before the age of six. Early diagnosis is of great significance for the\\nrehabilitation of autistic patients.\\nThe purpose of our research and design is to train the model and make a facial expression\\nrecognition system based on the normal expression, so as to verify the abnormal expression.\\nThis system can test the facial expression of autistic children and judge the difference\\nbetween autistic children and normal children.\\nIn this study, FER2013 and CK þwere used as the main facial expression training\\nsamples. At the same time, we collected the facial expression image data of 16 Chinese\\nchildren as a supplementary sample of facial expression. With the help of VGG19 andResnet18 algorithm models of deep convolution neural network, according to the hospital\\nautism diagnosis scale and diagnosis process, this paper studies and designs an information\\nsystem for the diagnosis of autism by facial expression data. After the actual test of recruiting\\ntesters, the recognition rate of the system is 81.4 percent. It can effectively distinguish\\nwhether the expression of children is normal or not. It provides a practical information system\\nfor the diagnosis of autism. This paper will continue to collect more children ’s facial\\nexpression data from different countries and regions as training samples to further improve\\nthe recognition rate of facial expressions.\\nThe autism diagnosis information system designed in this study has the following\\nimportant significance:\\n(1) Autism can be diagnosed as early as possible by using this system. The best time to\\ntreat autism is before the age of six. The earlier the diagnosis of autism is made, the\\nless the treatment cost and the higher the probability of recovery. Early diagnosis is\\nof great value in alleviating the burden on families and society of autistic patients.\\nThe system can be published in the form of app or web pages and disseminated\\nthrough the Internet. The system can be installed and used on different devices, such\\nas computers, mobile phones, tablets, etc. It has good applicability. Through thisLHTsystem, autism can be diagnosed conveniently, and time can be saved for the early\\ntreatment of autism patients, especially those in underdeveloped areas.\\n(2) It can make the diagnosis of autism more objective. The whole diagnosis process is\\ncompleted by the system. Because artificial intelligence technology is used to\\nrecognize facial expressions without human intervention, the diagnosis results are\\nobjective and accurate.\\n(3) Reduce the intensity of doctors ’work. Before the system was used, it took an hour for\\ndoctors to diagnose an autistic patient. By using this system, doctors can save a lot of\\ntime and pay attention to the treatment of autism.\\n(4) The facial expression database used in the training of this system contains different\\nraces in the world. Therefore, this system can not only diagnose children in different\\ncountries and regions but also diagnose suspected autism patients all over the world.\\n(5) This research designs the system according to the actual business. The early design\\nof the system adopts the suggestions of several doctors, so it is designed and\\nmanufactured according to the actual needs of doctors. Although there are some\\npapers on autism diagnosis by facial expressions at home and abroad, there are still\\nfew autism diagnosis systems developed which can be used in practice.\\n(6) This paper uses the latest in-depth learning technology to improve the accuracy of\\nfacial expression recognition. Previous traditional techniques and methods have low\\nrecognition rate of facial expressions. In recent years, with the development of\\nartificial intelligence technology and the improvement of computing speed, the\\nconvolutional neural network has greatly improved the accuracy of facial expression\\nrecognition, which is the innovation of this research in technology.\\n2. Facial expression database and its recognition technology\\n2.1 Facial expression database\\nFacial expression is an important way for peopl e to express their emotions. In the social\\nprocess, facial expression is an important way to judge the attitude and inner feelings of\\nthe other party ( Lanlan, 2018 ).Mehrabian (2008) found that in a conversation, the change\\nof facial expression played the most important role. Of these, 55 percent are facial\\nexpressions, 38 percent are voic e and only 7 percent are words ( Mei and Hu, 2015 ).\\nCompared with voice, expression can convey more abundant information. Recognition\\nand understanding of facial expressions i s very important for communicating with\\nothers ( Shen et al. , 2013 ). In 1972, Ekman demonstrated through empirical research that\\nhuman beings have six basic facial expression s: happiness, sadness, anger, fear, disgust\\nand surprise ( Ekman, 1992 ). In subsequent studies, neutral expression has also been\\nadded to the basic expression, and it is gen erally believed that there are seven basic\\nexpressions in facial expression.\\nWith the continuous development of computer software and hardware technology, people\\nhave a deeper understanding of facial expression recognition technology. In order to better\\nstudy facial expression recognition technology, many international research institutions\\nhave established standard facial expression databases, the main facial expression databases\\nare as follows:\\n(1) JAFFE\\nThe database stores facial expression data of Japanese women. It contains 213 facial images\\nof ten Japanese women. There are seven types of facial expressions, namely neutral, happy,Facial\\nexpressions for\\nautism\\ndiagnosissad, surprise, anger, disgust and fear. The resolution of each image is 256 3256 pixels.\\nEveryone has seven kinds of pictures of facial expressions.\\n(2) CKþ\\nThe expression database was collected under laboratory conditions. It includes African\\nAmericans, Asians and South Americans. The resolution of each image is 640*480 pixels. It\\ncontains 593 expression sequences of 123 people, 69 percent of whom are female and 31\\npercent are male. Each sequence begins and ends with neutral expression, which includes the\\nprocess from calm to strong expression. CK þis a facial expression data set with many\\napplications. The reliability of various facial expression evaluation experiments using this\\ndatabase is very high. It includes seven types of facial expressions: anger, contempt, disgust,\\nfear, happy, sadness and surprise.\\n(3) FER2013\\nThere are 35,887 facial images in the library, and there are seven facial expression types:\\nangry, disgust, fear, happy, sad, surprise and neutral. The resolution of each image is 48*48\\npixels. All the images are gray images. There are three sample sets: 28,709 images in the\\ntraining set; 3,589 images in the validation set and 3,589 images in the test set.\\n(4) MMI\\nThe expression database can be divided into two parts: one is a dynamic data set composed of\\nmore than 2,900 video sequences. The other part is a static data set consisting of a large\\nnumber of high resolution images. There are seven types of expression in the library.\\n(5) AFEW\\nAll the facial images in the database are edited from the movies and contain seven basic facial\\nexpressions.\\n(6) SFEW\\nThe expression library is a static frame image extracted from the AFEW data set, which\\ncontains seven basic expressions.\\n2.2 Facial expression recognition process\\nThe process of facial expression recognition includes two stages as shown in Figure 1 : One is\\nthe training stage and the other is the recognition stage. The training and recognition stages\\ncan be divided into three parts: the pretreatment of facial expression images, the extraction of\\nfacial expression features and the classification of facial expressions. The training stage is to\\ntrain the model in order to achieve the purpose that the model can be used. The recognition\\nstage is to recognize and classify the expression of the test image ( Du, 2018 ).\\nThe two stages of expression recognition process include the following processes: First,\\nface detection is carried out on the image in the expression database, including the location,\\nFigure 1.\\nFacial expression\\nrecognition processLHTalignment and clipping of the face area. This is the basis of the follow-up process. Only when\\nthe expression area is accurately obtained, the following series of work will be more accurate.\\nAfter the face area is detected, the image needs to be preprocessed in order to eliminate the\\nnoise caused by the influence of acquisition equipment and environment and avoid the\\ninterference of feature extraction. Then it is the feature extraction step, which aims to extract\\nthe features that can represent the essence of expression from the preprocessed facial images.\\nIn this process, in order to avoid the high dimension of feature extraction and affect the\\nefficiency of the algorithm, we need to reduce the dimension of extracted features in order to\\nextract the most representative expression features. Finally, the extracted facial features are\\nclassified to determine which type of facial expression is.\\n2.3 Facial expression recognition technology\\nFacial expression recognition technology mainly includes traditional machine learning\\ntechnology and deep learning technology. The two technologies have similarities and\\ndifferent characteristics.\\n(1) Traditional machine learning technology\\nFacial expression recognition algorithm based on traditional machine learning includes three\\nsteps: image preprocessing, facial expression feature extraction and feature classification.\\nFirst, for the convenience of feature extraction, it is necessary to preprocess the image,\\nwhich can effectively avoid the interference of various noises and leave the key information\\nneeded by the face. The pretreatment process includes image gray processing, face\\nalignment, face size tailoring, data enhancement, brightness, pose normalization, etc. ( Li and\\nDeng, 2018 ).\\nSecond, the traditional feature extraction methods include directional gradient histogram\\nfeature, Gabor filter feature, local directional pattern feature and enhanced local binary\\nalgorithm. Because these methods are artificial design, time-consuming and laborious, and\\nhave certain limitations and often have better effect in feature extraction in small sample\\nimage set, most of the current studies are based on deep learning feature extraction method.\\nThere are many basic machine learning methods for expression classification, such as\\nsupport vector machine (SVM), hidden Markov model (HMM) and k-nearest classification\\nalgorithm.\\n(2) Deep learning technology\\nFacial expression recognition algorithm based on deep learning also needs image\\npreprocessing. The difference is that it often combines feature extraction and feature\\nclassification into an end-to-end model, which greatly simplifies the process of facial\\nexpression recognition. In addition to end-to-end learning, deep learning algorithm can be\\nused to extract facial expression features, and then other independent classifiers can be used.\\nFor example, SVM or random forest algorithm is used to process the extracted features and\\nclassify them.\\nIn this paper, we construct a facial expression recognition model based on deep learning\\ntechnology, extract facial expression feature data of children and classify them into groups,\\nso as to diagnose autism.\\n2.4 Driving role of facial expression data\\nResearch on facial expression recognition has been applied in a series of life scenarios. In\\nchildren ’s education, advanced human-computer interaction, medical diagnosis and other\\naspects have played an important role ( Cai, 2018 ).\\nIn distance education or classroom teaching, teachers can better improve students ’\\nlearning quality by observing students ’emotional changes in the classroom and adjustingFacial\\nexpressions for\\nautism\\ndiagnosisteaching plans in time. Advanced human-computer interaction can make human-computer\\ninteraction more harmonious. For example, intelligent robots can automatically respond to\\nthe facial expressions of their interlocutors. In medical diagnosis, facial expressions also play\\nan important role in the prevention and diagnosis of diseases. For example, this article is to\\ndiagnose autism by analyzing children ’s facial expressions.\\n3. Autism and facial expression diagnosis\\n3.1 Autism and its development\\nAutism is a neurodevelopmental disorder, which is collectively referred to as autism\\nspectrum disorder ( Duan et al., 2015 ).\\nSince Kanner, an American child psychiatrist, first reported autism in 1943, the incidence\\nof autism has risen rapidly worldwide. In the 1980s, about 3 –5 out of every 10,000 people\\nsuffered from the disease, while in 2000, 6.7 out of every 1,000 children suffered from the\\ndisease ( Vismara and Rogers, 2008 ). According to the National Center for Health Statistics,\\nthe probability of autism among children aged 3 –14 in the United States reached 2.76 percent\\nin 2016 ( Zablotsky et al., 2017 ).\\nThere is no statistical survey on autistic children in China. However, according to the data\\nof the report on the development of China ’s autism education and rehabilitation industry II,\\nthe number of people with autism in China is estimated to exceed 10 million, of which 2 million\\nare autistic children. At the same time, it is growing at the rate of nearly 200,000 annually\\n(Beijing Wucai Deer Autism Research Institute, 2017 ).\\nAutism brings serious financial burden to both society and family. Families with autistic\\nchildren, on the one hand, spend a lot of time caring for their children, while working hours are\\nreduced so that work income is reduced. On the other hand, the cost of family rehabilitation\\ntreatment for autistic children is huge, which increases the family ’s financial burden ( Wu and\\nChen, 2018 ). According to the survey on the occupational and economic burden of preschool\\nautistic children ’s families, 33 percent of parents of autistic children reported that their\\ncaregiving problems seriously affected their careers, and their annual income was\\nsignificantly lower than that of ordinary families, with an average loss of income of 30,957\\nyuan per year. Meanwhile, the average annual cost of autistic children ’s families for children ’s\\neducation and training is significantly higher than that of ordinary families ( Yang and Wang,\\n2014). The society and the government also need to invest a lot of money in the rehabilitation\\neducation of autistic children. At the same time, autism also brings high subjective load and\\ndepression to the families of patients, which has a negative impact on their quality of life\\n(Singh et al., 2017 ;Wang et al., 2018 ). It can be seen that the incidence of autism in children is\\nrelatively serious, and the harm to society and family is enormous.\\n3.2 Diagnosis of autism through facial expressions\\n3.2.1 Facial expression recognition disorder. Autistic children have facial expression\\nrecognition obstacles, which are mainly manifested in their inability to recognize facial\\nexpressions ( Liuet al., 2015 ). It is easy to distinguish autistic children from normal children by\\nobserving their facial expressions. Therefore, we combine facial expression recognition\\ntechnology to extract facial expression response feature vectors and use artificial intelligence\\ntechnology to distinguish normal group and autistic group based on these facial features.\\n3.2.2 The principle of diagnosing autism through facial expressions. A large number of\\nstudies have pointed out that autistic patients have deficiencies in facial expression\\nrecognition and understanding. This is the core source of impaired social function in\\nautistic patients ( Yang et al. , 2017 ). Autistic children are more difficult to identify other\\npeople ’s emotional behavior, and it is difficult to make appropriate judgment and responseLHT(Shen et al. , 2013 ). Overseas research on facial expression recognition ability of autistic\\npatients has been carried out not only in children but also in adults. Most studies believe that\\nthe ability of facial expression recognition of autistic patients is low. Baron-Cohen et al. (1997)\\nused standard facial expression maps to study the recognition of different emotional types in\\nautistic adults. It was found that autistic adults had better recognition of some basic facial\\nexpressions, such as happiness, but relatively complex facial expressions such as surprise\\nrecognition were difficult to recognize.\\nAt present, the main diagnostic criteria of autism are: IDC-10, DSM-IV, the autism child\\nbehavior scale (ABC), the children autism rating scale (CARS) and the Clancy behavior scale\\n(CABS) ( Wang, 2007 ).\\nAfter consulting a large number of literatures and investigating the actual situation of the\\nhospital, now the hospital mainly uses CABS (filled by parents), ABC (filled by parents) and\\nCARS (filled by doctors) to diagnose autism. After a detailed review of the test items of the\\nthree scales, these scales all contain the test items to judge autism through children ’s facial\\nexpressions. There were 14 items in the CABS scale, of which the seventh item was\\ninexplicable laughter and the tenth item was not looking at each other ’s face. Avoiding eye\\ncontact was related to expression. There were 57 items in the ABC scale, of which the seventh\\nitem was non-communicative smile, the seventeenth item did not respond to other people ’s\\nfacial expressions, and the twenty-fourth item was active avoidance of eye contact with\\nothers. Fifteen items of the CARS scale, the third of which is emotional response, pleasure and\\nunhappiness and interest, are expressed by changes in facial expression and posture. These\\nscales basically include the items of autism detection by children ’s facial expressions, which\\nshow that the diagnosis of autism can be more accurate by facial expressions. With the\\nprogress of artificial intelligence technology, facial expression recognition technology can\\nobjectively and effectively reflect the mental health of children and can be used in early\\ndiagnosis of autism ( Yanbin et al., 2018 ).\\nWe also communicated with doctors of Hubei Maternal and Child Health Hospital, Wuhan\\nChildren ’s Hospital and Guangzhou Women and Children ’s Medical Center many times, and\\nactually checked the process of using the above autism diagnostic scale to diagnose children.\\nThe doctor observes the tester ’s reaction to determine whether the tester is autistic after\\nrequesting the tester to make the corresponding expression. Doctors point out that facial\\nexpression is an important part of autism diagnosis. In terms of system design, they put\\nforward requirements and suggestions for the process of diagnosing autism through facialexpression.\\n4. Research and development of autism diagnosis information system\\n4.1 Facial expression database selection\\nThe expression databases in this study mainly come from two public expression databases\\nCKþand FER2013. In addition, 16 Chinese children ’s expression data were collected as\\nsupplementary samples. The two public expression databases are standard and international\\nand have been widely used, including facial expression data of adults and children. Each\\nsample in the database contains seven expressions: angry, disgust, fear, happy, sad, surprise\\nand neutral. Because children ’s facial expressions are different from adults, in order to\\nimprove the recognition rate of children ’s facial expressions, we collected facial expression\\ndata of 16 children aged 5 to 8 in China. Seven expressions were collected from each child. We\\ncombine Chinese children ’s facial expression data and public expression database as our\\nsystem ’s facial expression database.\\n4.1.1 FER2013 facial expression database. The reason for choosing FER2013 expression\\ndatabase is that it has more samples and is more mature than other expression databases. It\\nhas advantages in model training. At the same time, it has been used in many studies (see\\nPlate 1 ).Facial\\nexpressions for\\nautism\\ndiagnosis4.1.2 CK þFacial expression database. CKþfacial expression database was selected\\nbecause it was collected in the laboratory, so its accuracy is relatively high ( Lucey et al., 2010 )\\n(seePlate 2 ).\\n4.1.3 Facial expression data of Chinese children. At present, the mature facial expression\\ndatabases at home and abroad are mainly based on adult male or female facial expression\\nimages. Therefore, it is urgent to establish a facial expression database for children.\\nFacial images of children are quite diffe rent from those of adults. Children have\\nrounder faces, larger eyes and less prominent bones. Because of these differences,\\nchildren ’s facial features are less obvious and more difficult to recognize than adults.\\nBecause of the particularity of children, it is very difficult to collect children ’sf a c i a l\\nimages. In order to improve the recognition rate of children ’s facial expressions, we\\ncooperated with Amy Education School in Zhengzhou. Sixteen healthy children as\\nvolunteers were recruited to collect facial expression data. Each of them collected seven\\nkinds of expressions, totaling 112 pictures. These children are between 5 and 8 years old,\\nincluding 8 boys and 8 girls. The acquisition environment is quiet and there is no\\nexternal interference. High-d efinition cameras are used t o collect facial expression\\nimages, which are processed professionally. Bef ore collecting facial expression data,\\nparents have been informed of the purpose o f collecting facial expression data. After\\nquestioning with parents, all the children who participated in the collection of facial\\nexpression data had no history of autism.\\nWe loaded the expression data into the training sample library. The purpose of collecting\\nChinese children ’s facial expression data is to increase the number of Chinese children ’s facial\\nexpression samples in training samples and improve the recognition rate of the system for\\nchildren ’s facial expression. The collection process and the collected children ’s facial\\nexpression data are shown in Plate 3 .\\n4.2 Network topology\\nAccording to the network environment and equipment of the information service platform,\\nthe network topology can be divided into four levels. The network topology diagram is shown\\ninFigure 2 .\\nThe first layer is the application layer, which consists of users, computers and various\\nsmart devices. Smart devices include smart tablet computer, smartphones and other\\nelectronic devices. Users access and use the information service platform through computers\\nand various smart devices.\\nPlate 1.\\nFER2013 facialexpression database\\nPlate 2.CKþFacial expression\\ndatabaseLHTThe second layer is the communication layer, mainly based on the internet network\\nenvironment, providing access channels for users and systems.\\nThe third layer is the application server layer, which is composed of firewall and\\napplication server and has an ontology display system for autism. The application server\\nmanages various business functions, handles various business requests submitted by users\\nand can access the database server for various data exchange.\\nThe fourth layer is the database server layer, which stores all kinds of data and knowledge\\nresources of the information service platform.\\n4.3 System architecture\\nThe smart diagnosis system of autism adopts client/server architecture. The client includes\\ndifferent versions of programs suitable for computers and smartphones. The system\\narchitecture diagram is shown in Figure 3 .\\nThe client includes three main modules: user interaction, image acquisition and face\\ndetection. User interaction module is responsible for human-computer interaction. According\\nto the requirements of the autism diagnostic scale, users who diagnose are required to make\\nappropriate expressions and feedback by prompting pictures and voice guidance. Through\\nthe camera, the image acquisition module can dynamically capture facial expression images.\\nAt the right time, the system will collect facial expression images and transmit them to the\\nface detection module. Face detection module recognizes the valid face features and\\ncompresses the image and transfers it to the server through the internet or mobile Internet.\\nThe server includes six main modules: image processing, feature extraction, group\\nclassification, automatic diagnosis, training model and data management. The image\\nprocessing module can receive the expression image transmitted by the client and then\\nprocess the expression image and transfer it to other modules on the server side. The feature\\nextraction module receives the facial expression images provided by the image processing\\nmodule and extracts the facial expression features. The group classification module is\\nFigure 2.\\nNetwork topology\\ndiagramPlate 3.\\nCollection of facial\\nexpression data of\\nChinese childrenFacial\\nexpressions for\\nautism\\ndiagnosisresponsible for group classification and correctly classifies the expression images into the\\nmost matching expressions among the seven kinds of expressions. The automatic diagnosis\\nmodule gives the diagnosis of autism by comparing the facial expressions that the tester is\\nrequired to imitate and the facial expressions that the tester actually makes. Model training\\nmodule is the core module of the system, which is responsible for recognizing and processing\\nthe newly collected facial expression images. The data management module mainly manages\\nfacial expression data, including storing and reading facial expression images transmitted by\\nthe client.\\nThe system server stores facial expression feature files, which are formed by feature\\nextraction of facial expression database. The expression feature file is HDF5 file format. The\\nexpression recognition system running on the server can read the expression feature file at\\nany time. If new facial expression samples are collected, the model can be retrained and the\\nfacial expression feature files can be updated.\\nThe client collects the tester ’s facial expression data by high-definition camera and\\ntransmits the facial expression data to the server by JSON file according to TCP communication\\nprotocol. The facial expression recognition system running on the server processes the collected\\nfacial expression data and then feeds the recognition results back to the tester through the\\nnetwork and stores the recognition results and facial expression data in the server database.\\nFacial data and diagnostic system are stored on a server, and the recognition results and facial\\ndata are stored in the SQL Server database. The diagnostic system reads data from the\\ndatabase through SQL structured query language. The response time of the whole database\\noperation and communication process should not exceed 5 s.\\n4.4 System architecture\\n4.4.1 VGG19 model. Researchers from the Oxford University and the Google Brain have\\njointly developed the convolutional neural network VGG. VGNet consists of 11, 13, 16 and 19\\nlayers of neural networks .VGNet constructs 16 –19 layers of neural networks by stacking\\nsmall convolution cores of 3 33 and maximum pooling layers of 2 32 repeatedly. VGGNet\\nFigure 3.\\nSystem architectureLHThas strong scalability and greatly reduces the error rate when extending. When migrating to\\nother image data, it has good generalization ability and simple structure.\\n4.4.2 ResNet18 model. ResNet was proposed by Kaiming He and others of Microsoft\\nResearch institute. They have successfully trained 152 layers of neural networks by using\\nResNet unit. The structure of ResNet can accelerate the training of the neural network, and\\nthe accuracy of the model has been greatly improved.\\n4.4.3 Graphic of deep learning framework. The deep learning framework used in this paper\\nfor facial expression recognition is shown in Plate 4 .\\nThe whole process includes image input, image preprocessing, model building, model\\ntraining, model testing and output of expression recognition results. There are two kinds of\\ndeep learning algorithms used in this paper: VGG19 and ResNet18. ResNet18 solves the\\nproblem of network performance degradation caused by the high depth of VGG19. By\\ntraining the two models and synthesizing the two convolutional neural network models, the\\nfacial expression features of autistic children can be extracted accurately.\\n4.4.4 Image preprocessing. The purpose of image preprocessing is to achieve uniform\\nnormalization of the final input image. The process is shown in Figure 4 .\\nConverting an image to a grayscale image can reduce the computational complexity of the\\nlatter pixel level and also reflect the overall and local distribution and characteristics of the\\nimage. Then, image transformation is used to enhance data by zooming, rotating, cutting and\\ntranslating, and the image is located in the center of the window. The contrast and brightness\\nof the image can be improved by histogram equalization to reduce the influence of\\nillumination on expression feature learning. In order to make the image uniform, it is planned\\nto transform the image size into the same size by normalizing the image size. Finally, the\\nmask is used to remove the occlusion of non-face areas.\\n4.4.5 Model training. Before model training, we need to enhance the image data. We choose\\nSGD random gradient descent algorithm as the optimization method. The batch size is still\\n128 by default, and the learning rate is set to 0.01 initially. In addition, the initialization of\\nPlate 4.\\nFramework of deep\\nlearning\\nFigure 4.\\nImage preprocessingFacial\\nexpressions for\\nautism\\ndiagnosisPlate 5.\\nFacial expression\\nrecognition resultsLHTnetwork parameters is also very important. We have adopted a random initialization method\\nto train the two network algorithms. The core code of Python is as follows:\\n# Model training\\ndeftrain (epoch ):\\nifepoch >learning_rate_decay_start andlearning_rate_decay_start >=0:\\nfrac =(epoch -learning_rate_decay_start )/ /learning_rate_decay_every\\ndecay_factor =learning_rate_decay_rate **frac\\ncurrent_lr =opt.lr *decay_factor\\nutils .set_lr (optimizer ,current_lr )# set the decayed rate\\nelse :\\ncurrent_lr =opt.lr\\nforbatch_idx ,(inputs ,targets )inenumerate (trainloader ):\\nifuse_cuda :\\ninputs ,targets =inputs .cuda (),targets .cuda ()\\noptimizer .zero_grad ()\\nutils .clip_gradient (optimizer ,0.1)\\noptimizer .step ()\\ncorrect +=predicted .eq(targets .data ).cpu().sum ()\\n4.4.6 Recognition results. Through the trained model, we use some children ’s facial\\nexpressions pictures and videos to test, and get the probability of various expressions and the\\nfinal prediction results of the model. As shown in Plate 5 , the histogram shows the probability\\nof each type of facial expression, and the histogram of maximum probability is the final\\nrecognized facial expression. After testing, the recognition rate of children ’s facial expression\\nreaches 81.4 percent, which can effectively distinguish whether children ’s facial expression is\\nnormal or not.\\n5. System validation\\n5.1 Testing environment\\nIn this study, two kinds of mobile phones, personal computers and servers are selected as test\\nenvironments. The hardware and software environments are shown in Table I .\\n5.2 Diagnostic procedure and interface of diagnostic system\\nThe diagnostic process is shown in Figure 5 . First, the system randomly displays one of the\\nseven kinds of facial expressions for the tester to imitate. The system will prompt the tester\\nTesting equipment Hardware environment Software environment\\nOPPO R17 mobile phone CPU:SDM670 RAM:8GB Android\\nIPhone 8 mobile phone CPU:A11 RAM:2GB iOSPersonal computer CPU:Intel i7 RAM:16GB Windows 10Server CPU:Intel W2133 RAM:16GB Windows Server 2019\\nFigure 5.\\nAutomatic diagnostic\\nprocedureTable I.\\nTesting environmentFacial\\nexpressions for\\nautism\\ndiagnosisto imitate the facial expression by pictures a nd sounds. For example, the system displays\\nhappy cartoon smiling faces, plays happy children ’s songs and induces children to make\\nhappy expressions. The system displays t he same expression example three times and\\ncollects the tester ’se x p r e s s i o nd a t aa tt h es a m et i m e .T h e nt h es y s t e mc o m p a r e st h e\\nexpression examples and the actual collect ed expression data and gives the diagnosis\\nresults.\\n5.2.1 Diagnostic procedure. The diagnostic process is shown in Figure 5 . First, the system\\nrandomly displays one of the seven kinds of facial expressions for the tester to imitate. The\\nsystem will prompt the tester to imitate the facial expression by pictures and sounds. For\\nexample, the system displays happy cartoon smiling faces, plays happy children ’s songs and\\ninduces children to make happy expressions. The system displays the same expression\\nexample three times and collects the tester ’s expression data at the same time. Then the\\nsystem compares the expression examples and the actual collected expression data and gives\\nthe diagnosis results.\\n5.2.2 Interface of diagnostic system. The system diagnostic interface is designed according\\nto the diagnostic process (see Plate 6 ).\\n5.3 System testing\\n5.3.1 Test sample. We recruited ten normal children and ten autistic children and divided\\nthem into normal children group and autistic children group for comparative verification.\\nThe accuracy of the system is verified by the actual test of the autism diagnosis information\\nsystem.\\nThe normal group of children was provided by Amy Education School in Zhengzhou,\\nwhich cooperated with us. Ten healthy children as volunteers were recruited as the normal\\nPlate 6.\\n(a) The main interface\\nof the autism smart\\ndiagnosis informationsystem, including\\nsystem introduction,\\nknowledgeintroduction of autism\\nand other functions. (b)\\nThe facial expressionthat the system\\nprompts the tester to\\nsimulate after starting\\nthe diagnostic process.\\n(c) The expressionanalysis after\\ndiagnosis. (d) The\\nresult given by thesystem after three\\ndiagnosesLHTgroup for testing. These children were between 5 and 8 years old, including 5 boys and 5 girls.\\nParents were informed of the purpose and content of the experiment before the experiment.\\nChildren who participated in the experiment had no history of autism after being asked by\\ntheir parents.\\nThe autistic children were provided by Guangzhou Children ’s Care Center, which\\ncooperated with us. Ten volunteers of autistic children were recruited as the autistic children\\ngroup for testing. These children were aged between 3 and 6 years old, including 5 boys and 5\\ngirls. Parents were informed of the purpose and content of the experiment before the\\nexperiment. The selected children with autism were diagnosed by a professional physician.\\n5.3.2 Test environment and process. All the tests were conducted in quiet classrooms\\nwithout noise and external factors. Through our autism diagnosis information system, each\\nchild was prompted by pictures and sounds to imitate seven kinds of facial expressions and\\nprompted to make corresponding facial responses according to the facial expressions on the\\npictures. At this time, the camera will capture their facial expressions, and after system\\nanalysis, they will be saved in the form of pictures in the computer of the test system (see\\nPlate 7 ).\\n5.3.3 Test result. We used the system to test the normal combination and autistic children\\nrespectively. Finally, we compared the recognition rate of the two groups.\\nFrom Table II , the average recognition rate of each expression is angry 80 percent, disgust\\n70 percent, fear 80 percent, happy 100 percent, sad 80 percent, surprise 70 percent and neutral\\n90 percent.\\nTest child 1 only had a disgusting expression recognition error, and other facial\\nexpression recognition was correct, then the average recognition rate of the seven facial\\nFacial expressionNumber of test children\\nCorrect identification (Y 5Yes, No 5N)\\nAverage recognition rate % 1234567891 0\\nAngry Y Y Y N Y Y Y Y Y N 80\\nDisgust N Y Y Y N Y N Y Y Y 70Fear Y Y Y N Y Y Y Y N Y 80Happy Y Y Y Y Y Y Y Y Y Y 100S a d YYNYYYYYYN 8 0\\nSurprise Y N Y N Y Y N Y Y Y 70\\nNeutral Y Y Y Y Y Y Y Y Y N 90Plate 7.\\n(a) Test environment\\nfor normal children\\ngroup. (b) Test\\nenvironment for\\nautistic children group\\nTable II.\\nTest results in normal\\nchildren groupFacial\\nexpressions for\\nautism\\ndiagnosisexpressions of test child 1 was 85.7 percent. According to this method, the average\\nrecognition rates of the seven expressions from test child 1 to test child 10 were 85.7 percent,\\n85.7 percent, 85.7 percent, 57.1 percent, 85.7 percent, 100 percent, 71.4 percent, 100 percent,\\n85.7 percent and 57.1 percent, respectively. The average recognition rate is 81.4 percent.\\nJudging by the 60 percent threshold, there are two test children ’s facial expression\\nrecognition rates at 57.1 percent. This shows that in real environment, the algorithm of the\\nsystem is affected by the environment and light, and the accuracy will be affected to a certain\\nextent. However, according to the accuracy of 81.4 percent, it can basically meet the\\npreliminary diagnostic requirements of whether the expression is abnormal or not. In the\\nfuture, more real samples will be added to further improve the accuracy of the system\\nalgorithm.\\nThe experimental results show that the errors mainly concentrate on the expressions of\\ndisgust and surprise. The main reasons are as follows:\\n(1) Disgust and surprise have only minor local changes in the faces of the two kinds of\\nexpressions, and there is no significant distinguishing feature.\\n(2) Some of the participants had little change in the two facial expressions, did not have\\nthe obvious features of the corresponding categories, approached neutral expressions\\nand were easy to confuse.\\nFrom Table III , the average recognition rate of each expression is angry 50 percent, disgust 10\\npercent, fear 30 percent, happy 60 percent, sad 60 percent, surprise 20 percent and neutral 10\\npercent.\\nThe average recognition rates of the seven expressions from test child 1 to test child 10\\nwere 28.5 percent, 28.5 percent, 28.5 percent, 42.9 percent, 42.9 percent, 57.1 percent, 28.5\\npercent, 42.9 percent, 28.5 percent and 14.3 percent, and the average recognition rate is 34.3\\npercent.\\nFacial expressionNumber of test children\\nCorrect identification (Y 5Yes, No 5N)\\nAverage recognition rate % 1234567891 0\\nAngry Y N N N Y Y N Y Y N 50\\nDisgust N N N Y N N N N N N 10\\nFear N N Y N Y Y N N N N 30Happy N Y Y Y N Y N Y N Y 60S a d NYNYYYNYYN 6 0Surprise Y N N N N N Y N N N 20Neutral N N N N N N Y N N N 10\\nFacial expression Normal children group Autistic children group\\nAngry 80% 50%\\nDisgust 70% 10%Fear 80% 30%Happy 100% 60%Sad 80% 60%Surprise 70% 20%\\nNeutral 90% 10%\\nAverage recognition rate % 81.4% 34.3%Table III.\\nTest results in autistic\\nchildren group\\nTable IV.\\nComparisons of two\\ngroups of children ’s\\nfacial expression\\nrecognition rateLHTThe experimental results show that the recognition rate of happiness and sadness is\\nhigher in the seven expressions. Testing children showed difficulty in identifying complex\\nfacial expressions such as neutrality and aversion (see Table IV ).\\nThe experimental results showed that the recognition rate of facial expressions in autistic\\nchildren was significantly lower than that in normal children. All the autistic children who\\nparticipated in the test had a facial recognition rate of less than 60 percent. Therefore, if the\\naccuracy rate of facial expression diagnosis by the system was less than 60 percent, the tester\\nwould have a tendency to suffer from autism. The lower the recognition rate, the higher the\\ntendency of autism.\\n6. Conclusion\\nIn the era of rapid development of information technology, the processing of a large number\\nof health data has brought new opportunities and challenges to medical research. The\\nincidence of autism is increasing, which has attracted more and more attention from all\\naspects of society. The use of information technology, especially artificial intelligence\\ntechnology, to build an autism diagnosis system has become an urgent need for doctors and\\npatients. In this paper, an autism diagnosis system based on deep convolution neural network\\nand expression data is constructed. After testing, it can meet the design requirements of\\nautism diagnosis. The public can download and use the system through the network to\\ndiagnose autism conveniently. In addition, we will expand the function of the system,\\nincrease the recognition of children ’s physical movement and realize the diagnosis of autism\\nfrom multiple perspectives.\\nBecause the average age of children using and collecting facial expression data is between\\n3 and 8 years old, the system can recognize children aged 3 –6 years old. Therefore, through\\nthis system, autism can be diagnosed as soon as possible. The earlier the diagnosis and\\ntreatment of autism is, the better the rehabilitation effect. Therefore, it is of great significance\\nfor the treatment of autism.\\nBecause the training samples of the system adopt the international open facial expression\\ndatabase, which contains the facial expression data of children and adults in different\\ncountries and regions, the system can diagnose autism for children and adults in different\\ncountries and regions.\\nOf course, the system also needs to be improved through practical use. Next, we will\\narrange for the system to be tested in a large number of cooperative hospitals. Next, there are\\ntwo main tasks to be done. The first is to collect more data of normal and autistic children ’s\\nfacial expressions, improve the recognition effect of the system on children ’s facial\\nexpressions and establish a special database of children ’s facial expressions. The second is to\\nimprove the system function, according to the results of facial expression diagnosis of autistic\\nchildren for a detailed classification, to distinguish between severe, moderate and mild autism\\npatients, in order to facilitate the treatment of doctors.\\nThis study hopes to be helpful to the diagnosis of autism in remote and underdeveloped\\nareas, so as to promote the early diagnosis and treatment of autistic children and reduce the\\nmedical costs and burdens of autistic families and society. Therefore, this study has more\\nimportant social significance and application value.\\nReferences\\nBaron-Cohen, S., Wheelwright, S., Jolliffe, T. (1997), “Is there a “language of the eyes ”? Evidence from\\nnormal adults, and adults with autism or asperger syndrome ”,Visual Cognition , Vol. 4 No. 3,\\npp. 311-331.\\nBeijing Wucai Deer Autism Research Institute (2017), Report on the Development of Autism Education\\nand Rehabilitation Industry in China 2 , Huaxia Publishing House, Beijing.Facial\\nexpressions for\\nautism\\ndiagnosisCai, Y. (2018), “Facial tracking and facial expression recognition based on in-depth learning ”\\nSoutheast University.\\nDu, J. (2018), “Research on face expression recognition based on Kernel relieff ”Zhengzhou University.\\nDuan, Y., Wu, X. and Jinfeng (2015), “Research progress on etiology and treatment of autism ”,Chinese\\nScience: Life Science , Vol. 9, pp. 820-844.\\nEkman, P. (1992), “An argument for basic emotions ”,Cognition and Emotion , Vol. 6 Nos 3-4,\\npp. 169-200.\\nLanlan (2018), “Research on facial expression recognition method based on multi-feature fusion ”, Jilin\\nUniversity.\\nLi, S. and Deng, W. (2018), “Deep facial expression recognition: a survey ”arXiv preprint arXiv:\\n1804.08348.\\nLiu, Y., Huo, W. and Hu, X. (2015), “Summary of research on facial expression recognition of autistic\\nchildren ”,Modern Special Education , Vol. 8, pp. 35-39.\\nLucey, P., Cohn, J.F., Kanade, T., Saragih, J. and Ambadar, Z. (2010), “The extended cohn-kanade\\ndataset (ckþ): a complete dataset for action unit and emotion-specified expression ”,2010 IEEE\\nComputer Society Conference on Computer Vision and Pattern Recognition-Workshops ,\\n2010, IEEE.\\nMehrabian, A. (2008), “Communication without words ”,Communication Theory , Vol. 6, pp. 193-200.\\nMei, J. and Hu, B. (2015), “Research and implementation of real-time face expression recognition\\nmethod ”,Information and Technology , Vol. 44 No. 4, pp. 145-148.\\nOrganization W H (1992), The ICD-10 Classification of Mental and Behavioural Disorders: Clinical\\nDescriptions and Diagnostic Guidelines , World Health Organization, Geneva.\\nSegal, D.L. (2010), “Diagnostic and statistical manual of mental disorders (DSM-IV-TR) ”,The Corsini\\nEncyclopedia of Psychology , Vol. 1 No. 16, pp. 1-3.\\nShen, X., He, Z. and Ding, X. (2013), “Computer facial expression recognition training to improve the\\nfacial expression recognition ability of autistic children ”,Sci-tech Horizon , Vol. 25, pp. 12-13.\\nSingh, P., Ghosh, S. and Nandi, S. (2017), “Subjective burden and depression in mothers of children\\nwith autism spectrum disorder in India: moderating effect of social support ”,Journal of Autism\\nand Developmental Disorders , Vol. 47 No. 10, pp. 3097-3111.\\nVismara, L.A. and Rogers, S.J. (2008), “The early start denver model ”,Journal of Early Intervention ,\\nVol. 31 No. 1, pp. 91-108.\\nWang, H. (2007), “Psychological and behavioral characteristics, diagnosis and evaluation of autistic\\nchildren ”,Chinese Journal of Rehabilitation Medicine , Vol. 22 No. 9, pp. 853-856.\\nWang, G. and Lu, M. (2015), “Research on educational games for children with autism spectrum\\ndisorders ”,Modern Special Education , Vol. 14, pp. 38-40.\\nWang, Y., Xiao, L. and Chen, R. (2018), “Social impairment of children with autism spectrum disorder\\naffects parental quality of life in different ways ”,Psychiatry Research , Vol. 2018 No. 266,\\npp. 168-174.\\nWu, X. and Chen, S. (2018), “Research progress on quality of life and its influencing factors of primary\\ncaregivers for autistic children ”,General Nursing , Vol. 16 No. 18, pp. 2206-2208.\\nYan, S. (2008), “Experimental study on facial expression processing of autistic children ”, East China\\nNormal University.\\nYanbin, H., Fuxing, W., Heping, X., Jing, A., Yuxin, W. and Huashan, L. (2018), “Facial processing\\ncharacteristics of autism spectrum disorders: meta-analysis of eye movement research ”,\\nProgress in Psychological Science , Vol. 1, pp. 26-41.\\nYang, Y. and Wang, M. (2014), “Employment and financial burdens of families with preschool-aged\\nchildren with autism ”,Chinese Journal of Clinical Psychology , Vol. 22 No. 2, pp. 295-297, 361.LHTYang, J., Xing, H., Shao, Z. and Yuan, J. (2017), “Facial expression sensitivity deficits in patients with\\nautism spectrum disorder: impact of task nature and implications for intervention ”,Chinese\\nScience: Life Science , Vol. 47 No. 4, pp. 443-452.\\nZablotsky, B., Black, L.I. and Blumberg, S.J. (2017), “Estimated prevalence of children with diagnosed\\ndevelopmental disabilities in the United States, 2014-2016 ”,NCHS Data Brief , Vol. 291, pp. 1-8.\\nCorresponding author\\nWang Zhao can be contacted at: creativesoft@sohu.com\\nFor instructions on how to order reprints of this article, please visit our website:\\nwww.emeraldgrouppublishing.com/licensing/reprints.htmOr contact us for further details: permissions@emeraldinsight.comFacial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020_clean.txt'})]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "\n",
    "for text in texts:\n",
    "    document=text_splitter.split_documents(text)\n",
    "    document[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Research and development of\\nautism diagnosis information\\nsystem based on deep convolution\\nneural network and facial\\nexpression data\\nWang Zhao and Long Lu\\nSchool of Information Management, Wuhan University, Wuhan, China\\nAbstract\\nPurpose –Facial expression provides abundant information for social interaction, and the analysis and\\nutilization of facial expression data are playing a huge driving role in all areas of society. Facial expression data\\ncan reflect people ’s mental state. In health care, the analysis and processing of facial expression data can\\npromote the improvement of people ’s health. This paper introduces several important public facial expression\\ndatabases and describes the process of facial expression recognition. The standard facial expression database\\nFER2013 and CK þwere used as the main training samples. At the same time, the facial expression image data\\nof 16 Chinese children were collected as supplementary samples. With the help of VGG19 and Resnet18', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='of 16 Chinese children were collected as supplementary samples. With the help of VGG19 and Resnet18\\nalgorithm models of deep convolution neural network, this paper studies and develops an information system\\nfor the diagnosis of autism by facial expression data.\\nDesign/methodology/approach –The facial expression data of the training samples are based on the\\nstandard expression database FER2013 and CK þ. FER2013 and CK þdatabases are a common facial\\nexpression data set, which is suitable for the research of facial expression recognition. On the basis of FER2013and CK þfacial expression database, this paper uses the machine learning model support vector machine\\n(SVM) and deep convolution neural network model CNN, VGG19 and Resnet18 to complete the facial\\nexpression recognition.\\nFindings –In this study, ten normal children and ten autistic patients were recruited to test the accuracy of the', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='expression recognition.\\nFindings –In this study, ten normal children and ten autistic patients were recruited to test the accuracy of the\\ninformation system and the diagnostic effect of autism. After testing, the accuracy rate of facial expression\\nrecognition is 81.4 percent. This information system can easily identify autistic children. The feasibility ofrecognizing autism through facial expression is verified.\\nResearch limitations/implications –The CK þfacial expression database contains some adult facial\\nexpression images. In order to improve the accuracy of facial expression recognition for children, more facial\\nexpression data of children will be collected as training samples. Therefore, the recognition rate of the\\ninformation system will be further improved.\\nOriginality/value –This research uses facial expression data and the latest artificial intelligence technology,', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='information system will be further improved.\\nOriginality/value –This research uses facial expression data and the latest artificial intelligence technology,\\nwhich is advanced in technology. The diagnostic accuracy of autism is higher than that of traditional systems,so this study is innovative. Research topics come from the actual needs of doctors, and the contents and\\nmethods of research have been discussed with doctors many times. The system can diagnose autism as early as\\npossible, promote the early treatment and rehabilitation of patients, and then reduce the economic and mental\\nburden of patients. Therefore, this information system has good social benefits and application value.\\nKeywords Facial expression data, FER2013, CK þ, Deep convolution neural network, VGG19, Resnet18,\\nAutism, Diagnostic information system\\nPaper type Research paper\\n1. Introduction\\nFacial expression recognition is an important social cognitive skill. Emotions are expressed', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='Autism, Diagnostic information system\\nPaper type Research paper\\n1. Introduction\\nFacial expression recognition is an important social cognitive skill. Emotions are expressed\\nby facial expressions. Therefore, recognition and understanding of facial expressions is theFacial\\nexpressions for\\nautism\\ndiagnosis\\nThis research has been possible thanks to the support of projects: National Natural Science Foundation\\nof China (No. 61772375) and Independent Research Project of School of Information ManagementWuhan University (No: 413100032).The current issue and full text archive of this journal is available on Emerald Insight at:\\nhttps://www.emerald.com/insight/0737-8831.htm\\nReceived 31 August 2019\\nRevised 16 December 2019\\nAccepted 23 January 2020\\nLibrary Hi Tech\\n© Emerald Publishing Limited\\n0737-8831\\nDOI 10.1108/LHT-08-2019-0176basis of communication and interpersonal relationships with others. Abnormal expression is', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='Accepted 23 January 2020\\nLibrary Hi Tech\\n© Emerald Publishing Limited\\n0737-8831\\nDOI 10.1108/LHT-08-2019-0176basis of communication and interpersonal relationships with others. Abnormal expression is\\na prominent manifestation of autism, and it is also one of the criteria for the diagnosis of\\nautism. Doctors can diagnose autism by responding to abnormal facial expressions in\\nchildren.\\nAutism, also known as autism or autism disorders, is a representative disease of\\ngeneralized developmental disorders. In recent years, the incidence of autism in children has\\nbecome higher and higher, experiencing a transition from rare diseases to epidemics. At\\npresent, research on autism is still in its infancy at home and abroad, and research methods\\nand tools are still developing.\\nThe main symptoms of autism include impaired social and interpersonal communication,\\nlanguage retardation, repetitive behavior and sensory dysfunction. It is difficult for autistic', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='The main symptoms of autism include impaired social and interpersonal communication,\\nlanguage retardation, repetitive behavior and sensory dysfunction. It is difficult for autistic\\npatients to correctly recognize faces and explain facial emotions. They have different\\nemotional expressions from ordinary people, and they cannot correctly perceive and\\nunderstand some basic expressions such as anger ( Yan, 2008 ).\\nAt present, the diagnostic methods for autism spectrum disorders include: traditional\\nstandard DSM-IV-TR ( Segal, 2010 ) and ICD-10 ( Organization W H, 1992 ), various autism\\ndiagnostic assessment scales such as “Childhood Autism Rating Scale (CARS) ”,“the autism\\nchild behavior scale (ABC) ”and autism behavior rating scale and questionnaire interviews\\n(Wang and Lu, 2015 ). Most of these methods rely on doctors ’direct observation of the\\npatient ’s expression, speech and behavior based on their experience. Diagnostic results are', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='(Wang and Lu, 2015 ). Most of these methods rely on doctors ’direct observation of the\\npatient ’s expression, speech and behavior based on their experience. Diagnostic results are\\neasily disturbed by external factors such as hospital level, physician ’s subjective level,\\npatient ’s education level, age and so on. There are relatively large subjective factors, resulting\\nin a certain degree of missed diagnosis and misdiagnosis. It takes about 1 –2 h for each autistic\\npatient to diagnose, so doctors have a lot of work to do. The best period of treatment for\\nautistic patients is before the age of six. Early diagnosis is of great significance for the\\nrehabilitation of autistic patients.\\nThe purpose of our research and design is to train the model and make a facial expression\\nrecognition system based on the normal expression, so as to verify the abnormal expression.\\nThis system can test the facial expression of autistic children and judge the difference', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='recognition system based on the normal expression, so as to verify the abnormal expression.\\nThis system can test the facial expression of autistic children and judge the difference\\nbetween autistic children and normal children.\\nIn this study, FER2013 and CK þwere used as the main facial expression training\\nsamples. At the same time, we collected the facial expression image data of 16 Chinese\\nchildren as a supplementary sample of facial expression. With the help of VGG19 andResnet18 algorithm models of deep convolution neural network, according to the hospital\\nautism diagnosis scale and diagnosis process, this paper studies and designs an information\\nsystem for the diagnosis of autism by facial expression data. After the actual test of recruiting\\ntesters, the recognition rate of the system is 81.4 percent. It can effectively distinguish\\nwhether the expression of children is normal or not. It provides a practical information system', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='testers, the recognition rate of the system is 81.4 percent. It can effectively distinguish\\nwhether the expression of children is normal or not. It provides a practical information system\\nfor the diagnosis of autism. This paper will continue to collect more children ’s facial\\nexpression data from different countries and regions as training samples to further improve\\nthe recognition rate of facial expressions.\\nThe autism diagnosis information system designed in this study has the following\\nimportant significance:\\n(1) Autism can be diagnosed as early as possible by using this system. The best time to\\ntreat autism is before the age of six. The earlier the diagnosis of autism is made, the\\nless the treatment cost and the higher the probability of recovery. Early diagnosis is\\nof great value in alleviating the burden on families and society of autistic patients.\\nThe system can be published in the form of app or web pages and disseminated', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='of great value in alleviating the burden on families and society of autistic patients.\\nThe system can be published in the form of app or web pages and disseminated\\nthrough the Internet. The system can be installed and used on different devices, such\\nas computers, mobile phones, tablets, etc. It has good applicability. Through thisLHTsystem, autism can be diagnosed conveniently, and time can be saved for the early\\ntreatment of autism patients, especially those in underdeveloped areas.\\n(2) It can make the diagnosis of autism more objective. The whole diagnosis process is\\ncompleted by the system. Because artificial intelligence technology is used to\\nrecognize facial expressions without human intervention, the diagnosis results are\\nobjective and accurate.\\n(3) Reduce the intensity of doctors ’work. Before the system was used, it took an hour for\\ndoctors to diagnose an autistic patient. By using this system, doctors can save a lot of\\ntime and pay attention to the treatment of autism.', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='doctors to diagnose an autistic patient. By using this system, doctors can save a lot of\\ntime and pay attention to the treatment of autism.\\n(4) The facial expression database used in the training of this system contains different\\nraces in the world. Therefore, this system can not only diagnose children in different\\ncountries and regions but also diagnose suspected autism patients all over the world.\\n(5) This research designs the system according to the actual business. The early design\\nof the system adopts the suggestions of several doctors, so it is designed and\\nmanufactured according to the actual needs of doctors. Although there are some\\npapers on autism diagnosis by facial expressions at home and abroad, there are still\\nfew autism diagnosis systems developed which can be used in practice.\\n(6) This paper uses the latest in-depth learning technology to improve the accuracy of\\nfacial expression recognition. Previous traditional techniques and methods have low', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='(6) This paper uses the latest in-depth learning technology to improve the accuracy of\\nfacial expression recognition. Previous traditional techniques and methods have low\\nrecognition rate of facial expressions. In recent years, with the development of\\nartificial intelligence technology and the improvement of computing speed, the\\nconvolutional neural network has greatly improved the accuracy of facial expression\\nrecognition, which is the innovation of this research in technology.\\n2. Facial expression database and its recognition technology\\n2.1 Facial expression database\\nFacial expression is an important way for peopl e to express their emotions. In the social\\nprocess, facial expression is an important way to judge the attitude and inner feelings of\\nthe other party ( Lanlan, 2018 ).Mehrabian (2008) found that in a conversation, the change\\nof facial expression played the most important role. Of these, 55 percent are facial', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='the other party ( Lanlan, 2018 ).Mehrabian (2008) found that in a conversation, the change\\nof facial expression played the most important role. Of these, 55 percent are facial\\nexpressions, 38 percent are voic e and only 7 percent are words ( Mei and Hu, 2015 ).\\nCompared with voice, expression can convey more abundant information. Recognition\\nand understanding of facial expressions i s very important for communicating with\\nothers ( Shen et al. , 2013 ). In 1972, Ekman demonstrated through empirical research that\\nhuman beings have six basic facial expression s: happiness, sadness, anger, fear, disgust\\nand surprise ( Ekman, 1992 ). In subsequent studies, neutral expression has also been\\nadded to the basic expression, and it is gen erally believed that there are seven basic\\nexpressions in facial expression.\\nWith the continuous development of computer software and hardware technology, people\\nhave a deeper understanding of facial expression recognition technology. In order to better', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='With the continuous development of computer software and hardware technology, people\\nhave a deeper understanding of facial expression recognition technology. In order to better\\nstudy facial expression recognition technology, many international research institutions\\nhave established standard facial expression databases, the main facial expression databases\\nare as follows:\\n(1) JAFFE\\nThe database stores facial expression data of Japanese women. It contains 213 facial images\\nof ten Japanese women. There are seven types of facial expressions, namely neutral, happy,Facial\\nexpressions for\\nautism\\ndiagnosissad, surprise, anger, disgust and fear. The resolution of each image is 256 3256 pixels.\\nEveryone has seven kinds of pictures of facial expressions.\\n(2) CKþ\\nThe expression database was collected under laboratory conditions. It includes African\\nAmericans, Asians and South Americans. The resolution of each image is 640*480 pixels. It', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='(2) CKþ\\nThe expression database was collected under laboratory conditions. It includes African\\nAmericans, Asians and South Americans. The resolution of each image is 640*480 pixels. It\\ncontains 593 expression sequences of 123 people, 69 percent of whom are female and 31\\npercent are male. Each sequence begins and ends with neutral expression, which includes the\\nprocess from calm to strong expression. CK þis a facial expression data set with many\\napplications. The reliability of various facial expression evaluation experiments using this\\ndatabase is very high. It includes seven types of facial expressions: anger, contempt, disgust,\\nfear, happy, sadness and surprise.\\n(3) FER2013\\nThere are 35,887 facial images in the library, and there are seven facial expression types:\\nangry, disgust, fear, happy, sad, surprise and neutral. The resolution of each image is 48*48\\npixels. All the images are gray images. There are three sample sets: 28,709 images in the', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='angry, disgust, fear, happy, sad, surprise and neutral. The resolution of each image is 48*48\\npixels. All the images are gray images. There are three sample sets: 28,709 images in the\\ntraining set; 3,589 images in the validation set and 3,589 images in the test set.\\n(4) MMI\\nThe expression database can be divided into two parts: one is a dynamic data set composed of\\nmore than 2,900 video sequences. The other part is a static data set consisting of a large\\nnumber of high resolution images. There are seven types of expression in the library.\\n(5) AFEW\\nAll the facial images in the database are edited from the movies and contain seven basic facial\\nexpressions.\\n(6) SFEW\\nThe expression library is a static frame image extracted from the AFEW data set, which\\ncontains seven basic expressions.\\n2.2 Facial expression recognition process\\nThe process of facial expression recognition includes two stages as shown in Figure 1 : One is', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='contains seven basic expressions.\\n2.2 Facial expression recognition process\\nThe process of facial expression recognition includes two stages as shown in Figure 1 : One is\\nthe training stage and the other is the recognition stage. The training and recognition stages\\ncan be divided into three parts: the pretreatment of facial expression images, the extraction of\\nfacial expression features and the classification of facial expressions. The training stage is to\\ntrain the model in order to achieve the purpose that the model can be used. The recognition\\nstage is to recognize and classify the expression of the test image ( Du, 2018 ).\\nThe two stages of expression recognition process include the following processes: First,\\nface detection is carried out on the image in the expression database, including the location,\\nFigure 1.\\nFacial expression\\nrecognition processLHTalignment and clipping of the face area. This is the basis of the follow-up process. Only when', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='Figure 1.\\nFacial expression\\nrecognition processLHTalignment and clipping of the face area. This is the basis of the follow-up process. Only when\\nthe expression area is accurately obtained, the following series of work will be more accurate.\\nAfter the face area is detected, the image needs to be preprocessed in order to eliminate the\\nnoise caused by the influence of acquisition equipment and environment and avoid the\\ninterference of feature extraction. Then it is the feature extraction step, which aims to extract\\nthe features that can represent the essence of expression from the preprocessed facial images.\\nIn this process, in order to avoid the high dimension of feature extraction and affect the\\nefficiency of the algorithm, we need to reduce the dimension of extracted features in order to\\nextract the most representative expression features. Finally, the extracted facial features are\\nclassified to determine which type of facial expression is.\\n2.3 Facial expression recognition technology', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='classified to determine which type of facial expression is.\\n2.3 Facial expression recognition technology\\nFacial expression recognition technology mainly includes traditional machine learning\\ntechnology and deep learning technology. The two technologies have similarities and\\ndifferent characteristics.\\n(1) Traditional machine learning technology\\nFacial expression recognition algorithm based on traditional machine learning includes three\\nsteps: image preprocessing, facial expression feature extraction and feature classification.\\nFirst, for the convenience of feature extraction, it is necessary to preprocess the image,\\nwhich can effectively avoid the interference of various noises and leave the key information\\nneeded by the face. The pretreatment process includes image gray processing, face\\nalignment, face size tailoring, data enhancement, brightness, pose normalization, etc. ( Li and\\nDeng, 2018 ).\\nSecond, the traditional feature extraction methods include directional gradient histogram', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='alignment, face size tailoring, data enhancement, brightness, pose normalization, etc. ( Li and\\nDeng, 2018 ).\\nSecond, the traditional feature extraction methods include directional gradient histogram\\nfeature, Gabor filter feature, local directional pattern feature and enhanced local binary\\nalgorithm. Because these methods are artificial design, time-consuming and laborious, and\\nhave certain limitations and often have better effect in feature extraction in small sample\\nimage set, most of the current studies are based on deep learning feature extraction method.\\nThere are many basic machine learning methods for expression classification, such as\\nsupport vector machine (SVM), hidden Markov model (HMM) and k-nearest classification\\nalgorithm.\\n(2) Deep learning technology\\nFacial expression recognition algorithm based on deep learning also needs image\\npreprocessing. The difference is that it often combines feature extraction and feature', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='(2) Deep learning technology\\nFacial expression recognition algorithm based on deep learning also needs image\\npreprocessing. The difference is that it often combines feature extraction and feature\\nclassification into an end-to-end model, which greatly simplifies the process of facial\\nexpression recognition. In addition to end-to-end learning, deep learning algorithm can be\\nused to extract facial expression features, and then other independent classifiers can be used.\\nFor example, SVM or random forest algorithm is used to process the extracted features and\\nclassify them.\\nIn this paper, we construct a facial expression recognition model based on deep learning\\ntechnology, extract facial expression feature data of children and classify them into groups,\\nso as to diagnose autism.\\n2.4 Driving role of facial expression data\\nResearch on facial expression recognition has been applied in a series of life scenarios. In', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='so as to diagnose autism.\\n2.4 Driving role of facial expression data\\nResearch on facial expression recognition has been applied in a series of life scenarios. In\\nchildren ’s education, advanced human-computer interaction, medical diagnosis and other\\naspects have played an important role ( Cai, 2018 ).\\nIn distance education or classroom teaching, teachers can better improve students ’\\nlearning quality by observing students ’emotional changes in the classroom and adjustingFacial\\nexpressions for\\nautism\\ndiagnosisteaching plans in time. Advanced human-computer interaction can make human-computer\\ninteraction more harmonious. For example, intelligent robots can automatically respond to\\nthe facial expressions of their interlocutors. In medical diagnosis, facial expressions also play\\nan important role in the prevention and diagnosis of diseases. For example, this article is to\\ndiagnose autism by analyzing children ’s facial expressions.\\n3. Autism and facial expression diagnosis', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='an important role in the prevention and diagnosis of diseases. For example, this article is to\\ndiagnose autism by analyzing children ’s facial expressions.\\n3. Autism and facial expression diagnosis\\n3.1 Autism and its development\\nAutism is a neurodevelopmental disorder, which is collectively referred to as autism\\nspectrum disorder ( Duan et al., 2015 ).\\nSince Kanner, an American child psychiatrist, first reported autism in 1943, the incidence\\nof autism has risen rapidly worldwide. In the 1980s, about 3 –5 out of every 10,000 people\\nsuffered from the disease, while in 2000, 6.7 out of every 1,000 children suffered from the\\ndisease ( Vismara and Rogers, 2008 ). According to the National Center for Health Statistics,\\nthe probability of autism among children aged 3 –14 in the United States reached 2.76 percent\\nin 2016 ( Zablotsky et al., 2017 ).\\nThere is no statistical survey on autistic children in China. However, according to the data', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='in 2016 ( Zablotsky et al., 2017 ).\\nThere is no statistical survey on autistic children in China. However, according to the data\\nof the report on the development of China ’s autism education and rehabilitation industry II,\\nthe number of people with autism in China is estimated to exceed 10 million, of which 2 million\\nare autistic children. At the same time, it is growing at the rate of nearly 200,000 annually\\n(Beijing Wucai Deer Autism Research Institute, 2017 ).\\nAutism brings serious financial burden to both society and family. Families with autistic\\nchildren, on the one hand, spend a lot of time caring for their children, while working hours are\\nreduced so that work income is reduced. On the other hand, the cost of family rehabilitation\\ntreatment for autistic children is huge, which increases the family ’s financial burden ( Wu and\\nChen, 2018 ). According to the survey on the occupational and economic burden of preschool', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='treatment for autistic children is huge, which increases the family ’s financial burden ( Wu and\\nChen, 2018 ). According to the survey on the occupational and economic burden of preschool\\nautistic children ’s families, 33 percent of parents of autistic children reported that their\\ncaregiving problems seriously affected their careers, and their annual income was\\nsignificantly lower than that of ordinary families, with an average loss of income of 30,957\\nyuan per year. Meanwhile, the average annual cost of autistic children ’s families for children ’s\\neducation and training is significantly higher than that of ordinary families ( Yang and Wang,\\n2014). The society and the government also need to invest a lot of money in the rehabilitation\\neducation of autistic children. At the same time, autism also brings high subjective load and\\ndepression to the families of patients, which has a negative impact on their quality of life', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='education of autistic children. At the same time, autism also brings high subjective load and\\ndepression to the families of patients, which has a negative impact on their quality of life\\n(Singh et al., 2017 ;Wang et al., 2018 ). It can be seen that the incidence of autism in children is\\nrelatively serious, and the harm to society and family is enormous.\\n3.2 Diagnosis of autism through facial expressions\\n3.2.1 Facial expression recognition disorder. Autistic children have facial expression\\nrecognition obstacles, which are mainly manifested in their inability to recognize facial\\nexpressions ( Liuet al., 2015 ). It is easy to distinguish autistic children from normal children by\\nobserving their facial expressions. Therefore, we combine facial expression recognition\\ntechnology to extract facial expression response feature vectors and use artificial intelligence\\ntechnology to distinguish normal group and autistic group based on these facial features.', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='technology to extract facial expression response feature vectors and use artificial intelligence\\ntechnology to distinguish normal group and autistic group based on these facial features.\\n3.2.2 The principle of diagnosing autism through facial expressions. A large number of\\nstudies have pointed out that autistic patients have deficiencies in facial expression\\nrecognition and understanding. This is the core source of impaired social function in\\nautistic patients ( Yang et al. , 2017 ). Autistic children are more difficult to identify other\\npeople ’s emotional behavior, and it is difficult to make appropriate judgment and responseLHT(Shen et al. , 2013 ). Overseas research on facial expression recognition ability of autistic\\npatients has been carried out not only in children but also in adults. Most studies believe that\\nthe ability of facial expression recognition of autistic patients is low. Baron-Cohen et al. (1997)', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='patients has been carried out not only in children but also in adults. Most studies believe that\\nthe ability of facial expression recognition of autistic patients is low. Baron-Cohen et al. (1997)\\nused standard facial expression maps to study the recognition of different emotional types in\\nautistic adults. It was found that autistic adults had better recognition of some basic facial\\nexpressions, such as happiness, but relatively complex facial expressions such as surprise\\nrecognition were difficult to recognize.\\nAt present, the main diagnostic criteria of autism are: IDC-10, DSM-IV, the autism child\\nbehavior scale (ABC), the children autism rating scale (CARS) and the Clancy behavior scale\\n(CABS) ( Wang, 2007 ).\\nAfter consulting a large number of literatures and investigating the actual situation of the\\nhospital, now the hospital mainly uses CABS (filled by parents), ABC (filled by parents) and', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='After consulting a large number of literatures and investigating the actual situation of the\\nhospital, now the hospital mainly uses CABS (filled by parents), ABC (filled by parents) and\\nCARS (filled by doctors) to diagnose autism. After a detailed review of the test items of the\\nthree scales, these scales all contain the test items to judge autism through children ’s facial\\nexpressions. There were 14 items in the CABS scale, of which the seventh item was\\ninexplicable laughter and the tenth item was not looking at each other ’s face. Avoiding eye\\ncontact was related to expression. There were 57 items in the ABC scale, of which the seventh\\nitem was non-communicative smile, the seventeenth item did not respond to other people ’s\\nfacial expressions, and the twenty-fourth item was active avoidance of eye contact with\\nothers. Fifteen items of the CARS scale, the third of which is emotional response, pleasure and', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='facial expressions, and the twenty-fourth item was active avoidance of eye contact with\\nothers. Fifteen items of the CARS scale, the third of which is emotional response, pleasure and\\nunhappiness and interest, are expressed by changes in facial expression and posture. These\\nscales basically include the items of autism detection by children ’s facial expressions, which\\nshow that the diagnosis of autism can be more accurate by facial expressions. With the\\nprogress of artificial intelligence technology, facial expression recognition technology can\\nobjectively and effectively reflect the mental health of children and can be used in early\\ndiagnosis of autism ( Yanbin et al., 2018 ).\\nWe also communicated with doctors of Hubei Maternal and Child Health Hospital, Wuhan\\nChildren ’s Hospital and Guangzhou Women and Children ’s Medical Center many times, and\\nactually checked the process of using the above autism diagnostic scale to diagnose children.', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='Children ’s Hospital and Guangzhou Women and Children ’s Medical Center many times, and\\nactually checked the process of using the above autism diagnostic scale to diagnose children.\\nThe doctor observes the tester ’s reaction to determine whether the tester is autistic after\\nrequesting the tester to make the corresponding expression. Doctors point out that facial\\nexpression is an important part of autism diagnosis. In terms of system design, they put\\nforward requirements and suggestions for the process of diagnosing autism through facialexpression.\\n4. Research and development of autism diagnosis information system\\n4.1 Facial expression database selection\\nThe expression databases in this study mainly come from two public expression databases\\nCKþand FER2013. In addition, 16 Chinese children ’s expression data were collected as\\nsupplementary samples. The two public expression databases are standard and international', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='CKþand FER2013. In addition, 16 Chinese children ’s expression data were collected as\\nsupplementary samples. The two public expression databases are standard and international\\nand have been widely used, including facial expression data of adults and children. Each\\nsample in the database contains seven expressions: angry, disgust, fear, happy, sad, surprise\\nand neutral. Because children ’s facial expressions are different from adults, in order to\\nimprove the recognition rate of children ’s facial expressions, we collected facial expression\\ndata of 16 children aged 5 to 8 in China. Seven expressions were collected from each child. We\\ncombine Chinese children ’s facial expression data and public expression database as our\\nsystem ’s facial expression database.\\n4.1.1 FER2013 facial expression database. The reason for choosing FER2013 expression\\ndatabase is that it has more samples and is more mature than other expression databases. It', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='4.1.1 FER2013 facial expression database. The reason for choosing FER2013 expression\\ndatabase is that it has more samples and is more mature than other expression databases. It\\nhas advantages in model training. At the same time, it has been used in many studies (see\\nPlate 1 ).Facial\\nexpressions for\\nautism\\ndiagnosis4.1.2 CK þFacial expression database. CKþfacial expression database was selected\\nbecause it was collected in the laboratory, so its accuracy is relatively high ( Lucey et al., 2010 )\\n(seePlate 2 ).\\n4.1.3 Facial expression data of Chinese children. At present, the mature facial expression\\ndatabases at home and abroad are mainly based on adult male or female facial expression\\nimages. Therefore, it is urgent to establish a facial expression database for children.\\nFacial images of children are quite diffe rent from those of adults. Children have\\nrounder faces, larger eyes and less prominent bones. Because of these differences,', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='Facial images of children are quite diffe rent from those of adults. Children have\\nrounder faces, larger eyes and less prominent bones. Because of these differences,\\nchildren ’s facial features are less obvious and more difficult to recognize than adults.\\nBecause of the particularity of children, it is very difficult to collect children ’sf a c i a l\\nimages. In order to improve the recognition rate of children ’s facial expressions, we\\ncooperated with Amy Education School in Zhengzhou. Sixteen healthy children as\\nvolunteers were recruited to collect facial expression data. Each of them collected seven\\nkinds of expressions, totaling 112 pictures. These children are between 5 and 8 years old,\\nincluding 8 boys and 8 girls. The acquisition environment is quiet and there is no\\nexternal interference. High-d efinition cameras are used t o collect facial expression\\nimages, which are processed professionally. Bef ore collecting facial expression data,', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='external interference. High-d efinition cameras are used t o collect facial expression\\nimages, which are processed professionally. Bef ore collecting facial expression data,\\nparents have been informed of the purpose o f collecting facial expression data. After\\nquestioning with parents, all the children who participated in the collection of facial\\nexpression data had no history of autism.\\nWe loaded the expression data into the training sample library. The purpose of collecting\\nChinese children ’s facial expression data is to increase the number of Chinese children ’s facial\\nexpression samples in training samples and improve the recognition rate of the system for\\nchildren ’s facial expression. The collection process and the collected children ’s facial\\nexpression data are shown in Plate 3 .\\n4.2 Network topology\\nAccording to the network environment and equipment of the information service platform,', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='expression data are shown in Plate 3 .\\n4.2 Network topology\\nAccording to the network environment and equipment of the information service platform,\\nthe network topology can be divided into four levels. The network topology diagram is shown\\ninFigure 2 .\\nThe first layer is the application layer, which consists of users, computers and various\\nsmart devices. Smart devices include smart tablet computer, smartphones and other\\nelectronic devices. Users access and use the information service platform through computers\\nand various smart devices.\\nPlate 1.\\nFER2013 facialexpression database\\nPlate 2.CKþFacial expression\\ndatabaseLHTThe second layer is the communication layer, mainly based on the internet network\\nenvironment, providing access channels for users and systems.\\nThe third layer is the application server layer, which is composed of firewall and\\napplication server and has an ontology display system for autism. The application server', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='The third layer is the application server layer, which is composed of firewall and\\napplication server and has an ontology display system for autism. The application server\\nmanages various business functions, handles various business requests submitted by users\\nand can access the database server for various data exchange.\\nThe fourth layer is the database server layer, which stores all kinds of data and knowledge\\nresources of the information service platform.\\n4.3 System architecture\\nThe smart diagnosis system of autism adopts client/server architecture. The client includes\\ndifferent versions of programs suitable for computers and smartphones. The system\\narchitecture diagram is shown in Figure 3 .\\nThe client includes three main modules: user interaction, image acquisition and face\\ndetection. User interaction module is responsible for human-computer interaction. According\\nto the requirements of the autism diagnostic scale, users who diagnose are required to make', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='detection. User interaction module is responsible for human-computer interaction. According\\nto the requirements of the autism diagnostic scale, users who diagnose are required to make\\nappropriate expressions and feedback by prompting pictures and voice guidance. Through\\nthe camera, the image acquisition module can dynamically capture facial expression images.\\nAt the right time, the system will collect facial expression images and transmit them to the\\nface detection module. Face detection module recognizes the valid face features and\\ncompresses the image and transfers it to the server through the internet or mobile Internet.\\nThe server includes six main modules: image processing, feature extraction, group\\nclassification, automatic diagnosis, training model and data management. The image\\nprocessing module can receive the expression image transmitted by the client and then\\nprocess the expression image and transfer it to other modules on the server side. The feature', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='processing module can receive the expression image transmitted by the client and then\\nprocess the expression image and transfer it to other modules on the server side. The feature\\nextraction module receives the facial expression images provided by the image processing\\nmodule and extracts the facial expression features. The group classification module is\\nFigure 2.\\nNetwork topology\\ndiagramPlate 3.\\nCollection of facial\\nexpression data of\\nChinese childrenFacial\\nexpressions for\\nautism\\ndiagnosisresponsible for group classification and correctly classifies the expression images into the\\nmost matching expressions among the seven kinds of expressions. The automatic diagnosis\\nmodule gives the diagnosis of autism by comparing the facial expressions that the tester is\\nrequired to imitate and the facial expressions that the tester actually makes. Model training\\nmodule is the core module of the system, which is responsible for recognizing and processing', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='required to imitate and the facial expressions that the tester actually makes. Model training\\nmodule is the core module of the system, which is responsible for recognizing and processing\\nthe newly collected facial expression images. The data management module mainly manages\\nfacial expression data, including storing and reading facial expression images transmitted by\\nthe client.\\nThe system server stores facial expression feature files, which are formed by feature\\nextraction of facial expression database. The expression feature file is HDF5 file format. The\\nexpression recognition system running on the server can read the expression feature file at\\nany time. If new facial expression samples are collected, the model can be retrained and the\\nfacial expression feature files can be updated.\\nThe client collects the tester ’s facial expression data by high-definition camera and\\ntransmits the facial expression data to the server by JSON file according to TCP communication', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='The client collects the tester ’s facial expression data by high-definition camera and\\ntransmits the facial expression data to the server by JSON file according to TCP communication\\nprotocol. The facial expression recognition system running on the server processes the collected\\nfacial expression data and then feeds the recognition results back to the tester through the\\nnetwork and stores the recognition results and facial expression data in the server database.\\nFacial data and diagnostic system are stored on a server, and the recognition results and facial\\ndata are stored in the SQL Server database. The diagnostic system reads data from the\\ndatabase through SQL structured query language. The response time of the whole database\\noperation and communication process should not exceed 5 s.\\n4.4 System architecture\\n4.4.1 VGG19 model. Researchers from the Oxford University and the Google Brain have\\njointly developed the convolutional neural network VGG. VGNet consists of 11, 13, 16 and 19', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='4.4 System architecture\\n4.4.1 VGG19 model. Researchers from the Oxford University and the Google Brain have\\njointly developed the convolutional neural network VGG. VGNet consists of 11, 13, 16 and 19\\nlayers of neural networks .VGNet constructs 16 –19 layers of neural networks by stacking\\nsmall convolution cores of 3 33 and maximum pooling layers of 2 32 repeatedly. VGGNet\\nFigure 3.\\nSystem architectureLHThas strong scalability and greatly reduces the error rate when extending. When migrating to\\nother image data, it has good generalization ability and simple structure.\\n4.4.2 ResNet18 model. ResNet was proposed by Kaiming He and others of Microsoft\\nResearch institute. They have successfully trained 152 layers of neural networks by using\\nResNet unit. The structure of ResNet can accelerate the training of the neural network, and\\nthe accuracy of the model has been greatly improved.\\n4.4.3 Graphic of deep learning framework. The deep learning framework used in this paper', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='the accuracy of the model has been greatly improved.\\n4.4.3 Graphic of deep learning framework. The deep learning framework used in this paper\\nfor facial expression recognition is shown in Plate 4 .\\nThe whole process includes image input, image preprocessing, model building, model\\ntraining, model testing and output of expression recognition results. There are two kinds of\\ndeep learning algorithms used in this paper: VGG19 and ResNet18. ResNet18 solves the\\nproblem of network performance degradation caused by the high depth of VGG19. By\\ntraining the two models and synthesizing the two convolutional neural network models, the\\nfacial expression features of autistic children can be extracted accurately.\\n4.4.4 Image preprocessing. The purpose of image preprocessing is to achieve uniform\\nnormalization of the final input image. The process is shown in Figure 4 .\\nConverting an image to a grayscale image can reduce the computational complexity of the', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='normalization of the final input image. The process is shown in Figure 4 .\\nConverting an image to a grayscale image can reduce the computational complexity of the\\nlatter pixel level and also reflect the overall and local distribution and characteristics of the\\nimage. Then, image transformation is used to enhance data by zooming, rotating, cutting and\\ntranslating, and the image is located in the center of the window. The contrast and brightness\\nof the image can be improved by histogram equalization to reduce the influence of\\nillumination on expression feature learning. In order to make the image uniform, it is planned\\nto transform the image size into the same size by normalizing the image size. Finally, the\\nmask is used to remove the occlusion of non-face areas.\\n4.4.5 Model training. Before model training, we need to enhance the image data. We choose\\nSGD random gradient descent algorithm as the optimization method. The batch size is still', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='4.4.5 Model training. Before model training, we need to enhance the image data. We choose\\nSGD random gradient descent algorithm as the optimization method. The batch size is still\\n128 by default, and the learning rate is set to 0.01 initially. In addition, the initialization of\\nPlate 4.\\nFramework of deep\\nlearning\\nFigure 4.\\nImage preprocessingFacial\\nexpressions for\\nautism\\ndiagnosisPlate 5.\\nFacial expression\\nrecognition resultsLHTnetwork parameters is also very important. We have adopted a random initialization method\\nto train the two network algorithms. The core code of Python is as follows:\\n# Model training\\ndeftrain (epoch ):\\nifepoch >learning_rate_decay_start andlearning_rate_decay_start >=0:\\nfrac =(epoch -learning_rate_decay_start )/ /learning_rate_decay_every\\ndecay_factor =learning_rate_decay_rate **frac\\ncurrent_lr =opt.lr *decay_factor\\nutils .set_lr (optimizer ,current_lr )# set the decayed rate\\nelse :\\ncurrent_lr =opt.lr\\nforbatch_idx ,(inputs ,targets )inenumerate (trainloader ):', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='current_lr =opt.lr *decay_factor\\nutils .set_lr (optimizer ,current_lr )# set the decayed rate\\nelse :\\ncurrent_lr =opt.lr\\nforbatch_idx ,(inputs ,targets )inenumerate (trainloader ):\\nifuse_cuda :\\ninputs ,targets =inputs .cuda (),targets .cuda ()\\noptimizer .zero_grad ()\\nutils .clip_gradient (optimizer ,0.1)\\noptimizer .step ()\\ncorrect +=predicted .eq(targets .data ).cpu().sum ()\\n4.4.6 Recognition results. Through the trained model, we use some children ’s facial\\nexpressions pictures and videos to test, and get the probability of various expressions and the\\nfinal prediction results of the model. As shown in Plate 5 , the histogram shows the probability\\nof each type of facial expression, and the histogram of maximum probability is the final\\nrecognized facial expression. After testing, the recognition rate of children ’s facial expression\\nreaches 81.4 percent, which can effectively distinguish whether children ’s facial expression is\\nnormal or not.\\n5. System validation\\n5.1 Testing environment', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='reaches 81.4 percent, which can effectively distinguish whether children ’s facial expression is\\nnormal or not.\\n5. System validation\\n5.1 Testing environment\\nIn this study, two kinds of mobile phones, personal computers and servers are selected as test\\nenvironments. The hardware and software environments are shown in Table I .\\n5.2 Diagnostic procedure and interface of diagnostic system\\nThe diagnostic process is shown in Figure 5 . First, the system randomly displays one of the\\nseven kinds of facial expressions for the tester to imitate. The system will prompt the tester\\nTesting equipment Hardware environment Software environment\\nOPPO R17 mobile phone CPU:SDM670 RAM:8GB Android\\nIPhone 8 mobile phone CPU:A11 RAM:2GB iOSPersonal computer CPU:Intel i7 RAM:16GB Windows 10Server CPU:Intel W2133 RAM:16GB Windows Server 2019\\nFigure 5.\\nAutomatic diagnostic\\nprocedureTable I.\\nTesting environmentFacial\\nexpressions for\\nautism', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='Figure 5.\\nAutomatic diagnostic\\nprocedureTable I.\\nTesting environmentFacial\\nexpressions for\\nautism\\ndiagnosisto imitate the facial expression by pictures a nd sounds. For example, the system displays\\nhappy cartoon smiling faces, plays happy children ’s songs and induces children to make\\nhappy expressions. The system displays t he same expression example three times and\\ncollects the tester ’se x p r e s s i o nd a t aa tt h es a m et i m e .T h e nt h es y s t e mc o m p a r e st h e\\nexpression examples and the actual collect ed expression data and gives the diagnosis\\nresults.\\n5.2.1 Diagnostic procedure. The diagnostic process is shown in Figure 5 . First, the system\\nrandomly displays one of the seven kinds of facial expressions for the tester to imitate. The\\nsystem will prompt the tester to imitate the facial expression by pictures and sounds. For\\nexample, the system displays happy cartoon smiling faces, plays happy children ’s songs and', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='system will prompt the tester to imitate the facial expression by pictures and sounds. For\\nexample, the system displays happy cartoon smiling faces, plays happy children ’s songs and\\ninduces children to make happy expressions. The system displays the same expression\\nexample three times and collects the tester ’s expression data at the same time. Then the\\nsystem compares the expression examples and the actual collected expression data and gives\\nthe diagnosis results.\\n5.2.2 Interface of diagnostic system. The system diagnostic interface is designed according\\nto the diagnostic process (see Plate 6 ).\\n5.3 System testing\\n5.3.1 Test sample. We recruited ten normal children and ten autistic children and divided\\nthem into normal children group and autistic children group for comparative verification.\\nThe accuracy of the system is verified by the actual test of the autism diagnosis information\\nsystem.\\nThe normal group of children was provided by Amy Education School in Zhengzhou,', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='The accuracy of the system is verified by the actual test of the autism diagnosis information\\nsystem.\\nThe normal group of children was provided by Amy Education School in Zhengzhou,\\nwhich cooperated with us. Ten healthy children as volunteers were recruited as the normal\\nPlate 6.\\n(a) The main interface\\nof the autism smart\\ndiagnosis informationsystem, including\\nsystem introduction,\\nknowledgeintroduction of autism\\nand other functions. (b)\\nThe facial expressionthat the system\\nprompts the tester to\\nsimulate after starting\\nthe diagnostic process.\\n(c) The expressionanalysis after\\ndiagnosis. (d) The\\nresult given by thesystem after three\\ndiagnosesLHTgroup for testing. These children were between 5 and 8 years old, including 5 boys and 5 girls.\\nParents were informed of the purpose and content of the experiment before the experiment.\\nChildren who participated in the experiment had no history of autism after being asked by\\ntheir parents.', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='Parents were informed of the purpose and content of the experiment before the experiment.\\nChildren who participated in the experiment had no history of autism after being asked by\\ntheir parents.\\nThe autistic children were provided by Guangzhou Children ’s Care Center, which\\ncooperated with us. Ten volunteers of autistic children were recruited as the autistic children\\ngroup for testing. These children were aged between 3 and 6 years old, including 5 boys and 5\\ngirls. Parents were informed of the purpose and content of the experiment before the\\nexperiment. The selected children with autism were diagnosed by a professional physician.\\n5.3.2 Test environment and process. All the tests were conducted in quiet classrooms\\nwithout noise and external factors. Through our autism diagnosis information system, each\\nchild was prompted by pictures and sounds to imitate seven kinds of facial expressions and\\nprompted to make corresponding facial responses according to the facial expressions on the', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='child was prompted by pictures and sounds to imitate seven kinds of facial expressions and\\nprompted to make corresponding facial responses according to the facial expressions on the\\npictures. At this time, the camera will capture their facial expressions, and after system\\nanalysis, they will be saved in the form of pictures in the computer of the test system (see\\nPlate 7 ).\\n5.3.3 Test result. We used the system to test the normal combination and autistic children\\nrespectively. Finally, we compared the recognition rate of the two groups.\\nFrom Table II , the average recognition rate of each expression is angry 80 percent, disgust\\n70 percent, fear 80 percent, happy 100 percent, sad 80 percent, surprise 70 percent and neutral\\n90 percent.\\nTest child 1 only had a disgusting expression recognition error, and other facial\\nexpression recognition was correct, then the average recognition rate of the seven facial\\nFacial expressionNumber of test children\\nCorrect identification (Y 5Yes, No 5N)', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='expression recognition was correct, then the average recognition rate of the seven facial\\nFacial expressionNumber of test children\\nCorrect identification (Y 5Yes, No 5N)\\nAverage recognition rate % 1234567891 0\\nAngry Y Y Y N Y Y Y Y Y N 80\\nDisgust N Y Y Y N Y N Y Y Y 70Fear Y Y Y N Y Y Y Y N Y 80Happy Y Y Y Y Y Y Y Y Y Y 100S a d YYNYYYYYYN 8 0\\nSurprise Y N Y N Y Y N Y Y Y 70\\nNeutral Y Y Y Y Y Y Y Y Y N 90Plate 7.\\n(a) Test environment\\nfor normal children\\ngroup. (b) Test\\nenvironment for\\nautistic children group\\nTable II.\\nTest results in normal\\nchildren groupFacial\\nexpressions for\\nautism\\ndiagnosisexpressions of test child 1 was 85.7 percent. According to this method, the average\\nrecognition rates of the seven expressions from test child 1 to test child 10 were 85.7 percent,\\n85.7 percent, 85.7 percent, 57.1 percent, 85.7 percent, 100 percent, 71.4 percent, 100 percent,\\n85.7 percent and 57.1 percent, respectively. The average recognition rate is 81.4 percent.', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='85.7 percent, 85.7 percent, 57.1 percent, 85.7 percent, 100 percent, 71.4 percent, 100 percent,\\n85.7 percent and 57.1 percent, respectively. The average recognition rate is 81.4 percent.\\nJudging by the 60 percent threshold, there are two test children ’s facial expression\\nrecognition rates at 57.1 percent. This shows that in real environment, the algorithm of the\\nsystem is affected by the environment and light, and the accuracy will be affected to a certain\\nextent. However, according to the accuracy of 81.4 percent, it can basically meet the\\npreliminary diagnostic requirements of whether the expression is abnormal or not. In the\\nfuture, more real samples will be added to further improve the accuracy of the system\\nalgorithm.\\nThe experimental results show that the errors mainly concentrate on the expressions of\\ndisgust and surprise. The main reasons are as follows:\\n(1) Disgust and surprise have only minor local changes in the faces of the two kinds of', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='disgust and surprise. The main reasons are as follows:\\n(1) Disgust and surprise have only minor local changes in the faces of the two kinds of\\nexpressions, and there is no significant distinguishing feature.\\n(2) Some of the participants had little change in the two facial expressions, did not have\\nthe obvious features of the corresponding categories, approached neutral expressions\\nand were easy to confuse.\\nFrom Table III , the average recognition rate of each expression is angry 50 percent, disgust 10\\npercent, fear 30 percent, happy 60 percent, sad 60 percent, surprise 20 percent and neutral 10\\npercent.\\nThe average recognition rates of the seven expressions from test child 1 to test child 10\\nwere 28.5 percent, 28.5 percent, 28.5 percent, 42.9 percent, 42.9 percent, 57.1 percent, 28.5\\npercent, 42.9 percent, 28.5 percent and 14.3 percent, and the average recognition rate is 34.3\\npercent.\\nFacial expressionNumber of test children\\nCorrect identification (Y 5Yes, No 5N)', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='percent, 42.9 percent, 28.5 percent and 14.3 percent, and the average recognition rate is 34.3\\npercent.\\nFacial expressionNumber of test children\\nCorrect identification (Y 5Yes, No 5N)\\nAverage recognition rate % 1234567891 0\\nAngry Y N N N Y Y N Y Y N 50\\nDisgust N N N Y N N N N N N 10\\nFear N N Y N Y Y N N N N 30Happy N Y Y Y N Y N Y N Y 60S a d NYNYYYNYYN 6 0Surprise Y N N N N N Y N N N 20Neutral N N N N N N Y N N N 10\\nFacial expression Normal children group Autistic children group\\nAngry 80% 50%\\nDisgust 70% 10%Fear 80% 30%Happy 100% 60%Sad 80% 60%Surprise 70% 20%\\nNeutral 90% 10%\\nAverage recognition rate % 81.4% 34.3%Table III.\\nTest results in autistic\\nchildren group\\nTable IV.\\nComparisons of two\\ngroups of children ’s\\nfacial expression\\nrecognition rateLHTThe experimental results show that the recognition rate of happiness and sadness is\\nhigher in the seven expressions. Testing children showed difficulty in identifying complex', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='recognition rateLHTThe experimental results show that the recognition rate of happiness and sadness is\\nhigher in the seven expressions. Testing children showed difficulty in identifying complex\\nfacial expressions such as neutrality and aversion (see Table IV ).\\nThe experimental results showed that the recognition rate of facial expressions in autistic\\nchildren was significantly lower than that in normal children. All the autistic children who\\nparticipated in the test had a facial recognition rate of less than 60 percent. Therefore, if the\\naccuracy rate of facial expression diagnosis by the system was less than 60 percent, the tester\\nwould have a tendency to suffer from autism. The lower the recognition rate, the higher the\\ntendency of autism.\\n6. Conclusion\\nIn the era of rapid development of information technology, the processing of a large number\\nof health data has brought new opportunities and challenges to medical research. The', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='6. Conclusion\\nIn the era of rapid development of information technology, the processing of a large number\\nof health data has brought new opportunities and challenges to medical research. The\\nincidence of autism is increasing, which has attracted more and more attention from all\\naspects of society. The use of information technology, especially artificial intelligence\\ntechnology, to build an autism diagnosis system has become an urgent need for doctors and\\npatients. In this paper, an autism diagnosis system based on deep convolution neural network\\nand expression data is constructed. After testing, it can meet the design requirements of\\nautism diagnosis. The public can download and use the system through the network to\\ndiagnose autism conveniently. In addition, we will expand the function of the system,\\nincrease the recognition of children ’s physical movement and realize the diagnosis of autism\\nfrom multiple perspectives.', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='increase the recognition of children ’s physical movement and realize the diagnosis of autism\\nfrom multiple perspectives.\\nBecause the average age of children using and collecting facial expression data is between\\n3 and 8 years old, the system can recognize children aged 3 –6 years old. Therefore, through\\nthis system, autism can be diagnosed as soon as possible. The earlier the diagnosis and\\ntreatment of autism is, the better the rehabilitation effect. Therefore, it is of great significance\\nfor the treatment of autism.\\nBecause the training samples of the system adopt the international open facial expression\\ndatabase, which contains the facial expression data of children and adults in different\\ncountries and regions, the system can diagnose autism for children and adults in different\\ncountries and regions.\\nOf course, the system also needs to be improved through practical use. Next, we will\\narrange for the system to be tested in a large number of cooperative hospitals. Next, there are', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='Of course, the system also needs to be improved through practical use. Next, we will\\narrange for the system to be tested in a large number of cooperative hospitals. Next, there are\\ntwo main tasks to be done. The first is to collect more data of normal and autistic children ’s\\nfacial expressions, improve the recognition effect of the system on children ’s facial\\nexpressions and establish a special database of children ’s facial expressions. The second is to\\nimprove the system function, according to the results of facial expression diagnosis of autistic\\nchildren for a detailed classification, to distinguish between severe, moderate and mild autism\\npatients, in order to facilitate the treatment of doctors.\\nThis study hopes to be helpful to the diagnosis of autism in remote and underdeveloped\\nareas, so as to promote the early diagnosis and treatment of autistic children and reduce the\\nmedical costs and burdens of autistic families and society. Therefore, this study has more', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='areas, so as to promote the early diagnosis and treatment of autistic children and reduce the\\nmedical costs and burdens of autistic families and society. Therefore, this study has more\\nimportant social significance and application value.\\nReferences\\nBaron-Cohen, S., Wheelwright, S., Jolliffe, T. (1997), “Is there a “language of the eyes ”? Evidence from\\nnormal adults, and adults with autism or asperger syndrome ”,Visual Cognition , Vol. 4 No. 3,\\npp. 311-331.\\nBeijing Wucai Deer Autism Research Institute (2017), Report on the Development of Autism Education\\nand Rehabilitation Industry in China 2 , Huaxia Publishing House, Beijing.Facial\\nexpressions for\\nautism\\ndiagnosisCai, Y. (2018), “Facial tracking and facial expression recognition based on in-depth learning ”\\nSoutheast University.\\nDu, J. (2018), “Research on face expression recognition based on Kernel relieff ”Zhengzhou University.\\nDuan, Y., Wu, X. and Jinfeng (2015), “Research progress on etiology and treatment of autism ”,Chinese', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='Duan, Y., Wu, X. and Jinfeng (2015), “Research progress on etiology and treatment of autism ”,Chinese\\nScience: Life Science , Vol. 9, pp. 820-844.\\nEkman, P. (1992), “An argument for basic emotions ”,Cognition and Emotion , Vol. 6 Nos 3-4,\\npp. 169-200.\\nLanlan (2018), “Research on facial expression recognition method based on multi-feature fusion ”, Jilin\\nUniversity.\\nLi, S. and Deng, W. (2018), “Deep facial expression recognition: a survey ”arXiv preprint arXiv:\\n1804.08348.\\nLiu, Y., Huo, W. and Hu, X. (2015), “Summary of research on facial expression recognition of autistic\\nchildren ”,Modern Special Education , Vol. 8, pp. 35-39.\\nLucey, P., Cohn, J.F., Kanade, T., Saragih, J. and Ambadar, Z. (2010), “The extended cohn-kanade\\ndataset (ckþ): a complete dataset for action unit and emotion-specified expression ”,2010 IEEE\\nComputer Society Conference on Computer Vision and Pattern Recognition-Workshops ,\\n2010, IEEE.', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='dataset (ckþ): a complete dataset for action unit and emotion-specified expression ”,2010 IEEE\\nComputer Society Conference on Computer Vision and Pattern Recognition-Workshops ,\\n2010, IEEE.\\nMehrabian, A. (2008), “Communication without words ”,Communication Theory , Vol. 6, pp. 193-200.\\nMei, J. and Hu, B. (2015), “Research and implementation of real-time face expression recognition\\nmethod ”,Information and Technology , Vol. 44 No. 4, pp. 145-148.\\nOrganization W H (1992), The ICD-10 Classification of Mental and Behavioural Disorders: Clinical\\nDescriptions and Diagnostic Guidelines , World Health Organization, Geneva.\\nSegal, D.L. (2010), “Diagnostic and statistical manual of mental disorders (DSM-IV-TR) ”,The Corsini\\nEncyclopedia of Psychology , Vol. 1 No. 16, pp. 1-3.\\nShen, X., He, Z. and Ding, X. (2013), “Computer facial expression recognition training to improve the\\nfacial expression recognition ability of autistic children ”,Sci-tech Horizon , Vol. 25, pp. 12-13.', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='facial expression recognition ability of autistic children ”,Sci-tech Horizon , Vol. 25, pp. 12-13.\\nSingh, P., Ghosh, S. and Nandi, S. (2017), “Subjective burden and depression in mothers of children\\nwith autism spectrum disorder in India: moderating effect of social support ”,Journal of Autism\\nand Developmental Disorders , Vol. 47 No. 10, pp. 3097-3111.\\nVismara, L.A. and Rogers, S.J. (2008), “The early start denver model ”,Journal of Early Intervention ,\\nVol. 31 No. 1, pp. 91-108.\\nWang, H. (2007), “Psychological and behavioral characteristics, diagnosis and evaluation of autistic\\nchildren ”,Chinese Journal of Rehabilitation Medicine , Vol. 22 No. 9, pp. 853-856.\\nWang, G. and Lu, M. (2015), “Research on educational games for children with autism spectrum\\ndisorders ”,Modern Special Education , Vol. 14, pp. 38-40.\\nWang, Y., Xiao, L. and Chen, R. (2018), “Social impairment of children with autism spectrum disorder', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='disorders ”,Modern Special Education , Vol. 14, pp. 38-40.\\nWang, Y., Xiao, L. and Chen, R. (2018), “Social impairment of children with autism spectrum disorder\\naffects parental quality of life in different ways ”,Psychiatry Research , Vol. 2018 No. 266,\\npp. 168-174.\\nWu, X. and Chen, S. (2018), “Research progress on quality of life and its influencing factors of primary\\ncaregivers for autistic children ”,General Nursing , Vol. 16 No. 18, pp. 2206-2208.\\nYan, S. (2008), “Experimental study on facial expression processing of autistic children ”, East China\\nNormal University.\\nYanbin, H., Fuxing, W., Heping, X., Jing, A., Yuxin, W. and Huashan, L. (2018), “Facial processing\\ncharacteristics of autism spectrum disorders: meta-analysis of eye movement research ”,\\nProgress in Psychological Science , Vol. 1, pp. 26-41.\\nYang, Y. and Wang, M. (2014), “Employment and financial burdens of families with preschool-aged', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='Progress in Psychological Science , Vol. 1, pp. 26-41.\\nYang, Y. and Wang, M. (2014), “Employment and financial burdens of families with preschool-aged\\nchildren with autism ”,Chinese Journal of Clinical Psychology , Vol. 22 No. 2, pp. 295-297, 361.LHTYang, J., Xing, H., Shao, Z. and Yuan, J. (2017), “Facial expression sensitivity deficits in patients with\\nautism spectrum disorder: impact of task nature and implications for intervention ”,Chinese\\nScience: Life Science , Vol. 47 No. 4, pp. 443-452.\\nZablotsky, B., Black, L.I. and Blumberg, S.J. (2017), “Estimated prevalence of children with diagnosed\\ndevelopmental disabilities in the United States, 2014-2016 ”,NCHS Data Brief , Vol. 291, pp. 1-8.\\nCorresponding author\\nWang Zhao can be contacted at: creativesoft@sohu.com\\nFor instructions on how to order reprints of this article, please visit our website:\\nwww.emeraldgrouppublishing.com/licensing/reprints.htmOr contact us for further details: permissions@emeraldinsight.comFacial', metadata={'source': 'papers/zhao2020_clean.txt'}),\n",
       " Document(page_content='www.emeraldgrouppublishing.com/licensing/reprints.htmOr contact us for further details: permissions@emeraldinsight.comFacial\\nexpressions for\\nautism\\ndiagnosis', metadata={'source': 'papers/zhao2020_clean.txt'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_api_key=os.environ.get('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings,HuggingFaceInferenceAPIEmbeddings,OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "db1=FAISS.from_documents(document,HuggingFaceEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('Query Questions - Sheet1.csv',index_col='SN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SN\n",
      "1     [of great value in alleviating the burden on f...\n",
      "2     [The main symptoms of autism include impaired ...\n",
      "3     [The main symptoms of autism include impaired ...\n",
      "4     [Accepted 23 January 2020\\nLibrary Hi Tech\\n© ...\n",
      "5     [After consulting a large number of literature...\n",
      "6     [reaches 81.4 percent, which can effectively d...\n",
      "7     [The main symptoms of autism include impaired ...\n",
      "8     [an important role in the prevention and diagn...\n",
      "9     [an important role in the prevention and diagn...\n",
      "10    [increase the recognition of children ’s physi...\n",
      "11    [The main symptoms of autism include impaired ...\n",
      "12    [an important role in the prevention and diagn...\n",
      "13    [The main symptoms of autism include impaired ...\n",
      "Name: Questions , dtype: object\n"
     ]
    }
   ],
   "source": [
    "def q_a_similarity(query):\n",
    "    result=db1.similarity_search(query)\n",
    "    return [result[0].page_content, result[1].page_content, result[2].page_content, result[3].page_content]\n",
    "# df=pd.read_csv('questions.csv')\n",
    "res=df['Questions '].apply(q_a_similarity)\n",
    "print(res)\n",
    "\n",
    "df['answers']=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the variety of Multimodal and Multi-m...</td>\n",
       "      <td>[of great value in alleviating the burden on f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Autism Spectrum Disorder, how it is ca...</td>\n",
       "      <td>[The main symptoms of autism include impaired ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the cure of Autism Spectrum Disorder</td>\n",
       "      <td>[The main symptoms of autism include impaired ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are Stereotypical and maladaptive behavio...</td>\n",
       "      <td>[Accepted 23 January 2020\\nLibrary Hi Tech\\n© ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How relevant is eye contact and how it can be ...</td>\n",
       "      <td>[After consulting a large number of literature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How can cross country trials help in developme...</td>\n",
       "      <td>[reaches 81.4 percent, which can effectively d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How early infants cry can help in the early de...</td>\n",
       "      <td>[The main symptoms of autism include impaired ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are various methods to detect  Atypical P...</td>\n",
       "      <td>[an important role in the prevention and diagn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What kind of facial expressions can be used to...</td>\n",
       "      <td>[an important role in the prevention and diagn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What are methods to detect Autism from home vi...</td>\n",
       "      <td>[increase the recognition of children ’s physi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is Still-Face Paradigm in Early Screening...</td>\n",
       "      <td>[The main symptoms of autism include impaired ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is West Syndrome?</td>\n",
       "      <td>[an important role in the prevention and diagn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is the utility of Behavior and interactio...</td>\n",
       "      <td>[The main symptoms of autism include impaired ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Questions   \\\n",
       "SN                                                      \n",
       "1   What are the variety of Multimodal and Multi-m...   \n",
       "2   What is Autism Spectrum Disorder, how it is ca...   \n",
       "3        What is the cure of Autism Spectrum Disorder   \n",
       "4   What are Stereotypical and maladaptive behavio...   \n",
       "5   How relevant is eye contact and how it can be ...   \n",
       "6   How can cross country trials help in developme...   \n",
       "7   How early infants cry can help in the early de...   \n",
       "8   What are various methods to detect  Atypical P...   \n",
       "9   What kind of facial expressions can be used to...   \n",
       "10  What are methods to detect Autism from home vi...   \n",
       "11  What is Still-Face Paradigm in Early Screening...   \n",
       "12                            What is West Syndrome?    \n",
       "13  What is the utility of Behavior and interactio...   \n",
       "\n",
       "                                              answers  \n",
       "SN                                                     \n",
       "1   [of great value in alleviating the burden on f...  \n",
       "2   [The main symptoms of autism include impaired ...  \n",
       "3   [The main symptoms of autism include impaired ...  \n",
       "4   [Accepted 23 January 2020\\nLibrary Hi Tech\\n© ...  \n",
       "5   [After consulting a large number of literature...  \n",
       "6   [reaches 81.4 percent, which can effectively d...  \n",
       "7   [The main symptoms of autism include impaired ...  \n",
       "8   [an important role in the prevention and diagn...  \n",
       "9   [an important role in the prevention and diagn...  \n",
       "10  [increase the recognition of children ’s physi...  \n",
       "11  [The main symptoms of autism include impaired ...  \n",
       "12  [an important role in the prevention and diagn...  \n",
       "13  [The main symptoms of autism include impaired ...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ans1']=df['answers'].apply(lambda x:x[0])\n",
    "df['ans2']=df['answers'].apply(lambda x:x[1])\n",
    "df['ans3']=df['answers'].apply(lambda x:x[2])\n",
    "df['ans4']=df['answers'].apply(lambda x:x[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='answers',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
